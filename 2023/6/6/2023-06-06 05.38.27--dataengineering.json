{"kind": "Listing", "data": {"after": "t3_141lm26", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A couple years ago I put my career on pause and moved back home to be closer to an ailing parent. I was a data analyst in my previous role with only two years experience, where I mostly worked in Excel, Power BI, a bit of SQL and Python. During this pause I beefed up my SQL, Python and stats knowledge and built some portfolio projects in Streamlit to showcase my interests and abilities. I also learned Git along the way as well.\n\nAbout a year ago I started job hunting where I was mostly looking for analyst roles. As I progressed in the job hunt I became increasingly interested in analytics engineering but felt like my skills weren't quite there yet. At the time I was thinking the best path would be getting another DA role and using it to catapult into an engineering role down the line. Fast forward a year, countless job applications, numerous interviews later, I finally had the most incredible breakthrough.\n\nLast week someone in my network connected me to the owner of a software development company that builds AI tools for customers. We exchanged a couple texts, hoped on a call, got talking about my journey in the world of data and how I ultimately want to get into analytics engineering. He then asked me if I was comfortable with window functions in SQL (I am) and if I know basic data structures (I do). At that point he asked me if I could send him a copy of my CV and if I could meet with him and some of his teammates for a dinner later in the week. I go to the dinner, meet the teams senior software engineers, we hit it off over some tacos, and they essentially create a role for me out of thin air! It was all very serendipitous and I still have no idea how this happened. They even told me that they weren't in the market for hiring but liked my story/journey/tenacity so much that they wanted me to come and work for them. The owner even told me as we were shaking hands and saying goodbye that he doesn't care about my background and that \"it is all about investing in the right people\".\n\nYes, they are a legitimate company and have several large clients that you have heard of. I received their job offer today and I nearly threw up after reading it. The title is Data Automation Developer, so most of my work is going to be in automation testing and they have also thrown in some ELT tasks as well. It's a fully remote role and one that I never imagined I would have in my career. While I don't feel like I am being setup for failure, and I do really like the team, I can't help but feel immense imposter syndrome when I look at the job posting that they created and I see all sorts of things I have no experience in. Is this even real life?? I understand the idea of hiring on someones potential but this is all incredibly daunting. Has anyone had a similar experience to this?", "author_fullname": "t2_jitcg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I feel like I won the job lottery and it has sent my imposter syndrome into the stratosphere", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141vfwl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 75, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 75, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686008502.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686006525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A couple years ago I put my career on pause and moved back home to be closer to an ailing parent. I was a data analyst in my previous role with only two years experience, where I mostly worked in Excel, Power BI, a bit of SQL and Python. During this pause I beefed up my SQL, Python and stats knowledge and built some portfolio projects in Streamlit to showcase my interests and abilities. I also learned Git along the way as well.&lt;/p&gt;\n\n&lt;p&gt;About a year ago I started job hunting where I was mostly looking for analyst roles. As I progressed in the job hunt I became increasingly interested in analytics engineering but felt like my skills weren&amp;#39;t quite there yet. At the time I was thinking the best path would be getting another DA role and using it to catapult into an engineering role down the line. Fast forward a year, countless job applications, numerous interviews later, I finally had the most incredible breakthrough.&lt;/p&gt;\n\n&lt;p&gt;Last week someone in my network connected me to the owner of a software development company that builds AI tools for customers. We exchanged a couple texts, hoped on a call, got talking about my journey in the world of data and how I ultimately want to get into analytics engineering. He then asked me if I was comfortable with window functions in SQL (I am) and if I know basic data structures (I do). At that point he asked me if I could send him a copy of my CV and if I could meet with him and some of his teammates for a dinner later in the week. I go to the dinner, meet the teams senior software engineers, we hit it off over some tacos, and they essentially create a role for me out of thin air! It was all very serendipitous and I still have no idea how this happened. They even told me that they weren&amp;#39;t in the market for hiring but liked my story/journey/tenacity so much that they wanted me to come and work for them. The owner even told me as we were shaking hands and saying goodbye that he doesn&amp;#39;t care about my background and that &amp;quot;it is all about investing in the right people&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Yes, they are a legitimate company and have several large clients that you have heard of. I received their job offer today and I nearly threw up after reading it. The title is Data Automation Developer, so most of my work is going to be in automation testing and they have also thrown in some ELT tasks as well. It&amp;#39;s a fully remote role and one that I never imagined I would have in my career. While I don&amp;#39;t feel like I am being setup for failure, and I do really like the team, I can&amp;#39;t help but feel immense imposter syndrome when I look at the job posting that they created and I see all sorts of things I have no experience in. Is this even real life?? I understand the idea of hiring on someones potential but this is all incredibly daunting. Has anyone had a similar experience to this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "141vfwl", "is_robot_indexable": true, "report_reasons": null, "author": "yror007", "discussion_type": null, "num_comments": 22, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141vfwl/i_feel_like_i_won_the_job_lottery_and_it_has_sent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141vfwl/i_feel_like_i_won_the_job_lottery_and_it_has_sent/", "subreddit_subscribers": 109114, "created_utc": 1686006525.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apologize if this question has been asked countless times. As a newly hired DE I was advised by my Supervisor to improve my SQL knowledge from a 5 to 8/10 and be more versed in BigQuery and GCP\n\nMy previous experience as a Software Developer didn't provide difficult database tasks, so my SQL skills are limited to the basic CRUD and creation of schema and tables. \n\nGoing back to my question, how do I proceed from here? I checked the links in the faq but I still feel lost.", "author_fullname": "t2_c58xvng2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I get better in SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1415jdb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 62, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 62, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685948603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologize if this question has been asked countless times. As a newly hired DE I was advised by my Supervisor to improve my SQL knowledge from a 5 to 8/10 and be more versed in BigQuery and GCP&lt;/p&gt;\n\n&lt;p&gt;My previous experience as a Software Developer didn&amp;#39;t provide difficult database tasks, so my SQL skills are limited to the basic CRUD and creation of schema and tables. &lt;/p&gt;\n\n&lt;p&gt;Going back to my question, how do I proceed from here? I checked the links in the faq but I still feel lost.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1415jdb", "is_robot_indexable": true, "report_reasons": null, "author": "dehydratedcatnip", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1415jdb/how_do_i_get_better_in_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1415jdb/how_do_i_get_better_in_sql/", "subreddit_subscribers": 109114, "created_utc": 1685948603.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_brkxjomi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I\u2019ve had the definition wrong this entire time\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 125, "top_awarded_type": null, "hide_score": false, "name": "t3_1420fjz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IFuK97ynhfsSPbLctWG4Nu9kNdsbn1tYITeoa2-9rxo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686017984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9uviprh35b4b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9uviprh35b4b1.jpg?auto=webp&amp;v=enabled&amp;s=b854077dd9f4cc89f247e44d63a04c45f3e13183", "width": 556, "height": 500}, "resolutions": [{"url": "https://preview.redd.it/9uviprh35b4b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fc60879835dacd0fc93b0a0944ee45ce943074f", "width": 108, "height": 97}, {"url": "https://preview.redd.it/9uviprh35b4b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81a30ee8d32ecc13cb413e31f972976c6fae5ad9", "width": 216, "height": 194}, {"url": "https://preview.redd.it/9uviprh35b4b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fcfb0b91c224266641f631f8d29d0b65971bf9d", "width": 320, "height": 287}], "variants": {}, "id": "U3wE6gNxgunkDMWgxqPlc8ScfOmNpg9hno-IJl-hp7E"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "1420fjz", "is_robot_indexable": true, "report_reasons": null, "author": "Straight_House8628", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1420fjz/ive_had_the_definition_wrong_this_entire_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9uviprh35b4b1.jpg", "subreddit_subscribers": 109114, "created_utc": 1686017984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm curious how others document their data architecture. I wonder if there are any standards around this or if there is a lot of variance.  Would be great if you could include what types of diagrams and sections you would see along with how this presented to new employees or stakeholders.\n\nFor me, I'm built 2 data architectures as the a lone wolf data engineer.  My current documentation includes \n\n* An architecture diagram that shows all the cloud technologies and connections between them.\n* Brief descriptions of the phases like 'Data Ingestion', 'Orchestration', 'Reporting'.  \n\nThis documentation is mostly for my own reference and truth be told I'm wondering what else I should be including.", "author_fullname": "t2_5065w9mg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you or your company document your data architecture? What is typically included in the documentation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141lrjm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685986040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious how others document their data architecture. I wonder if there are any standards around this or if there is a lot of variance.  Would be great if you could include what types of diagrams and sections you would see along with how this presented to new employees or stakeholders.&lt;/p&gt;\n\n&lt;p&gt;For me, I&amp;#39;m built 2 data architectures as the a lone wolf data engineer.  My current documentation includes &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;An architecture diagram that shows all the cloud technologies and connections between them.&lt;/li&gt;\n&lt;li&gt;Brief descriptions of the phases like &amp;#39;Data Ingestion&amp;#39;, &amp;#39;Orchestration&amp;#39;, &amp;#39;Reporting&amp;#39;.&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This documentation is mostly for my own reference and truth be told I&amp;#39;m wondering what else I should be including.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "141lrjm", "is_robot_indexable": true, "report_reasons": null, "author": "kvotheTHEinquisitor", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141lrjm/how_do_you_or_your_company_document_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141lrjm/how_do_you_or_your_company_document_your_data/", "subreddit_subscribers": 109114, "created_utc": 1685986040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a Sr. DE assigned to me 50/50 with another team. Im not a dataengineer and she has an actual data engineer manager. Task assigned was to import data\n\nThe tables were 6 in total.  1 table had 95 million rows, but the other were mapping tables with less than 1k rows.\n\nTalked to her a couple of times now on why its not done. and so last thursday i removed her from all her other workloads outside of this and canceled all her meetings with me\n\nBut is there any reason it would take this long to do this task? she keeps telling me that it needs to further optimized and has been pretty evasive at this point\n\nId personally just \"select \\* from table 1\" at this point.  We can query Redshift directly from databricks. but i dont know anything about the limitations and im not that familar with databricks. i just query it adhoc.\n\nso before i make a big deal with this.  how long would you expect a Sr. DE with 20 years experience to do this task?", "author_fullname": "t2_9q93psld7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is this data engineer on my team dicking me around with bs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141si4x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686000542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Sr. DE assigned to me 50/50 with another team. Im not a dataengineer and she has an actual data engineer manager. Task assigned was to import data&lt;/p&gt;\n\n&lt;p&gt;The tables were 6 in total.  1 table had 95 million rows, but the other were mapping tables with less than 1k rows.&lt;/p&gt;\n\n&lt;p&gt;Talked to her a couple of times now on why its not done. and so last thursday i removed her from all her other workloads outside of this and canceled all her meetings with me&lt;/p&gt;\n\n&lt;p&gt;But is there any reason it would take this long to do this task? she keeps telling me that it needs to further optimized and has been pretty evasive at this point&lt;/p&gt;\n\n&lt;p&gt;Id personally just &amp;quot;select * from table 1&amp;quot; at this point.  We can query Redshift directly from databricks. but i dont know anything about the limitations and im not that familar with databricks. i just query it adhoc.&lt;/p&gt;\n\n&lt;p&gt;so before i make a big deal with this.  how long would you expect a Sr. DE with 20 years experience to do this task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "141si4x", "is_robot_indexable": true, "report_reasons": null, "author": "AntiquePassage7229", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141si4x/is_this_data_engineer_on_my_team_dicking_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141si4x/is_this_data_engineer_on_my_team_dicking_me/", "subreddit_subscribers": 109114, "created_utc": 1686000542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Not only am I getting mountains of emails from vendors, but now emails from leadership and department heads who are apt to respond to a flashy vendor email.\n\nI\u2019m skeptical of most of these products to say the least. But it\u2019s not always bad to have the business seeing more value to data.\n\nBut I\u2019m curious how others are tempering expectations as well as a general outlook on utilizing these technologies?\n\nMy thoughts are it\u2019s doubtful any business has a well proven product with GPT being so new, and it\u2019s the same buzzwords all over again but with AI replacing RPA.", "author_fullname": "t2_ao7u40a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is everyone handling the wave of Chat GPT-based software products?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141ijx5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685979661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not only am I getting mountains of emails from vendors, but now emails from leadership and department heads who are apt to respond to a flashy vendor email.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m skeptical of most of these products to say the least. But it\u2019s not always bad to have the business seeing more value to data.&lt;/p&gt;\n\n&lt;p&gt;But I\u2019m curious how others are tempering expectations as well as a general outlook on utilizing these technologies?&lt;/p&gt;\n\n&lt;p&gt;My thoughts are it\u2019s doubtful any business has a well proven product with GPT being so new, and it\u2019s the same buzzwords all over again but with AI replacing RPA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "141ijx5", "is_robot_indexable": true, "report_reasons": null, "author": "minormisgnomer", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141ijx5/how_is_everyone_handling_the_wave_of_chat/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141ijx5/how_is_everyone_handling_the_wave_of_chat/", "subreddit_subscribers": 109114, "created_utc": 1685979661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a49okn69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are data teams ticket takers or decision makers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_141mm5c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kpT-5cEDFtmFInhplih-1oOkSQD5NNJ3F57ni9_g0iM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685987733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "towardsdatascience.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://towardsdatascience.com/data-ticket-takers-vs-decision-makers-a6cf957b507a?source=friends_link&amp;sk=8e7d47f140ed3f52fb28c20afbaef857", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?auto=webp&amp;v=enabled&amp;s=4bb49ecea64a7f1202f6feca5180cf599a787581", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c37175c432cbcfcfe8a8137b16e632f4277ae695", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f74ae6673c837d75931c755e99a0195512544ad", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0fcfc0e5f281cbf54e60f111b9561bbbca7f4ba4", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d49462121ab79fe5e96a0c9f9d787e5924b2146c", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e07e5cc45eeb9d189ed12770abf052b30581f1dc", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=143ac4f8ddf1b3360a7fda0604683595d20dd195", "width": 1080, "height": 607}], "variants": {}, "id": "Uoz-O1WeN4siWLyEn-9XyIVY3xbQocAz9iebFjC37C4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "141mm5c", "is_robot_indexable": true, "report_reasons": null, "author": "Top-Substance2185", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141mm5c/are_data_teams_ticket_takers_or_decision_makers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://towardsdatascience.com/data-ticket-takers-vs-decision-makers-a6cf957b507a?source=friends_link&amp;sk=8e7d47f140ed3f52fb28c20afbaef857", "subreddit_subscribers": 109114, "created_utc": 1685987733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I relatively often see people referring that they do the E(t)L of ELT with basic Python and the T with SQL.  \nWhat is your approach to the EL in Python?\n\nI am recent graduate in data science and after my internship got a full time position in a non-tech company (football/soccer club) with no data infrastructure, and no employees with a computer or data science background.  \nMy thoughts would be to built api wrappers in Python, which is already done as I have used them for apps and reports through my internship, and then use pandas (or polars) to do the (t) from .json to dataframes/tables. Do the insert, update and delete using sqlalchemy and schedule the scripts using cron on a physical machine in the office (i.e. my old windows computer).  \nIt is a very basic and fragile setup, but on a limited budget, it is to my knowledge, a good way to start and then convince the organization to invest more resources and me learning in tools and migrate to dagster, Airflow, dbt etc.\n\nWhat are your suggestions of how to do the EL in Python for very basic usage, and how do you approach the E(t)L with Python (custom api wrappers, pandas, sqlachemy, json objects, cron)?  \nWould love to see some examples if possible :)", "author_fullname": "t2_226yoed5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to do the \"basic\" EL in Python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141ib4n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685979180.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I relatively often see people referring that they do the E(t)L of ELT with basic Python and the T with SQL.&lt;br/&gt;\nWhat is your approach to the EL in Python?&lt;/p&gt;\n\n&lt;p&gt;I am recent graduate in data science and after my internship got a full time position in a non-tech company (football/soccer club) with no data infrastructure, and no employees with a computer or data science background.&lt;br/&gt;\nMy thoughts would be to built api wrappers in Python, which is already done as I have used them for apps and reports through my internship, and then use pandas (or polars) to do the (t) from .json to dataframes/tables. Do the insert, update and delete using sqlalchemy and schedule the scripts using cron on a physical machine in the office (i.e. my old windows computer).&lt;br/&gt;\nIt is a very basic and fragile setup, but on a limited budget, it is to my knowledge, a good way to start and then convince the organization to invest more resources and me learning in tools and migrate to dagster, Airflow, dbt etc.&lt;/p&gt;\n\n&lt;p&gt;What are your suggestions of how to do the EL in Python for very basic usage, and how do you approach the E(t)L with Python (custom api wrappers, pandas, sqlachemy, json objects, cron)?&lt;br/&gt;\nWould love to see some examples if possible :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "141ib4n", "is_robot_indexable": true, "report_reasons": null, "author": "C_Ronsholt", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141ib4n/how_to_do_the_basic_el_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141ib4n/how_to_do_the_basic_el_in_python/", "subreddit_subscribers": 109114, "created_utc": 1685979180.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Good afternoon /r/dataengineering, I wanted to write up a quick post sharing how I and my company structure many of our ETL pipelines using [Meerschaum Compose](https://meerschaum.io/reference/compose/).\n\n&gt;***Disclaimer:*** *I am the author of Meerschaum and Compose, but that open source work is not affiliated with my company. Not looking to sell anything, just to share what works for us.*\n\nWe've implemented our core system's APIs as an [instance connector](https://meerschaum.io/reference/connectors/instance-connectors/), which is simply the destination of ETL. Whereas internal transformations follow this pattern:\n\n    ingestion source (extract) -&gt; PostgreSQL -&gt; PostgreSQL (transform)\n\nthe final step of our standard pipeline is like this:\n\n    PostgreSQL -&gt; Core system (load via our instance connector)\n\nWe usually organize new pipelines with three [compose files](https://meerschaum.io/reference/compose/), where the first stage ingests client data, the second transforms it, and final stage loads it via our internal instance connector:\n\n* `mrsm-compose-00-ingest.yaml`\n* `mrsm-compose-01-etl.yaml`\n* `mrsm-compose-02-publish.yaml`\n\nWe run everything on a shared base image with commonly used [plugins](https://meerschaum.io/reference/plugins/writing-plugins/) (i.e. pulling out of our datalake on S3), and integration-specific plugins live in the project next to the compose files.\n\nI know there are plenty of other tools out there for a similar workflow (a la Meltano). We deal with a lot of time-series data, so this fits our use case nicely and is quick to onboard new engineers. Most of our transformations are in SQL, and we have plugins in the base image for commonly ingested sources (e.g. Salesforce's API).\n\nWhat do you think of this workflow? In my experience it works well on the scale of &lt;100 million rows unless your data are mostly immutable.", "author_fullname": "t2_db74x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We run our ETL pipelines via Meerschaum Compose", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141q100", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685995449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good afternoon &lt;a href=\"/r/dataengineering\"&gt;/r/dataengineering&lt;/a&gt;, I wanted to write up a quick post sharing how I and my company structure many of our ETL pipelines using &lt;a href=\"https://meerschaum.io/reference/compose/\"&gt;Meerschaum Compose&lt;/a&gt;.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Disclaimer:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;I am the author of Meerschaum and Compose, but that open source work is not affiliated with my company. Not looking to sell anything, just to share what works for us.&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;We&amp;#39;ve implemented our core system&amp;#39;s APIs as an &lt;a href=\"https://meerschaum.io/reference/connectors/instance-connectors/\"&gt;instance connector&lt;/a&gt;, which is simply the destination of ETL. Whereas internal transformations follow this pattern:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ingestion source (extract) -&amp;gt; PostgreSQL -&amp;gt; PostgreSQL (transform)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;the final step of our standard pipeline is like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;PostgreSQL -&amp;gt; Core system (load via our instance connector)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;We usually organize new pipelines with three &lt;a href=\"https://meerschaum.io/reference/compose/\"&gt;compose files&lt;/a&gt;, where the first stage ingests client data, the second transforms it, and final stage loads it via our internal instance connector:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;mrsm-compose-00-ingest.yaml&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;mrsm-compose-01-etl.yaml&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;mrsm-compose-02-publish.yaml&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We run everything on a shared base image with commonly used &lt;a href=\"https://meerschaum.io/reference/plugins/writing-plugins/\"&gt;plugins&lt;/a&gt; (i.e. pulling out of our datalake on S3), and integration-specific plugins live in the project next to the compose files.&lt;/p&gt;\n\n&lt;p&gt;I know there are plenty of other tools out there for a similar workflow (a la Meltano). We deal with a lot of time-series data, so this fits our use case nicely and is quick to onboard new engineers. Most of our transformations are in SQL, and we have plugins in the base image for commonly ingested sources (e.g. Salesforce&amp;#39;s API).&lt;/p&gt;\n\n&lt;p&gt;What do you think of this workflow? In my experience it works well on the scale of &amp;lt;100 million rows unless your data are mostly immutable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "141q100", "is_robot_indexable": true, "report_reasons": null, "author": "Obliterative_hippo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141q100/we_run_our_etl_pipelines_via_meerschaum_compose/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141q100/we_run_our_etl_pipelines_via_meerschaum_compose/", "subreddit_subscribers": 109114, "created_utc": 1685995449.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've got a database set up like so:\n\n    1. Campaign table references:\n       - None\n    \n    2. AdGroup table references:\n       - `Campaign` table via the `campaign_id` foreign key.\n    \n    3. AdGroupAd table references:\n       - `AdGroup` table via the `ad_group_id` foreign key.\n\nIn reality, there's several more tables with many more relationships. So inserting data has become quite nuanced. For example, if I insert a new \\`AdGroupAd\\` record, I must first make sure that it has a corresponding \\`AdGroup\\`. By extension, I must make sure there's also a corresponding \\`Campaign\\`. This gets a little overwhelming after awhile.\n\nI was thinking, I can create a table called \\`LoadingZone\\` where I can insert the entire raw data--which looks like this:\n\n    [\n        {\n            \"campaign\": \"international_stuff\",\n            \"ad_group\": \"regional_1\",\n            \"ad\": \"blue\"\n        },\n        {\n            \"campaign\": \"international_stuff\",\n            \"ad_group\": \"regional_2\",\n            \"ad\": \"blue\"\n        },\n        {\n            \"campaign\": \"local_stuff\",\n            \"ad_group\": \"holiday_special\",\n            \"ad\": \"ice_cream\"\n        }\n    ]\n\nIn theory, my \"LoadingZone\" table will have a trigger before insert. The trigger will iterate through each row in there and try inserting everything to their respective table (in the correct order) with \\`ON CONFLICT IGNORE\\`.\n\nAlternatively, I can make code that does this from outside the database, and expect errors to arise if a conflict ever occurs.\n\nWhich way is better?  \n\n\nEdit: No replies yet, but I think I'm going to go with the trigger approach. Because the database should ideally be more optimized at triaging data to tables than any Python code I write to do the same. Secondly, if there is an error in the middle of inserting hierarchy data to several tables... that sounds like a mess. With a trigger, I believe the entire hierarchy (multiple inserts) should either fail or pass. That sounds ideal to me.", "author_fullname": "t2_k648rah9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "With a database containing hierarchical tables, is it better to have a simple insert design and let the database handle all the nuances of which table needs which data first?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141jg50", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685987667.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685981434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a database set up like so:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;1. Campaign table references:\n   - None\n\n2. AdGroup table references:\n   - `Campaign` table via the `campaign_id` foreign key.\n\n3. AdGroupAd table references:\n   - `AdGroup` table via the `ad_group_id` foreign key.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;In reality, there&amp;#39;s several more tables with many more relationships. So inserting data has become quite nuanced. For example, if I insert a new `AdGroupAd` record, I must first make sure that it has a corresponding `AdGroup`. By extension, I must make sure there&amp;#39;s also a corresponding `Campaign`. This gets a little overwhelming after awhile.&lt;/p&gt;\n\n&lt;p&gt;I was thinking, I can create a table called `LoadingZone` where I can insert the entire raw data--which looks like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[\n    {\n        &amp;quot;campaign&amp;quot;: &amp;quot;international_stuff&amp;quot;,\n        &amp;quot;ad_group&amp;quot;: &amp;quot;regional_1&amp;quot;,\n        &amp;quot;ad&amp;quot;: &amp;quot;blue&amp;quot;\n    },\n    {\n        &amp;quot;campaign&amp;quot;: &amp;quot;international_stuff&amp;quot;,\n        &amp;quot;ad_group&amp;quot;: &amp;quot;regional_2&amp;quot;,\n        &amp;quot;ad&amp;quot;: &amp;quot;blue&amp;quot;\n    },\n    {\n        &amp;quot;campaign&amp;quot;: &amp;quot;local_stuff&amp;quot;,\n        &amp;quot;ad_group&amp;quot;: &amp;quot;holiday_special&amp;quot;,\n        &amp;quot;ad&amp;quot;: &amp;quot;ice_cream&amp;quot;\n    }\n]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;In theory, my &amp;quot;LoadingZone&amp;quot; table will have a trigger before insert. The trigger will iterate through each row in there and try inserting everything to their respective table (in the correct order) with `ON CONFLICT IGNORE`.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, I can make code that does this from outside the database, and expect errors to arise if a conflict ever occurs.&lt;/p&gt;\n\n&lt;p&gt;Which way is better?  &lt;/p&gt;\n\n&lt;p&gt;Edit: No replies yet, but I think I&amp;#39;m going to go with the trigger approach. Because the database should ideally be more optimized at triaging data to tables than any Python code I write to do the same. Secondly, if there is an error in the middle of inserting hierarchy data to several tables... that sounds like a mess. With a trigger, I believe the entire hierarchy (multiple inserts) should either fail or pass. That sounds ideal to me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "141jg50", "is_robot_indexable": true, "report_reasons": null, "author": "MrChadWood", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141jg50/with_a_database_containing_hierarchical_tables_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141jg50/with_a_database_containing_hierarchical_tables_is/", "subreddit_subscribers": 109114, "created_utc": 1685981434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I've done dp-203, dp-900 and az-900 but felt it wasn't sufficient. After getting those, I now work using SQL Server, T-SQL, SSMS and SSIS in Visual Studio. \n\nI really want to do DE in Azure and have been getting job interviews and doing well but my Azure technology experience let's me down a little. \n\nI find it MUCH better learning on the job with projects and a little guidance but don't have the privilege. \n\nIve tried learning more through YouTube but most are just basic explanations or basic \"copy this table into azure\" and nothing juicy enough for me to get some good experience and ideas. \n\nI've tried doing some solo projects but without the knowledge I tend to bugger something up and no idea how to fix it. \n\nDoes anyone have any good resources to help be get good with learning ADF, data warehousing, modelling and a little Power BI? \n\nHope this makes sense!\n\nThanks!", "author_fullname": "t2_6o5h731v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help getting DE experience in Azure stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141h76i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685976997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve done dp-203, dp-900 and az-900 but felt it wasn&amp;#39;t sufficient. After getting those, I now work using SQL Server, T-SQL, SSMS and SSIS in Visual Studio. &lt;/p&gt;\n\n&lt;p&gt;I really want to do DE in Azure and have been getting job interviews and doing well but my Azure technology experience let&amp;#39;s me down a little. &lt;/p&gt;\n\n&lt;p&gt;I find it MUCH better learning on the job with projects and a little guidance but don&amp;#39;t have the privilege. &lt;/p&gt;\n\n&lt;p&gt;Ive tried learning more through YouTube but most are just basic explanations or basic &amp;quot;copy this table into azure&amp;quot; and nothing juicy enough for me to get some good experience and ideas. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried doing some solo projects but without the knowledge I tend to bugger something up and no idea how to fix it. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any good resources to help be get good with learning ADF, data warehousing, modelling and a little Power BI? &lt;/p&gt;\n\n&lt;p&gt;Hope this makes sense!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "141h76i", "is_robot_indexable": true, "report_reasons": null, "author": "anonymous6156", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141h76i/help_getting_de_experience_in_azure_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141h76i/help_getting_de_experience_in_azure_stack/", "subreddit_subscribers": 109114, "created_utc": 1685976997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Seeing alot of news around Doris and it's performance benchmarks.\n\n\nSo, is it the next big thing to pocket?", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Apache Doris the next big thing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141eswm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685972153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seeing alot of news around Doris and it&amp;#39;s performance benchmarks.&lt;/p&gt;\n\n&lt;p&gt;So, is it the next big thing to pocket?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "141eswm", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141eswm/is_apache_doris_the_next_big_thing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141eswm/is_apache_doris_the_next_big_thing/", "subreddit_subscribers": 109114, "created_utc": 1685972153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, My organization is working on our first dbt project. We are Azure partners and I am figuring out how to deploy dbt in Azure infrastructure. Right now we have two options under consideration:\n\n1: Install dbt on a VM and schedule from ADF.\n\n2: Install dbt on Azure Container Instances after dockerizing and schedule from ADF. \n\n&amp;#x200B;\n\nWhich option do you advice ?. If there is a better alternative, please advice on it.", "author_fullname": "t2_o78u2p44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt deployment in Docker or VM ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1418rxw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685957562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, My organization is working on our first dbt project. We are Azure partners and I am figuring out how to deploy dbt in Azure infrastructure. Right now we have two options under consideration:&lt;/p&gt;\n\n&lt;p&gt;1: Install dbt on a VM and schedule from ADF.&lt;/p&gt;\n\n&lt;p&gt;2: Install dbt on Azure Container Instances after dockerizing and schedule from ADF. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Which option do you advice ?. If there is a better alternative, please advice on it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1418rxw", "is_robot_indexable": true, "report_reasons": null, "author": "SignificanceNo136", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1418rxw/dbt_deployment_in_docker_or_vm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1418rxw/dbt_deployment_in_docker_or_vm/", "subreddit_subscribers": 109114, "created_utc": 1685957562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI am modeling a postgres database and I'm wondering about filling via several sources ( other databases, CSV, txt, user etc... ). There's potentially a lot of data to extract and transform before integrating it into my database. The only purpose of this database is to feed applications, not analysis, just simple SQL request. Is it therefore correct to create a data pipeline or to say that the preparation process is a pipeline although the database is not a datawarehouse?\n\nThank you.", "author_fullname": "t2_q95335vn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Probably a stupid question... Can a pipeline be used to fill an OLTP database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1416a3t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685951228.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685950661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am modeling a postgres database and I&amp;#39;m wondering about filling via several sources ( other databases, CSV, txt, user etc... ). There&amp;#39;s potentially a lot of data to extract and transform before integrating it into my database. The only purpose of this database is to feed applications, not analysis, just simple SQL request. Is it therefore correct to create a data pipeline or to say that the preparation process is a pipeline although the database is not a datawarehouse?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1416a3t", "is_robot_indexable": true, "report_reasons": null, "author": "Aquilae2", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1416a3t/probably_a_stupid_question_can_a_pipeline_be_used/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1416a3t/probably_a_stupid_question_can_a_pipeline_be_used/", "subreddit_subscribers": 109114, "created_utc": 1685950661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI need couple of advices regarding career.\n\nCurrently I'm working in a bank as developer (Oracle technologies) and I have good experience with data analysis, ETL and BI (also some experience in Microsoft technologies)\nNow I'm planning to switch to real data engineering path. Knowing Python, I'm currently learning Pandas and other libraries and I have my eye on AWS and GCP.\n\nNow, my questions are:\n- What's the easiest way to find remote job,  and get into field professionaly? Also part-time option would be the best, so I could continue with the current job while obtaining experience with data engineering until I'm fully ready to switch jobs.\n\n- I have registered small company here in my country and I would like to use it for data engineering jobs. Is it easier / harder to get hired / payed as b2b? Should I mention this option to recruiters (I'm from Bosnia and Herzegovina, EU - if that makes any difference)\n\nAlso, any other advices about this would be appreciated.\n\nThanks in advance!", "author_fullname": "t2_swnotw4d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching career to data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141i3gu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685978762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I need couple of advices regarding career.&lt;/p&gt;\n\n&lt;p&gt;Currently I&amp;#39;m working in a bank as developer (Oracle technologies) and I have good experience with data analysis, ETL and BI (also some experience in Microsoft technologies)\nNow I&amp;#39;m planning to switch to real data engineering path. Knowing Python, I&amp;#39;m currently learning Pandas and other libraries and I have my eye on AWS and GCP.&lt;/p&gt;\n\n&lt;p&gt;Now, my questions are:\n- What&amp;#39;s the easiest way to find remote job,  and get into field professionaly? Also part-time option would be the best, so I could continue with the current job while obtaining experience with data engineering until I&amp;#39;m fully ready to switch jobs.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I have registered small company here in my country and I would like to use it for data engineering jobs. Is it easier / harder to get hired / payed as b2b? Should I mention this option to recruiters (I&amp;#39;m from Bosnia and Herzegovina, EU - if that makes any difference)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Also, any other advices about this would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "141i3gu", "is_robot_indexable": true, "report_reasons": null, "author": "Doza071", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141i3gu/switching_career_to_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141i3gu/switching_career_to_data_engineer/", "subreddit_subscribers": 109114, "created_utc": 1685978762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, fellow data enthusiasts! \ud83d\udc4b\n\nWe just launched the alpha release of [Sherloq](https://sherloqdata.io/?utm_source=redditl&amp;utm_medium=post&amp;utm_campaign=data_page) \ud83e\udd29 With this free Chrome extension, you can effortlessly save your SQL queries from any web editor you use, organize it in a folder and keep your KPI calculations aligned with your teammates. No more lost queries! It also offers a snippet tool with convenient keyboard shortcuts for repetitive code. Whether you're a data scientist or a developer, Sherloq empowers you to stay organized and efficient.\n\nWe know the struggle of organizing and managing your SQL queries scattered across various platforms like Slack or notes. That's why we're excited to introduce our collaborative data platform that designed to solve this headache!\n\n&amp;#x200B;\n\n[Here is an example of a saved SQL snippet injected with a keyboard shortcut t code edit](https://i.redd.it/uhz6ec3up54b1.gif)\n\n&amp;#x200B;\n\nWe're constantly working on enhancing Sherloq's features and usability. And we want you to be a part of it! Join us for the [alpha release and try out Sherloq for free](https://chrome.google.com/webstore/detail/sherloq-save-share-simpli/kjndilccgkemibeimjdefmjkhfddobfk). Explore our cool features and let us know your feedback or feature requests. Your input will help shape the platform and make it even more powerful!", "author_fullname": "t2_vgo9vaei", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sherloq: Chrome extension for query management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 17, "top_awarded_type": null, "hide_score": false, "media_metadata": {"uhz6ec3up54b1": {"status": "valid", "e": "AnimatedImage", "m": "image/gif", "p": [{"y": 69, "x": 108, "u": "https://preview.redd.it/uhz6ec3up54b1.gif?width=108&amp;crop=smart&amp;format=png8&amp;v=enabled&amp;s=2421174a67d48580a7e2bbdb3ea5b8a1bd7365d3"}, {"y": 139, "x": 216, "u": "https://preview.redd.it/uhz6ec3up54b1.gif?width=216&amp;crop=smart&amp;format=png8&amp;v=enabled&amp;s=3ebc2f3b88ba013eefaf9510f8aaf64593873398"}, {"y": 206, "x": 320, "u": "https://preview.redd.it/uhz6ec3up54b1.gif?width=320&amp;crop=smart&amp;format=png8&amp;v=enabled&amp;s=8112e6763498acc65c322aefdf2ccabea3bed35f"}], "s": {"y": 388, "gif": "https://i.redd.it/uhz6ec3up54b1.gif", "mp4": "https://preview.redd.it/uhz6ec3up54b1.gif?format=mp4&amp;v=enabled&amp;s=8ef4dd7473e0889f83d51f7c8281d3c56e3acef6", "x": 600}, "id": "uhz6ec3up54b1"}}, "name": "t3_1416wy0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HkDpKsZnvP7_a8s_jXIuL34m-kSy77e7K-IAMhWnW1Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1685952341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, fellow data enthusiasts! \ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;We just launched the alpha release of &lt;a href=\"https://sherloqdata.io/?utm_source=redditl&amp;amp;utm_medium=post&amp;amp;utm_campaign=data_page\"&gt;Sherloq&lt;/a&gt; \ud83e\udd29 With this free Chrome extension, you can effortlessly save your SQL queries from any web editor you use, organize it in a folder and keep your KPI calculations aligned with your teammates. No more lost queries! It also offers a snippet tool with convenient keyboard shortcuts for repetitive code. Whether you&amp;#39;re a data scientist or a developer, Sherloq empowers you to stay organized and efficient.&lt;/p&gt;\n\n&lt;p&gt;We know the struggle of organizing and managing your SQL queries scattered across various platforms like Slack or notes. That&amp;#39;s why we&amp;#39;re excited to introduce our collaborative data platform that designed to solve this headache!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/uhz6ec3up54b1.gif\"&gt;Here is an example of a saved SQL snippet injected with a keyboard shortcut t code edit&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re constantly working on enhancing Sherloq&amp;#39;s features and usability. And we want you to be a part of it! Join us for the &lt;a href=\"https://chrome.google.com/webstore/detail/sherloq-save-share-simpli/kjndilccgkemibeimjdefmjkhfddobfk\"&gt;alpha release and try out Sherloq for free&lt;/a&gt;. Explore our cool features and let us know your feedback or feature requests. Your input will help shape the platform and make it even more powerful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mYUPF3GbHOtlFa6UY8QuXuhwaWJPNu0ouFkcdj9h6W4.jpg?auto=webp&amp;v=enabled&amp;s=a6ddbe5ebc94895fcc16642562572ba1cfbf4a27", "width": 1024, "height": 131}, "resolutions": [{"url": "https://external-preview.redd.it/mYUPF3GbHOtlFa6UY8QuXuhwaWJPNu0ouFkcdj9h6W4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a21afc50ed0b4f2a167730c1f0690e39818af994", "width": 108, "height": 13}, {"url": "https://external-preview.redd.it/mYUPF3GbHOtlFa6UY8QuXuhwaWJPNu0ouFkcdj9h6W4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5262580a59e0d01dd1ab8f24234f0921d5bed6be", "width": 216, "height": 27}, {"url": "https://external-preview.redd.it/mYUPF3GbHOtlFa6UY8QuXuhwaWJPNu0ouFkcdj9h6W4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ff6b5e6278956b2b095f3768e649f5a29a3a510", "width": 320, "height": 40}, {"url": "https://external-preview.redd.it/mYUPF3GbHOtlFa6UY8QuXuhwaWJPNu0ouFkcdj9h6W4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bea029fd1205c3f80b838c3b333f01f2ce39f85a", "width": 640, "height": 81}, {"url": "https://external-preview.redd.it/mYUPF3GbHOtlFa6UY8QuXuhwaWJPNu0ouFkcdj9h6W4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b35941c2de2b4191f2d234aeca6084f9c3c5e5cd", "width": 960, "height": 122}], "variants": {}, "id": "WIejn18f-7pW7brSGEj-Lt7qSAGVJ5ofhiDCuH9gKco"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1416wy0", "is_robot_indexable": true, "report_reasons": null, "author": "hellosherloq1", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1416wy0/sherloq_chrome_extension_for_query_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1416wy0/sherloq_chrome_extension_for_query_management/", "subreddit_subscribers": 109114, "created_utc": 1685952341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have been a long time lurker in this sub. Recently, I decided to kick off my own data engineering agency. I have experience working with plenty of tools in the modern data stack + Python and Postgres, but starting an agency seems to be a different ballgame. Advice or help from folks in our field would be much appreciated.\n\nLanding page: [danai.dev](https://danai.dev)   \n\n\n**PLAN:**\n\nSo far my plan is to target \n\n* E-commerce companies\n* Industrial and Process Automation companies \n* Venture Capital firms\n\n(I have experience working in e-com and manufacturing/industry)\n\nMostly, I want to target any company that has a lot of data, but don't have the expertise to build their own data team. \n\nQuestion is on how to get clients? Current idea is to prospect and send Cold Emails to people who may find this valuable, while writing and sharing DE related articles and blogs to get outreach.", "author_fullname": "t2_f0ng6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for kicking of a new DE Agency", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1415tgo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685949386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have been a long time lurker in this sub. Recently, I decided to kick off my own data engineering agency. I have experience working with plenty of tools in the modern data stack + Python and Postgres, but starting an agency seems to be a different ballgame. Advice or help from folks in our field would be much appreciated.&lt;/p&gt;\n\n&lt;p&gt;Landing page: &lt;a href=\"https://danai.dev\"&gt;danai.dev&lt;/a&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;PLAN:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So far my plan is to target &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;E-commerce companies&lt;/li&gt;\n&lt;li&gt;Industrial and Process Automation companies &lt;/li&gt;\n&lt;li&gt;Venture Capital firms&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;(I have experience working in e-com and manufacturing/industry)&lt;/p&gt;\n\n&lt;p&gt;Mostly, I want to target any company that has a lot of data, but don&amp;#39;t have the expertise to build their own data team. &lt;/p&gt;\n\n&lt;p&gt;Question is on how to get clients? Current idea is to prospect and send Cold Emails to people who may find this valuable, while writing and sharing DE related articles and blogs to get outreach.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1415tgo", "is_robot_indexable": true, "report_reasons": null, "author": "bodpoq", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1415tgo/advice_for_kicking_of_a_new_de_agency/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1415tgo/advice_for_kicking_of_a_new_de_agency/", "subreddit_subscribers": 109114, "created_utc": 1685949386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uypc8pwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dealing with Stress, Anxiety, and Hardship in the Workplace - For Data Teams.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_141goay", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/oW2ST2TqJh8lwUnOU7z_uPNio5HJuUT1-8T4kZEtJGU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685975962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringcentral.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataengineeringcentral.substack.com/p/dealing-with-stress-anxiety-and-hardship", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1Y7lxIaCpednikHFqFth7tEilJrtWdz4_q4UQUvdWus.jpg?auto=webp&amp;v=enabled&amp;s=99414398c70bd3631b59ba8ac60fae8cb3c69ca4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/1Y7lxIaCpednikHFqFth7tEilJrtWdz4_q4UQUvdWus.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4793c125b4ed7daf2542151156cffb135801b63a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/1Y7lxIaCpednikHFqFth7tEilJrtWdz4_q4UQUvdWus.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f156d5ca38d969003d42eb37cccafd04c2e1c09", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/1Y7lxIaCpednikHFqFth7tEilJrtWdz4_q4UQUvdWus.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53fb1a728fa17f641cfb1638c6c9b8bc0bd6d86d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/1Y7lxIaCpednikHFqFth7tEilJrtWdz4_q4UQUvdWus.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3776a5aaaa38f5a96bea915d2dfd7e78842045a2", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/1Y7lxIaCpednikHFqFth7tEilJrtWdz4_q4UQUvdWus.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=270b84fa85f88739e0f81ec4bb2c04d426d74af9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/1Y7lxIaCpednikHFqFth7tEilJrtWdz4_q4UQUvdWus.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3489454e18359fdaf3e767c5ac7d03f8303f5bce", "width": 1080, "height": 540}], "variants": {}, "id": "2KFxKuutbxGF5hKkkkpCRiynNtuLX_DvDGRncEwMZwA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "141goay", "is_robot_indexable": true, "report_reasons": null, "author": "DarkClear3881", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141goay/dealing_with_stress_anxiety_and_hardship_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataengineeringcentral.substack.com/p/dealing-with-stress-anxiety-and-hardship", "subreddit_subscribers": 109114, "created_utc": 1685975962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for some advice here. There is lots to be found on automating the load of a Data Vault warehouse, with hubs, links &amp; satellites. But unless you want a transactional system driven (as opposed to domain driven) warehouse, there needs to be some form of modelling in the staging area prior to ingestion. \n\nExample: our Salesforce Account table holds leads, customers, companies, stores etc. and each of these would arguably sit in a different hub. An automated solution will just build off the source, which means the source (staging area) would need these modelled out into different tables - account_lead; account_customer; account_store etc.\n\nOur raw data sits in Snowflake, replicated there by FiveTran. Prior to ingesting into DV, what would your tool of choice be to load your staging tables?\n\n- basic sql;\n- dbt;\n- SQLMesh;\n- something else?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Staging area before data vault", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141bqzf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685965221.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some advice here. There is lots to be found on automating the load of a Data Vault warehouse, with hubs, links &amp;amp; satellites. But unless you want a transactional system driven (as opposed to domain driven) warehouse, there needs to be some form of modelling in the staging area prior to ingestion. &lt;/p&gt;\n\n&lt;p&gt;Example: our Salesforce Account table holds leads, customers, companies, stores etc. and each of these would arguably sit in a different hub. An automated solution will just build off the source, which means the source (staging area) would need these modelled out into different tables - account_lead; account_customer; account_store etc.&lt;/p&gt;\n\n&lt;p&gt;Our raw data sits in Snowflake, replicated there by FiveTran. Prior to ingesting into DV, what would your tool of choice be to load your staging tables?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;basic sql;&lt;/li&gt;\n&lt;li&gt;dbt;&lt;/li&gt;\n&lt;li&gt;SQLMesh;&lt;/li&gt;\n&lt;li&gt;something else?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "141bqzf", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141bqzf/staging_area_before_data_vault/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141bqzf/staging_area_before_data_vault/", "subreddit_subscribers": 109114, "created_utc": 1685965221.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "  I have a DW/Datalake setup on AWS S3 with Delta format.\n  My data consumers get access to the data via:\n- athena - via glue catalog/ manifest backed\n- redshift - external tables\n- Databricks sql endpoint/notebooks\n\nI am trying to wrap my head around how to monitor Data Access and audit it? \n I do have redshift setup with saml and i can consume the redshift internals.\n Athena i can use cloudwatch and cloudtrail, but will cost me an arm and a leg.\n\n What do i do about Databricks ? How can i persist thehitory of runned queries? \n\nThx", "author_fullname": "t2_nxu067bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datalake - Data Access Audit", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1418y5z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685958055.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a DW/Datalake setup on AWS S3 with Delta format.\n  My data consumers get access to the data via:\n- athena - via glue catalog/ manifest backed\n- redshift - external tables\n- Databricks sql endpoint/notebooks&lt;/p&gt;\n\n&lt;p&gt;I am trying to wrap my head around how to monitor Data Access and audit it? \n I do have redshift setup with saml and i can consume the redshift internals.\n Athena i can use cloudwatch and cloudtrail, but will cost me an arm and a leg.&lt;/p&gt;\n\n&lt;p&gt;What do i do about Databricks ? How can i persist thehitory of runned queries? &lt;/p&gt;\n\n&lt;p&gt;Thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1418y5z", "is_robot_indexable": true, "report_reasons": null, "author": "InsightByte", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1418y5z/datalake_data_access_audit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1418y5z/datalake_data_access_audit/", "subreddit_subscribers": 109114, "created_utc": 1685958055.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say in a stream processing, how do you spot the missing entries in a subscriber? When do you decide to use a broker?", "author_fullname": "t2_6zaja793", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you detect missing entries in a subscriber?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141wywx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686009910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say in a stream processing, how do you spot the missing entries in a subscriber? When do you decide to use a broker?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "141wywx", "is_robot_indexable": true, "report_reasons": null, "author": "Ambitious_Cucumber96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141wywx/how_do_you_detect_missing_entries_in_a_subscriber/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141wywx/how_do_you_detect_missing_entries_in_a_subscriber/", "subreddit_subscribers": 109114, "created_utc": 1686009910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a panel round of interview with an autonomous vehicle company. This is divided as follows\n\n45 mins - Python/Backend coding\n45 mins - Systems Designs technical component\n30 mins - Leadership questions.\n\nWhat kind of questions can I expect for the first 2 rounds? Can I expect more data structure or algorithm python question or Pyspark/Pandas questions? I've never had one for Systems Design. Any idea what to expect? Thanks!!\n\nI have 3 yrs of experience.", "author_fullname": "t2_ej6rfpuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upcoming Data Engineer Interview.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141q2g7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685995526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a panel round of interview with an autonomous vehicle company. This is divided as follows&lt;/p&gt;\n\n&lt;p&gt;45 mins - Python/Backend coding\n45 mins - Systems Designs technical component\n30 mins - Leadership questions.&lt;/p&gt;\n\n&lt;p&gt;What kind of questions can I expect for the first 2 rounds? Can I expect more data structure or algorithm python question or Pyspark/Pandas questions? I&amp;#39;ve never had one for Systems Design. Any idea what to expect? Thanks!!&lt;/p&gt;\n\n&lt;p&gt;I have 3 yrs of experience.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "141q2g7", "is_robot_indexable": true, "report_reasons": null, "author": "AppointmentFit5600", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141q2g7/upcoming_data_engineer_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141q2g7/upcoming_data_engineer_interview/", "subreddit_subscribers": 109114, "created_utc": 1685995526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Guys, I am looking for a game, where I would be able to connect to retrieve a lot of data directly from the game, maybe about what is happening on the screen. The reason is to learn to build big data infrastructure based on large amount of data. I was trying League of Legend, but their API in game didn't work for me, and API is dedicated to retrieve historical information rather than live data. Any ideas?", "author_fullname": "t2_adsn5t2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Steam computer game with dedicated API to retrieve live information from game client.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141oykq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685993301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Guys, I am looking for a game, where I would be able to connect to retrieve a lot of data directly from the game, maybe about what is happening on the screen. The reason is to learn to build big data infrastructure based on large amount of data. I was trying League of Legend, but their API in game didn&amp;#39;t work for me, and API is dedicated to retrieve historical information rather than live data. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "141oykq", "is_robot_indexable": true, "report_reasons": null, "author": "StyleBitter3753", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141oykq/a_steam_computer_game_with_dedicated_api_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141oykq/a_steam_computer_game_with_dedicated_api_to/", "subreddit_subscribers": 109114, "created_utc": 1685993301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hello fellow DE,\n\nI've been working on a project that uses AWS Lambdas written in Python, which get triggered upon the arrival of a file in an S3 bucket, and then proceed to write to a MySQL RDS database.\n\nI'm looking for ways to effectively perform integration tests on this workflow. I already wrote unit test that I can run locally and in my Github CI/CD pipeline.\n\nHere are some of the specific areas where I'd appreciate some guidance:\n\n1. **Mocking AWS Services**: What are the best practices to mock AWS services, specifically S3, Lambda, and MySQL RDS? I have come across some tools like moto  \n and localstack, but I'm unsure about their pros and cons, and whether there are better alternatives. Should I even mock my infra for integration test?\n2. **Data Persistence**: With the writing process to a MySQL RDS database, how can I ensure the data is correctly written and also reset the state for each test?\n\nI'm open to any tools, libraries, or best practices that could make my integration testing process more efficient and effective. Any examples or guides would be especially helpful.\n\nThank you in advance for your valuable inputs!", "author_fullname": "t2_gmwe0m1d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integration Testing for AWS Lambdas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1417dae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685953640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow DE,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working on a project that uses AWS Lambdas written in Python, which get triggered upon the arrival of a file in an S3 bucket, and then proceed to write to a MySQL RDS database.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for ways to effectively perform integration tests on this workflow. I already wrote unit test that I can run locally and in my Github CI/CD pipeline.&lt;/p&gt;\n\n&lt;p&gt;Here are some of the specific areas where I&amp;#39;d appreciate some guidance:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Mocking AWS Services&lt;/strong&gt;: What are the best practices to mock AWS services, specifically S3, Lambda, and MySQL RDS? I have come across some tools like moto&lt;br/&gt;\nand localstack, but I&amp;#39;m unsure about their pros and cons, and whether there are better alternatives. Should I even mock my infra for integration test?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Persistence&lt;/strong&gt;: With the writing process to a MySQL RDS database, how can I ensure the data is correctly written and also reset the state for each test?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m open to any tools, libraries, or best practices that could make my integration testing process more efficient and effective. Any examples or guides would be especially helpful.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your valuable inputs!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1417dae", "is_robot_indexable": true, "report_reasons": null, "author": "baguetteFeuille", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1417dae/integration_testing_for_aws_lambdas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1417dae/integration_testing_for_aws_lambdas/", "subreddit_subscribers": 109114, "created_utc": 1685953640.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Been working in data science for about 4 years. I\u2019m switching to DE and slowly starting to apply for jobs. \n\nI know if I go in DS, I can land a senior DS positon. Realistically, could I land a senior DE job or will need to aim for DE given my background was mainly DS.", "author_fullname": "t2_t4jv8qi9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data science to data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141lm26", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685985733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been working in data science for about 4 years. I\u2019m switching to DE and slowly starting to apply for jobs. &lt;/p&gt;\n\n&lt;p&gt;I know if I go in DS, I can land a senior DS positon. Realistically, could I land a senior DE job or will need to aim for DE given my background was mainly DS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "141lm26", "is_robot_indexable": true, "report_reasons": null, "author": "NoStrang3rDang3r", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141lm26/data_science_to_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141lm26/data_science_to_data_engineer/", "subreddit_subscribers": 109114, "created_utc": 1685985733.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}