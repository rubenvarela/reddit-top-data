{"kind": "Listing", "data": {"after": "t3_142cvhu", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_brkxjomi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I\u2019ve had the definition wrong this entire time\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 125, "top_awarded_type": null, "hide_score": false, "name": "t3_1420fjz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 365, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 365, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IFuK97ynhfsSPbLctWG4Nu9kNdsbn1tYITeoa2-9rxo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686017984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9uviprh35b4b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9uviprh35b4b1.jpg?auto=webp&amp;v=enabled&amp;s=b854077dd9f4cc89f247e44d63a04c45f3e13183", "width": 556, "height": 500}, "resolutions": [{"url": "https://preview.redd.it/9uviprh35b4b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fc60879835dacd0fc93b0a0944ee45ce943074f", "width": 108, "height": 97}, {"url": "https://preview.redd.it/9uviprh35b4b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81a30ee8d32ecc13cb413e31f972976c6fae5ad9", "width": 216, "height": 194}, {"url": "https://preview.redd.it/9uviprh35b4b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fcfb0b91c224266641f631f8d29d0b65971bf9d", "width": 320, "height": 287}], "variants": {}, "id": "U3wE6gNxgunkDMWgxqPlc8ScfOmNpg9hno-IJl-hp7E"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "1420fjz", "is_robot_indexable": true, "report_reasons": null, "author": "Straight_House8628", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1420fjz/ive_had_the_definition_wrong_this_entire_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9uviprh35b4b1.jpg", "subreddit_subscribers": 109210, "created_utc": 1686017984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A couple years ago I put my career on pause and moved back home to be closer to an ailing parent. I was a data analyst in my previous role with only two years experience, where I mostly worked in Excel, Power BI, a bit of SQL and Python. During this pause I beefed up my SQL, Python and stats knowledge and built some portfolio projects in Streamlit to showcase my interests and abilities. I also learned Git along the way as well.\n\nAbout a year ago I started job hunting where I was mostly looking for analyst roles. As I progressed in the job hunt I became increasingly interested in analytics engineering but felt like my skills weren't quite there yet. At the time I was thinking the best path would be getting another DA role and using it to catapult into an engineering role down the line. Fast forward a year, countless job applications, numerous interviews later, I finally had the most incredible breakthrough.\n\nLast week someone in my network connected me to the owner of a software development company that builds AI tools for customers. We exchanged a couple texts, hoped on a call, got talking about my journey in the world of data and how I ultimately want to get into analytics engineering. He then asked me if I was comfortable with window functions in SQL (I am) and if I know basic data structures (I do). At that point he asked me if I could send him a copy of my CV and if I could meet with him and some of his teammates for a dinner later in the week. I go to the dinner, meet the teams senior software engineers, we hit it off over some tacos, and they essentially create a role for me out of thin air! It was all very serendipitous and I still have no idea how this happened. They even told me that they weren't in the market for hiring but liked my story/journey/tenacity so much that they wanted me to come and work for them. The owner even told me as we were shaking hands and saying goodbye that he doesn't care about my background and that \"it is all about investing in the right people\".\n\nYes, they are a legitimate company and have several large clients that you have heard of. I received their job offer today and I nearly threw up after reading it. The title is Data Automation Developer, so most of my work is going to be in automation testing and they have also thrown in some ELT tasks as well. It's a fully remote role and one that I never imagined I would have in my career. While I don't feel like I am being setup for failure, and I do really like the team, I can't help but feel immense imposter syndrome when I look at the job posting that they created and I see all sorts of things I have no experience in. Is this even real life?? I understand the idea of hiring on someones potential but this is all incredibly daunting. Has anyone had a similar experience to this?", "author_fullname": "t2_jitcg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I feel like I won the job lottery and it has sent my imposter syndrome into the stratosphere", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141vfwl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 141, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 141, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686008502.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686006525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A couple years ago I put my career on pause and moved back home to be closer to an ailing parent. I was a data analyst in my previous role with only two years experience, where I mostly worked in Excel, Power BI, a bit of SQL and Python. During this pause I beefed up my SQL, Python and stats knowledge and built some portfolio projects in Streamlit to showcase my interests and abilities. I also learned Git along the way as well.&lt;/p&gt;\n\n&lt;p&gt;About a year ago I started job hunting where I was mostly looking for analyst roles. As I progressed in the job hunt I became increasingly interested in analytics engineering but felt like my skills weren&amp;#39;t quite there yet. At the time I was thinking the best path would be getting another DA role and using it to catapult into an engineering role down the line. Fast forward a year, countless job applications, numerous interviews later, I finally had the most incredible breakthrough.&lt;/p&gt;\n\n&lt;p&gt;Last week someone in my network connected me to the owner of a software development company that builds AI tools for customers. We exchanged a couple texts, hoped on a call, got talking about my journey in the world of data and how I ultimately want to get into analytics engineering. He then asked me if I was comfortable with window functions in SQL (I am) and if I know basic data structures (I do). At that point he asked me if I could send him a copy of my CV and if I could meet with him and some of his teammates for a dinner later in the week. I go to the dinner, meet the teams senior software engineers, we hit it off over some tacos, and they essentially create a role for me out of thin air! It was all very serendipitous and I still have no idea how this happened. They even told me that they weren&amp;#39;t in the market for hiring but liked my story/journey/tenacity so much that they wanted me to come and work for them. The owner even told me as we were shaking hands and saying goodbye that he doesn&amp;#39;t care about my background and that &amp;quot;it is all about investing in the right people&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Yes, they are a legitimate company and have several large clients that you have heard of. I received their job offer today and I nearly threw up after reading it. The title is Data Automation Developer, so most of my work is going to be in automation testing and they have also thrown in some ELT tasks as well. It&amp;#39;s a fully remote role and one that I never imagined I would have in my career. While I don&amp;#39;t feel like I am being setup for failure, and I do really like the team, I can&amp;#39;t help but feel immense imposter syndrome when I look at the job posting that they created and I see all sorts of things I have no experience in. Is this even real life?? I understand the idea of hiring on someones potential but this is all incredibly daunting. Has anyone had a similar experience to this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "141vfwl", "is_robot_indexable": true, "report_reasons": null, "author": "yror007", "discussion_type": null, "num_comments": 37, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141vfwl/i_feel_like_i_won_the_job_lottery_and_it_has_sent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141vfwl/i_feel_like_i_won_the_job_lottery_and_it_has_sent/", "subreddit_subscribers": 109210, "created_utc": 1686006525.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm curious how others document their data architecture. I wonder if there are any standards around this or if there is a lot of variance.  Would be great if you could include what types of diagrams and sections you would see along with how this presented to new employees or stakeholders.\n\nFor me, I'm built 2 data architectures as the a lone wolf data engineer.  My current documentation includes \n\n* An architecture diagram that shows all the cloud technologies and connections between them.\n* Brief descriptions of the phases like 'Data Ingestion', 'Orchestration', 'Reporting'.  \n\nThis documentation is mostly for my own reference and truth be told I'm wondering what else I should be including.", "author_fullname": "t2_5065w9mg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you or your company document your data architecture? What is typically included in the documentation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141lrjm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685986040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious how others document their data architecture. I wonder if there are any standards around this or if there is a lot of variance.  Would be great if you could include what types of diagrams and sections you would see along with how this presented to new employees or stakeholders.&lt;/p&gt;\n\n&lt;p&gt;For me, I&amp;#39;m built 2 data architectures as the a lone wolf data engineer.  My current documentation includes &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;An architecture diagram that shows all the cloud technologies and connections between them.&lt;/li&gt;\n&lt;li&gt;Brief descriptions of the phases like &amp;#39;Data Ingestion&amp;#39;, &amp;#39;Orchestration&amp;#39;, &amp;#39;Reporting&amp;#39;.&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This documentation is mostly for my own reference and truth be told I&amp;#39;m wondering what else I should be including.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "141lrjm", "is_robot_indexable": true, "report_reasons": null, "author": "kvotheTHEinquisitor", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141lrjm/how_do_you_or_your_company_document_your_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141lrjm/how_do_you_or_your_company_document_your_data/", "subreddit_subscribers": 109210, "created_utc": 1685986040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a Sr. DE assigned to me 50/50 with another team. Im not a dataengineer and she has an actual data engineer manager. Task assigned was to import data\n\nThe tables were 6 in total.  1 table had 95 million rows, but the other were mapping tables with less than 1k rows.\n\nTalked to her a couple of times now on why its not done. and so last thursday i removed her from all her other workloads outside of this and canceled all her meetings with me\n\nBut is there any reason it would take this long to do this task? she keeps telling me that it needs to further optimized and has been pretty evasive at this point\n\nId personally just \"select \\* from table 1\" at this point.  We can query Redshift directly from databricks. but i dont know anything about the limitations and im not that familar with databricks. i just query it adhoc.\n\nso before i make a big deal with this.  how long would you expect a Sr. DE with 20 years experience to do this task?", "author_fullname": "t2_9q93psld7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is this data engineer on my team dicking me around with bs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141si4x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686000542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Sr. DE assigned to me 50/50 with another team. Im not a dataengineer and she has an actual data engineer manager. Task assigned was to import data&lt;/p&gt;\n\n&lt;p&gt;The tables were 6 in total.  1 table had 95 million rows, but the other were mapping tables with less than 1k rows.&lt;/p&gt;\n\n&lt;p&gt;Talked to her a couple of times now on why its not done. and so last thursday i removed her from all her other workloads outside of this and canceled all her meetings with me&lt;/p&gt;\n\n&lt;p&gt;But is there any reason it would take this long to do this task? she keeps telling me that it needs to further optimized and has been pretty evasive at this point&lt;/p&gt;\n\n&lt;p&gt;Id personally just &amp;quot;select * from table 1&amp;quot; at this point.  We can query Redshift directly from databricks. but i dont know anything about the limitations and im not that familar with databricks. i just query it adhoc.&lt;/p&gt;\n\n&lt;p&gt;so before i make a big deal with this.  how long would you expect a Sr. DE with 20 years experience to do this task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "141si4x", "is_robot_indexable": true, "report_reasons": null, "author": "AntiquePassage7229", "discussion_type": null, "num_comments": 64, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141si4x/is_this_data_engineer_on_my_team_dicking_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141si4x/is_this_data_engineer_on_my_team_dicking_me/", "subreddit_subscribers": 109210, "created_utc": 1686000542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a49okn69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are data teams ticket takers or decision makers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_141mm5c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kpT-5cEDFtmFInhplih-1oOkSQD5NNJ3F57ni9_g0iM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685987733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "towardsdatascience.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://towardsdatascience.com/data-ticket-takers-vs-decision-makers-a6cf957b507a?source=friends_link&amp;sk=8e7d47f140ed3f52fb28c20afbaef857", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?auto=webp&amp;v=enabled&amp;s=4bb49ecea64a7f1202f6feca5180cf599a787581", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c37175c432cbcfcfe8a8137b16e632f4277ae695", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f74ae6673c837d75931c755e99a0195512544ad", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0fcfc0e5f281cbf54e60f111b9561bbbca7f4ba4", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d49462121ab79fe5e96a0c9f9d787e5924b2146c", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e07e5cc45eeb9d189ed12770abf052b30581f1dc", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/c29SdWi1xl771_I4NksyuZnqWj4C4BddpjnDTo6MKE8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=143ac4f8ddf1b3360a7fda0604683595d20dd195", "width": 1080, "height": 607}], "variants": {}, "id": "Uoz-O1WeN4siWLyEn-9XyIVY3xbQocAz9iebFjC37C4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "141mm5c", "is_robot_indexable": true, "report_reasons": null, "author": "Top-Substance2185", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141mm5c/are_data_teams_ticket_takers_or_decision_makers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://towardsdatascience.com/data-ticket-takers-vs-decision-makers-a6cf957b507a?source=friends_link&amp;sk=8e7d47f140ed3f52fb28c20afbaef857", "subreddit_subscribers": 109210, "created_utc": 1685987733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are using the AWS ecosystem and currently dumping some big files in S3 and we want to transform those files and load them into MySQL (AWS - RDS).\n\nWe are currently considering using AWS Glue, but the documentation is a little confusing and the notebooks are not working for me. I am trying to create a remote development process connecting the S3 and glue to my local VSCode and use it to build and test code. This sounds like a complicated process for me. Is there a better approach that doesn't cost much?", "author_fullname": "t2_3yclh0lo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best ETL workflow for doing transformations on data present in S3 and loading it to MySQL table.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14255vr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686029900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are using the AWS ecosystem and currently dumping some big files in S3 and we want to transform those files and load them into MySQL (AWS - RDS).&lt;/p&gt;\n\n&lt;p&gt;We are currently considering using AWS Glue, but the documentation is a little confusing and the notebooks are not working for me. I am trying to create a remote development process connecting the S3 and glue to my local VSCode and use it to build and test code. This sounds like a complicated process for me. Is there a better approach that doesn&amp;#39;t cost much?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14255vr", "is_robot_indexable": true, "report_reasons": null, "author": "vishalw007", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14255vr/best_etl_workflow_for_doing_transformations_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14255vr/best_etl_workflow_for_doing_transformations_on/", "subreddit_subscribers": 109210, "created_utc": 1686029900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "On paper, I have a great job.  It's at a great tech company that pays very well for my area, and I enjoy working with my colleagues.  I've also been there for more than 2.5 years.\n\n\n\n\nHowever, for almost a year at work now, my team's responsibilities have gotten less interesting to me.  When I started, the core of my responsibilities involved your usual data pipeline development and optimizing existing code.  However, data engineers were also given the opportunity to work on collaborative projects in the area of backend engineering and devops, which I often did.  I found all of my previous work very interesting.\n\n\n\n\nMy team is now at the point where our data pipelines just need the very occasional maintenance, so much of our responsibilities now involve advising stakeholders and attending meetings to provide various metrics.  I'm in meetings for a large part of my day, and the coding I do now is pretty repetitive.  Since my company hired a lot in 2021-2022, there is also less of an opportunity now to work between teams, and we lost a lot of the culture that encouraged working on projects between teams.\n\n\n\n\nI feel like I am at a great company with great coworkers, I just don't have the opportunity to grow as an engineer.", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it unreasonable to want to find a new job only because my current work is not interesting?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142fmvl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686057059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On paper, I have a great job.  It&amp;#39;s at a great tech company that pays very well for my area, and I enjoy working with my colleagues.  I&amp;#39;ve also been there for more than 2.5 years.&lt;/p&gt;\n\n&lt;p&gt;However, for almost a year at work now, my team&amp;#39;s responsibilities have gotten less interesting to me.  When I started, the core of my responsibilities involved your usual data pipeline development and optimizing existing code.  However, data engineers were also given the opportunity to work on collaborative projects in the area of backend engineering and devops, which I often did.  I found all of my previous work very interesting.&lt;/p&gt;\n\n&lt;p&gt;My team is now at the point where our data pipelines just need the very occasional maintenance, so much of our responsibilities now involve advising stakeholders and attending meetings to provide various metrics.  I&amp;#39;m in meetings for a large part of my day, and the coding I do now is pretty repetitive.  Since my company hired a lot in 2021-2022, there is also less of an opportunity now to work between teams, and we lost a lot of the culture that encouraged working on projects between teams.&lt;/p&gt;\n\n&lt;p&gt;I feel like I am at a great company with great coworkers, I just don&amp;#39;t have the opportunity to grow as an engineer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "142fmvl", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/142fmvl/is_it_unreasonable_to_want_to_find_a_new_job_only/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142fmvl/is_it_unreasonable_to_want_to_find_a_new_job_only/", "subreddit_subscribers": 109210, "created_utc": 1686057059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is HUDI a template against delta lake? What is one thing that it solves that the current models dont? Most importantly, when should one think of implementing HUDI , i mean the actual used case ?\n\nThe documentation out on their site is pain to understand. Pardon me if this has been already discussed here but yes, an expert's perspective about HUDI would be highly appreciated.", "author_fullname": "t2_nyd96q0y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm not understanding the purpose of HUDI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1426wgc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686035780.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686034648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is HUDI a template against delta lake? What is one thing that it solves that the current models dont? Most importantly, when should one think of implementing HUDI , i mean the actual used case ?&lt;/p&gt;\n\n&lt;p&gt;The documentation out on their site is pain to understand. Pardon me if this has been already discussed here but yes, an expert&amp;#39;s perspective about HUDI would be highly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1426wgc", "is_robot_indexable": true, "report_reasons": null, "author": "theaitribe", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1426wgc/im_not_understanding_the_purpose_of_hudi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1426wgc/im_not_understanding_the_purpose_of_hudi/", "subreddit_subscribers": 109210, "created_utc": 1686034648.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Good afternoon /r/dataengineering, I wanted to write up a quick post sharing how I and my company structure many of our ETL pipelines using [Meerschaum Compose](https://meerschaum.io/reference/compose/).\n\n&gt;***Disclaimer:*** *I am the author of Meerschaum and Compose, but that open source work is not affiliated with my company. Not looking to sell anything, just to share what works for us.*\n\nWe've implemented our core system's APIs as an [instance connector](https://meerschaum.io/reference/connectors/instance-connectors/), which is simply the destination of ETL. Whereas internal transformations follow this pattern:\n\n    ingestion source (extract) -&gt; PostgreSQL -&gt; PostgreSQL (transform)\n\nthe final step of our standard pipeline is like this:\n\n    PostgreSQL -&gt; Core system (load via our instance connector)\n\nWe usually organize new pipelines with three [compose files](https://meerschaum.io/reference/compose/), where the first stage ingests client data, the second transforms it, and final stage loads it via our internal instance connector:\n\n* `mrsm-compose-00-ingest.yaml`\n* `mrsm-compose-01-etl.yaml`\n* `mrsm-compose-02-publish.yaml`\n\nWe run everything on a shared base image with commonly used [plugins](https://meerschaum.io/reference/plugins/writing-plugins/) (i.e. pulling out of our datalake on S3), and integration-specific plugins live in the project next to the compose files.\n\nI know there are plenty of other tools out there for a similar workflow (a la Meltano). We deal with a lot of time-series data, so this fits our use case nicely and is quick to onboard new engineers. Most of our transformations are in SQL, and we have plugins in the base image for commonly ingested sources (e.g. Salesforce's API).\n\nWhat do you think of this workflow? In my experience it works well on the scale of &lt;100 million rows unless your data are mostly immutable.", "author_fullname": "t2_db74x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We run our ETL pipelines via Meerschaum Compose", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141q100", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685995449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good afternoon &lt;a href=\"/r/dataengineering\"&gt;/r/dataengineering&lt;/a&gt;, I wanted to write up a quick post sharing how I and my company structure many of our ETL pipelines using &lt;a href=\"https://meerschaum.io/reference/compose/\"&gt;Meerschaum Compose&lt;/a&gt;.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Disclaimer:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;I am the author of Meerschaum and Compose, but that open source work is not affiliated with my company. Not looking to sell anything, just to share what works for us.&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;We&amp;#39;ve implemented our core system&amp;#39;s APIs as an &lt;a href=\"https://meerschaum.io/reference/connectors/instance-connectors/\"&gt;instance connector&lt;/a&gt;, which is simply the destination of ETL. Whereas internal transformations follow this pattern:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ingestion source (extract) -&amp;gt; PostgreSQL -&amp;gt; PostgreSQL (transform)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;the final step of our standard pipeline is like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;PostgreSQL -&amp;gt; Core system (load via our instance connector)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;We usually organize new pipelines with three &lt;a href=\"https://meerschaum.io/reference/compose/\"&gt;compose files&lt;/a&gt;, where the first stage ingests client data, the second transforms it, and final stage loads it via our internal instance connector:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;mrsm-compose-00-ingest.yaml&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;mrsm-compose-01-etl.yaml&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;mrsm-compose-02-publish.yaml&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We run everything on a shared base image with commonly used &lt;a href=\"https://meerschaum.io/reference/plugins/writing-plugins/\"&gt;plugins&lt;/a&gt; (i.e. pulling out of our datalake on S3), and integration-specific plugins live in the project next to the compose files.&lt;/p&gt;\n\n&lt;p&gt;I know there are plenty of other tools out there for a similar workflow (a la Meltano). We deal with a lot of time-series data, so this fits our use case nicely and is quick to onboard new engineers. Most of our transformations are in SQL, and we have plugins in the base image for commonly ingested sources (e.g. Salesforce&amp;#39;s API).&lt;/p&gt;\n\n&lt;p&gt;What do you think of this workflow? In my experience it works well on the scale of &amp;lt;100 million rows unless your data are mostly immutable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "141q100", "is_robot_indexable": true, "report_reasons": null, "author": "Obliterative_hippo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141q100/we_run_our_etl_pipelines_via_meerschaum_compose/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141q100/we_run_our_etl_pipelines_via_meerschaum_compose/", "subreddit_subscribers": 109210, "created_utc": 1685995449.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say in a stream processing, how do you spot the missing entries in a subscriber? When do you decide to use a broker?", "author_fullname": "t2_6zaja793", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you detect missing entries in a subscriber?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141wywx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686009910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say in a stream processing, how do you spot the missing entries in a subscriber? When do you decide to use a broker?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "141wywx", "is_robot_indexable": true, "report_reasons": null, "author": "Ambitious_Cucumber96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/141wywx/how_do_you_detect_missing_entries_in_a_subscriber/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/141wywx/how_do_you_detect_missing_entries_in_a_subscriber/", "subreddit_subscribers": 109210, "created_utc": 1686009910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Blog] Modern Data Architecture: Data Lakes, Data Lakehouses and Data Mesh", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_142hvm3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/p92sma0n5QT9bNqNV10xSWZOBqQ_BLs-vft5oV9UZ4U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686061770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/pulse/dremio-modern-data-architecture-lakes-lakehouses-mesh-alex-merced", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/M0Mmxel-fRn_znsBGILJ2GSM2Eo1HMfm_foQym-Z8wc.jpg?auto=webp&amp;v=enabled&amp;s=69797846034b3ce8e4f0a8bb2bf679651b6032a8", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/M0Mmxel-fRn_znsBGILJ2GSM2Eo1HMfm_foQym-Z8wc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cf650d471314bab7e6c637b4951db7927856d28", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/M0Mmxel-fRn_znsBGILJ2GSM2Eo1HMfm_foQym-Z8wc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab1ad6015da62b3b5a656a37e027014fa3e2b00d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/M0Mmxel-fRn_znsBGILJ2GSM2Eo1HMfm_foQym-Z8wc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a309836d3f9157cd2484e144026660ed34abeff", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/M0Mmxel-fRn_znsBGILJ2GSM2Eo1HMfm_foQym-Z8wc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c6089a0a8a2291a0e36c0e48d415f68ed761913", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/M0Mmxel-fRn_znsBGILJ2GSM2Eo1HMfm_foQym-Z8wc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d004a6260959d1e74589182881a7cab2ea84dd0d", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/M0Mmxel-fRn_znsBGILJ2GSM2Eo1HMfm_foQym-Z8wc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9d94d3e8b7467079a6e2e0f216b4f12e593d010", "width": 1080, "height": 565}], "variants": {}, "id": "sam90P28s5CV067X6he2za03vQHZF4DA_RzyAIf2U1I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "142hvm3", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142hvm3/blog_modern_data_architecture_data_lakes_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/pulse/dremio-modern-data-architecture-lakes-lakehouses-mesh-alex-merced", "subreddit_subscribers": 109210, "created_utc": 1686061770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working as a Data Scientist in a small, relatively new Data Science team (&lt;10 data scientists) as part of a larger company (&lt;1000 employees). We've grown quickly in the last year and are noticing that we're severely lacking in data engineering skills. While my boss is trying his best to get us a proper data engineer, I have gotten sign-off to get some cross-training to tide us over. \n\nSince we're almost exclusively working with structured data, I want to focus on SQL databases (we're running MySQL at the moment) and data modeling, as that's where we'd probably get the most bang for our buck.  \nI do have some basic knowledge in SQL: I can do simple JOINs and have heard of keys and normalization, but beyond that, things get hazy fast.\n\nWhat's the best way for me to get a decent foundation? Is there a book I should absolutely read? A coursera/udemy course? A lecture I should attend in University? Some 5-day crash-course Bootcamp?\n\nIs there something that I should definitely learn that I don't yet know I should learn?\n\nThanks a lot for your insights!", "author_fullname": "t2_uda7ln5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cross-Training from Data Scientist, how and what?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142fqh5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686057272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working as a Data Scientist in a small, relatively new Data Science team (&amp;lt;10 data scientists) as part of a larger company (&amp;lt;1000 employees). We&amp;#39;ve grown quickly in the last year and are noticing that we&amp;#39;re severely lacking in data engineering skills. While my boss is trying his best to get us a proper data engineer, I have gotten sign-off to get some cross-training to tide us over. &lt;/p&gt;\n\n&lt;p&gt;Since we&amp;#39;re almost exclusively working with structured data, I want to focus on SQL databases (we&amp;#39;re running MySQL at the moment) and data modeling, as that&amp;#39;s where we&amp;#39;d probably get the most bang for our buck.&lt;br/&gt;\nI do have some basic knowledge in SQL: I can do simple JOINs and have heard of keys and normalization, but beyond that, things get hazy fast.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best way for me to get a decent foundation? Is there a book I should absolutely read? A coursera/udemy course? A lecture I should attend in University? Some 5-day crash-course Bootcamp?&lt;/p&gt;\n\n&lt;p&gt;Is there something that I should definitely learn that I don&amp;#39;t yet know I should learn?&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for your insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "142fqh5", "is_robot_indexable": true, "report_reasons": null, "author": "invalidConsciousness", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142fqh5/crosstraining_from_data_scientist_how_and_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142fqh5/crosstraining_from_data_scientist_how_and_what/", "subreddit_subscribers": 109210, "created_utc": 1686057272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, so title really says all I currently have 6+ years of experience in DE. BS in CS and MS in Data Science. I have been applying for primarily DE jobs and SE jobs.\n\nI would say my rates are about for every 50 or so jobs applied I get about 5 or so first round/phone screenings. So, I am getting interviews but in those 5 first round/phone screenings 0-1 continue with me. Which is pretty annoying because I cannot seem to comprehend on how one can make a decision to not continue the process just from a 10-30 min phone call - without the ability to show my skill set in a technical manner. To me, considering my education and years of experience, that means other applicants must have at least an MS or PhD and many more years of experience than me?\n\nIs anybody else having troubles getting a job atm in the industry? Family/friends have been telling me the tech market is doing pretty bad atm is this true? My issue is that I am getting interviews so it almost makes me think otherwise like why would I be getting interviews if the market is in a bad place? Are companies just filling in quotas/reqs and interviewing people and not actually hiring/filling the roles?\n\nWhat are some other jobs somebody in DE can easily change to? I have been applying to data analyst related jobs recently, any other recommendations?\n\nI will likely post my resume another time to ask for some advice on changes I can make but just wanted to hear the thoughts of others.", "author_fullname": "t2_9vildhm3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recently laid off, getting interviews but not landing anything - thoughts on current state of job opportunities?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142a3n1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686050888.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686043555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, so title really says all I currently have 6+ years of experience in DE. BS in CS and MS in Data Science. I have been applying for primarily DE jobs and SE jobs.&lt;/p&gt;\n\n&lt;p&gt;I would say my rates are about for every 50 or so jobs applied I get about 5 or so first round/phone screenings. So, I am getting interviews but in those 5 first round/phone screenings 0-1 continue with me. Which is pretty annoying because I cannot seem to comprehend on how one can make a decision to not continue the process just from a 10-30 min phone call - without the ability to show my skill set in a technical manner. To me, considering my education and years of experience, that means other applicants must have at least an MS or PhD and many more years of experience than me?&lt;/p&gt;\n\n&lt;p&gt;Is anybody else having troubles getting a job atm in the industry? Family/friends have been telling me the tech market is doing pretty bad atm is this true? My issue is that I am getting interviews so it almost makes me think otherwise like why would I be getting interviews if the market is in a bad place? Are companies just filling in quotas/reqs and interviewing people and not actually hiring/filling the roles?&lt;/p&gt;\n\n&lt;p&gt;What are some other jobs somebody in DE can easily change to? I have been applying to data analyst related jobs recently, any other recommendations?&lt;/p&gt;\n\n&lt;p&gt;I will likely post my resume another time to ask for some advice on changes I can make but just wanted to hear the thoughts of others.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "142a3n1", "is_robot_indexable": true, "report_reasons": null, "author": "ReasonableOpinion40", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142a3n1/recently_laid_off_getting_interviews_but_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142a3n1/recently_laid_off_getting_interviews_but_not/", "subreddit_subscribers": 109210, "created_utc": 1686043555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nFirst of all, I thank God for being part of this amazing data community. Whether knowingly or unknowingly, this subreddit has helped me a lot.\n\nI'm planning to change jobs and target the position of a senior data engineer. Currently, I am a data engineer in my organization. I am also working on side projects with the goal of showcasing real impact on my CV and to potential employers.\n\nIn my current side project:\n\nI am extracting data from an API, transforming it using Spark, and loading it into PostgreSQL. The end product will be built on Streamlit.\n\nI am also working with Airflow as an orchestrator, although I have never worked with it before. Additionally, I am exploring Great Expectations.\n\nI have two questions:\n\n1) Does the above project look good for a candidate applying for a senior data engineer role?\n\n2) What factors should be considered when building a side project as a senior data engineer?\n\nPlease let me know if you have any suggestions or feedback. Thank you.", "author_fullname": "t2_ci308gob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Side project as a senior data engineer.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142768o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686035378.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;First of all, I thank God for being part of this amazing data community. Whether knowingly or unknowingly, this subreddit has helped me a lot.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to change jobs and target the position of a senior data engineer. Currently, I am a data engineer in my organization. I am also working on side projects with the goal of showcasing real impact on my CV and to potential employers.&lt;/p&gt;\n\n&lt;p&gt;In my current side project:&lt;/p&gt;\n\n&lt;p&gt;I am extracting data from an API, transforming it using Spark, and loading it into PostgreSQL. The end product will be built on Streamlit.&lt;/p&gt;\n\n&lt;p&gt;I am also working with Airflow as an orchestrator, although I have never worked with it before. Additionally, I am exploring Great Expectations.&lt;/p&gt;\n\n&lt;p&gt;I have two questions:&lt;/p&gt;\n\n&lt;p&gt;1) Does the above project look good for a candidate applying for a senior data engineer role?&lt;/p&gt;\n\n&lt;p&gt;2) What factors should be considered when building a side project as a senior data engineer?&lt;/p&gt;\n\n&lt;p&gt;Please let me know if you have any suggestions or feedback. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "142768o", "is_robot_indexable": true, "report_reasons": null, "author": "Delicious_Attempt_99", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142768o/side_project_as_a_senior_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142768o/side_project_as_a_senior_data_engineer/", "subreddit_subscribers": 109210, "created_utc": 1686035378.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am willing to learn from stratch how to data modeling entities in an IoT context in order to map thoese entities in a relational database (or another paradigm of database if more suitable).\n\nLet me define the entities in their gerarchy:\n\n\\- Plants\n\n\\- Machines\n\n\\- Sensors\n\nThe sensors output data with different frenquencies. Should I have a table with all measures from a single machine resulting in a sparse table or should I have a table for each sensor containing the measurements? Where should I start about designing this?\n\nFeel free to source me references or books also, thanks!", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to data modeling in IoT context", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1426hhj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686033496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am willing to learn from stratch how to data modeling entities in an IoT context in order to map thoese entities in a relational database (or another paradigm of database if more suitable).&lt;/p&gt;\n\n&lt;p&gt;Let me define the entities in their gerarchy:&lt;/p&gt;\n\n&lt;p&gt;- Plants&lt;/p&gt;\n\n&lt;p&gt;- Machines&lt;/p&gt;\n\n&lt;p&gt;- Sensors&lt;/p&gt;\n\n&lt;p&gt;The sensors output data with different frenquencies. Should I have a table with all measures from a single machine resulting in a sparse table or should I have a table for each sensor containing the measurements? Where should I start about designing this?&lt;/p&gt;\n\n&lt;p&gt;Feel free to source me references or books also, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1426hhj", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1426hhj/how_to_data_modeling_in_iot_context/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1426hhj/how_to_data_modeling_in_iot_context/", "subreddit_subscribers": 109210, "created_utc": 1686033496.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, everyone! :)  \n\n\nThat's my first time working on Google Cloud and I have to move data between buckets and from buckets to BigQuery.   \n\n\nI've extensive experience with Spark (on-prem Hadoop and Databricks) and no experience at all with Beam (powering Dataflow). Also, I'm not afraid of learning.  But, I'm indeed trying to reduce:  \n\\- cognitive load  \n\\- infra management  \n\\- cloud costs  \n\n\nSpark can follow a bucket for new files and has a connector to BigQuery. So I'll need:  \n\\- Dataproc  \n\\- Buckets for checkpointing\n\nBeam can follow a bucket for new files, so it should be the same as Spark. I suppose it needs some sort of checkpointing as well, or maybe it works better with new files notification to Pub/Sub?  \n\n\n**So, I'm trying to compare and get to a decision:**  \n1. How straightforward is it to use (and to monitor!) Dataflow vs Dataproc?  \n2. Is Dataflow worth the time spent learning Beam?  \n3. If my transformations are simple, maybe I should just go for new files notification to Pub/Sub and plain code using Google Storage read and BigQuery write APIs?  \n4. How to estimate cloud costs so I can compare them?  \n5. More considerations?  \n\n\nThank you :)", "author_fullname": "t2_vd6ewwka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dataflow or Spark on Dataproc?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_142l4u8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686068280.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, everyone! :)  &lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s my first time working on Google Cloud and I have to move data between buckets and from buckets to BigQuery.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve extensive experience with Spark (on-prem Hadoop and Databricks) and no experience at all with Beam (powering Dataflow). Also, I&amp;#39;m not afraid of learning.  But, I&amp;#39;m indeed trying to reduce:&lt;br/&gt;\n- cognitive load&lt;br/&gt;\n- infra management&lt;br/&gt;\n- cloud costs  &lt;/p&gt;\n\n&lt;p&gt;Spark can follow a bucket for new files and has a connector to BigQuery. So I&amp;#39;ll need:&lt;br/&gt;\n- Dataproc&lt;br/&gt;\n- Buckets for checkpointing&lt;/p&gt;\n\n&lt;p&gt;Beam can follow a bucket for new files, so it should be the same as Spark. I suppose it needs some sort of checkpointing as well, or maybe it works better with new files notification to Pub/Sub?  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So, I&amp;#39;m trying to compare and get to a decision:&lt;/strong&gt;&lt;br/&gt;\n1. How straightforward is it to use (and to monitor!) Dataflow vs Dataproc?&lt;br/&gt;\n2. Is Dataflow worth the time spent learning Beam?&lt;br/&gt;\n3. If my transformations are simple, maybe I should just go for new files notification to Pub/Sub and plain code using Google Storage read and BigQuery write APIs?&lt;br/&gt;\n4. How to estimate cloud costs so I can compare them?&lt;br/&gt;\n5. More considerations?  &lt;/p&gt;\n\n&lt;p&gt;Thank you :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "142l4u8", "is_robot_indexable": true, "report_reasons": null, "author": "yfeltz", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142l4u8/dataflow_or_spark_on_dataproc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142l4u8/dataflow_or_spark_on_dataproc/", "subreddit_subscribers": 109210, "created_utc": 1686068280.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you think having both of them together in a architecture is an added benefit ? Given that the lakehouse is not performant at some point ? \n\nIf you have both the components, then i wonder if i have to stick to medallion architecture, then all the marts are in DWH , i hope this will be a ideal solution", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lakehouse architecture + Data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142jtdu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686065755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you think having both of them together in a architecture is an added benefit ? Given that the lakehouse is not performant at some point ? &lt;/p&gt;\n\n&lt;p&gt;If you have both the components, then i wonder if i have to stick to medallion architecture, then all the marts are in DWH , i hope this will be a ideal solution&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "142jtdu", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142jtdu/lakehouse_architecture_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142jtdu/lakehouse_architecture_data_warehouse/", "subreddit_subscribers": 109210, "created_utc": 1686065755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As in the title, I am searching for a well-documented repository or a detailed article presenting a real project, business use case utilizing Prefect.\n\nArticles from the Prefect website are short, mainly focus on improvements achieved and mostly lack of any schemas.\n\nI'd love something like this project using Airflow: [GoodReads](https://github.com/san089/goodreads_etl_pipeline).  \nI specifically look for information how the authors dealt with deployment and on what kind of data they work.   \n", "author_fullname": "t2_861irqdh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I cannot find any more complex Prefect use cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142jiit", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1686072277.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686065124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As in the title, I am searching for a well-documented repository or a detailed article presenting a real project, business use case utilizing Prefect.&lt;/p&gt;\n\n&lt;p&gt;Articles from the Prefect website are short, mainly focus on improvements achieved and mostly lack of any schemas.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love something like this project using Airflow: &lt;a href=\"https://github.com/san089/goodreads_etl_pipeline\"&gt;GoodReads&lt;/a&gt;.&lt;br/&gt;\nI specifically look for information how the authors dealt with deployment and on what kind of data they work.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zeCetiiP1x3hhDERavKldzjfdXBD2inB68isrgowVcc.jpg?auto=webp&amp;v=enabled&amp;s=74c38cfef315de9b926f7ab3bccc3b7660188ae4", "width": 1140, "height": 440}, "resolutions": [{"url": "https://external-preview.redd.it/zeCetiiP1x3hhDERavKldzjfdXBD2inB68isrgowVcc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c78253e6276fd6521640ae19209ca4e37760bea", "width": 108, "height": 41}, {"url": "https://external-preview.redd.it/zeCetiiP1x3hhDERavKldzjfdXBD2inB68isrgowVcc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0db8b158a4e97dc9c4e85f2ab9502f7accd28669", "width": 216, "height": 83}, {"url": "https://external-preview.redd.it/zeCetiiP1x3hhDERavKldzjfdXBD2inB68isrgowVcc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c920222c33ed1d035155be4ed988e79c06eb67ba", "width": 320, "height": 123}, {"url": "https://external-preview.redd.it/zeCetiiP1x3hhDERavKldzjfdXBD2inB68isrgowVcc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d664c6b62b7e8d4438f129ff7cfc78447dde326b", "width": 640, "height": 247}, {"url": "https://external-preview.redd.it/zeCetiiP1x3hhDERavKldzjfdXBD2inB68isrgowVcc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32ae121d0a994b26fa19730d98554ccd46600f4e", "width": 960, "height": 370}, {"url": "https://external-preview.redd.it/zeCetiiP1x3hhDERavKldzjfdXBD2inB68isrgowVcc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e6277e9f4b2f9ffdfb13ffa6d6ce01c692768b4", "width": 1080, "height": 416}], "variants": {}, "id": "5cj5oE6uE4XBn9peCzMgaY3obDqr-_50VHDfEx-T050"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "142jiit", "is_robot_indexable": true, "report_reasons": null, "author": "Lihor", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142jiit/i_cannot_find_any_more_complex_prefect_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142jiit/i_cannot_find_any_more_complex_prefect_use_cases/", "subreddit_subscribers": 109210, "created_utc": 1686065124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking into using Athena + dbt for our data workloads and I am interested in best practices / industry standards how to use this combo as efficiently as possible. I would like to use Iceberg as storage format. \n\nSome of my questions but I would be more than happy if you add anything else you think it would be good to know.\n\n1. How do you structure buckets? I am thinking of having one bucket for \u201craw\u201d data, then bucket for \u201cloaded data by Athena\u201d (i.e. raw data transformed to glue tables), some \u201cwarehouse-like\u201d bucket and finally \u201cdata-martish\u201d bucket for analysis. Each bucket then would correspond to one glue database. \n2. The very first layer of data is \u201crefreshed\u201d automatically? Meaning if I upload new raw data, the tables are updated automatically or I need glue crawler/something else for that?\n3. Following layers have to be periodically recomputed (ie triggering dbt models) - you schedule this e.g. in Airflow?\n4. You use dbt-athena-community library and/or anything else?\n5. dbt always creates folder structure like \u201cschema/tableName/tableHash/paritionHash/partition/data\u201d. Are these hashes necessary to have stored like this? I think it is because of creating temporal tables so dbt knows which table is the main one and which is temp one but it would be nicer to have it more clean.\n6. Glue crawlers are used only to explore new datasets or also new partitions (i.e. existing table with new partition)? Or new partition is read automatically - no need for \u201crefresh\u201d?\n7. Anything else I should be aware of?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Athena + dbt users - best practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ib76", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686062677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking into using Athena + dbt for our data workloads and I am interested in best practices / industry standards how to use this combo as efficiently as possible. I would like to use Iceberg as storage format. &lt;/p&gt;\n\n&lt;p&gt;Some of my questions but I would be more than happy if you add anything else you think it would be good to know.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How do you structure buckets? I am thinking of having one bucket for \u201craw\u201d data, then bucket for \u201cloaded data by Athena\u201d (i.e. raw data transformed to glue tables), some \u201cwarehouse-like\u201d bucket and finally \u201cdata-martish\u201d bucket for analysis. Each bucket then would correspond to one glue database. &lt;/li&gt;\n&lt;li&gt;The very first layer of data is \u201crefreshed\u201d automatically? Meaning if I upload new raw data, the tables are updated automatically or I need glue crawler/something else for that?&lt;/li&gt;\n&lt;li&gt;Following layers have to be periodically recomputed (ie triggering dbt models) - you schedule this e.g. in Airflow?&lt;/li&gt;\n&lt;li&gt;You use dbt-athena-community library and/or anything else?&lt;/li&gt;\n&lt;li&gt;dbt always creates folder structure like \u201cschema/tableName/tableHash/paritionHash/partition/data\u201d. Are these hashes necessary to have stored like this? I think it is because of creating temporal tables so dbt knows which table is the main one and which is temp one but it would be nicer to have it more clean.&lt;/li&gt;\n&lt;li&gt;Glue crawlers are used only to explore new datasets or also new partitions (i.e. existing table with new partition)? Or new partition is read automatically - no need for \u201crefresh\u201d?&lt;/li&gt;\n&lt;li&gt;Anything else I should be aware of?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "142ib76", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142ib76/athena_dbt_users_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142ib76/athena_dbt_users_best_practices/", "subreddit_subscribers": 109210, "created_utc": 1686062677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any ETL tools that extract data from SaaS platforms such as Salesforce and Hubspot in a streaming / CDC fashion? Running Fivetran in high frequency gets costly. Any other suggestions on how to get \u201crealtime\u201d data from these common Business SaaS tools. Use case is to do automation from these change events", "author_fullname": "t2_cmjwin", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming ETL tools for SaaS products?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142i57z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686062337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any ETL tools that extract data from SaaS platforms such as Salesforce and Hubspot in a streaming / CDC fashion? Running Fivetran in high frequency gets costly. Any other suggestions on how to get \u201crealtime\u201d data from these common Business SaaS tools. Use case is to do automation from these change events&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "142i57z", "is_robot_indexable": true, "report_reasons": null, "author": "tiltaltti", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142i57z/streaming_etl_tools_for_saas_products/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142i57z/streaming_etl_tools_for_saas_products/", "subreddit_subscribers": 109210, "created_utc": 1686062337.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ap2l3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: DataFusion is an underrated DataFrame library that let's you run fast localhost queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_142gprs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/v17Env0P5R8HxJAGG8WGe8blqw3sH5VL2hEvYqAk9L0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686059351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/64y5d28tje4b1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/64y5d28tje4b1.png?auto=webp&amp;v=enabled&amp;s=0d230a0eb95a1ad78354d065a40bb9a60e30ea8d", "width": 1338, "height": 1592}, "resolutions": [{"url": "https://preview.redd.it/64y5d28tje4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c88eb63f2a7b0f554ddd8d6140b14a1c5bbabe95", "width": 108, "height": 128}, {"url": "https://preview.redd.it/64y5d28tje4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=050b2ff3a33f5b0989921f617747bed7b8a6c08f", "width": 216, "height": 257}, {"url": "https://preview.redd.it/64y5d28tje4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3baa8aa581988b14876a09acb924e1aa3fe05666", "width": 320, "height": 380}, {"url": "https://preview.redd.it/64y5d28tje4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=505303d9685b6318065209ad4b4cd32d6d9306d5", "width": 640, "height": 761}, {"url": "https://preview.redd.it/64y5d28tje4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cb3e8458756053a3abc148d14412836abafe109d", "width": 960, "height": 1142}, {"url": "https://preview.redd.it/64y5d28tje4b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f606e6a8d821055907ce754437efbae3cd854ef3", "width": 1080, "height": 1285}], "variants": {}, "id": "-3IiHgvqoGDltZdWDVS-0c089DXcaMu6d0qfjCviwlI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "142gprs", "is_robot_indexable": true, "report_reasons": null, "author": "MrPowersAAHHH", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142gprs/psa_datafusion_is_an_underrated_dataframe_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/64y5d28tje4b1.png", "subreddit_subscribers": 109210, "created_utc": 1686059351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a situation where, among the many data sources, the raw format may change frequently.  This is because the entities generating the data may change or have their code changed.  \n\nIs this just handled by more ETL / ELT work?  The principal here being its best to have the raw data. \n\nOr would it make sense to have a normalization layer, that isn\u2019t analytics, and the analytics pipeline consumes this data?\n\nThanks for thoughts.  I could find any useful material on handling raw data that may change formats.", "author_fullname": "t2_qj7u92h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to best support raw data that has changing formats", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142fri3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686057332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a situation where, among the many data sources, the raw format may change frequently.  This is because the entities generating the data may change or have their code changed.  &lt;/p&gt;\n\n&lt;p&gt;Is this just handled by more ETL / ELT work?  The principal here being its best to have the raw data. &lt;/p&gt;\n\n&lt;p&gt;Or would it make sense to have a normalization layer, that isn\u2019t analytics, and the analytics pipeline consumes this data?&lt;/p&gt;\n\n&lt;p&gt;Thanks for thoughts.  I could find any useful material on handling raw data that may change formats.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "142fri3", "is_robot_indexable": true, "report_reasons": null, "author": "madmyersreal", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142fri3/how_to_best_support_raw_data_that_has_changing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142fri3/how_to_best_support_raw_data_that_has_changing/", "subreddit_subscribers": 109210, "created_utc": 1686057332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI've been really looking forward to spark connect and finally being able to work in my local IDE as well. However, it seems to me that currently the delta library is not working with spark connect. However, in this [video](https://www.youtube.com/live/vTd3OqDzjuo?feature=share) the lead developer mentions the delta package specifically and that is possible using protobuf. Has someone made it work so far and what are your experiences with spark connect?", "author_fullname": "t2_bk0fqe9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark connect + Delta", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142fphr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1686057215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been really looking forward to spark connect and finally being able to work in my local IDE as well. However, it seems to me that currently the delta library is not working with spark connect. However, in this &lt;a href=\"https://www.youtube.com/live/vTd3OqDzjuo?feature=share\"&gt;video&lt;/a&gt; the lead developer mentions the delta package specifically and that is possible using protobuf. Has someone made it work so far and what are your experiences with spark connect?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uuEDnN6MuTZ98hsAbub6IMBKG8EdIwZJ6foFWFNpZr4.jpg?auto=webp&amp;v=enabled&amp;s=2d05cf1f77c1ab48b175910d05cf18e34f2d70f6", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/uuEDnN6MuTZ98hsAbub6IMBKG8EdIwZJ6foFWFNpZr4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac263a64861a0f0d0770aa07379ccd824a27bf8b", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/uuEDnN6MuTZ98hsAbub6IMBKG8EdIwZJ6foFWFNpZr4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=530b0395e538a5b25103a7e93a7b5cac942009ab", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/uuEDnN6MuTZ98hsAbub6IMBKG8EdIwZJ6foFWFNpZr4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db96dfc1e20f3f895b6caa12fd4b9c4debb0c1d7", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/uuEDnN6MuTZ98hsAbub6IMBKG8EdIwZJ6foFWFNpZr4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1663804cacf85d4bf7e1768ab7e491e8eed2c44f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/uuEDnN6MuTZ98hsAbub6IMBKG8EdIwZJ6foFWFNpZr4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5986f6f7dd6adcfbf015ec40b1d8783f07a778af", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/uuEDnN6MuTZ98hsAbub6IMBKG8EdIwZJ6foFWFNpZr4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35819345822a28c9077066d605e32da792ac236b", "width": 1080, "height": 607}], "variants": {}, "id": "fZjMsp4g602eFrlNRwRND3O_r9aeS5AIqnfKK3OsiBA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "142fphr", "is_robot_indexable": true, "report_reasons": null, "author": "Agreeable_Bake_783", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142fphr/spark_connect_delta/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142fphr/spark_connect_delta/", "subreddit_subscribers": 109210, "created_utc": 1686057215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We\u2019re doing a migration of our on-prem (Virginia) data center to AWS (us-west). It\u2019s happening in stages with different servers being migrated at different times.\n\nAll of our databases are going in wave 1 but our ETL server (SAS it pains me to say) is remaining on-prem til a later wave. So ETLs will soon have data moving from the west coast to the east coast and back again. Performance of ETLs writing &gt; 1 million rows is abysmal in our testing so far.\n\nKeeping in mind this is a temporary situation so we don\u2019t want to refactor our whole code base, any thoughts come to mind? I\u2019ve tried increasing the row batch size (which helped significantly, but only up to a point) and am investigating parallelism to push more data simultaneously over the pipe.", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Latency ugh", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142f84c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686056184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We\u2019re doing a migration of our on-prem (Virginia) data center to AWS (us-west). It\u2019s happening in stages with different servers being migrated at different times.&lt;/p&gt;\n\n&lt;p&gt;All of our databases are going in wave 1 but our ETL server (SAS it pains me to say) is remaining on-prem til a later wave. So ETLs will soon have data moving from the west coast to the east coast and back again. Performance of ETLs writing &amp;gt; 1 million rows is abysmal in our testing so far.&lt;/p&gt;\n\n&lt;p&gt;Keeping in mind this is a temporary situation so we don\u2019t want to refactor our whole code base, any thoughts come to mind? I\u2019ve tried increasing the row batch size (which helped significantly, but only up to a point) and am investigating parallelism to push more data simultaneously over the pipe.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "142f84c", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142f84c/latency_ugh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142f84c/latency_ugh/", "subreddit_subscribers": 109210, "created_utc": 1686056184.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I posted the below help 5 days ago, posting again as I have some follow up doubts:\n\n  \nI am working on a project that requires filtering of huge volumes of data with extremely low latency.\n\nSome general context: My organisation's product is a web app. On this webapp, we have to build a feature where the frontend UI will have multiple filters that the user can select, these selected filters should be applied on a huge amount of data and after the filters are applied, some very basic summary metrics should be calculated and shown to the user in frontend.\n\nA bit more specific context(I am assuming the absolute worst case numbers here):\n\n1. The webapp is assumed to have a total of 100 concurrent users\n2. Huge amount of data = we're planning to store 1 month of data, and average is 1 million records per day with 50 columns. So lets say 30 million records **per user**. At the moment, nobody is using any of this data, and its just sitting in log files in S3. Considering 100 users the scale can go upto 3 billion records(30 million \\* 100 users)\n3. Basic summary metrics = count of email IDs, count of IP addresses, count of mobile numbers etc\n4. It's ok if the data is not absolute real-time. What I mean to say here is that if the max(date) in the dashboard is yesterday's, its fine\n5. Idea is simple groupby and aggregations on 1 single table, no joins\n\nI am a rookie DE working in a team of SWEs, and we can build something here from scratch(means that the data which is currently stored in S3, we can process that and store in a DW if required)\n\nCan you help me with a solution as to what should be my best data storage + data processing solution for the above use case, considering the below concerns:\n\n1. data volume\n2. latency(after all the filters are applied, a latency of max 10 seconds is affordable)\n3. cost  \n\n\nThe recommendations so far:\n\n* Postgres\n* Spark+Iceberg\n* Clickhouse\n* ElasticSearch\n\n&amp;#x200B;\n\nA few follow up doubts: \n\n* Will DuckDB be a good solution for this? Or will it be a bad idea for user facing analytics? DuckDB is a very tempting option considering how easy it is to setup and use, although I am not reading much about it being used in terms of user facing analytics\n* Is self hosting Clickhouse a good idea or can be too difficult?\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThank you", "author_fullname": "t2_cu6910tv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[REPOST] Low latency filtering and basic operations on huge volume of data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142cvhu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686050728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I posted the below help 5 days ago, posting again as I have some follow up doubts:&lt;/p&gt;\n\n&lt;p&gt;I am working on a project that requires filtering of huge volumes of data with extremely low latency.&lt;/p&gt;\n\n&lt;p&gt;Some general context: My organisation&amp;#39;s product is a web app. On this webapp, we have to build a feature where the frontend UI will have multiple filters that the user can select, these selected filters should be applied on a huge amount of data and after the filters are applied, some very basic summary metrics should be calculated and shown to the user in frontend.&lt;/p&gt;\n\n&lt;p&gt;A bit more specific context(I am assuming the absolute worst case numbers here):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The webapp is assumed to have a total of 100 concurrent users&lt;/li&gt;\n&lt;li&gt;Huge amount of data = we&amp;#39;re planning to store 1 month of data, and average is 1 million records per day with 50 columns. So lets say 30 million records &lt;strong&gt;per user&lt;/strong&gt;. At the moment, nobody is using any of this data, and its just sitting in log files in S3. Considering 100 users the scale can go upto 3 billion records(30 million * 100 users)&lt;/li&gt;\n&lt;li&gt;Basic summary metrics = count of email IDs, count of IP addresses, count of mobile numbers etc&lt;/li&gt;\n&lt;li&gt;It&amp;#39;s ok if the data is not absolute real-time. What I mean to say here is that if the max(date) in the dashboard is yesterday&amp;#39;s, its fine&lt;/li&gt;\n&lt;li&gt;Idea is simple groupby and aggregations on 1 single table, no joins&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I am a rookie DE working in a team of SWEs, and we can build something here from scratch(means that the data which is currently stored in S3, we can process that and store in a DW if required)&lt;/p&gt;\n\n&lt;p&gt;Can you help me with a solution as to what should be my best data storage + data processing solution for the above use case, considering the below concerns:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;data volume&lt;/li&gt;\n&lt;li&gt;latency(after all the filters are applied, a latency of max 10 seconds is affordable)&lt;/li&gt;\n&lt;li&gt;cost&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The recommendations so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Postgres&lt;/li&gt;\n&lt;li&gt;Spark+Iceberg&lt;/li&gt;\n&lt;li&gt;Clickhouse&lt;/li&gt;\n&lt;li&gt;ElasticSearch&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;A few follow up doubts: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Will DuckDB be a good solution for this? Or will it be a bad idea for user facing analytics? DuckDB is a very tempting option considering how easy it is to setup and use, although I am not reading much about it being used in terms of user facing analytics&lt;/li&gt;\n&lt;li&gt;Is self hosting Clickhouse a good idea or can be too difficult?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "142cvhu", "is_robot_indexable": true, "report_reasons": null, "author": "table_data", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/142cvhu/repost_low_latency_filtering_and_basic_operations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/142cvhu/repost_low_latency_filtering_and_basic_operations/", "subreddit_subscribers": 109210, "created_utc": 1686050728.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}