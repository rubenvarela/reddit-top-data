{"kind": "Listing", "data": {"after": "t3_141s2xr", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There are several things I would like to download from Reddit before they kill off API access: \n\n- Every single thread I have commented on, for the purpose of being able to train an LLM to write like me. Reddit is by far the largest collection of text I have written. I have already filed a new CCPA request to get all my comments, but IIRC last time I made a request I only got my comments by themselves, not what they were replying to, so I need a way to automatically download all the context. \n\n- Every single post I have upvoted or saved, if possible. \n\n- Specific subreddits, particularly /r/HFY. I would like to save all the Reddit serials that I enjoy reading on my phone before API access is cut off and I no longer have a comfortable way to read them anymore. \n\nWhat are the best tools to do this with, saving as much metadata as possible in a machine-readable format? \n\nAny other tools for downloading from Reddit, even if not important for my particular use case, are also welcome. I am posting this because at my current point in searching, I have not yet found any good compilation of all the tools available.", "author_fullname": "t2_qp8d2", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Best tools for downloading Reddit before API access is cut off?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1423a1q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 256, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 256, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {"gid_2": 1}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686024982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are several things I would like to download from Reddit before they kill off API access: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Every single thread I have commented on, for the purpose of being able to train an LLM to write like me. Reddit is by far the largest collection of text I have written. I have already filed a new CCPA request to get all my comments, but IIRC last time I made a request I only got my comments by themselves, not what they were replying to, so I need a way to automatically download all the context. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Every single post I have upvoted or saved, if possible. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Specific subreddits, particularly &lt;a href=\"/r/HFY\"&gt;/r/HFY&lt;/a&gt;. I would like to save all the Reddit serials that I enjoy reading on my phone before API access is cut off and I no longer have a comfortable way to read them anymore. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What are the best tools to do this with, saving as much metadata as possible in a machine-readable format? &lt;/p&gt;\n\n&lt;p&gt;Any other tools for downloading from Reddit, even if not important for my particular use case, are also welcome. I am posting this because at my current point in searching, I have not yet found any good compilation of all the tools available.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "11TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1423a1q", "is_robot_indexable": true, "report_reasons": null, "author": "happysmash27", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1423a1q/best_tools_for_downloading_reddit_before_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1423a1q/best_tools_for_downloading_reddit_before_api/", "subreddit_subscribers": 686277, "created_utc": 1686024982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Datahoarders! We were thinking that it's time to run another giveaway thread. This time, we are giving away an **IronWolf 125 1TB SSD** to one lucky winner in this thread.\n\nThe prize is: one **IronWolf 125 1TB SSD**\n\n*How to enter:*\nJust reply to this post once with a comment about what you are thankful for. We ask entrants to please include the terms **RunWithIronWolf** and **Seagate** in your comment to be considered for the prize drawing.\n\nFeel free to let us know what project(s) this would help with!\n\nSelection process/rules\n\nOne entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. Entries are open until June 23, 2023 at 23:59 UTC. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, and not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.\n\nGeographic restrictions:\n\nOur policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions (e.g. the obvious shipping restrictions to Russia and Belarus currently)\n\nUS\n\nCanada (will require a basic skills-based question if winner is chosen by law)\n\nBrazil\n\nSouth America\n\nUnited Kingdom\n\nGermany\n\nFrance\n\nIberia\n\nAustralia\n\nNew Zealand\n\nKorea\n\nIndia\n\nMalaysia\n\nSingapore\n\nChina", "author_fullname": "t2_16nn7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Official Giveaway: June 2023 Seagate IronWolf Giveaway!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141lmhg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": "", "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "OFFICIAL", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685985755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Datahoarders! We were thinking that it&amp;#39;s time to run another giveaway thread. This time, we are giving away an &lt;strong&gt;IronWolf 125 1TB SSD&lt;/strong&gt; to one lucky winner in this thread.&lt;/p&gt;\n\n&lt;p&gt;The prize is: one &lt;strong&gt;IronWolf 125 1TB SSD&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;How to enter:&lt;/em&gt;\nJust reply to this post once with a comment about what you are thankful for. We ask entrants to please include the terms &lt;strong&gt;RunWithIronWolf&lt;/strong&gt; and &lt;strong&gt;Seagate&lt;/strong&gt; in your comment to be considered for the prize drawing.&lt;/p&gt;\n\n&lt;p&gt;Feel free to let us know what project(s) this would help with!&lt;/p&gt;\n\n&lt;p&gt;Selection process/rules&lt;/p&gt;\n\n&lt;p&gt;One entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. Entries are open until June 23, 2023 at 23:59 UTC. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, and not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.&lt;/p&gt;\n\n&lt;p&gt;Geographic restrictions:&lt;/p&gt;\n\n&lt;p&gt;Our policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions (e.g. the obvious shipping restrictions to Russia and Belarus currently)&lt;/p&gt;\n\n&lt;p&gt;US&lt;/p&gt;\n\n&lt;p&gt;Canada (will require a basic skills-based question if winner is chosen by law)&lt;/p&gt;\n\n&lt;p&gt;Brazil&lt;/p&gt;\n\n&lt;p&gt;South America&lt;/p&gt;\n\n&lt;p&gt;United Kingdom&lt;/p&gt;\n\n&lt;p&gt;Germany&lt;/p&gt;\n\n&lt;p&gt;France&lt;/p&gt;\n\n&lt;p&gt;Iberia&lt;/p&gt;\n\n&lt;p&gt;Australia&lt;/p&gt;\n\n&lt;p&gt;New Zealand&lt;/p&gt;\n\n&lt;p&gt;Korea&lt;/p&gt;\n\n&lt;p&gt;India&lt;/p&gt;\n\n&lt;p&gt;Malaysia&lt;/p&gt;\n\n&lt;p&gt;Singapore&lt;/p&gt;\n\n&lt;p&gt;China&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "81f0a58e-b3f5-11ea-95d7-0e4db8ecc231", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "OFFICIAL SEAGATE", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "141lmhg", "is_robot_indexable": true, "report_reasons": null, "author": "Seagate_Surfer", "discussion_type": null, "num_comments": 241, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/141lmhg/official_giveaway_june_2023_seagate_ironwolf/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/141lmhg/official_giveaway_june_2023_seagate_ironwolf/", "subreddit_subscribers": 686277, "created_utc": 1685985755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nWith the recent news about RARBG going down, and us being saved by some archivist's scraped DB dumps. I wanted to discuss mirroring more sites.\nParticularly I am interested in making scraped databases of RuTracker and Nyaa.si. Both of these have large amounts of highly popular content.\nI wanted to discuss techniques for mirroring these.\nNyaa.si has a public API where you can query search results as RSS. However, they limit the number of pages. And to mirror, don't we need to be able to arbitrarily go down page after page? How do we make queries to be able to mirror most of Nyaa?\nI was throwing out this question to see if anyone has an idea of how this is typically circumvented, in order to get lots of results to mirror.", "author_fullname": "t2_ctq800ysv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirroring torrent sites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1424g6h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686028010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the recent news about RARBG going down, and us being saved by some archivist&amp;#39;s scraped DB dumps. I wanted to discuss mirroring more sites.\nParticularly I am interested in making scraped databases of RuTracker and Nyaa.si. Both of these have large amounts of highly popular content.\nI wanted to discuss techniques for mirroring these.\nNyaa.si has a public API where you can query search results as RSS. However, they limit the number of pages. And to mirror, don&amp;#39;t we need to be able to arbitrarily go down page after page? How do we make queries to be able to mirror most of Nyaa?\nI was throwing out this question to see if anyone has an idea of how this is typically circumvented, in order to get lots of results to mirror.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1424g6h", "is_robot_indexable": true, "report_reasons": null, "author": "Busy-Paramedic-4994", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1424g6h/mirroring_torrent_sites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1424g6h/mirroring_torrent_sites/", "subreddit_subscribers": 686277, "created_utc": 1686028010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Countries that Blocked Wikipedia in the Past", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 97, "top_awarded_type": null, "hide_score": false, "name": "t3_142i24d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_d2xe0c33", "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yPq6Nps7r4-BhVyLakclucA9du1ePiYFZCO9uH4osRA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "vpnnetwork", "selftext": "", "author_fullname": "t2_d2xe0c33", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Countries that Blocked Wikipedia in the Past", "link_flair_richtext": [], "subreddit_name_prefixed": "r/vpnnetwork", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 97, "top_awarded_type": null, "hide_score": false, "name": "t3_142i0q6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yPq6Nps7r4-BhVyLakclucA9du1ePiYFZCO9uH4osRA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686062072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ffankjn2se4b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ffankjn2se4b1.png?auto=webp&amp;v=enabled&amp;s=e91c991e6bb342296223091b140056ec239737c2", "width": 960, "height": 667}, "resolutions": [{"url": "https://preview.redd.it/ffankjn2se4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0725ca1928f7b29edc93cb5d646422804bde4a6a", "width": 108, "height": 75}, {"url": "https://preview.redd.it/ffankjn2se4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d13703ab582bddef10545078d8fed011c4ac837b", "width": 216, "height": 150}, {"url": "https://preview.redd.it/ffankjn2se4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b12f0aa4a55da52ef826b73d11b818dd9d21ee84", "width": 320, "height": 222}, {"url": "https://preview.redd.it/ffankjn2se4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e5cef9a373a5282bd446ff385cac583b55a1b3d", "width": 640, "height": 444}, {"url": "https://preview.redd.it/ffankjn2se4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=331fccc98377cc8251f72d6ab2fdfaff695df6d7", "width": 960, "height": 667}], "variants": {}, "id": "E0sApFH_-2BuRQIXCvBhZ7kLmWbW26BxUOT9UwMOG5U"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ead29b9a-58f1-11ed-9b9a-86d24560bd17", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_740hil", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "142i0q6", "is_robot_indexable": true, "report_reasons": null, "author": "More_Breakfast2084", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/vpnnetwork/comments/142i0q6/countries_that_blocked_wikipedia_in_the_past/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ffankjn2se4b1.png", "subreddit_subscribers": 14869, "created_utc": 1686062072.0, "num_crossposts": 4, "media": null, "is_video": false}], "created": 1686062152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ffankjn2se4b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ffankjn2se4b1.png?auto=webp&amp;v=enabled&amp;s=e91c991e6bb342296223091b140056ec239737c2", "width": 960, "height": 667}, "resolutions": [{"url": "https://preview.redd.it/ffankjn2se4b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0725ca1928f7b29edc93cb5d646422804bde4a6a", "width": 108, "height": 75}, {"url": "https://preview.redd.it/ffankjn2se4b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d13703ab582bddef10545078d8fed011c4ac837b", "width": 216, "height": 150}, {"url": "https://preview.redd.it/ffankjn2se4b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b12f0aa4a55da52ef826b73d11b818dd9d21ee84", "width": 320, "height": 222}, {"url": "https://preview.redd.it/ffankjn2se4b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e5cef9a373a5282bd446ff385cac583b55a1b3d", "width": 640, "height": 444}, {"url": "https://preview.redd.it/ffankjn2se4b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=331fccc98377cc8251f72d6ab2fdfaff695df6d7", "width": 960, "height": 667}], "variants": {}, "id": "E0sApFH_-2BuRQIXCvBhZ7kLmWbW26BxUOT9UwMOG5U"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "142i24d", "is_robot_indexable": true, "report_reasons": null, "author": "More_Breakfast2084", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_142i0q6", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142i24d/countries_that_blocked_wikipedia_in_the_past/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ffankjn2se4b1.png", "subreddit_subscribers": 686277, "created_utc": 1686062152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey so big question:\n\nLet's say I had all of my Twitter data, Reddit data and Discord data, is there any possible way I can display it all in some sort of chart or search engine where I can search through it? Like some sort of third party GUI or application.", "author_fullname": "t2_2alh77oh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Presentation of Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142cub8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686050646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey so big question:&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I had all of my Twitter data, Reddit data and Discord data, is there any possible way I can display it all in some sort of chart or search engine where I can search through it? Like some sort of third party GUI or application.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142cub8", "is_robot_indexable": true, "report_reasons": null, "author": "Zaxoosh", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142cub8/presentation_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142cub8/presentation_of_data/", "subreddit_subscribers": 686277, "created_utc": 1686050646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello Datahorder community!\n\nI just have a question referance backups of all your precious and (in my case sometimes) usless data! I have for a long time admired the setups people have on here and I have for a very long time wanted a data hording setup of my own (since I was about 15 as im a little bit sad!). I currently have 1TB on Google Drive and I am currently looking at purchasing a Synology DS923+ with 8TB of storage (which I will keep in RAID5 giving me 6TB usable). \n\n&amp;#x200B;\n\nI was just wanting to use you experts as a sounding bored, If i was to get say 3 6TB external HDD's and keep 1 in my bedroom (which will be backed up to daily, or everyother day), 1 in my office (weekly backed up) and 1 in my parents house (monthly backed up). \n\nWould this be sufficent? I feel like paying for a backup to google drive etc defeats purchasing the NAS. I am trying to become self hosted in as many ways as possible.\n\n&amp;#x200B;\n\nThank you for any advice\n\n&amp;#x200B;\n\nAdded:\n\nOn this NAS will be a few docker containers for testing some stuff and just general files and photos\n\nI am also looking at purchasing a older server at some point and that will run my testing and dockers and will backup to the NAS.", "author_fullname": "t2_1wgqbq6b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup suggestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141shz5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686000533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Datahorder community!&lt;/p&gt;\n\n&lt;p&gt;I just have a question referance backups of all your precious and (in my case sometimes) usless data! I have for a long time admired the setups people have on here and I have for a very long time wanted a data hording setup of my own (since I was about 15 as im a little bit sad!). I currently have 1TB on Google Drive and I am currently looking at purchasing a Synology DS923+ with 8TB of storage (which I will keep in RAID5 giving me 6TB usable). &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was just wanting to use you experts as a sounding bored, If i was to get say 3 6TB external HDD&amp;#39;s and keep 1 in my bedroom (which will be backed up to daily, or everyother day), 1 in my office (weekly backed up) and 1 in my parents house (monthly backed up). &lt;/p&gt;\n\n&lt;p&gt;Would this be sufficent? I feel like paying for a backup to google drive etc defeats purchasing the NAS. I am trying to become self hosted in as many ways as possible.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for any advice&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Added:&lt;/p&gt;\n\n&lt;p&gt;On this NAS will be a few docker containers for testing some stuff and just general files and photos&lt;/p&gt;\n\n&lt;p&gt;I am also looking at purchasing a older server at some point and that will run my testing and dockers and will backup to the NAS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141shz5", "is_robot_indexable": true, "report_reasons": null, "author": "Zetta666", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141shz5/backup_suggestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141shz5/backup_suggestion/", "subreddit_subscribers": 686277, "created_utc": 1686000533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "how should i visually format an archive of someone's tweets/twitter account? im just focusing on the tweets (both with and without media)\n\nmost of the programs/apps i find for this only pull the media &amp; data separately (and a lot don't go past the 3k limit.. this account has 12k..), i want it similar to how the tweets look with the text &amp; media and date in the same box, that you can scroll chronologically. like a pdf, but im unsure if it's possible to put thousands of tweets into a pdf... has anyone else done this before? what did you do?\n\ni did actually manage to get *something* with Twitter Media Downloader (which even grabbed more than 3k tweets) so if there's nothing i'll just use that. but yeah just wondering if it's possible to maybe display the csv file (from twitter media downloader) visually..? or is it even possible to run a script or program that's capable of taking tweets from a specific account into a visual format? does such a thing exist with twitters current API situation?\n\ni've been trying to figure this out for a few days now, any help is appreciated", "author_fullname": "t2_tkcopl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to display an archive of someone's twitter account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1428b1l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686038535.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;how should i visually format an archive of someone&amp;#39;s tweets/twitter account? im just focusing on the tweets (both with and without media)&lt;/p&gt;\n\n&lt;p&gt;most of the programs/apps i find for this only pull the media &amp;amp; data separately (and a lot don&amp;#39;t go past the 3k limit.. this account has 12k..), i want it similar to how the tweets look with the text &amp;amp; media and date in the same box, that you can scroll chronologically. like a pdf, but im unsure if it&amp;#39;s possible to put thousands of tweets into a pdf... has anyone else done this before? what did you do?&lt;/p&gt;\n\n&lt;p&gt;i did actually manage to get &lt;em&gt;something&lt;/em&gt; with Twitter Media Downloader (which even grabbed more than 3k tweets) so if there&amp;#39;s nothing i&amp;#39;ll just use that. but yeah just wondering if it&amp;#39;s possible to maybe display the csv file (from twitter media downloader) visually..? or is it even possible to run a script or program that&amp;#39;s capable of taking tweets from a specific account into a visual format? does such a thing exist with twitters current API situation?&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;ve been trying to figure this out for a few days now, any help is appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1428b1l", "is_robot_indexable": true, "report_reasons": null, "author": "Hootie_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1428b1l/how_to_display_an_archive_of_someones_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1428b1l/how_to_display_an_archive_of_someones_twitter/", "subreddit_subscribers": 686277, "created_utc": 1686038535.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey there!\n\nI have downloaded alot of data from a site called justcritics or smth along the lines.... \n\nAnyway, i've got millions of files (videos and images) which i now need to filter out and categorize.\n\nNow i notice that ntfs/windows isnt well suited for that job, the explorer is constantly crashing and filling up the cache when having a folder with more than 100 images/videos.\n\nCan anyone recommend a good solution for that?", "author_fullname": "t2_onogq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing Content", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141jcc6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685981219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there!&lt;/p&gt;\n\n&lt;p&gt;I have downloaded alot of data from a site called justcritics or smth along the lines.... &lt;/p&gt;\n\n&lt;p&gt;Anyway, i&amp;#39;ve got millions of files (videos and images) which i now need to filter out and categorize.&lt;/p&gt;\n\n&lt;p&gt;Now i notice that ntfs/windows isnt well suited for that job, the explorer is constantly crashing and filling up the cache when having a folder with more than 100 images/videos.&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend a good solution for that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141jcc6", "is_robot_indexable": true, "report_reasons": null, "author": "AllCowsAreBurgers", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141jcc6/managing_content/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141jcc6/managing_content/", "subreddit_subscribers": 686277, "created_utc": 1685981219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a server with about 200TB that uses FTP and Rsync which I want to backup. I have a ton of 4TB drives laying around so I thought I might as well use them for offline backup. I intend to use a temporary 24slots/4U server with UnRAID to do the backup so I get individual drives that can be stored offline later.  \n\n\nMy question is.. does there exist a tool (or can rsync do this?) that can allow me to transfer the data using one of those two protocols and keep track of the status using a local log file? My idea is that the logfile could be used to keep track of what has been downloaded without having to access the data on the drives (which may already be stored offline) and let the client only download new/updated files. Preferably with some way of pausing it after n TB so I can swap the 24 TB drives with empty once when full before continuing where it left off.", "author_fullname": "t2_lh08w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FTP or Rsync to offline backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142c4um", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686048908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a server with about 200TB that uses FTP and Rsync which I want to backup. I have a ton of 4TB drives laying around so I thought I might as well use them for offline backup. I intend to use a temporary 24slots/4U server with UnRAID to do the backup so I get individual drives that can be stored offline later.  &lt;/p&gt;\n\n&lt;p&gt;My question is.. does there exist a tool (or can rsync do this?) that can allow me to transfer the data using one of those two protocols and keep track of the status using a local log file? My idea is that the logfile could be used to keep track of what has been downloaded without having to access the data on the drives (which may already be stored offline) and let the client only download new/updated files. Preferably with some way of pausing it after n TB so I can swap the 24 TB drives with empty once when full before continuing where it left off.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142c4um", "is_robot_indexable": true, "report_reasons": null, "author": "NeoID", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142c4um/ftp_or_rsync_to_offline_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142c4um/ftp_or_rsync_to_offline_backup/", "subreddit_subscribers": 686277, "created_utc": 1686048908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there any cases out there that can hold the same amount of drives as the 7 XL (18 I believe) or more, for a regular nas build? I\u2019ve seen people say the Lian Li D8000 but it\u2019s discontinued.", "author_fullname": "t2_ba3y5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rival to Fractal Define 7 XL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141o74a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685991024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any cases out there that can hold the same amount of drives as the 7 XL (18 I believe) or more, for a regular nas build? I\u2019ve seen people say the Lian Li D8000 but it\u2019s discontinued.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141o74a", "is_robot_indexable": true, "report_reasons": null, "author": "Conorsavage", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141o74a/rival_to_fractal_define_7_xl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141o74a/rival_to_fractal_define_7_xl/", "subreddit_subscribers": 686277, "created_utc": 1685991024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve heard that Reddit users will get free access to the API, so shouldn\u2019t tools like Reddit Media Downloader still work after the changes are implemented? Or will it stop working?\n\nThanks", "author_fullname": "t2_gvdnvhdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit media downloader and Reddit API changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141m2m3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685986652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve heard that Reddit users will get free access to the API, so shouldn\u2019t tools like Reddit Media Downloader still work after the changes are implemented? Or will it stop working?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141m2m3", "is_robot_indexable": true, "report_reasons": null, "author": "Fresh_Air13", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141m2m3/reddit_media_downloader_and_reddit_api_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141m2m3/reddit_media_downloader_and_reddit_api_changes/", "subreddit_subscribers": 686277, "created_utc": 1685986652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I have tons of iPhone text/iMessage chats with at least 30gb of attachments across them.\nI know how to find and extract the attachments from the iTunes backup files.\n\nThe issue I am having is that the iTunes backup doesn\u2019t have all the attachments (photos, videos, PDF, vCard) since I have iCloud messages turned on, so the attachments in the cloud won\u2019t appear in the local iTunes backup.\n\nHowever, when I turn off messages in iCloud (and press the option to download and delete messages from the cloud to my phone) many attachments don\u2019t download, they just have a \u201c?\u201d icon. I can only access those attachments when I turn iCloud messages back on.\n\nHow can get get all message attachments from the cloud onto my phone and into a local iTunes backup of the \u201cdisable and delete\u201d from iCloud doesn\u2019t wort properly?", "author_fullname": "t2_e6drpuch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting photos from iPhone backup - cloud issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_142iue7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686063763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have tons of iPhone text/iMessage chats with at least 30gb of attachments across them.\nI know how to find and extract the attachments from the iTunes backup files.&lt;/p&gt;\n\n&lt;p&gt;The issue I am having is that the iTunes backup doesn\u2019t have all the attachments (photos, videos, PDF, vCard) since I have iCloud messages turned on, so the attachments in the cloud won\u2019t appear in the local iTunes backup.&lt;/p&gt;\n\n&lt;p&gt;However, when I turn off messages in iCloud (and press the option to download and delete messages from the cloud to my phone) many attachments don\u2019t download, they just have a \u201c?\u201d icon. I can only access those attachments when I turn iCloud messages back on.&lt;/p&gt;\n\n&lt;p&gt;How can get get all message attachments from the cloud onto my phone and into a local iTunes backup of the \u201cdisable and delete\u201d from iCloud doesn\u2019t wort properly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142iue7", "is_robot_indexable": true, "report_reasons": null, "author": "aviator_L1011", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142iue7/extracting_photos_from_iphone_backup_cloud_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142iue7/extracting_photos_from_iphone_backup_cloud_issues/", "subreddit_subscribers": 686277, "created_utc": 1686063763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm stumped, everyway I look at this it's telling me that there is 12tb of free space. But the storage space says it's full? Is there some parity that is causing this? When I try to copy files it says it doesn't have any space?", "author_fullname": "t2_14mc49ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Storage doesn't add up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "media_metadata": {"47ng4qtaie4b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 59, "x": 108, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f28f178d7a57e520d8eb5bc17768da695c7a5f7"}, {"y": 119, "x": 216, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba3819f9cea7a682cb2b9c78c706be9f2be90bef"}, {"y": 176, "x": 320, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=645a1825317bdb51375012e1cb5e7c2de6e46216"}, {"y": 353, "x": 640, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8f1bce6bc6108cc35035455873af1ae702cec43"}, {"y": 529, "x": 960, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=892c654c9b7ca22542d053b21ce52ea8a11c7d22"}, {"y": 596, "x": 1080, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=855a319f0069dc89a10d7b45fe69d6213783d5b3"}], "s": {"y": 787, "x": 1426, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=1426&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=af9177f96c3c757bf55bdf20e61f9f832ab4ee51"}, "id": "47ng4qtaie4b1"}, "dy4voyqaie4b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e799259a1adbdddf21d5ca6d0d3f0fdb3d277231"}, {"y": 110, "x": 216, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d320b39d69a2937c35e34c8353b2a0244c57d6ec"}, {"y": 164, "x": 320, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7748d91f793360bdb3095a504b56b9c8eb65987"}, {"y": 328, "x": 640, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d4743621adcc21877c690ec1f3036193b8c20ae"}, {"y": 493, "x": 960, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce917567b54bbb46d683bd3c58036a8e7a508c2c"}, {"y": 554, "x": 1080, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ca63d7a43d6ebf6ce752c5348d4fe90450c7c7d"}], "s": {"y": 690, "x": 1343, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=1343&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=9f7a41c0721a5caf8e2c89a46e8c3c4aa01aebd3"}, "id": "dy4voyqaie4b1"}}, "name": "t3_142gfsf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "dy4voyqaie4b1", "id": 284483225}, {"media_id": "47ng4qtaie4b1", "id": 284483226}]}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6Rq8GMRqYT6bqthCL3YezWbwO08iXHo6hmp4vzMemzg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686058744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m stumped, everyway I look at this it&amp;#39;s telling me that there is 12tb of free space. But the storage space says it&amp;#39;s full? Is there some parity that is causing this? When I try to copy files it says it doesn&amp;#39;t have any space?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/142gfsf", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142gfsf", "is_robot_indexable": true, "report_reasons": null, "author": "Cor_Brain", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142gfsf/storage_doesnt_add_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/142gfsf", "subreddit_subscribers": 686277, "created_utc": 1686058744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Community, \n\nI wanted to get your views on the different ways of working with storage.   \nFor example, would it be better to work against a NAS in the network, or against local storage which usually is faster and then backup to the NAS?  \nMaybe a mix model?\n\nI wanted to see how you guys are usually working and what do you feel comfortable / already tested.  \nThank you for sharing your thoughts \ud83d\ude09", "author_fullname": "t2_meba9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Workflows and different ways of working with storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1428wiz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686040255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Community, &lt;/p&gt;\n\n&lt;p&gt;I wanted to get your views on the different ways of working with storage.&lt;br/&gt;\nFor example, would it be better to work against a NAS in the network, or against local storage which usually is faster and then backup to the NAS?&lt;br/&gt;\nMaybe a mix model?&lt;/p&gt;\n\n&lt;p&gt;I wanted to see how you guys are usually working and what do you feel comfortable / already tested.&lt;br/&gt;\nThank you for sharing your thoughts \ud83d\ude09&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1428wiz", "is_robot_indexable": true, "report_reasons": null, "author": "fscheps", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1428wiz/workflows_and_different_ways_of_working_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1428wiz/workflows_and_different_ways_of_working_with/", "subreddit_subscribers": 686277, "created_utc": 1686040255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It will mostly be a media/jellyfin server, but will do some general use as well. A VM and/or some containers. \n\nFor storage I am looking at either 4x16TB or 6x16TB for storage. Which would be best, and what RAID/UNRAID config should I run for 4 and 6 drives? Should the drives be the same or should I split between 2 or 3 brands? There will be some data that would suck to lose. Most of it is media though. Not critical, but a big pain to find again and would preferably want to avoid doing so. I am willing to pay more for an extra drive or two if necessary.\n\nAs for other hardware. How powerful cpu should I get for running maybe a VM, a container and a jellyfin server? And what about memory?\n\nBig thanks in advance for any input!", "author_fullname": "t2_y5g3q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to build a home server. 4 or 6 drives? How powerful hardware", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1420occ", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686018563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It will mostly be a media/jellyfin server, but will do some general use as well. A VM and/or some containers. &lt;/p&gt;\n\n&lt;p&gt;For storage I am looking at either 4x16TB or 6x16TB for storage. Which would be best, and what RAID/UNRAID config should I run for 4 and 6 drives? Should the drives be the same or should I split between 2 or 3 brands? There will be some data that would suck to lose. Most of it is media though. Not critical, but a big pain to find again and would preferably want to avoid doing so. I am willing to pay more for an extra drive or two if necessary.&lt;/p&gt;\n\n&lt;p&gt;As for other hardware. How powerful cpu should I get for running maybe a VM, a container and a jellyfin server? And what about memory?&lt;/p&gt;\n\n&lt;p&gt;Big thanks in advance for any input!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1420occ", "is_robot_indexable": true, "report_reasons": null, "author": "TheOptiGamer", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1420occ/looking_to_build_a_home_server_4_or_6_drives_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1420occ/looking_to_build_a_home_server_4_or_6_drives_how/", "subreddit_subscribers": 686277, "created_utc": 1686018563.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone!\nThanks for reading!\n\nPlease excuse me if my noob question makes you cringe.\n\nI have been looking for a solution to set all my hdds in one single enclosure so I can access to all of them. Lurking here I read about the Terramaster d5-330 and I got one.\n\nHere is where my problems started. I followed the official guide, watching the videos and spent the last 24hrs reading all I can find on how to set it up. I failed. \n\nI want to use it in SINGLE mode that for what I understand should be pretty much plug and play, but the thunderbolt ports on my ROG ZEPHIRUS g15 only detect it as PD (power device) and I don\u2019t get the prompt to install the drivers nor the raid shows up in my device manager window.\n\nI tried force the update on the thuderbolt to install the official drivers but it got me nowhere. I installed the rain manager and the raid pro manager software. Obviously the drive doesn\u2019t show up so also that is a nonstarter. \n\nI tried several recommended hdd in different all the bays of the D5-330 with always the same results. Btw the light of the drives stay off all the time. I tested the drives before and after, and it works no problem. \n\nIs my first time fiddling with a das so I am sure I can mess up something but I can\u2019t see what \u2026 I am stupid or is this unit defective?\n\nThanks!", "author_fullname": "t2_3cl0d5g5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help/troubleshooting: Terramaster D5-330 failing to be recognized, is it a dudd?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1420fb0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686017968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!\nThanks for reading!&lt;/p&gt;\n\n&lt;p&gt;Please excuse me if my noob question makes you cringe.&lt;/p&gt;\n\n&lt;p&gt;I have been looking for a solution to set all my hdds in one single enclosure so I can access to all of them. Lurking here I read about the Terramaster d5-330 and I got one.&lt;/p&gt;\n\n&lt;p&gt;Here is where my problems started. I followed the official guide, watching the videos and spent the last 24hrs reading all I can find on how to set it up. I failed. &lt;/p&gt;\n\n&lt;p&gt;I want to use it in SINGLE mode that for what I understand should be pretty much plug and play, but the thunderbolt ports on my ROG ZEPHIRUS g15 only detect it as PD (power device) and I don\u2019t get the prompt to install the drivers nor the raid shows up in my device manager window.&lt;/p&gt;\n\n&lt;p&gt;I tried force the update on the thuderbolt to install the official drivers but it got me nowhere. I installed the rain manager and the raid pro manager software. Obviously the drive doesn\u2019t show up so also that is a nonstarter. &lt;/p&gt;\n\n&lt;p&gt;I tried several recommended hdd in different all the bays of the D5-330 with always the same results. Btw the light of the drives stay off all the time. I tested the drives before and after, and it works no problem. &lt;/p&gt;\n\n&lt;p&gt;Is my first time fiddling with a das so I am sure I can mess up something but I can\u2019t see what \u2026 I am stupid or is this unit defective?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1420fb0", "is_robot_indexable": true, "report_reasons": null, "author": "FresconeFrizzantino", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1420fb0/helptroubleshooting_terramaster_d5330_failing_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1420fb0/helptroubleshooting_terramaster_d5330_failing_to/", "subreddit_subscribers": 686277, "created_utc": 1686017968.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "want to buy a lot of drives and I have found no posts talking about their support once something goes wrong", "author_fullname": "t2_gw5a2p3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have RMA experience with Solidigm?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141zgm0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686015673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;want to buy a lot of drives and I have found no posts talking about their support once something goes wrong&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141zgm0", "is_robot_indexable": true, "report_reasons": null, "author": "masturbaiter696969", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141zgm0/anyone_have_rma_experience_with_solidigm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141zgm0/anyone_have_rma_experience_with_solidigm/", "subreddit_subscribers": 686277, "created_utc": 1686015673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title, but I'll elaborate just in case. I did some research on the topic and found threads from 5+ years ago about Macbooks so I think it's ok to ask -\n\nI am using this storage space to hold video files and misc. assets, and have always been told to place an abundance of caution on how the hard drive is removed from the computer. Thoughtless unplugging of a hard drive could cause hours upon hours of lost work and I would like to avoid that. \n\nI've placed this external drive underneath my desk along with my docking station to save space &amp; keep them out of the way. A perfect scenario would be that I can plug my laptop in and get straight to work, and then unplug safely when I'm done, keeping the drive where it is without touching it. \n\nIs this possible, should I just eject before I unplug the USB connection to my laptop? Thank you!", "author_fullname": "t2_6md0o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2 TB external drive connected to USB docking station - do I need to worry about ejecting the drive after using the dock?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141l4zw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685984789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title, but I&amp;#39;ll elaborate just in case. I did some research on the topic and found threads from 5+ years ago about Macbooks so I think it&amp;#39;s ok to ask -&lt;/p&gt;\n\n&lt;p&gt;I am using this storage space to hold video files and misc. assets, and have always been told to place an abundance of caution on how the hard drive is removed from the computer. Thoughtless unplugging of a hard drive could cause hours upon hours of lost work and I would like to avoid that. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve placed this external drive underneath my desk along with my docking station to save space &amp;amp; keep them out of the way. A perfect scenario would be that I can plug my laptop in and get straight to work, and then unplug safely when I&amp;#39;m done, keeping the drive where it is without touching it. &lt;/p&gt;\n\n&lt;p&gt;Is this possible, should I just eject before I unplug the USB connection to my laptop? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141l4zw", "is_robot_indexable": true, "report_reasons": null, "author": "delmarman", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141l4zw/2_tb_external_drive_connected_to_usb_docking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141l4zw/2_tb_external_drive_connected_to_usb_docking/", "subreddit_subscribers": 686277, "created_utc": 1685984789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone been able to download youtube videos on the ```http_dash_segments``` protocol at speed &gt; 500KB/sec ? \n\nWhen I download a video using the ```https``` protocol, I can get speed like 10MB/sec, but other formats of the same video that are listed on the ```http_dash_segments``` protocol, speed is always capped, that is with ```Youtube Download [yt-dlp]``` or ```jDownloader```. \n\nThanks", "author_fullname": "t2_ch4bnghs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "youtube ideos: http_dash_segments protocol speed capped?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14234ic", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686024601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone been able to download youtube videos on the &lt;code&gt;http_dash_segments&lt;/code&gt; protocol at speed &amp;gt; 500KB/sec ? &lt;/p&gt;\n\n&lt;p&gt;When I download a video using the &lt;code&gt;https&lt;/code&gt; protocol, I can get speed like 10MB/sec, but other formats of the same video that are listed on the &lt;code&gt;http_dash_segments&lt;/code&gt; protocol, speed is always capped, that is with &lt;code&gt;Youtube Download [yt-dlp]&lt;/code&gt; or &lt;code&gt;jDownloader&lt;/code&gt;. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14234ic", "is_robot_indexable": true, "report_reasons": null, "author": "cybercastor", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14234ic/youtube_ideos_http_dash_segments_protocol_speed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14234ic/youtube_ideos_http_dash_segments_protocol_speed/", "subreddit_subscribers": 686277, "created_utc": 1686024601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have all of my photos in Google photos, but am paying every month for overflow storage. On the plus side is quite easy to manage, but I am looking for alternative options. Is iCloud any better? Should I invest in an external hard drive? No professional work just personal memories, thanks!", "author_fullname": "t2_h6mc8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice on how to store years worth of digital photos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ga7f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686058414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have all of my photos in Google photos, but am paying every month for overflow storage. On the plus side is quite easy to manage, but I am looking for alternative options. Is iCloud any better? Should I invest in an external hard drive? No professional work just personal memories, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142ga7f", "is_robot_indexable": true, "report_reasons": null, "author": "onceuponanadventure", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142ga7f/looking_for_advice_on_how_to_store_years_worth_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142ga7f/looking_for_advice_on_how_to_store_years_worth_of/", "subreddit_subscribers": 686277, "created_utc": 1686058414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for the best option to to store all of my photos and videos that I am putting into digital form on a cloud service. I am using an external hard drive to store but that isn't 100% fool proof. Whats the best bang for my buck service that I won't have to worry about? May store some documents there as well but not many. Thanks\n\n(I have verizon cloud, but it's wonky to use, slow and seems to not work correctly for what I am trying to do with it)", "author_fullname": "t2_hfg20qfg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Storage (Personal Digitalized Photos/Videos)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142g0xk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686057890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for the best option to to store all of my photos and videos that I am putting into digital form on a cloud service. I am using an external hard drive to store but that isn&amp;#39;t 100% fool proof. Whats the best bang for my buck service that I won&amp;#39;t have to worry about? May store some documents there as well but not many. Thanks&lt;/p&gt;\n\n&lt;p&gt;(I have verizon cloud, but it&amp;#39;s wonky to use, slow and seems to not work correctly for what I am trying to do with it)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142g0xk", "is_robot_indexable": true, "report_reasons": null, "author": "Aries-Baby-11", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142g0xk/cloud_storage_personal_digitalized_photosvideos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142g0xk/cloud_storage_personal_digitalized_photosvideos/", "subreddit_subscribers": 686277, "created_utc": 1686057890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm curious if anyone did manage to archive Synology's old DSM and package versions?\n\n[web.archive.org](https://web.archive.org) has archived a lot of it, but it didn't archive the larger files. [https://web.archive.org/web/20210416132208/https://archive.synology.com/download](https://web.archive.org/web/20210416132208/https://archive.synology.com/download)\n\nThere were 2 threads about Synology deleting all it's old  DSM and package versions:\n\n1. [https://www.reddit.com/r/DataHoarder/comments/10da7a9/official\\_synology\\_download\\_site\\_closing\\_legacy/](https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/)\n2. [https://www.reddit.com/r/DataHoarder/comments/12wp46q/reminder\\_synology\\_legacy\\_dsm\\_firmware\\_package/](https://www.reddit.com/r/DataHoarder/comments/12wp46q/reminder_synology_legacy_dsm_firmware_package/)\n\nWhile there were people who started downloading all the old DSM and package versions it seemed like everyone gave up before they had downloaded everything (myself included).", "author_fullname": "t2_33srm1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did anyone manage to archive Synology's old DSM and package versions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1424mqq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686028490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious if anyone did manage to archive Synology&amp;#39;s old DSM and package versions?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://web.archive.org\"&gt;web.archive.org&lt;/a&gt; has archived a lot of it, but it didn&amp;#39;t archive the larger files. &lt;a href=\"https://web.archive.org/web/20210416132208/https://archive.synology.com/download\"&gt;https://web.archive.org/web/20210416132208/https://archive.synology.com/download&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There were 2 threads about Synology deleting all it&amp;#39;s old  DSM and package versions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/\"&gt;https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/12wp46q/reminder_synology_legacy_dsm_firmware_package/\"&gt;https://www.reddit.com/r/DataHoarder/comments/12wp46q/reminder_synology_legacy_dsm_firmware_package/&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;While there were people who started downloading all the old DSM and package versions it seemed like everyone gave up before they had downloaded everything (myself included).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "186TB local", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1424mqq", "is_robot_indexable": true, "report_reasons": null, "author": "DaveR007", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1424mqq/did_anyone_manage_to_archive_synologys_old_dsm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1424mqq/did_anyone_manage_to_archive_synologys_old_dsm/", "subreddit_subscribers": 686277, "created_utc": 1686028490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for something like [https://www.amazon.com/SanDisk-Cruzer-Flash-Drive-SDCZ33-032G-A11/dp/B008C7C5OC](https://www.amazon.com/SanDisk-Cruzer-Flash-Drive-SDCZ33-032G-A11/dp/B008C7C5OC) but with USB C.  \nSomething like [https://www.amazon.com/Yubico-YubiKey-Factor-Authentication-Security/dp/B07HBTBJ5S](https://www.amazon.com/Yubico-YubiKey-Factor-Authentication-Security/dp/B07HBTBJ5S) but a memory stick.  \n\n\nHas anyone came across anything similar? Im failing to find anything.  \nP.S. if there is more appropriate channel, let me know.", "author_fullname": "t2_173zc0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Physically smallest USB C memory stick", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141kvwp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685984292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for something like &lt;a href=\"https://www.amazon.com/SanDisk-Cruzer-Flash-Drive-SDCZ33-032G-A11/dp/B008C7C5OC\"&gt;https://www.amazon.com/SanDisk-Cruzer-Flash-Drive-SDCZ33-032G-A11/dp/B008C7C5OC&lt;/a&gt; but with USB C.&lt;br/&gt;\nSomething like &lt;a href=\"https://www.amazon.com/Yubico-YubiKey-Factor-Authentication-Security/dp/B07HBTBJ5S\"&gt;https://www.amazon.com/Yubico-YubiKey-Factor-Authentication-Security/dp/B07HBTBJ5S&lt;/a&gt; but a memory stick.  &lt;/p&gt;\n\n&lt;p&gt;Has anyone came across anything similar? Im failing to find anything.&lt;br/&gt;\nP.S. if there is more appropriate channel, let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141kvwp", "is_robot_indexable": true, "report_reasons": null, "author": "grannyno", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141kvwp/physically_smallest_usb_c_memory_stick/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141kvwp/physically_smallest_usb_c_memory_stick/", "subreddit_subscribers": 686277, "created_utc": 1685984292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone, I\u2019ve recently gotten into Data Hoarding as I\u2019d like to begin archiving 2 of my favorite YouTube channels. I have 2x 2TB External HDD, one as my main and the second as a backup, although I might get a 3rd one as another backup (I\u2019ve checked beforehand and the 2 channels together won\u2019t even add up to 3/4 of a TB, so I\u2019m okay with 2TB Drives *for now*). \n\nMoving on, I\u2019ve seen people on this subreddit talk about bit rot and using checksums as a way to detect it. Therefore, **what\u2019s the best way to have checksums while still updating my drives with new videos?** My initial thought was to just have 2 checksums, one for 1 channel and one for the other, however, I ended up finding 2 issues with this: (1) if I keep adding new videos to one channel\u2019s file, then the checksum will keep changing, right? So I\u2019ll have to keep updating the checksum every few days? (2) If bitrot does occur, and I see a different checksum for YT Channel #1\u2019s file on one HDD, I wouldn\u2019t know which specific file is corrupted, just that one (or perhaps a couple) file(s) out of 500 are corrupted. \n\nSo, my next thought was to make checksums for each video, that way if bitrot does occur, I can pinpoint which file is corrupted, however, this would mean having 500+ checksums to check each time I\u2019m looking over one of my HDD\u2019s, which sounds way too inefficient unless there\u2019s a program or application that can do it automatically. \n\nSo, once again, **what\u2019s the best way to have checksums while still updating my drives with new videos?** Would it be best if I sort the videos yearly and then make checksums for each year? Monthly? \n\nWould really appreciate any help I can get!", "author_fullname": "t2_lgtie35c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to Data Hoarding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141v5ga", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686005937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I\u2019ve recently gotten into Data Hoarding as I\u2019d like to begin archiving 2 of my favorite YouTube channels. I have 2x 2TB External HDD, one as my main and the second as a backup, although I might get a 3rd one as another backup (I\u2019ve checked beforehand and the 2 channels together won\u2019t even add up to 3/4 of a TB, so I\u2019m okay with 2TB Drives &lt;em&gt;for now&lt;/em&gt;). &lt;/p&gt;\n\n&lt;p&gt;Moving on, I\u2019ve seen people on this subreddit talk about bit rot and using checksums as a way to detect it. Therefore, &lt;strong&gt;what\u2019s the best way to have checksums while still updating my drives with new videos?&lt;/strong&gt; My initial thought was to just have 2 checksums, one for 1 channel and one for the other, however, I ended up finding 2 issues with this: (1) if I keep adding new videos to one channel\u2019s file, then the checksum will keep changing, right? So I\u2019ll have to keep updating the checksum every few days? (2) If bitrot does occur, and I see a different checksum for YT Channel #1\u2019s file on one HDD, I wouldn\u2019t know which specific file is corrupted, just that one (or perhaps a couple) file(s) out of 500 are corrupted. &lt;/p&gt;\n\n&lt;p&gt;So, my next thought was to make checksums for each video, that way if bitrot does occur, I can pinpoint which file is corrupted, however, this would mean having 500+ checksums to check each time I\u2019m looking over one of my HDD\u2019s, which sounds way too inefficient unless there\u2019s a program or application that can do it automatically. &lt;/p&gt;\n\n&lt;p&gt;So, once again, &lt;strong&gt;what\u2019s the best way to have checksums while still updating my drives with new videos?&lt;/strong&gt; Would it be best if I sort the videos yearly and then make checksums for each year? Monthly? &lt;/p&gt;\n\n&lt;p&gt;Would really appreciate any help I can get!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141v5ga", "is_robot_indexable": true, "report_reasons": null, "author": "iMainQuake", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/141v5ga/new_to_data_hoarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141v5ga/new_to_data_hoarding/", "subreddit_subscribers": 686277, "created_utc": 1686005937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to test something.", "author_fullname": "t2_cqgnh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easiest and quickest way to use a lot of cellular 4g lte or 5g data on a smartphone or tablet ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141s2xr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685999649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to test something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "141s2xr", "is_robot_indexable": true, "report_reasons": null, "author": "ng4ever", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141s2xr/easiest_and_quickest_way_to_use_a_lot_of_cellular/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141s2xr/easiest_and_quickest_way_to_use_a_lot_of_cellular/", "subreddit_subscribers": 686277, "created_utc": 1685999649.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}