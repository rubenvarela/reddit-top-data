{"kind": "Listing", "data": {"after": "t3_1424mqq", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There are several things I would like to download from Reddit before they kill off API access: \n\n- Every single thread I have commented on, for the purpose of being able to train an LLM to write like me. Reddit is by far the largest collection of text I have written. I have already filed a new CCPA request to get all my comments, but IIRC last time I made a request I only got my comments by themselves, not what they were replying to, so I need a way to automatically download all the context. \n\n- Every single post I have upvoted or saved, if possible. \n\n- Specific subreddits, particularly /r/HFY. I would like to save all the Reddit serials that I enjoy reading on my phone before API access is cut off and I no longer have a comfortable way to read them anymore. \n\nWhat are the best tools to do this with, saving as much metadata as possible in a machine-readable format? \n\nAny other tools for downloading from Reddit, even if not important for my particular use case, are also welcome. I am posting this because at my current point in searching, I have not yet found any good compilation of all the tools available.", "author_fullname": "t2_qp8d2", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Best tools for downloading Reddit before API access is cut off?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1423a1q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 305, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 305, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {"gid_2": 1}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686024982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are several things I would like to download from Reddit before they kill off API access: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Every single thread I have commented on, for the purpose of being able to train an LLM to write like me. Reddit is by far the largest collection of text I have written. I have already filed a new CCPA request to get all my comments, but IIRC last time I made a request I only got my comments by themselves, not what they were replying to, so I need a way to automatically download all the context. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Every single post I have upvoted or saved, if possible. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Specific subreddits, particularly &lt;a href=\"/r/HFY\"&gt;/r/HFY&lt;/a&gt;. I would like to save all the Reddit serials that I enjoy reading on my phone before API access is cut off and I no longer have a comfortable way to read them anymore. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What are the best tools to do this with, saving as much metadata as possible in a machine-readable format? &lt;/p&gt;\n\n&lt;p&gt;Any other tools for downloading from Reddit, even if not important for my particular use case, are also welcome. I am posting this because at my current point in searching, I have not yet found any good compilation of all the tools available.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "11TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1423a1q", "is_robot_indexable": true, "report_reasons": null, "author": "happysmash27", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1423a1q/best_tools_for_downloading_reddit_before_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1423a1q/best_tools_for_downloading_reddit_before_api/", "subreddit_subscribers": 686284, "created_utc": 1686024982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "ArchiveTeam has been archiving Reddit posts for a while now, but we are running out of time. [So far, we have archived 10.81 billion links, with 150 million to go](https://tracker.archiveteam.org/reddit/).\n\nRecent news of the [Reddit API cost changes](https://www.reddit.com/r/Save3rdPartyApps/comments/13yh0jf/dont_let_reddit_kill_3rd_party_apps/) will force many of the top 3rd party Reddit apps to shut down. This will not only affect how people use Reddit, but it will also cause issues with many subreddit moderation bots which rely on the API to function. Many subreddits have agreed to shut down for 48 hours on June 12th, while others will be gone *indefinitely* unless this issue is resolved.\n\n#Here is how you can help:\n\n### [Choose the \"host\" that matches your current PC, probably Windows or macOS](https://www.virtualbox.org/wiki/Downloads)\n\n### [Download ArchiveTeam Warrior](https://tracker.archiveteam.org/)\n\n1. In VirtualBox, click File &gt; Import Appliance and open the file.\n2. Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser.\n\nOnce you\u2019ve started your warrior:\n\n1. Go to http://localhost:8001/ and check the Settings page.\n2. Choose a username \u2014 we\u2019ll show your progress on the leaderboard.\n3. Go to the \"All projects\" tab and select ArchiveTeam\u2019s Choice to let your warrior work on the most urgent project. (This will be Reddit).\n\n### Alternative Method: Docker\n#### **[Download Docker on your \"host\" (Windows, macOS, Linux)](https://docs.docker.com/get-docker/)**\n### **[Follow the instructions on the ArchiveTeam website to set up Docker](https://wiki.archiveteam.org/index.php/Running_Archive_Team_Projects_with_Docker)**\n\nWhen setting up the project container, it will ask you to enter this command:\n\n```docker run -d --name archiveteam --label=com.centurylinklabs.watchtower.enable=true --restart=unless-stopped [image address] --concurrent 1 [username]```\n\nMake sure to replace the [image address] with the Reddit project address (removing brackets): ```atdr.meo.ws/archiveteam/reddit-grab```\nAlso change the [username] to whatever you'd like, no need to register for anything.\n\n####More information about running this project:\n\n**[Information about setting up the project](https://github.com/ArchiveTeam/reddit-grab)**\n\n**[ArchiveTeam Wiki page on the Reddit project](https://wiki.archiveteam.org/index.php?title=Reddit)**\n\n##**IMPORTANT: Do NOT modify scripts or the Warrior client!**", "author_fullname": "t2_ucb1wzy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ArchiveTeam has saved over 10.8 BILLION Reddit links so far. We need YOUR help running ArchiveTeam Warrior to archive subreddits before they're gone indefinitely after June 12th!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142l1i0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 139, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 139, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686068100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ArchiveTeam has been archiving Reddit posts for a while now, but we are running out of time. &lt;a href=\"https://tracker.archiveteam.org/reddit/\"&gt;So far, we have archived 10.81 billion links, with 150 million to go&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Recent news of the &lt;a href=\"https://www.reddit.com/r/Save3rdPartyApps/comments/13yh0jf/dont_let_reddit_kill_3rd_party_apps/\"&gt;Reddit API cost changes&lt;/a&gt; will force many of the top 3rd party Reddit apps to shut down. This will not only affect how people use Reddit, but it will also cause issues with many subreddit moderation bots which rely on the API to function. Many subreddits have agreed to shut down for 48 hours on June 12th, while others will be gone &lt;em&gt;indefinitely&lt;/em&gt; unless this issue is resolved.&lt;/p&gt;\n\n&lt;h1&gt;Here is how you can help:&lt;/h1&gt;\n\n&lt;h3&gt;&lt;a href=\"https://www.virtualbox.org/wiki/Downloads\"&gt;Choose the &amp;quot;host&amp;quot; that matches your current PC, probably Windows or macOS&lt;/a&gt;&lt;/h3&gt;\n\n&lt;h3&gt;&lt;a href=\"https://tracker.archiveteam.org/\"&gt;Download ArchiveTeam Warrior&lt;/a&gt;&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;In VirtualBox, click File &amp;gt; Import Appliance and open the file.&lt;/li&gt;\n&lt;li&gt;Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Once you\u2019ve started your warrior:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Go to http://localhost:8001/ and check the Settings page.&lt;/li&gt;\n&lt;li&gt;Choose a username \u2014 we\u2019ll show your progress on the leaderboard.&lt;/li&gt;\n&lt;li&gt;Go to the &amp;quot;All projects&amp;quot; tab and select ArchiveTeam\u2019s Choice to let your warrior work on the most urgent project. (This will be Reddit).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h3&gt;Alternative Method: Docker&lt;/h3&gt;\n\n&lt;h4&gt;&lt;strong&gt;&lt;a href=\"https://docs.docker.com/get-docker/\"&gt;Download Docker on your &amp;quot;host&amp;quot; (Windows, macOS, Linux)&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt;\n\n&lt;h3&gt;&lt;strong&gt;&lt;a href=\"https://wiki.archiveteam.org/index.php/Running_Archive_Team_Projects_with_Docker\"&gt;Follow the instructions on the ArchiveTeam website to set up Docker&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt;\n\n&lt;p&gt;When setting up the project container, it will ask you to enter this command:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;docker run -d --name archiveteam --label=com.centurylinklabs.watchtower.enable=true --restart=unless-stopped [image address] --concurrent 1 [username]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Make sure to replace the [image address] with the Reddit project address (removing brackets): &lt;code&gt;atdr.meo.ws/archiveteam/reddit-grab&lt;/code&gt;\nAlso change the [username] to whatever you&amp;#39;d like, no need to register for anything.&lt;/p&gt;\n\n&lt;h4&gt;More information about running this project:&lt;/h4&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;a href=\"https://github.com/ArchiveTeam/reddit-grab\"&gt;Information about setting up the project&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;a href=\"https://wiki.archiveteam.org/index.php?title=Reddit\"&gt;ArchiveTeam Wiki page on the Reddit project&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h2&gt;&lt;strong&gt;IMPORTANT: Do NOT modify scripts or the Warrior client!&lt;/strong&gt;&lt;/h2&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "6TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142l1i0", "is_robot_indexable": true, "report_reasons": null, "author": "BananaBus43", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/142l1i0/archiveteam_has_saved_over_108_billion_reddit/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/142l1i0/archiveteam_has_saved_over_108_billion_reddit/", "subreddit_subscribers": 686284, "created_utc": 1686068100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nWith the recent news about RARBG going down, and us being saved by some archivist's scraped DB dumps. I wanted to discuss mirroring more sites.\nParticularly I am interested in making scraped databases of RuTracker and Nyaa.si. Both of these have large amounts of highly popular content.\nI wanted to discuss techniques for mirroring these.\nNyaa.si has a public API where you can query search results as RSS. However, they limit the number of pages. And to mirror, don't we need to be able to arbitrarily go down page after page? How do we make queries to be able to mirror most of Nyaa?\nI was throwing out this question to see if anyone has an idea of how this is typically circumvented, in order to get lots of results to mirror.", "author_fullname": "t2_ctq800ysv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirroring torrent sites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1424g6h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686028010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the recent news about RARBG going down, and us being saved by some archivist&amp;#39;s scraped DB dumps. I wanted to discuss mirroring more sites.\nParticularly I am interested in making scraped databases of RuTracker and Nyaa.si. Both of these have large amounts of highly popular content.\nI wanted to discuss techniques for mirroring these.\nNyaa.si has a public API where you can query search results as RSS. However, they limit the number of pages. And to mirror, don&amp;#39;t we need to be able to arbitrarily go down page after page? How do we make queries to be able to mirror most of Nyaa?\nI was throwing out this question to see if anyone has an idea of how this is typically circumvented, in order to get lots of results to mirror.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1424g6h", "is_robot_indexable": true, "report_reasons": null, "author": "Busy-Paramedic-4994", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1424g6h/mirroring_torrent_sites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1424g6h/mirroring_torrent_sites/", "subreddit_subscribers": 686284, "created_utc": 1686028010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey so big question:\n\nLet's say I had all of my Twitter data, Reddit data and Discord data, is there any possible way I can display it all in some sort of chart or search engine where I can search through it? Like some sort of third party GUI or application.", "author_fullname": "t2_2alh77oh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Presentation of Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142cub8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686050646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey so big question:&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I had all of my Twitter data, Reddit data and Discord data, is there any possible way I can display it all in some sort of chart or search engine where I can search through it? Like some sort of third party GUI or application.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142cub8", "is_robot_indexable": true, "report_reasons": null, "author": "Zaxoosh", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142cub8/presentation_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142cub8/presentation_of_data/", "subreddit_subscribers": 686284, "created_utc": 1686050646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "how should i visually format an archive of someone's tweets/twitter account? im just focusing on the tweets (both with and without media)\n\nmost of the programs/apps i find for this only pull the media &amp; data separately (and a lot don't go past the 3k limit.. this account has 12k..), i want it similar to how the tweets look with the text &amp; media and date in the same box, that you can scroll chronologically. like a pdf, but im unsure if it's possible to put thousands of tweets into a pdf... has anyone else done this before? what did you do?\n\ni did actually manage to get *something* with Twitter Media Downloader (which even grabbed more than 3k tweets) so if there's nothing i'll just use that. but yeah just wondering if it's possible to maybe display the csv file (from twitter media downloader) visually..? or is it even possible to run a script or program that's capable of taking tweets from a specific account into a visual format? does such a thing exist with twitters current API situation?\n\ni've been trying to figure this out for a few days now, any help is appreciated", "author_fullname": "t2_tkcopl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to display an archive of someone's twitter account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1428b1l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686038535.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;how should i visually format an archive of someone&amp;#39;s tweets/twitter account? im just focusing on the tweets (both with and without media)&lt;/p&gt;\n\n&lt;p&gt;most of the programs/apps i find for this only pull the media &amp;amp; data separately (and a lot don&amp;#39;t go past the 3k limit.. this account has 12k..), i want it similar to how the tweets look with the text &amp;amp; media and date in the same box, that you can scroll chronologically. like a pdf, but im unsure if it&amp;#39;s possible to put thousands of tweets into a pdf... has anyone else done this before? what did you do?&lt;/p&gt;\n\n&lt;p&gt;i did actually manage to get &lt;em&gt;something&lt;/em&gt; with Twitter Media Downloader (which even grabbed more than 3k tweets) so if there&amp;#39;s nothing i&amp;#39;ll just use that. but yeah just wondering if it&amp;#39;s possible to maybe display the csv file (from twitter media downloader) visually..? or is it even possible to run a script or program that&amp;#39;s capable of taking tweets from a specific account into a visual format? does such a thing exist with twitters current API situation?&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;ve been trying to figure this out for a few days now, any help is appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1428b1l", "is_robot_indexable": true, "report_reasons": null, "author": "Hootie_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1428b1l/how_to_display_an_archive_of_someones_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1428b1l/how_to_display_an_archive_of_someones_twitter/", "subreddit_subscribers": 686284, "created_utc": 1686038535.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello Datahorder community!\n\nI just have a question referance backups of all your precious and (in my case sometimes) usless data! I have for a long time admired the setups people have on here and I have for a very long time wanted a data hording setup of my own (since I was about 15 as im a little bit sad!). I currently have 1TB on Google Drive and I am currently looking at purchasing a Synology DS923+ with 8TB of storage (which I will keep in RAID5 giving me 6TB usable). \n\n&amp;#x200B;\n\nI was just wanting to use you experts as a sounding bored, If i was to get say 3 6TB external HDD's and keep 1 in my bedroom (which will be backed up to daily, or everyother day), 1 in my office (weekly backed up) and 1 in my parents house (monthly backed up). \n\nWould this be sufficent? I feel like paying for a backup to google drive etc defeats purchasing the NAS. I am trying to become self hosted in as many ways as possible.\n\n&amp;#x200B;\n\nThank you for any advice\n\n&amp;#x200B;\n\nAdded:\n\nOn this NAS will be a few docker containers for testing some stuff and just general files and photos\n\nI am also looking at purchasing a older server at some point and that will run my testing and dockers and will backup to the NAS.", "author_fullname": "t2_1wgqbq6b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup suggestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141shz5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686000533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Datahorder community!&lt;/p&gt;\n\n&lt;p&gt;I just have a question referance backups of all your precious and (in my case sometimes) usless data! I have for a long time admired the setups people have on here and I have for a very long time wanted a data hording setup of my own (since I was about 15 as im a little bit sad!). I currently have 1TB on Google Drive and I am currently looking at purchasing a Synology DS923+ with 8TB of storage (which I will keep in RAID5 giving me 6TB usable). &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was just wanting to use you experts as a sounding bored, If i was to get say 3 6TB external HDD&amp;#39;s and keep 1 in my bedroom (which will be backed up to daily, or everyother day), 1 in my office (weekly backed up) and 1 in my parents house (monthly backed up). &lt;/p&gt;\n\n&lt;p&gt;Would this be sufficent? I feel like paying for a backup to google drive etc defeats purchasing the NAS. I am trying to become self hosted in as many ways as possible.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for any advice&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Added:&lt;/p&gt;\n\n&lt;p&gt;On this NAS will be a few docker containers for testing some stuff and just general files and photos&lt;/p&gt;\n\n&lt;p&gt;I am also looking at purchasing a older server at some point and that will run my testing and dockers and will backup to the NAS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141shz5", "is_robot_indexable": true, "report_reasons": null, "author": "Zetta666", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141shz5/backup_suggestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141shz5/backup_suggestion/", "subreddit_subscribers": 686284, "created_utc": 1686000533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi fellow data hoarders, I have a question that I hope someone can answer. I'm interested in backing up some subreddits that I like, especially the ones that are at risk of being banned or quarantined. I know there are some tools that can download subreddits in a CSV format or just the images, but I'm wondering if there is a way to publish them to lemmy, which is a federated and open source alternative to reddit.\n\n&amp;#x200B;\n\nThe reason I want to publish subreddits to lemmy is because I think it would be a great way to preserve the content and discussions that are valuable to me and others, and also to encourage more people to switch to lemmy and its benefits.\n\n&amp;#x200B;\n\nAnother reason to switch to lemmy is because reddit is going through some major changes that might affect third-party apps and NSFW content. Reddit recently announced that it has filed paperwork to go public, which could mean more pressure from investors and advertisers to monetize the site and censor controversial content. Reddit also announced new pricing for its API access, which could make third-party apps unsustainable or unaffordable. Many of the largest subreddits are planning to go dark for 48 hours in protest of these changes.\n\n&amp;#x200B;\n\nSo, is there anyone who knows how to backup subreddits and publish them to lemmy? Or is there anyone who is interested in working on such a project? I think it would be a worthwhile endeavor for data hoarders and lemmy users alike. Let me know what you think. Thanks!", "author_fullname": "t2_l38eqkli", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to backup subreddits and publish them to Lemmy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142hvhj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686061762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow data hoarders, I have a question that I hope someone can answer. I&amp;#39;m interested in backing up some subreddits that I like, especially the ones that are at risk of being banned or quarantined. I know there are some tools that can download subreddits in a CSV format or just the images, but I&amp;#39;m wondering if there is a way to publish them to lemmy, which is a federated and open source alternative to reddit.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The reason I want to publish subreddits to lemmy is because I think it would be a great way to preserve the content and discussions that are valuable to me and others, and also to encourage more people to switch to lemmy and its benefits.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Another reason to switch to lemmy is because reddit is going through some major changes that might affect third-party apps and NSFW content. Reddit recently announced that it has filed paperwork to go public, which could mean more pressure from investors and advertisers to monetize the site and censor controversial content. Reddit also announced new pricing for its API access, which could make third-party apps unsustainable or unaffordable. Many of the largest subreddits are planning to go dark for 48 hours in protest of these changes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So, is there anyone who knows how to backup subreddits and publish them to lemmy? Or is there anyone who is interested in working on such a project? I think it would be a worthwhile endeavor for data hoarders and lemmy users alike. Let me know what you think. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142hvhj", "is_robot_indexable": true, "report_reasons": null, "author": "thebigvsbattlesfan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142hvhj/is_it_possible_to_backup_subreddits_and_publish/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142hvhj/is_it_possible_to_backup_subreddits_and_publish/", "subreddit_subscribers": 686284, "created_utc": 1686061762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a server with about 200TB that uses FTP and Rsync which I want to backup. I have a ton of 4TB drives laying around so I thought I might as well use them for offline backup. I intend to use a temporary 24slots/4U server with UnRAID to do the backup so I get individual drives that can be stored offline later.  \n\n\nMy question is.. does there exist a tool (or can rsync do this?) that can allow me to transfer the data using one of those two protocols and keep track of the status using a local log file? My idea is that the logfile could be used to keep track of what has been downloaded without having to access the data on the drives (which may already be stored offline) and let the client only download new/updated files. Preferably with some way of pausing it after n TB so I can swap the 24 TB drives with empty once when full before continuing where it left off.", "author_fullname": "t2_lh08w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FTP or Rsync to offline backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142c4um", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686048908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a server with about 200TB that uses FTP and Rsync which I want to backup. I have a ton of 4TB drives laying around so I thought I might as well use them for offline backup. I intend to use a temporary 24slots/4U server with UnRAID to do the backup so I get individual drives that can be stored offline later.  &lt;/p&gt;\n\n&lt;p&gt;My question is.. does there exist a tool (or can rsync do this?) that can allow me to transfer the data using one of those two protocols and keep track of the status using a local log file? My idea is that the logfile could be used to keep track of what has been downloaded without having to access the data on the drives (which may already be stored offline) and let the client only download new/updated files. Preferably with some way of pausing it after n TB so I can swap the 24 TB drives with empty once when full before continuing where it left off.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142c4um", "is_robot_indexable": true, "report_reasons": null, "author": "NeoID", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142c4um/ftp_or_rsync_to_offline_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142c4um/ftp_or_rsync_to_offline_backup/", "subreddit_subscribers": 686284, "created_utc": 1686048908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there any cases out there that can hold the same amount of drives as the 7 XL (18 I believe) or more, for a regular nas build? I\u2019ve seen people say the Lian Li D8000 but it\u2019s discontinued.", "author_fullname": "t2_ba3y5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rival to Fractal Define 7 XL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141o74a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685991024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any cases out there that can hold the same amount of drives as the 7 XL (18 I believe) or more, for a regular nas build? I\u2019ve seen people say the Lian Li D8000 but it\u2019s discontinued.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141o74a", "is_robot_indexable": true, "report_reasons": null, "author": "Conorsavage", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141o74a/rival_to_fractal_define_7_xl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141o74a/rival_to_fractal_define_7_xl/", "subreddit_subscribers": 686284, "created_utc": 1685991024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I have tons of iPhone text/iMessage chats with at least 30gb of attachments across them.\nI know how to find and extract the attachments from the iTunes backup files.\n\nThe issue I am having is that the iTunes backup doesn\u2019t have all the attachments (photos, videos, PDF, vCard) since I have iCloud messages turned on, so the attachments in the cloud won\u2019t appear in the local iTunes backup.\n\nHowever, when I turn off messages in iCloud (and press the option to download and delete messages from the cloud to my phone) many attachments don\u2019t download, they just have a \u201c?\u201d icon. I can only access those attachments when I turn iCloud messages back on.\n\nHow can get get all message attachments from the cloud onto my phone and into a local iTunes backup of the \u201cdisable and delete\u201d from iCloud doesn\u2019t wort properly?", "author_fullname": "t2_e6drpuch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting photos from iPhone backup - cloud issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142iue7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686063763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have tons of iPhone text/iMessage chats with at least 30gb of attachments across them.\nI know how to find and extract the attachments from the iTunes backup files.&lt;/p&gt;\n\n&lt;p&gt;The issue I am having is that the iTunes backup doesn\u2019t have all the attachments (photos, videos, PDF, vCard) since I have iCloud messages turned on, so the attachments in the cloud won\u2019t appear in the local iTunes backup.&lt;/p&gt;\n\n&lt;p&gt;However, when I turn off messages in iCloud (and press the option to download and delete messages from the cloud to my phone) many attachments don\u2019t download, they just have a \u201c?\u201d icon. I can only access those attachments when I turn iCloud messages back on.&lt;/p&gt;\n\n&lt;p&gt;How can get get all message attachments from the cloud onto my phone and into a local iTunes backup of the \u201cdisable and delete\u201d from iCloud doesn\u2019t wort properly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142iue7", "is_robot_indexable": true, "report_reasons": null, "author": "aviator_L1011", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142iue7/extracting_photos_from_iphone_backup_cloud_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142iue7/extracting_photos_from_iphone_backup_cloud_issues/", "subreddit_subscribers": 686284, "created_utc": 1686063763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "TLDR: What size for autocad type files? Dual port vs. adapter? Are expensive flash drives worth the cost?\n\n\nI hope I\u2019m posting to the right sub for this question. I don\u2019t normally keep up with techy things until I find they\u2019re applicable to my daily life, so I\u2019m sorry if I seem a bit illiterate on this subject. \n\nI am an architecture student who has been asked to buy a flash drive for storing projects. We use computer programs such as revit, rhino, auto cad, photoshop and more. With these programs, I imagine I\u2019ll be needing a lot of space over the course of my 7 year education. My initial thought was \u201cjust buy a terabyte or more, to be safe\u201d, but they can get pricy, so maybe buying multiple, smaller drives is wiser? I could always consolidate onto an external hard-drive. \n\nCurrently, I own a Mac (thinking about switching when I get the funds) whereas the school computers are windows, so I\u2019m looking into the dual USB3.0/USB-C drives. But it seems like these may be more of a gimmick and I might be better off with an adapter. I\u2019m worried about one end always being exposed (especially around my messy architectural modeling supplies).\n\nFinally, I find that the price range is all over the place, ranging from $20 to well over $100. As a student, cheaper suits my bank account, but I also can\u2019t have the thing dying on me in the middle of finals. I\u2019ve read that overheating is a big problem for some.\n\nHoping you guys have some recommendations for me, or could at least steer me in the right direction! (Bonus points for key-chain connect ability, double bonus for indicator lights!)\n\nThanks in advance for any and all advice!", "author_fullname": "t2_3r9m60q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flash drive recommendations for the not so tech savvy:", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142f6ay", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686056079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: What size for autocad type files? Dual port vs. adapter? Are expensive flash drives worth the cost?&lt;/p&gt;\n\n&lt;p&gt;I hope I\u2019m posting to the right sub for this question. I don\u2019t normally keep up with techy things until I find they\u2019re applicable to my daily life, so I\u2019m sorry if I seem a bit illiterate on this subject. &lt;/p&gt;\n\n&lt;p&gt;I am an architecture student who has been asked to buy a flash drive for storing projects. We use computer programs such as revit, rhino, auto cad, photoshop and more. With these programs, I imagine I\u2019ll be needing a lot of space over the course of my 7 year education. My initial thought was \u201cjust buy a terabyte or more, to be safe\u201d, but they can get pricy, so maybe buying multiple, smaller drives is wiser? I could always consolidate onto an external hard-drive. &lt;/p&gt;\n\n&lt;p&gt;Currently, I own a Mac (thinking about switching when I get the funds) whereas the school computers are windows, so I\u2019m looking into the dual USB3.0/USB-C drives. But it seems like these may be more of a gimmick and I might be better off with an adapter. I\u2019m worried about one end always being exposed (especially around my messy architectural modeling supplies).&lt;/p&gt;\n\n&lt;p&gt;Finally, I find that the price range is all over the place, ranging from $20 to well over $100. As a student, cheaper suits my bank account, but I also can\u2019t have the thing dying on me in the middle of finals. I\u2019ve read that overheating is a big problem for some.&lt;/p&gt;\n\n&lt;p&gt;Hoping you guys have some recommendations for me, or could at least steer me in the right direction! (Bonus points for key-chain connect ability, double bonus for indicator lights!)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any and all advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142f6ay", "is_robot_indexable": true, "report_reasons": null, "author": "txkelly", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142f6ay/flash_drive_recommendations_for_the_not_so_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142f6ay/flash_drive_recommendations_for_the_not_so_tech/", "subreddit_subscribers": 686284, "created_utc": 1686056079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys,\n\nI've been trying to organise all my drives and started wondering... Is it possible to combine drives/folders into 1 virtual drive in Windows 11? \n\nI've got so many games from so many different platforms (Steam, Epic, EA etc...) all spread across 6 different drives on my PC at the moment. I know the solution is to buy a big drive and transfer everything into 1 drive but cmon... the cost of living these days is no joke XD\n\nAnyways is this possible? Can I create some sort of virtual drive that can link all my games across 6 drives into 1? Now that would be amazing!", "author_fullname": "t2_5251j5ai", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Combining multiple drives/folders into one", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1429p61", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686042446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to organise all my drives and started wondering... Is it possible to combine drives/folders into 1 virtual drive in Windows 11? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got so many games from so many different platforms (Steam, Epic, EA etc...) all spread across 6 different drives on my PC at the moment. I know the solution is to buy a big drive and transfer everything into 1 drive but cmon... the cost of living these days is no joke XD&lt;/p&gt;\n\n&lt;p&gt;Anyways is this possible? Can I create some sort of virtual drive that can link all my games across 6 drives into 1? Now that would be amazing!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1429p61", "is_robot_indexable": true, "report_reasons": null, "author": "SnooChipmunks4789", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1429p61/combining_multiple_drivesfolders_into_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1429p61/combining_multiple_drivesfolders_into_one/", "subreddit_subscribers": 686284, "created_utc": 1686042446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to save all the torrent files from a specific user in rutracker (the person has INCREDIBLY rare stuff - I mean stuff you can't even BUY because there were only very limited editions of it and they are sold out everywhere and I worry about a RARBG situation happening).\n\nConsulting the person's page states they have 3000+ torrents, but when I go to the page I am limited to 10 pages with 50 files each. If I search for something that I'm sure the person has, it shows up in the search results, it's just the person's list that is capped to 500 results.\n\nIs there any way to \"unlock\" their full catalogue?", "author_fullname": "t2_csnfseuy9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hoarding specific users from rutracker: list is limited to 10 pages?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1429727", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686041098.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to save all the torrent files from a specific user in rutracker (the person has INCREDIBLY rare stuff - I mean stuff you can&amp;#39;t even BUY because there were only very limited editions of it and they are sold out everywhere and I worry about a RARBG situation happening).&lt;/p&gt;\n\n&lt;p&gt;Consulting the person&amp;#39;s page states they have 3000+ torrents, but when I go to the page I am limited to 10 pages with 50 files each. If I search for something that I&amp;#39;m sure the person has, it shows up in the search results, it&amp;#39;s just the person&amp;#39;s list that is capped to 500 results.&lt;/p&gt;\n\n&lt;p&gt;Is there any way to &amp;quot;unlock&amp;quot; their full catalogue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1429727", "is_robot_indexable": true, "report_reasons": null, "author": "Thick_Blackberry3377", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1429727/hoarding_specific_users_from_rutracker_list_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1429727/hoarding_specific_users_from_rutracker_list_is/", "subreddit_subscribers": 686284, "created_utc": 1686041098.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Community, \n\nI wanted to get your views on the different ways of working with storage.   \nFor example, would it be better to work against a NAS in the network, or against local storage which usually is faster and then backup to the NAS?  \nMaybe a mix model?\n\nI wanted to see how you guys are usually working and what do you feel comfortable / already tested.  \nThank you for sharing your thoughts \ud83d\ude09", "author_fullname": "t2_meba9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Workflows and different ways of working with storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1428wiz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686040255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Community, &lt;/p&gt;\n\n&lt;p&gt;I wanted to get your views on the different ways of working with storage.&lt;br/&gt;\nFor example, would it be better to work against a NAS in the network, or against local storage which usually is faster and then backup to the NAS?&lt;br/&gt;\nMaybe a mix model?&lt;/p&gt;\n\n&lt;p&gt;I wanted to see how you guys are usually working and what do you feel comfortable / already tested.&lt;br/&gt;\nThank you for sharing your thoughts \ud83d\ude09&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1428wiz", "is_robot_indexable": true, "report_reasons": null, "author": "fscheps", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1428wiz/workflows_and_different_ways_of_working_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1428wiz/workflows_and_different_ways_of_working_with/", "subreddit_subscribers": 686284, "created_utc": 1686040255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It will mostly be a media/jellyfin server, but will do some general use as well. A VM and/or some containers. \n\nFor storage I am looking at either 4x16TB or 6x16TB for storage. Which would be best, and what RAID/UNRAID config should I run for 4 and 6 drives? Should the drives be the same or should I split between 2 or 3 brands? There will be some data that would suck to lose. Most of it is media though. Not critical, but a big pain to find again and would preferably want to avoid doing so. I am willing to pay more for an extra drive or two if necessary.\n\nAs for other hardware. How powerful cpu should I get for running maybe a VM, a container and a jellyfin server? And what about memory?\n\nBig thanks in advance for any input!", "author_fullname": "t2_y5g3q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to build a home server. 4 or 6 drives? How powerful hardware", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1420occ", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.47, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686018563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It will mostly be a media/jellyfin server, but will do some general use as well. A VM and/or some containers. &lt;/p&gt;\n\n&lt;p&gt;For storage I am looking at either 4x16TB or 6x16TB for storage. Which would be best, and what RAID/UNRAID config should I run for 4 and 6 drives? Should the drives be the same or should I split between 2 or 3 brands? There will be some data that would suck to lose. Most of it is media though. Not critical, but a big pain to find again and would preferably want to avoid doing so. I am willing to pay more for an extra drive or two if necessary.&lt;/p&gt;\n\n&lt;p&gt;As for other hardware. How powerful cpu should I get for running maybe a VM, a container and a jellyfin server? And what about memory?&lt;/p&gt;\n\n&lt;p&gt;Big thanks in advance for any input!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1420occ", "is_robot_indexable": true, "report_reasons": null, "author": "TheOptiGamer", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1420occ/looking_to_build_a_home_server_4_or_6_drives_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1420occ/looking_to_build_a_home_server_4_or_6_drives_how/", "subreddit_subscribers": 686284, "created_utc": 1686018563.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone!\nThanks for reading!\n\nPlease excuse me if my noob question makes you cringe.\n\nI have been looking for a solution to set all my hdds in one single enclosure so I can access to all of them. Lurking here I read about the Terramaster d5-330 and I got one.\n\nHere is where my problems started. I followed the official guide, watching the videos and spent the last 24hrs reading all I can find on how to set it up. I failed. \n\nI want to use it in SINGLE mode that for what I understand should be pretty much plug and play, but the thunderbolt ports on my ROG ZEPHIRUS g15 only detect it as PD (power device) and I don\u2019t get the prompt to install the drivers nor the raid shows up in my device manager window.\n\nI tried force the update on the thuderbolt to install the official drivers but it got me nowhere. I installed the rain manager and the raid pro manager software. Obviously the drive doesn\u2019t show up so also that is a nonstarter. \n\nI tried several recommended hdd in different all the bays of the D5-330 with always the same results. Btw the light of the drives stay off all the time. I tested the drives before and after, and it works no problem. \n\nIs my first time fiddling with a das so I am sure I can mess up something but I can\u2019t see what \u2026 I am stupid or is this unit defective?\n\nThanks!", "author_fullname": "t2_3cl0d5g5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help/troubleshooting: Terramaster D5-330 failing to be recognized, is it a dudd?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1420fb0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686017968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!\nThanks for reading!&lt;/p&gt;\n\n&lt;p&gt;Please excuse me if my noob question makes you cringe.&lt;/p&gt;\n\n&lt;p&gt;I have been looking for a solution to set all my hdds in one single enclosure so I can access to all of them. Lurking here I read about the Terramaster d5-330 and I got one.&lt;/p&gt;\n\n&lt;p&gt;Here is where my problems started. I followed the official guide, watching the videos and spent the last 24hrs reading all I can find on how to set it up. I failed. &lt;/p&gt;\n\n&lt;p&gt;I want to use it in SINGLE mode that for what I understand should be pretty much plug and play, but the thunderbolt ports on my ROG ZEPHIRUS g15 only detect it as PD (power device) and I don\u2019t get the prompt to install the drivers nor the raid shows up in my device manager window.&lt;/p&gt;\n\n&lt;p&gt;I tried force the update on the thuderbolt to install the official drivers but it got me nowhere. I installed the rain manager and the raid pro manager software. Obviously the drive doesn\u2019t show up so also that is a nonstarter. &lt;/p&gt;\n\n&lt;p&gt;I tried several recommended hdd in different all the bays of the D5-330 with always the same results. Btw the light of the drives stay off all the time. I tested the drives before and after, and it works no problem. &lt;/p&gt;\n\n&lt;p&gt;Is my first time fiddling with a das so I am sure I can mess up something but I can\u2019t see what \u2026 I am stupid or is this unit defective?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1420fb0", "is_robot_indexable": true, "report_reasons": null, "author": "FresconeFrizzantino", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1420fb0/helptroubleshooting_terramaster_d5330_failing_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1420fb0/helptroubleshooting_terramaster_d5330_failing_to/", "subreddit_subscribers": 686284, "created_utc": 1686017968.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "want to buy a lot of drives and I have found no posts talking about their support once something goes wrong", "author_fullname": "t2_gw5a2p3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have RMA experience with Solidigm?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141zgm0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686015673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;want to buy a lot of drives and I have found no posts talking about their support once something goes wrong&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "141zgm0", "is_robot_indexable": true, "report_reasons": null, "author": "masturbaiter696969", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141zgm0/anyone_have_rma_experience_with_solidigm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141zgm0/anyone_have_rma_experience_with_solidigm/", "subreddit_subscribers": 686284, "created_utc": 1686015673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello All,\n\nWe are a creative studio made by two people. We are looking for some suggestion concerning storage hardware and - eventually - methods.\n\nWe work as designers and artists and we produce a large and heterogeneous amount of files (3D, video, images, etc). We have no experience in NAS storage and we have done backups manually until now, but we are open to change our workflow if it worth the effort.\n\nWhat you experts suggest?  Go with the NAS (we heard enthusiastic opinion about the WD MyCloud) or buy for example a WD MyBook instead?\n\nThank you.", "author_fullname": "t2_dt415yx4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage suggestion (NAS vs 'just storage')", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ku0k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686067691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\n\n&lt;p&gt;We are a creative studio made by two people. We are looking for some suggestion concerning storage hardware and - eventually - methods.&lt;/p&gt;\n\n&lt;p&gt;We work as designers and artists and we produce a large and heterogeneous amount of files (3D, video, images, etc). We have no experience in NAS storage and we have done backups manually until now, but we are open to change our workflow if it worth the effort.&lt;/p&gt;\n\n&lt;p&gt;What you experts suggest?  Go with the NAS (we heard enthusiastic opinion about the WD MyCloud) or buy for example a WD MyBook instead?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142ku0k", "is_robot_indexable": true, "report_reasons": null, "author": "DwayneDardago", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142ku0k/storage_suggestion_nas_vs_just_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142ku0k/storage_suggestion_nas_vs_just_storage/", "subreddit_subscribers": 686284, "created_utc": 1686067691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have all of my photos in Google photos, but am paying every month for overflow storage. On the plus side is quite easy to manage, but I am looking for alternative options. Is iCloud any better? Should I invest in an external hard drive? No professional work just personal memories, thanks!", "author_fullname": "t2_h6mc8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice on how to store years worth of digital photos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ga7f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686058414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have all of my photos in Google photos, but am paying every month for overflow storage. On the plus side is quite easy to manage, but I am looking for alternative options. Is iCloud any better? Should I invest in an external hard drive? No professional work just personal memories, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142ga7f", "is_robot_indexable": true, "report_reasons": null, "author": "onceuponanadventure", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142ga7f/looking_for_advice_on_how_to_store_years_worth_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142ga7f/looking_for_advice_on_how_to_store_years_worth_of/", "subreddit_subscribers": 686284, "created_utc": 1686058414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey\n\nI'm reaching out to the community today seeking some advice and recommendations for a NAS solution for our small media company. We have specific requirements and would greatly appreciate any insights or suggestions you can provide.\n\nFirstly, we need a NAS solution that serves as both an archive storage and an edit NAS. We want to store our archive while being able to access assets from it for ongoing productions. The archive section doesn't need to be lightning-fast, but it should have a capacity of around 50TB. It would be ideal if we could expand the storage in the future as well.\n\nIn terms of editing, we currently have two edit stations, and we plan to connect them via a 10-gigabit network. Our primary camera, the Sony FX9, is quite data-intensive and can record at a maximum bitrate of 600Mbps. We want to ensure that our NAS setup is compatible with this camera. For the editing NAS, we require a minimum storage capacity of 8TB.\n\nInitially, we were considering options from QNAP or Synology for a standalone archive server, but we understand that a specialized solution might be more suitable for our needs. We're open to different suggestions and would like to get an idea of the budgetary implications as well. If possible, it would be great if you could provide recommendations and price estimates for both a budget-oriented option and a solution that you highly recommend.\n\nTo summarize, we're in search of a NAS solution that offers high-speed access to at least 8TB of storage for editing purposes and an additional 50TB for archiving. Any guidance or advice on suitable models, brands, or specific configurations would be greatly appreciated.\n\nThank you in advance for your help!", "author_fullname": "t2_ms2p8hgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a NAS Solution for a Small Media Company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142ccqy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686049446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m reaching out to the community today seeking some advice and recommendations for a NAS solution for our small media company. We have specific requirements and would greatly appreciate any insights or suggestions you can provide.&lt;/p&gt;\n\n&lt;p&gt;Firstly, we need a NAS solution that serves as both an archive storage and an edit NAS. We want to store our archive while being able to access assets from it for ongoing productions. The archive section doesn&amp;#39;t need to be lightning-fast, but it should have a capacity of around 50TB. It would be ideal if we could expand the storage in the future as well.&lt;/p&gt;\n\n&lt;p&gt;In terms of editing, we currently have two edit stations, and we plan to connect them via a 10-gigabit network. Our primary camera, the Sony FX9, is quite data-intensive and can record at a maximum bitrate of 600Mbps. We want to ensure that our NAS setup is compatible with this camera. For the editing NAS, we require a minimum storage capacity of 8TB.&lt;/p&gt;\n\n&lt;p&gt;Initially, we were considering options from QNAP or Synology for a standalone archive server, but we understand that a specialized solution might be more suitable for our needs. We&amp;#39;re open to different suggestions and would like to get an idea of the budgetary implications as well. If possible, it would be great if you could provide recommendations and price estimates for both a budget-oriented option and a solution that you highly recommend.&lt;/p&gt;\n\n&lt;p&gt;To summarize, we&amp;#39;re in search of a NAS solution that offers high-speed access to at least 8TB of storage for editing purposes and an additional 50TB for archiving. Any guidance or advice on suitable models, brands, or specific configurations would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142ccqy", "is_robot_indexable": true, "report_reasons": null, "author": "Hot_Adhesiveness2226", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142ccqy/looking_for_a_nas_solution_for_a_small_media/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142ccqy/looking_for_a_nas_solution_for_a_small_media/", "subreddit_subscribers": 686284, "created_utc": 1686049446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone been able to download youtube videos on the ```http_dash_segments``` protocol at speed &gt; 500KB/sec ? \n\nWhen I download a video using the ```https``` protocol, I can get speed like 10MB/sec, but other formats of the same video that are listed on the ```http_dash_segments``` protocol, speed is always capped, that is with ```Youtube Download [yt-dlp]``` or ```jDownloader```. \n\nThanks", "author_fullname": "t2_ch4bnghs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "youtube ideos: http_dash_segments protocol speed capped?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14234ic", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686024601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone been able to download youtube videos on the &lt;code&gt;http_dash_segments&lt;/code&gt; protocol at speed &amp;gt; 500KB/sec ? &lt;/p&gt;\n\n&lt;p&gt;When I download a video using the &lt;code&gt;https&lt;/code&gt; protocol, I can get speed like 10MB/sec, but other formats of the same video that are listed on the &lt;code&gt;http_dash_segments&lt;/code&gt; protocol, speed is always capped, that is with &lt;code&gt;Youtube Download [yt-dlp]&lt;/code&gt; or &lt;code&gt;jDownloader&lt;/code&gt;. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14234ic", "is_robot_indexable": true, "report_reasons": null, "author": "cybercastor", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14234ic/youtube_ideos_http_dash_segments_protocol_speed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14234ic/youtube_ideos_http_dash_segments_protocol_speed/", "subreddit_subscribers": 686284, "created_utc": 1686024601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to test something.", "author_fullname": "t2_cqgnh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easiest and quickest way to use a lot of cellular 4g lte or 5g data on a smartphone or tablet ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_141s2xr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685999649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to test something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "141s2xr", "is_robot_indexable": true, "report_reasons": null, "author": "ng4ever", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/141s2xr/easiest_and_quickest_way_to_use_a_lot_of_cellular/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/141s2xr/easiest_and_quickest_way_to_use_a_lot_of_cellular/", "subreddit_subscribers": 686284, "created_utc": 1685999649.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm stumped, everyway I look at this it's telling me that there is 12tb of free space. But the storage space says it's full? Is there some parity that is causing this? When I try to copy files it says it doesn't have any space?", "author_fullname": "t2_14mc49ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Storage doesn't add up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "media_metadata": {"47ng4qtaie4b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 59, "x": 108, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f28f178d7a57e520d8eb5bc17768da695c7a5f7"}, {"y": 119, "x": 216, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba3819f9cea7a682cb2b9c78c706be9f2be90bef"}, {"y": 176, "x": 320, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=645a1825317bdb51375012e1cb5e7c2de6e46216"}, {"y": 353, "x": 640, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8f1bce6bc6108cc35035455873af1ae702cec43"}, {"y": 529, "x": 960, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=892c654c9b7ca22542d053b21ce52ea8a11c7d22"}, {"y": 596, "x": 1080, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=855a319f0069dc89a10d7b45fe69d6213783d5b3"}], "s": {"y": 787, "x": 1426, "u": "https://preview.redd.it/47ng4qtaie4b1.jpg?width=1426&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=af9177f96c3c757bf55bdf20e61f9f832ab4ee51"}, "id": "47ng4qtaie4b1"}, "dy4voyqaie4b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e799259a1adbdddf21d5ca6d0d3f0fdb3d277231"}, {"y": 110, "x": 216, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d320b39d69a2937c35e34c8353b2a0244c57d6ec"}, {"y": 164, "x": 320, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7748d91f793360bdb3095a504b56b9c8eb65987"}, {"y": 328, "x": 640, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d4743621adcc21877c690ec1f3036193b8c20ae"}, {"y": 493, "x": 960, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce917567b54bbb46d683bd3c58036a8e7a508c2c"}, {"y": 554, "x": 1080, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ca63d7a43d6ebf6ce752c5348d4fe90450c7c7d"}], "s": {"y": 690, "x": 1343, "u": "https://preview.redd.it/dy4voyqaie4b1.jpg?width=1343&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=9f7a41c0721a5caf8e2c89a46e8c3c4aa01aebd3"}, "id": "dy4voyqaie4b1"}}, "name": "t3_142gfsf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "ups": 0, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "dy4voyqaie4b1", "id": 284483225}, {"media_id": "47ng4qtaie4b1", "id": 284483226}]}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6Rq8GMRqYT6bqthCL3YezWbwO08iXHo6hmp4vzMemzg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1686058744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m stumped, everyway I look at this it&amp;#39;s telling me that there is 12tb of free space. But the storage space says it&amp;#39;s full? Is there some parity that is causing this? When I try to copy files it says it doesn&amp;#39;t have any space?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/142gfsf", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142gfsf", "is_robot_indexable": true, "report_reasons": null, "author": "Cor_Brain", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142gfsf/storage_doesnt_add_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/142gfsf", "subreddit_subscribers": 686284, "created_utc": 1686058744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for the best option to to store all of my photos and videos that I am putting into digital form on a cloud service. I am using an external hard drive to store but that isn't 100% fool proof. Whats the best bang for my buck service that I won't have to worry about? May store some documents there as well but not many. Thanks\n\n(I have verizon cloud, but it's wonky to use, slow and seems to not work correctly for what I am trying to do with it)", "author_fullname": "t2_hfg20qfg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Storage (Personal Digitalized Photos/Videos)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_142g0xk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686057890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for the best option to to store all of my photos and videos that I am putting into digital form on a cloud service. I am using an external hard drive to store but that isn&amp;#39;t 100% fool proof. Whats the best bang for my buck service that I won&amp;#39;t have to worry about? May store some documents there as well but not many. Thanks&lt;/p&gt;\n\n&lt;p&gt;(I have verizon cloud, but it&amp;#39;s wonky to use, slow and seems to not work correctly for what I am trying to do with it)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "142g0xk", "is_robot_indexable": true, "report_reasons": null, "author": "Aries-Baby-11", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/142g0xk/cloud_storage_personal_digitalized_photosvideos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/142g0xk/cloud_storage_personal_digitalized_photosvideos/", "subreddit_subscribers": 686284, "created_utc": 1686057890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm curious if anyone did manage to archive Synology's old DSM and package versions?\n\n[web.archive.org](https://web.archive.org) has archived a lot of it, but it didn't archive the larger files. [https://web.archive.org/web/20210416132208/https://archive.synology.com/download](https://web.archive.org/web/20210416132208/https://archive.synology.com/download)\n\nThere were 2 threads about Synology deleting all it's old  DSM and package versions:\n\n1. [https://www.reddit.com/r/DataHoarder/comments/10da7a9/official\\_synology\\_download\\_site\\_closing\\_legacy/](https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/)\n2. [https://www.reddit.com/r/DataHoarder/comments/12wp46q/reminder\\_synology\\_legacy\\_dsm\\_firmware\\_package/](https://www.reddit.com/r/DataHoarder/comments/12wp46q/reminder_synology_legacy_dsm_firmware_package/)\n\nWhile there were people who started downloading all the old DSM and package versions it seemed like everyone gave up before they had downloaded everything (myself included).", "author_fullname": "t2_33srm1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did anyone manage to archive Synology's old DSM and package versions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1424mqq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1686028490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious if anyone did manage to archive Synology&amp;#39;s old DSM and package versions?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://web.archive.org\"&gt;web.archive.org&lt;/a&gt; has archived a lot of it, but it didn&amp;#39;t archive the larger files. &lt;a href=\"https://web.archive.org/web/20210416132208/https://archive.synology.com/download\"&gt;https://web.archive.org/web/20210416132208/https://archive.synology.com/download&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There were 2 threads about Synology deleting all it&amp;#39;s old  DSM and package versions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/\"&gt;https://www.reddit.com/r/DataHoarder/comments/10da7a9/official_synology_download_site_closing_legacy/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/12wp46q/reminder_synology_legacy_dsm_firmware_package/\"&gt;https://www.reddit.com/r/DataHoarder/comments/12wp46q/reminder_synology_legacy_dsm_firmware_package/&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;While there were people who started downloading all the old DSM and package versions it seemed like everyone gave up before they had downloaded everything (myself included).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "186TB local", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1424mqq", "is_robot_indexable": true, "report_reasons": null, "author": "DaveR007", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1424mqq/did_anyone_manage_to_archive_synologys_old_dsm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1424mqq/did_anyone_manage_to_archive_synologys_old_dsm/", "subreddit_subscribers": 686284, "created_utc": 1686028490.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}