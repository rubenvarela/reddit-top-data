{"kind": "Listing", "data": {"after": "t3_yvcz5r", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone! A few months ago I defended my **Master Thesis on Big Data** and got the maximum grade of 10.0 with honors. I want to thank this subreddit for the help and advice received in one of my previous posts. Also, if you want to build something similar and you think the project can be usefull for you, feel free to ask me for the Github page (I cannot attach it here since it contains my name and I think it is against the PII data community rules).\n\nAs a summary, I built an **ETL process** to get information about the latest music listened to by **Twitter** users (by searching for the hashtag #NowPlaying) and then queried **Spotify** to get the song and artist data involved. I used **Spark** to run the ETL process, **Cassandra** to store the data, a custom web application for the final visualization (**Flask** \\+ table with DataTables + graph with Graph.js) and **Airflow** to orchestrate the data flow.\n\nIn the end I could not include the Cloud part, except for a deployment in a virtual machine (using GCP's Compute Engine) to make it accessible to the evaluation board and which is currently deactivated. However, now that I have finished it I plan to make small extensions in GCP, such as implementing the Data Warehouse or making some visualizations in Big Query, but without focusing so much on the documentation work.\n\nAny feedback on your final impression of this project would be appreciated, as my idea is to try to use it to get a junior DE position in Europe! And enjoy my skills creating gifs with PowerPoint \ud83e\udd23\n\nhttps://i.redd.it/trlt7kqunzz91.gif", "author_fullname": "t2_2wafit96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Master's thesis finished - Thank you", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 52, "top_awarded_type": null, "hide_score": false, "media_metadata": {"trlt7kqunzz91": {"status": "valid", "e": "AnimatedImage", "m": "image/gif", "p": [{"y": 40, "x": 108, "u": "https://preview.redd.it/trlt7kqunzz91.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=dcf9f49fc1dc696cee4bb98bc3ef6a8ae5178019"}, {"y": 80, "x": 216, "u": "https://preview.redd.it/trlt7kqunzz91.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=6d3fbd27de6c495b3bfa89ef937758d8ad51a6c2"}, {"y": 119, "x": 320, "u": "https://preview.redd.it/trlt7kqunzz91.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=4596814c7dfda471353ac416d4da2b15dd9fc897"}, {"y": 238, "x": 640, "u": "https://preview.redd.it/trlt7kqunzz91.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=c52885fdc782f41661b32b886cec46260a825467"}, {"y": 357, "x": 960, "u": "https://preview.redd.it/trlt7kqunzz91.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=a14cc8d1fe20497aa1d14d8646fd4743035b52ec"}, {"y": 402, "x": 1080, "u": "https://preview.redd.it/trlt7kqunzz91.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=b70fe2a12f897cb1d40e894dbe006470ee81cb12"}], "s": {"y": 804, "gif": "https://i.redd.it/trlt7kqunzz91.gif", "mp4": "https://preview.redd.it/trlt7kqunzz91.gif?format=mp4&amp;s=60de0ef64f1ee84c3256d34cd0ff672c666c236f", "x": 2160}, "id": "trlt7kqunzz91"}}, "name": "t3_yve7sf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 116, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 116, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kqLkS9mk6LCtCnH7bpdRpeNBVb5ALLvK9vB_FPRClnY.jpg", "edited": 1668465119.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668463649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! A few months ago I defended my &lt;strong&gt;Master Thesis on Big Data&lt;/strong&gt; and got the maximum grade of 10.0 with honors. I want to thank this subreddit for the help and advice received in one of my previous posts. Also, if you want to build something similar and you think the project can be usefull for you, feel free to ask me for the Github page (I cannot attach it here since it contains my name and I think it is against the PII data community rules).&lt;/p&gt;\n\n&lt;p&gt;As a summary, I built an &lt;strong&gt;ETL process&lt;/strong&gt; to get information about the latest music listened to by &lt;strong&gt;Twitter&lt;/strong&gt; users (by searching for the hashtag #NowPlaying) and then queried &lt;strong&gt;Spotify&lt;/strong&gt; to get the song and artist data involved. I used &lt;strong&gt;Spark&lt;/strong&gt; to run the ETL process, &lt;strong&gt;Cassandra&lt;/strong&gt; to store the data, a custom web application for the final visualization (&lt;strong&gt;Flask&lt;/strong&gt; + table with DataTables + graph with Graph.js) and &lt;strong&gt;Airflow&lt;/strong&gt; to orchestrate the data flow.&lt;/p&gt;\n\n&lt;p&gt;In the end I could not include the Cloud part, except for a deployment in a virtual machine (using GCP&amp;#39;s Compute Engine) to make it accessible to the evaluation board and which is currently deactivated. However, now that I have finished it I plan to make small extensions in GCP, such as implementing the Data Warehouse or making some visualizations in Big Query, but without focusing so much on the documentation work.&lt;/p&gt;\n\n&lt;p&gt;Any feedback on your final impression of this project would be appreciated, as my idea is to try to use it to get a junior DE position in Europe! And enjoy my skills creating gifs with PowerPoint \ud83e\udd23&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/trlt7kqunzz91.gif\"&gt;https://i.redd.it/trlt7kqunzz91.gif&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "yve7sf", "is_robot_indexable": true, "report_reasons": null, "author": "Riesco", "discussion_type": null, "num_comments": 74, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yve7sf/masters_thesis_finished_thank_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yve7sf/masters_thesis_finished_thank_you/", "subreddit_subscribers": 80032, "created_utc": 1668463649.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_31ilk7t8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Faster Queries with Clustering: An introduction to Snowflake's most powerful optimization technique", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "name": "t3_yvgrmn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/gmAGDh8qasJbV_NaOlaaMuFC9crPpuaHwF6q1GhaqPw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668469922.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "select.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://select.dev/posts/introduction-to-snowflake-clustering", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NdzluZq_zdpoaRP0IUHF3qQqarfLS7xrseno1JQnvD0.jpg?auto=webp&amp;s=6835e48dcaaa57428ea3fa0cfbe41a6c20f57d83", "width": 2392, "height": 1014}, "resolutions": [{"url": "https://external-preview.redd.it/NdzluZq_zdpoaRP0IUHF3qQqarfLS7xrseno1JQnvD0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b4fcf770225967050ca3e813bd87683fb72620e", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/NdzluZq_zdpoaRP0IUHF3qQqarfLS7xrseno1JQnvD0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7738828c24c04310079260116cfb1e4148310b33", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/NdzluZq_zdpoaRP0IUHF3qQqarfLS7xrseno1JQnvD0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1563e248fbc5533fa2c58cd48ece7b7b72b5d0c1", "width": 320, "height": 135}, {"url": "https://external-preview.redd.it/NdzluZq_zdpoaRP0IUHF3qQqarfLS7xrseno1JQnvD0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f77f2787fa6b2f9339a66553fc3be295f6741e9", "width": 640, "height": 271}, {"url": "https://external-preview.redd.it/NdzluZq_zdpoaRP0IUHF3qQqarfLS7xrseno1JQnvD0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8671b4f1c957ce5dbded3857fceb2afc5c5611e0", "width": 960, "height": 406}, {"url": "https://external-preview.redd.it/NdzluZq_zdpoaRP0IUHF3qQqarfLS7xrseno1JQnvD0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fcd1b2c8bfb2bef04d268259860896b33c81042", "width": 1080, "height": 457}], "variants": {}, "id": "0QuV0iDdaZz8gZjudQrjgY9B4ptI8JunYztAMYJLa4Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yvgrmn", "is_robot_indexable": true, "report_reasons": null, "author": "ian-whitestone", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvgrmn/faster_queries_with_clustering_an_introduction_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://select.dev/posts/introduction-to-snowflake-clustering", "subreddit_subscribers": 80032, "created_utc": 1668469922.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Not sure who will need this but I built a very simple and small tool to generate PySpark schema from JSON. I built it to solve one of my problem that I was facing. I had to handle complex JSON this way.\n\nThere are few bugs there but ya here it it. [Click here to try!!](https://preetranjan.github.io/pyspark-schema-generator/)\n\nThe github is here, [View on Github](https://github.com/PreetRanjan/pyspark-schema-generator). It has few simple features like JSON format and compare.\n\n&amp;#x200B;\n\n[Screenshot of the project](https://preview.redd.it/4jg90xz3dyz91.png?width=1171&amp;format=png&amp;auto=webp&amp;s=bd3be25f3f8f9f72d82b6d4db4a76baea4fa49b9)", "author_fullname": "t2_r1dyavvd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A PySpark Schema Generator from JSON", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": false, "media_metadata": {"4jg90xz3dyz91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c5e8651d1e80e2a9e8e31e19550398b3f17d62a"}, {"y": 125, "x": 216, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d093adb5b206e4437bf1c7ad4b428630da032fd4"}, {"y": 185, "x": 320, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c044774924e48d401e62232355565c446ade3195"}, {"y": 371, "x": 640, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6afe62b9602814b90f37c564477d61ffdcc122eb"}, {"y": 557, "x": 960, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bee8a6aab5182eb60d4fa806938c0555566422fe"}, {"y": 627, "x": 1080, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f08668259f36fe9a91b6152b19b504d66529d304"}], "s": {"y": 680, "x": 1171, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=1171&amp;format=png&amp;auto=webp&amp;s=bd3be25f3f8f9f72d82b6d4db4a76baea4fa49b9"}, "id": "4jg90xz3dyz91"}}, "name": "t3_yv6lon", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bsXl4bc9J1PhGXcXoYCif1ZVExGpFQ_wVelMWBMk4uw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668447464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure who will need this but I built a very simple and small tool to generate PySpark schema from JSON. I built it to solve one of my problem that I was facing. I had to handle complex JSON this way.&lt;/p&gt;\n\n&lt;p&gt;There are few bugs there but ya here it it. &lt;a href=\"https://preetranjan.github.io/pyspark-schema-generator/\"&gt;Click here to try!!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The github is here, &lt;a href=\"https://github.com/PreetRanjan/pyspark-schema-generator\"&gt;View on Github&lt;/a&gt;. It has few simple features like JSON format and compare.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4jg90xz3dyz91.png?width=1171&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd3be25f3f8f9f72d82b6d4db4a76baea4fa49b9\"&gt;Screenshot of the project&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yv6lon", "is_robot_indexable": true, "report_reasons": null, "author": "RegentSphinx", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv6lon/a_pyspark_schema_generator_from_json/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yv6lon/a_pyspark_schema_generator_from_json/", "subreddit_subscribers": 80032, "created_utc": 1668447464.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When it comes to finding an entry level de job, which skills are absolutely required to know beforehand and which skills are nice to have skills  that you can learn on the job later on?\n\nFrom what I've read Python, SQL are pretty much the only required skills and the rest are stuff you can learn on the job. How true is this?\n\nFor example, if I am comfortable with Python and SQL but only have surface level experience with all the other de specific tools... would they even consider me?\n\nOr will most companies expect me to have prior professional experience with all these de related tools?\n\nI've been working on a few (very simple) projects using Python, pandas, sql, Airflow and tableau mainly and I'm wondering if this is enough to start applying for jobs.\n\nI haven't incorporated any data warehouse, data lake, data streaming or used any cloud providers in my projects mostly because there's too much to learn all at once and I wanted to focus on a specific set of technologies.\n\nI'm hoping that if I can demonstrate my proficiency in Python and sql they will hopefully see that I can pick up the rest on the job.\n\nOr is my thinking all wrong. Please advise. Thanks.", "author_fullname": "t2_yex15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Must know skills vs nice to have skills when job hunting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvhs6v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668472576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When it comes to finding an entry level de job, which skills are absolutely required to know beforehand and which skills are nice to have skills  that you can learn on the job later on?&lt;/p&gt;\n\n&lt;p&gt;From what I&amp;#39;ve read Python, SQL are pretty much the only required skills and the rest are stuff you can learn on the job. How true is this?&lt;/p&gt;\n\n&lt;p&gt;For example, if I am comfortable with Python and SQL but only have surface level experience with all the other de specific tools... would they even consider me?&lt;/p&gt;\n\n&lt;p&gt;Or will most companies expect me to have prior professional experience with all these de related tools?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working on a few (very simple) projects using Python, pandas, sql, Airflow and tableau mainly and I&amp;#39;m wondering if this is enough to start applying for jobs.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t incorporated any data warehouse, data lake, data streaming or used any cloud providers in my projects mostly because there&amp;#39;s too much to learn all at once and I wanted to focus on a specific set of technologies.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping that if I can demonstrate my proficiency in Python and sql they will hopefully see that I can pick up the rest on the job.&lt;/p&gt;\n\n&lt;p&gt;Or is my thinking all wrong. Please advise. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yvhs6v", "is_robot_indexable": true, "report_reasons": null, "author": "desperate-1", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvhs6v/must_know_skills_vs_nice_to_have_skills_when_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvhs6v/must_know_skills_vs_nice_to_have_skills_when_job/", "subreddit_subscribers": 80032, "created_utc": 1668472576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Compaction in Apache Iceberg: Fine-Tuning Your Iceberg Table\u2019s Data Files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 42, "top_awarded_type": null, "hide_score": false, "name": "t3_yv5yp3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rgDtRAuYhvrcxOof1JQwoWaI-fmBHaWJYZgrMpjPUcc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668446199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dremio.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dremio.com/subsurface/compaction-in-apache-iceberg-fine-tuning-your-iceberg-tables-data-files/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yaYNe53o4GMeUwzFdbE8thxQMn_6xNxvYLsd-zbgq2Y.jpg?auto=webp&amp;s=a2c26887f1d6d7a9997b7628f5677260b6b2fed6", "width": 1010, "height": 304}, "resolutions": [{"url": "https://external-preview.redd.it/yaYNe53o4GMeUwzFdbE8thxQMn_6xNxvYLsd-zbgq2Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=178f619f3812bd07fb2145e7d1e4de13e4871887", "width": 108, "height": 32}, {"url": "https://external-preview.redd.it/yaYNe53o4GMeUwzFdbE8thxQMn_6xNxvYLsd-zbgq2Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1530c337ccaf733a9bf6af78c7d40ebbd091973a", "width": 216, "height": 65}, {"url": "https://external-preview.redd.it/yaYNe53o4GMeUwzFdbE8thxQMn_6xNxvYLsd-zbgq2Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a22316ac4710a9fcee1c562fd23d9f4b0db2ba04", "width": 320, "height": 96}, {"url": "https://external-preview.redd.it/yaYNe53o4GMeUwzFdbE8thxQMn_6xNxvYLsd-zbgq2Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=838d3bed9b15b2f67e101dc3478066e7307e3aa9", "width": 640, "height": 192}, {"url": "https://external-preview.redd.it/yaYNe53o4GMeUwzFdbE8thxQMn_6xNxvYLsd-zbgq2Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e7a2fcedcba08b4fd2316420447e0df3cffa8ae", "width": 960, "height": 288}], "variants": {}, "id": "_3QLIej8-EtXQYfSFWVcRZq1EpXgFkyUELW9oAU7w9M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yv5yp3", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv5yp3/compaction_in_apache_iceberg_finetuning_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dremio.com/subsurface/compaction-in-apache-iceberg-fine-tuning-your-iceberg-tables-data-files/", "subreddit_subscribers": 80032, "created_utc": 1668446199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have a good suggestion where to prepare for GCP professional data engineer? What is the best course to take and take on google provided content on the same.", "author_fullname": "t2_9navdxud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP professional data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvair9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668455482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have a good suggestion where to prepare for GCP professional data engineer? What is the best course to take and take on google provided content on the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yvair9", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical-Grade2960", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvair9/gcp_professional_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvair9/gcp_professional_data_engineer/", "subreddit_subscribers": 80032, "created_utc": 1668455482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nI've got job offer from Gartner and Dunnhumby for the role of Data engineer. I've only 1 yoe in this field and I've working knowledge of ADF, Azure Synapse analytics,Databricks and PySpark.\n In Gartner, the work will be purely based on Azure Data stack (ADF, Azure Data Lake, Synapse) whereas in Dunnhumby it'll PySpark, Airflow, GCP, HBase. \n\nNow I don't have proper knowledge but I'm interested in all these.\nCan you please help me decide which would be better company to join considering the tech stack and future of DE.\n\nPS - I'm sorry if this post doesn't belong here but I'm really confused and I needed advice from professionals.", "author_fullname": "t2_6hnbzi4k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which DE job offer should I accept?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvl185", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668481470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got job offer from Gartner and Dunnhumby for the role of Data engineer. I&amp;#39;ve only 1 yoe in this field and I&amp;#39;ve working knowledge of ADF, Azure Synapse analytics,Databricks and PySpark.\n In Gartner, the work will be purely based on Azure Data stack (ADF, Azure Data Lake, Synapse) whereas in Dunnhumby it&amp;#39;ll PySpark, Airflow, GCP, HBase. &lt;/p&gt;\n\n&lt;p&gt;Now I don&amp;#39;t have proper knowledge but I&amp;#39;m interested in all these.\nCan you please help me decide which would be better company to join considering the tech stack and future of DE.&lt;/p&gt;\n\n&lt;p&gt;PS - I&amp;#39;m sorry if this post doesn&amp;#39;t belong here but I&amp;#39;m really confused and I needed advice from professionals.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yvl185", "is_robot_indexable": true, "report_reasons": null, "author": "dipanshusheoran", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvl185/which_de_job_offer_should_i_accept/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvl185/which_de_job_offer_should_i_accept/", "subreddit_subscribers": 80032, "created_utc": 1668481470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHi all,\n\nI'm a data engineering consultant so I start a project at a new customer regularly. I have the technical skills and tools down nicely so the biggest challenge when working with new data is figuring out and remembering what every database, schema, table and column means or is for. Is there a way or tool to document data for myself in a user-friendly manner? Like I can pull automatically or write down any of the objects (db, schema, table, column, column value, constraint, etc) and write my own notes by them, making everything easily findable etc.\n\nI thought about using trello boards per db and a ticket per table or something, writing down column descriptions and general info in there, but I feel like there must be a better way to go about it.\n\nAny tips, tricks and tools are greatly appreciated!", "author_fullname": "t2_5877uyxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Documenting data for personal reference", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvry50", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668504223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a data engineering consultant so I start a project at a new customer regularly. I have the technical skills and tools down nicely so the biggest challenge when working with new data is figuring out and remembering what every database, schema, table and column means or is for. Is there a way or tool to document data for myself in a user-friendly manner? Like I can pull automatically or write down any of the objects (db, schema, table, column, column value, constraint, etc) and write my own notes by them, making everything easily findable etc.&lt;/p&gt;\n\n&lt;p&gt;I thought about using trello boards per db and a ticket per table or something, writing down column descriptions and general info in there, but I feel like there must be a better way to go about it.&lt;/p&gt;\n\n&lt;p&gt;Any tips, tricks and tools are greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yvry50", "is_robot_indexable": true, "report_reasons": null, "author": "v0nm1ll3r", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvry50/documenting_data_for_personal_reference/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvry50/documenting_data_for_personal_reference/", "subreddit_subscribers": 80032, "created_utc": 1668504223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, im new to data engineering and stuff and currently working in project where we are trying to make own ETL framework like fivetran and etc. Currently im stuck with the problem of how can we detect any source database schema change after initial sync. Like if there new column added to the table or column datatype has been changed. Can anyone suggest how to approach this problem.\n\nPs. For now source db is postgresql", "author_fullname": "t2_bpdkk6n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to detect database schema changes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yv9kug", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668453883.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668453543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, im new to data engineering and stuff and currently working in project where we are trying to make own ETL framework like fivetran and etc. Currently im stuck with the problem of how can we detect any source database schema change after initial sync. Like if there new column added to the table or column datatype has been changed. Can anyone suggest how to approach this problem.&lt;/p&gt;\n\n&lt;p&gt;Ps. For now source db is postgresql&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yv9kug", "is_robot_indexable": true, "report_reasons": null, "author": "pakhira55", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv9kug/how_to_detect_database_schema_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yv9kug/how_to_detect_database_schema_changes/", "subreddit_subscribers": 80032, "created_utc": 1668453543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, i have been asked by my project manager to test the queries written by senior Devs acc to the usecases given to them to implement datamarts, currently I have very little knowledge about testing as I have never done it before and some advice from you guys will be really helpful", "author_fullname": "t2_fgjne05b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to test data Marts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yv9270", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668452460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, i have been asked by my project manager to test the queries written by senior Devs acc to the usecases given to them to implement datamarts, currently I have very little knowledge about testing as I have never done it before and some advice from you guys will be really helpful&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yv9270", "is_robot_indexable": true, "report_reasons": null, "author": "NoPreference7274", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv9270/how_to_test_data_marts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yv9270/how_to_test_data_marts/", "subreddit_subscribers": 80032, "created_utc": 1668452460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a new graduate and I am working as a data scientist in a startup for a 8 months. I started as a intern and now I am working full time. Unfortunately, my responsibilities are not exactly data science tasks. I can define it as data analysis and finding data sources for dev team. These tasks doesn't help me to improve my resume I can not develop myself in data field.\n\nActually I am more interested in data engineering and I want to switch to this field. I know Python and SQL but I don't have an expertise on these languages. Everyday I am visiting this subreddit and try to find a path. Some of them says \"SQL and Python is enough for entry level, just start to apply\". Other one says \"you should also build pipeline, make a project\" and maybe sometimes \"you should know dbt, airflow etc.\" so I feel I am lost. I was thinking this path before:\n\n* Improve yourself on SQL and start to solve leetcode.\n* Then learn cloud (aws).\n* After that build a pipeline, ETL project.\n* Now you can start to apply jobs.\n\nWhat do you think, what should I do now? This subreddit is so valuable and your opinions are really helpful for me. Thank you.", "author_fullname": "t2_qmoxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I start applying for jobs? I am lost.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yw0jg3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668526547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a new graduate and I am working as a data scientist in a startup for a 8 months. I started as a intern and now I am working full time. Unfortunately, my responsibilities are not exactly data science tasks. I can define it as data analysis and finding data sources for dev team. These tasks doesn&amp;#39;t help me to improve my resume I can not develop myself in data field.&lt;/p&gt;\n\n&lt;p&gt;Actually I am more interested in data engineering and I want to switch to this field. I know Python and SQL but I don&amp;#39;t have an expertise on these languages. Everyday I am visiting this subreddit and try to find a path. Some of them says &amp;quot;SQL and Python is enough for entry level, just start to apply&amp;quot;. Other one says &amp;quot;you should also build pipeline, make a project&amp;quot; and maybe sometimes &amp;quot;you should know dbt, airflow etc.&amp;quot; so I feel I am lost. I was thinking this path before:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Improve yourself on SQL and start to solve leetcode.&lt;/li&gt;\n&lt;li&gt;Then learn cloud (aws).&lt;/li&gt;\n&lt;li&gt;After that build a pipeline, ETL project.&lt;/li&gt;\n&lt;li&gt;Now you can start to apply jobs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What do you think, what should I do now? This subreddit is so valuable and your opinions are really helpful for me. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yw0jg3", "is_robot_indexable": true, "report_reasons": null, "author": "lost4line", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yw0jg3/should_i_start_applying_for_jobs_i_am_lost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yw0jg3/should_i_start_applying_for_jobs_i_am_lost/", "subreddit_subscribers": 80032, "created_utc": 1668526547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have the task of replicating the data in from an Azure SQL DB and an AWS RDS SQL DB into a Snowflake data warehouse, and I am unsure the most cost effective way to do this.\n\n* Azure SQL DB has 25 GB of data, and grows at about 1 GB per month\n* AWS RDS SQL DB has 1.2 TB of data, and grows at about 5 GB per month\n\nMy task is to get both of these databases fully replicated into the Snowflake instance, and then keep the Snowflake instance updated on a daily cadence for our business intelligence reports.\n\nIs Azure Data Factory a good way to handle this? I don't need to transform the data during replication, but I am finding it difficult to estimate the cost of running the ADF replication pipelines each once per day.\n\nI am also a little confused on if ADF can support just replicating the delta from each source after the initial bulk loading is done into the Snowflake warehouse. I see conflicting information online about ADF's ability to just handle the delta with Snowflake as a sink, and that SnowPipe would need to be used for that instead.\n\nAre there better options out there for this use-case rather than ADF or SnowPipe? Thanks in advance for anyone that can give me some advice.", "author_fullname": "t2_tql2kvxf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to set up replication pipelines from an Azure SQL DB + AWS RDS SQL DB into Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvw546", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668516253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have the task of replicating the data in from an Azure SQL DB and an AWS RDS SQL DB into a Snowflake data warehouse, and I am unsure the most cost effective way to do this.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Azure SQL DB has 25 GB of data, and grows at about 1 GB per month&lt;/li&gt;\n&lt;li&gt;AWS RDS SQL DB has 1.2 TB of data, and grows at about 5 GB per month&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My task is to get both of these databases fully replicated into the Snowflake instance, and then keep the Snowflake instance updated on a daily cadence for our business intelligence reports.&lt;/p&gt;\n\n&lt;p&gt;Is Azure Data Factory a good way to handle this? I don&amp;#39;t need to transform the data during replication, but I am finding it difficult to estimate the cost of running the ADF replication pipelines each once per day.&lt;/p&gt;\n\n&lt;p&gt;I am also a little confused on if ADF can support just replicating the delta from each source after the initial bulk loading is done into the Snowflake warehouse. I see conflicting information online about ADF&amp;#39;s ability to just handle the delta with Snowflake as a sink, and that SnowPipe would need to be used for that instead.&lt;/p&gt;\n\n&lt;p&gt;Are there better options out there for this use-case rather than ADF or SnowPipe? Thanks in advance for anyone that can give me some advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yvw546", "is_robot_indexable": true, "report_reasons": null, "author": "AzureNoob1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvw546/best_way_to_set_up_replication_pipelines_from_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvw546/best_way_to_set_up_replication_pipelines_from_an/", "subreddit_subscribers": 80032, "created_utc": 1668516253.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Relatively new to the space, coming from an SWE role, and am having trouble understanding the process between exploring data and turning it into a production pipeline.\n\nGeneral ETL pipelines make sense to me, but where I am confused is more 'analytics' pipelines -- ones that not only do transformations on data, but also do analysis during their transformations for visualizations, models, etc that require more contextual understanding of the data.\n\nIt is surprising to me that 'data scientists' will do EDA, provide the details via a notebook or something, and then DEs production-ize this after the fact, oftentimes using something entirely different that requires re-writing the query/code/etc (i.e., from pandas to pyspark).  \n\n\nFrom SWE perspective, it seems like it is an error-prone process and redundant.  \n\n\nI am also unclear on who owns new features/modifications from business during this loop.  \n\n\nDoes anyone have good case studies / anecdotal advice on how to unify this process? Or am I misrepresenting the process and it actually is supposed to be set up differently? Any pointers would be greatly appreciated.", "author_fullname": "t2_ub4hlwpr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good Examples of the Data Engineering Lifecycle", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvi2ld", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668473378.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Relatively new to the space, coming from an SWE role, and am having trouble understanding the process between exploring data and turning it into a production pipeline.&lt;/p&gt;\n\n&lt;p&gt;General ETL pipelines make sense to me, but where I am confused is more &amp;#39;analytics&amp;#39; pipelines -- ones that not only do transformations on data, but also do analysis during their transformations for visualizations, models, etc that require more contextual understanding of the data.&lt;/p&gt;\n\n&lt;p&gt;It is surprising to me that &amp;#39;data scientists&amp;#39; will do EDA, provide the details via a notebook or something, and then DEs production-ize this after the fact, oftentimes using something entirely different that requires re-writing the query/code/etc (i.e., from pandas to pyspark).  &lt;/p&gt;\n\n&lt;p&gt;From SWE perspective, it seems like it is an error-prone process and redundant.  &lt;/p&gt;\n\n&lt;p&gt;I am also unclear on who owns new features/modifications from business during this loop.  &lt;/p&gt;\n\n&lt;p&gt;Does anyone have good case studies / anecdotal advice on how to unify this process? Or am I misrepresenting the process and it actually is supposed to be set up differently? Any pointers would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yvi2ld", "is_robot_indexable": true, "report_reasons": null, "author": "No_Bat835", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvi2ld/good_examples_of_the_data_engineering_lifecycle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvi2ld/good_examples_of_the_data_engineering_lifecycle/", "subreddit_subscribers": 80032, "created_utc": 1668473378.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, \n\nA regular ol' SWE making some steps into the data engineering world. I'm liking the lake house architecture, and I'm looking to implement it within my start-up org. For me the ACID compliance and governance model through technologies like Trino, delta lake etc are really appealing to me. \n\nHowever, since I'm really new to this space I'm missing some key factors that relate to my core experience of a regular SWE, and that is gitops. In my mind, nothing that is deployable is not done somehow through a pipeline or some automated action from a git repo. I can't even comprehend doing it any other way anymore. \n\nHowever, from what I've seen, this doesn't seem to be the case in the data lake house world that I've seen. For example something like data bricks just looks like a Jupyter notebook with some loaded delta lake house connectors. Do DE's and DS just write ad hoc scripts here that create tables in the data lake? What's the industry best practice to versioning data engineering work?", "author_fullname": "t2_nsflng", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gitops in a Data lakehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvdydl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668463065.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, &lt;/p&gt;\n\n&lt;p&gt;A regular ol&amp;#39; SWE making some steps into the data engineering world. I&amp;#39;m liking the lake house architecture, and I&amp;#39;m looking to implement it within my start-up org. For me the ACID compliance and governance model through technologies like Trino, delta lake etc are really appealing to me. &lt;/p&gt;\n\n&lt;p&gt;However, since I&amp;#39;m really new to this space I&amp;#39;m missing some key factors that relate to my core experience of a regular SWE, and that is gitops. In my mind, nothing that is deployable is not done somehow through a pipeline or some automated action from a git repo. I can&amp;#39;t even comprehend doing it any other way anymore. &lt;/p&gt;\n\n&lt;p&gt;However, from what I&amp;#39;ve seen, this doesn&amp;#39;t seem to be the case in the data lake house world that I&amp;#39;ve seen. For example something like data bricks just looks like a Jupyter notebook with some loaded delta lake house connectors. Do DE&amp;#39;s and DS just write ad hoc scripts here that create tables in the data lake? What&amp;#39;s the industry best practice to versioning data engineering work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yvdydl", "is_robot_indexable": true, "report_reasons": null, "author": "CatabolicEdo", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvdydl/gitops_in_a_data_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvdydl/gitops_in_a_data_lakehouse/", "subreddit_subscribers": 80032, "created_utc": 1668463065.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dfcot0zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We are doing an on site team building activity for our tech department. Sure you can build robust data systems but can you engineer a structurally sound tower made out of spaghetti and marshmallows? Share your best team building stories.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_yvafx5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.58, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wQMEFYjgjz2PbAbtfMtMfFwWQji0KU5zL4LnU_5jQHY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668455320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/3qnba2u9i00a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?auto=webp&amp;s=ecea0a5a53b258fb24a3699beae6935584442fcf", "width": 2296, "height": 4080}, "resolutions": [{"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a021a716887160ebca3841d4244f77241275a51", "width": 108, "height": 191}, {"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da58ddec931053f7ae385ce79073ceb3a6164e0c", "width": 216, "height": 383}, {"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=53c202ab7c016deb6ffa2909caac13a663bf7546", "width": 320, "height": 568}, {"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce1aff6b77264b7813ace0833adfeb2cc4bbd1d9", "width": 640, "height": 1137}, {"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=965833813defd4538a6898920f331652eeaaa99c", "width": 960, "height": 1705}, {"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=195fa553b27c3f95e4494347c03f8fde779aa53f", "width": 1080, "height": 1919}], "variants": {}, "id": "ld8n5uhR9PLh6A55Rbuy5G_BECXU2C6Dvtn0B-viV48"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yvafx5", "is_robot_indexable": true, "report_reasons": null, "author": "DrRedmondNYC", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvafx5/we_are_doing_an_on_site_team_building_activity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/3qnba2u9i00a1.jpg", "subreddit_subscribers": 80032, "created_utc": 1668455320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dremio Contributes the Arrow Flight SQL JDBC Driver to the Apache Arrow Community \u2013 The Latest of Many Contributions | Dremio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_yv4z3z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/VzybhJXLFA8l35ZZiFK_kRskUhsSQIe6puq46uqnwxk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668444257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dremio.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dremio.com/blog/dremio-contributes-the-arrow-flight-sql-jdbc-driver-to-the-apache-arrow-community-the-latest-of-many-contributions/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?auto=webp&amp;s=6e6cd51218e5daa44fda46e871254d67b4f0b028", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3ff4e8f3ca79122c80a3edca5e76f391dbe6bdfe", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=33960553c96829ccee2547f8ab406130247c7384", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df3787d2c0c0fd87c0515f59c003c36736a729ec", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f2078e40e00eed2276fcc88816b7ba1fc0b1e1f", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ce6f95c31b9473961006a3d143d79d7513af6c5", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=511268e51ae4130135da26f6139fe780805c37b2", "width": 1080, "height": 565}], "variants": {}, "id": "dwgv-bjmtKMXlgmwTW_CSzpXGhwr6pO9hwIr945P_3g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yv4z3z", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv4z3z/dremio_contributes_the_arrow_flight_sql_jdbc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dremio.com/blog/dremio-contributes-the-arrow-flight-sql-jdbc-driver-to-the-apache-arrow-community-the-latest-of-many-contributions/", "subreddit_subscribers": 80032, "created_utc": 1668444257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Everyone, \n\nI am currently working on a project (personal but hoping to commercialize it at some point). The idea is a website allowing the user to upload data in the form of CSV/Excel and the website would basically generate a full dashboard containing several graphs analyzing the data. Eventually, I would want to export those dashboards as pdfs or emaemailsil. \n\nThe data extraction part I am ok with.  I am able to build the data parsing part easily with Pandas . I am looking now for the best way to generate dashboards on the web app (based on the result of the processing done in python ). \n\nI had a quick look at the possibilities and I found Streamlit and Dash as two options. They seemed however more adapted to a scenario of a company building its own BI system than a service website. My assumption is based on a quick research and gut feeling so please correct me if I am wrong \n\nAny suggestions for tools and libraries? \n\nThanks in advance", "author_fullname": "t2_kzkph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help need to pick the most adapted dashboard-generating tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yw1ap6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668528068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone, &lt;/p&gt;\n\n&lt;p&gt;I am currently working on a project (personal but hoping to commercialize it at some point). The idea is a website allowing the user to upload data in the form of CSV/Excel and the website would basically generate a full dashboard containing several graphs analyzing the data. Eventually, I would want to export those dashboards as pdfs or emaemailsil. &lt;/p&gt;\n\n&lt;p&gt;The data extraction part I am ok with.  I am able to build the data parsing part easily with Pandas . I am looking now for the best way to generate dashboards on the web app (based on the result of the processing done in python ). &lt;/p&gt;\n\n&lt;p&gt;I had a quick look at the possibilities and I found Streamlit and Dash as two options. They seemed however more adapted to a scenario of a company building its own BI system than a service website. My assumption is based on a quick research and gut feeling so please correct me if I am wrong &lt;/p&gt;\n\n&lt;p&gt;Any suggestions for tools and libraries? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yw1ap6", "is_robot_indexable": true, "report_reasons": null, "author": "mkhalil77", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yw1ap6/help_need_to_pick_the_most_adapted/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yw1ap6/help_need_to_pick_the_most_adapted/", "subreddit_subscribers": 80032, "created_utc": 1668528068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_wxj1rz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How vectorization improves database performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_yw0z51", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1_9V4zWdYJ3f5OrlMGMsnDDtjSBVUY5oJRVULRe_Gxw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668527414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "infoworld.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.infoworld.com/article/3678300/how-vectorization-improves-database-performance.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/r-UvBJGka4fePEWCkcar0bpX3ehruSP-JFBJl1yMb0g.jpg?auto=webp&amp;s=2de46562324f1234705ff004863ac441380bac90", "width": 1200, "height": 1231}, "resolutions": [{"url": "https://external-preview.redd.it/r-UvBJGka4fePEWCkcar0bpX3ehruSP-JFBJl1yMb0g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1d895959685c3071284a787c1c65c1c9ad04f16", "width": 108, "height": 110}, {"url": "https://external-preview.redd.it/r-UvBJGka4fePEWCkcar0bpX3ehruSP-JFBJl1yMb0g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=755bc79f8c465ef4017f8418baf23965dee9a37b", "width": 216, "height": 221}, {"url": "https://external-preview.redd.it/r-UvBJGka4fePEWCkcar0bpX3ehruSP-JFBJl1yMb0g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=047c69df98d016ed9e54f397f810b87cadc54251", "width": 320, "height": 328}, {"url": "https://external-preview.redd.it/r-UvBJGka4fePEWCkcar0bpX3ehruSP-JFBJl1yMb0g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e844915eea1f1cc02fed432baeeff3e39f59133d", "width": 640, "height": 656}, {"url": "https://external-preview.redd.it/r-UvBJGka4fePEWCkcar0bpX3ehruSP-JFBJl1yMb0g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d7ae9ae66c7aaa01f6dd78eef1d6b05ac42461a", "width": 960, "height": 984}, {"url": "https://external-preview.redd.it/r-UvBJGka4fePEWCkcar0bpX3ehruSP-JFBJl1yMb0g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6cfbb1d51a30e2591e2ba6f4f288afc00e729311", "width": 1080, "height": 1107}], "variants": {}, "id": "PSkYnX0bO_VeNWvfjdFzn2fYuF2JR9dne_Zc-Km4668"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "yw0z51", "is_robot_indexable": true, "report_reasons": null, "author": "lkang5280", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yw0z51/how_vectorization_improves_database_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.infoworld.com/article/3678300/how-vectorization-improves-database-performance.html", "subreddit_subscribers": 80032, "created_utc": 1668527414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI recently faced a decision I had to make at work whether to use asyncio or concurrent.futures ThreadPoolExecutor in order to do some concurrent fetching from a database for a backend service. (~30 fetching jobs, small and quick queries).\n\nI did some research online and found some very different opinions about these two approaches and couldn't quite decide what's best for this use case. \n\nThat had me wondering which approach do Data Engineers tend to use with such problems?", "author_fullname": "t2_bikhahe4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python concurrency for Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yw06ry", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668525781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently faced a decision I had to make at work whether to use asyncio or concurrent.futures ThreadPoolExecutor in order to do some concurrent fetching from a database for a backend service. (~30 fetching jobs, small and quick queries).&lt;/p&gt;\n\n&lt;p&gt;I did some research online and found some very different opinions about these two approaches and couldn&amp;#39;t quite decide what&amp;#39;s best for this use case. &lt;/p&gt;\n\n&lt;p&gt;That had me wondering which approach do Data Engineers tend to use with such problems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yw06ry", "is_robot_indexable": true, "report_reasons": null, "author": "ConsistentAd1477", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yw06ry/python_concurrency_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yw06ry/python_concurrency_for_data_engineers/", "subreddit_subscribers": 80032, "created_utc": 1668525781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an idea for a web application project that I want to build and I wanted to get peoples ideas on what the best approach should be. \n\nThe problem: A podcast I listen to doesn't name each podcast with a descriptive title. Each title is just a random phrase unrelated to description of the podcast. So it's difficult to go back and find a podcast or recommend an episode to a friend if you can't remember the title. \n\nSolution: I'd like to build a web application which consists of a search bar and has a list of episodes underneath the search bar. As you type into the search bar the list of episodes underneath gets filtered down. \n\nMy approach: I'm thinking that the first step would be to scrape the data and create a table with the title as one column and the episode description as the second column. Or potentially break the episode description into a number of tags and link the episode titles to their tags. \n\nHelp: After that I'm a bit lost. How should I best create these tables. How can I filter a list based on text in the search bar. Any ideas on this are appreciated. I'm sure this had also been done before so if there's a name for what I'm trying to do or specific learning resources please send them my way.", "author_fullname": "t2_303xrahc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to design a list which filters based on text in a search bar", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvyvat", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668522639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an idea for a web application project that I want to build and I wanted to get peoples ideas on what the best approach should be. &lt;/p&gt;\n\n&lt;p&gt;The problem: A podcast I listen to doesn&amp;#39;t name each podcast with a descriptive title. Each title is just a random phrase unrelated to description of the podcast. So it&amp;#39;s difficult to go back and find a podcast or recommend an episode to a friend if you can&amp;#39;t remember the title. &lt;/p&gt;\n\n&lt;p&gt;Solution: I&amp;#39;d like to build a web application which consists of a search bar and has a list of episodes underneath the search bar. As you type into the search bar the list of episodes underneath gets filtered down. &lt;/p&gt;\n\n&lt;p&gt;My approach: I&amp;#39;m thinking that the first step would be to scrape the data and create a table with the title as one column and the episode description as the second column. Or potentially break the episode description into a number of tags and link the episode titles to their tags. &lt;/p&gt;\n\n&lt;p&gt;Help: After that I&amp;#39;m a bit lost. How should I best create these tables. How can I filter a list based on text in the search bar. Any ideas on this are appreciated. I&amp;#39;m sure this had also been done before so if there&amp;#39;s a name for what I&amp;#39;m trying to do or specific learning resources please send them my way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yvyvat", "is_robot_indexable": true, "report_reasons": null, "author": "CorktoBoston2020", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvyvat/how_to_design_a_list_which_filters_based_on_text/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvyvat/how_to_design_a_list_which_filters_based_on_text/", "subreddit_subscribers": 80032, "created_utc": 1668522639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Everyone,   \n\n\nI joined a grad program for a company in Europe and they have put me as a Data Engineer. I have no idea of the responsibilities of a **Graduate Data Engineer**. All I understand is that the company uses Python and Google Cloud. **They asked me to upskill on PySpark, Kafka and Nifi** and I am scared because these aren't something I ever worked on during my study. Could someone help me from embarrasing myself at work and guide me in the right direction? I'm here because there is no mentoring or support group and they have not put me on a project. \n\nThanks in advance folks.", "author_fullname": "t2_l01nit35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Started as a Data Engineer with No Experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvu5e1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668510784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone,   &lt;/p&gt;\n\n&lt;p&gt;I joined a grad program for a company in Europe and they have put me as a Data Engineer. I have no idea of the responsibilities of a &lt;strong&gt;Graduate Data Engineer&lt;/strong&gt;. All I understand is that the company uses Python and Google Cloud. &lt;strong&gt;They asked me to upskill on PySpark, Kafka and Nifi&lt;/strong&gt; and I am scared because these aren&amp;#39;t something I ever worked on during my study. Could someone help me from embarrasing myself at work and guide me in the right direction? I&amp;#39;m here because there is no mentoring or support group and they have not put me on a project. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance folks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yvu5e1", "is_robot_indexable": true, "report_reasons": null, "author": "being_outlier", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvu5e1/started_as_a_data_engineer_with_no_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvu5e1/started_as_a_data_engineer_with_no_experience/", "subreddit_subscribers": 80032, "created_utc": 1668510784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was hired as a BI data engineer. Shortly after starting the role they decided to replace the entire business application stack. The company data maturity is nowhere near a reportable state. As a result my role was moved to the service engineering team to so analytics on log and performance data. I have now been put on a project to develop an analytics and reporting system for webpage performance using IIS logs. Naturally I want to use python, pandas, databricks, spark. Will this be appropriate for log analytics? \n\nThe company is averse to any work done in python, and they are pushing me into adopting C# based methods. They are also averse to using powerbi and want everything done in kibana. This is not what I envisioned for my career, if I stop using mainstream data engineering methods and tools I will struggle to get a data engineering job elsewhere in future.\n\nMy questions to you: is elasticsearch and kibana the best solution for analysing IIS logs? Could python/spark solutions be an appropriate solution? Does working with systems log data in solutions like elasticsearch and kibana still fall in the data engineering domain or am I shooting myself in the foot?\n\nI have to choose whats best for the business case but I don't know if it's going to be good for my career. I love log data far more than finance and sales data but I don't know if building monitors in c# and building kibana dashboards is data engineering anymore.", "author_fullname": "t2_r7bmwiee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IIS Log Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvt1sn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668507495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was hired as a BI data engineer. Shortly after starting the role they decided to replace the entire business application stack. The company data maturity is nowhere near a reportable state. As a result my role was moved to the service engineering team to so analytics on log and performance data. I have now been put on a project to develop an analytics and reporting system for webpage performance using IIS logs. Naturally I want to use python, pandas, databricks, spark. Will this be appropriate for log analytics? &lt;/p&gt;\n\n&lt;p&gt;The company is averse to any work done in python, and they are pushing me into adopting C# based methods. They are also averse to using powerbi and want everything done in kibana. This is not what I envisioned for my career, if I stop using mainstream data engineering methods and tools I will struggle to get a data engineering job elsewhere in future.&lt;/p&gt;\n\n&lt;p&gt;My questions to you: is elasticsearch and kibana the best solution for analysing IIS logs? Could python/spark solutions be an appropriate solution? Does working with systems log data in solutions like elasticsearch and kibana still fall in the data engineering domain or am I shooting myself in the foot?&lt;/p&gt;\n\n&lt;p&gt;I have to choose whats best for the business case but I don&amp;#39;t know if it&amp;#39;s going to be good for my career. I love log data far more than finance and sales data but I don&amp;#39;t know if building monitors in c# and building kibana dashboards is data engineering anymore.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yvt1sn", "is_robot_indexable": true, "report_reasons": null, "author": "va1kyrja-kara", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvt1sn/iis_log_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvt1sn/iis_log_analytics/", "subreddit_subscribers": 80032, "created_utc": 1668507495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How can we calculate credit used by Particular user or Role when warehouse is being shared between them.", "author_fullname": "t2_94sgtt7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Credit Usage per User/Role ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvpp3j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668496090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How can we calculate credit used by Particular user or Role when warehouse is being shared between them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yvpp3j", "is_robot_indexable": true, "report_reasons": null, "author": "manish__tomar", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvpp3j/snowflake_credit_usage_per_userrole/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvpp3j/snowflake_credit_usage_per_userrole/", "subreddit_subscribers": 80032, "created_utc": 1668496090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Relatively new to the space, coming from an SWE/Ops role, and am having trouble understanding the gap between the process and tooling for data scientists exploring data and DEs turning it into a production pipeline.\n\nIt is surprising to me that data scientists and BAs will do EDA, provide vague details via a notebook or something, and then DEs 'production-ize' this after the fact, oftentimes using something entirely different that requires re-writing and optimizing the query/code/etc (i.e., from pandas to spark).\n\nFrom SWE perspective, it seems like it is an error-prone process and redundant, and I haven't been able to find anything highlighting how this is generally handled.\n\nDoes anyone have good case studies / anecdotal advice on how this process is supposed to work? Any good examples of how DEs and DSs work together in your experience?  Any pointers would be greatly appreciated.", "author_fullname": "t2_hkw0srb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good examples of the DE Lifecycle and synergy between DE/DS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvkryj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668480753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Relatively new to the space, coming from an SWE/Ops role, and am having trouble understanding the gap between the process and tooling for data scientists exploring data and DEs turning it into a production pipeline.&lt;/p&gt;\n\n&lt;p&gt;It is surprising to me that data scientists and BAs will do EDA, provide vague details via a notebook or something, and then DEs &amp;#39;production-ize&amp;#39; this after the fact, oftentimes using something entirely different that requires re-writing and optimizing the query/code/etc (i.e., from pandas to spark).&lt;/p&gt;\n\n&lt;p&gt;From SWE perspective, it seems like it is an error-prone process and redundant, and I haven&amp;#39;t been able to find anything highlighting how this is generally handled.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have good case studies / anecdotal advice on how this process is supposed to work? Any good examples of how DEs and DSs work together in your experience?  Any pointers would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yvkryj", "is_robot_indexable": true, "report_reasons": null, "author": "suralo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvkryj/any_good_examples_of_the_de_lifecycle_and_synergy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvkryj/any_good_examples_of_the_de_lifecycle_and_synergy/", "subreddit_subscribers": 80032, "created_utc": 1668480753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to request for some aws access/permssions from the tech team. What permissions/access can I request. \nI would like to use redshift, aurora mysql, lambda, airflow.\n\nI have always used okta and things are pretty set up in my old job.\n\nSo what should I request for?", "author_fullname": "t2_3w9dfvgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What access &amp; Permissions do I need on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvcz5r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668460770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to request for some aws access/permssions from the tech team. What permissions/access can I request. \nI would like to use redshift, aurora mysql, lambda, airflow.&lt;/p&gt;\n\n&lt;p&gt;I have always used okta and things are pretty set up in my old job.&lt;/p&gt;\n\n&lt;p&gt;So what should I request for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yvcz5r", "is_robot_indexable": true, "report_reasons": null, "author": "mrmilata", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvcz5r/what_access_permissions_do_i_need_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvcz5r/what_access_permissions_do_i_need_on_aws/", "subreddit_subscribers": 80032, "created_utc": 1668460770.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}