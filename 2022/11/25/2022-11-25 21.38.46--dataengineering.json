{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for a large organization and share data permissions that 1,700 people in the org also have. I recently ran a query while investigating a dataset that returned sensitive information to the org. I was unaware of what it would return, stopped once I realized what it was, but am worried about the fallout. If it was an accident am I at risk of any repercussions?", "author_fullname": "t2_38elj5k6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Saw data I shouldn\u2019t have seen", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z3xgak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 78, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 78, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669331516.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a large organization and share data permissions that 1,700 people in the org also have. I recently ran a query while investigating a dataset that returned sensitive information to the org. I was unaware of what it would return, stopped once I realized what it was, but am worried about the fallout. If it was an accident am I at risk of any repercussions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z3xgak", "is_robot_indexable": true, "report_reasons": null, "author": "afdsrewtg", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z3xgak/saw_data_i_shouldnt_have_seen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z3xgak/saw_data_i_shouldnt_have_seen/", "subreddit_subscribers": 80934, "created_utc": 1669331516.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently a Java developer for almost 3 years. I like my job but I don't like my current situation with my company. So now I am job hunting applying for Java dev roles. However, a friend of mine suggested that they will refer me into an entry level Data Engineering role. I have a prior knowledge with Machine Learning and Statistics as it was implemented in my undergraduate thesis. I still have to have an interview but I want to be well informed prior the process. What should I expect? Is it a good career move to dabble into the DE field?\n\n&amp;#x200B;\n\n Enlighten me. Thank you!", "author_fullname": "t2_c58xvng2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Shift To Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z4bn7z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669377499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a Java developer for almost 3 years. I like my job but I don&amp;#39;t like my current situation with my company. So now I am job hunting applying for Java dev roles. However, a friend of mine suggested that they will refer me into an entry level Data Engineering role. I have a prior knowledge with Machine Learning and Statistics as it was implemented in my undergraduate thesis. I still have to have an interview but I want to be well informed prior the process. What should I expect? Is it a good career move to dabble into the DE field?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Enlighten me. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z4bn7z", "is_robot_indexable": true, "report_reasons": null, "author": "dehydratedcatnip", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z4bn7z/career_shift_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z4bn7z/career_shift_to_data_engineering/", "subreddit_subscribers": 80934, "created_utc": 1669377499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I live in europe and am job searching and getting some offers however the salaries here are really anywhere form 1/2 to 1/4 of what american salaries are. Instead of asking r/remotework or r/digitalnomad i figured I'd ask here for perhaps more precision. (also feel a more 'familial' atmosphere here) Anybody working a remote american job in europe? Any input/ideas?\n\n&amp;#x200B;\n\nedit: i have US citizenship", "author_fullname": "t2_2pxsf0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "remote working a US job in Europe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z4e5rh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669389400.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669385043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I live in europe and am job searching and getting some offers however the salaries here are really anywhere form 1/2 to 1/4 of what american salaries are. Instead of asking &lt;a href=\"/r/remotework\"&gt;r/remotework&lt;/a&gt; or &lt;a href=\"/r/digitalnomad\"&gt;r/digitalnomad&lt;/a&gt; i figured I&amp;#39;d ask here for perhaps more precision. (also feel a more &amp;#39;familial&amp;#39; atmosphere here) Anybody working a remote american job in europe? Any input/ideas?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;edit: i have US citizenship&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z4e5rh", "is_robot_indexable": true, "report_reasons": null, "author": "blue_trains_", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z4e5rh/remote_working_a_us_job_in_europe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z4e5rh/remote_working_a_us_job_in_europe/", "subreddit_subscribers": 80934, "created_utc": 1669385043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I have been assigned to do a data lake update based off of tables' changes from source system, the changes are fairly simple i.e. column length changes + few additional columns for about \\~100 tables, for the most part it's a pretty straight forward task i.e. truncate existing tables and then do schema changes and then repopulate everything from source, all of this can be done within a few hours, however, we have a couple of tables that are terribly huge i.e. 2 Billion to 5 billion records and they are a huge pain in the ass, I mean even with truncate and reload, they would take atleast a few days, just wanted to ask everyone how do you manage these schema changes? \n\nFor a few reasons, creating a new table and dumping everything from the old table to new table is not an option.\n\n  \nEdit 1: For reference,\n\nBefore:\n\n|A (varchar(10))|B (int)|C (decimal (18,2))|\n|:-|:-|:-|\n||||\n\nAfter:\n\n|A (varchar(20))|B (int)|X (int)|Y (bit)|C (decimal (18,4))|\n|:-|:-|:-|:-|:-|\n||||||\n\n  \nEdit 2: We are using SQL Server 2017", "author_fullname": "t2_2xpdiv8k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage schema changes for extremely large tables i.e. Billion row tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z3zzla", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669345533.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669338901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I have been assigned to do a data lake update based off of tables&amp;#39; changes from source system, the changes are fairly simple i.e. column length changes + few additional columns for about ~100 tables, for the most part it&amp;#39;s a pretty straight forward task i.e. truncate existing tables and then do schema changes and then repopulate everything from source, all of this can be done within a few hours, however, we have a couple of tables that are terribly huge i.e. 2 Billion to 5 billion records and they are a huge pain in the ass, I mean even with truncate and reload, they would take atleast a few days, just wanted to ask everyone how do you manage these schema changes? &lt;/p&gt;\n\n&lt;p&gt;For a few reasons, creating a new table and dumping everything from the old table to new table is not an option.&lt;/p&gt;\n\n&lt;p&gt;Edit 1: For reference,&lt;/p&gt;\n\n&lt;p&gt;Before:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;A (varchar(10))&lt;/th&gt;\n&lt;th align=\"left\"&gt;B (int)&lt;/th&gt;\n&lt;th align=\"left\"&gt;C (decimal (18,2))&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;After:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;A (varchar(20))&lt;/th&gt;\n&lt;th align=\"left\"&gt;B (int)&lt;/th&gt;\n&lt;th align=\"left\"&gt;X (int)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Y (bit)&lt;/th&gt;\n&lt;th align=\"left\"&gt;C (decimal (18,4))&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Edit 2: We are using SQL Server 2017&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z3zzla", "is_robot_indexable": true, "report_reasons": null, "author": "_whitezetsu", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z3zzla/how_do_you_manage_schema_changes_for_extremely/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z3zzla/how_do_you_manage_schema_changes_for_extremely/", "subreddit_subscribers": 80934, "created_utc": 1669338901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, so i have a databricks jobs in production, its an ML task, which takes as parameters different values, im using dbx, does anyone have experience how should i trigger job with different parameters in parallel,since its same logic and everything within a code, its just different set of some parameters.", "author_fullname": "t2_sx1wry60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Launch databricks jobs with different set of parameters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z4d45w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669382070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, so i have a databricks jobs in production, its an ML task, which takes as parameters different values, im using dbx, does anyone have experience how should i trigger job with different parameters in parallel,since its same logic and everything within a code, its just different set of some parameters.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z4d45w", "is_robot_indexable": true, "report_reasons": null, "author": "AcceptableProcess772", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z4d45w/launch_databricks_jobs_with_different_set_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z4d45w/launch_databricks_jobs_with_different_set_of/", "subreddit_subscribers": 80934, "created_utc": 1669382070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is for snowflake specifically, but for best practices it could be any warehouse really  \n\nApologies because I'm aware this is a basic question. Sadly when I search for anything like \"API data to [warehouse name, e.g. snowflake] I end up with a hell of a lot of spam in the search, many sponsered returns from no code tools like Hevo, or Snowflake API docs (SnowPipe) which seem more orientated to either data located on a server/local and using the SnowPipe API to load it.\n\nSince the data I have is accessible only by API, I need to call the APIs and store the response (JSON) somewhere, then load that into Snowflake. Since the end destination is snowflake (in JSON format), I'm wondering if it's bad practice/possible to add the API response directly into the data warehouse and skip the step where the response file is saved somewhere else first (e.g. Mongo, locally or a directory on a server elsewhere)\n\nGiven the lack of info online, I'm sensing it is bad practice to do this and I just wanted to check why exactly (e.g. schema related, corrupt files etc), and if not, what is the best way to implement it\n\nFor clarity, how I'd do this myself would be:\n\n1) Call API and save response locally (local for now as just testing, though might be in a VM dir)\n\n2) load these files into a snowflake staging area\n\n3) process the files in Snowflake into a table\n\nwith steps 1-2 in an airflow dag, maybe 3 also but haven't used airflow+snowflake yet so not sure", "author_fullname": "t2_1w1o79i7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice for getting data from A to B (Snowflake) via API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z4dih5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669383499.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669383217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is for snowflake specifically, but for best practices it could be any warehouse really  &lt;/p&gt;\n\n&lt;p&gt;Apologies because I&amp;#39;m aware this is a basic question. Sadly when I search for anything like &amp;quot;API data to [warehouse name, e.g. snowflake] I end up with a hell of a lot of spam in the search, many sponsered returns from no code tools like Hevo, or Snowflake API docs (SnowPipe) which seem more orientated to either data located on a server/local and using the SnowPipe API to load it.&lt;/p&gt;\n\n&lt;p&gt;Since the data I have is accessible only by API, I need to call the APIs and store the response (JSON) somewhere, then load that into Snowflake. Since the end destination is snowflake (in JSON format), I&amp;#39;m wondering if it&amp;#39;s bad practice/possible to add the API response directly into the data warehouse and skip the step where the response file is saved somewhere else first (e.g. Mongo, locally or a directory on a server elsewhere)&lt;/p&gt;\n\n&lt;p&gt;Given the lack of info online, I&amp;#39;m sensing it is bad practice to do this and I just wanted to check why exactly (e.g. schema related, corrupt files etc), and if not, what is the best way to implement it&lt;/p&gt;\n\n&lt;p&gt;For clarity, how I&amp;#39;d do this myself would be:&lt;/p&gt;\n\n&lt;p&gt;1) Call API and save response locally (local for now as just testing, though might be in a VM dir)&lt;/p&gt;\n\n&lt;p&gt;2) load these files into a snowflake staging area&lt;/p&gt;\n\n&lt;p&gt;3) process the files in Snowflake into a table&lt;/p&gt;\n\n&lt;p&gt;with steps 1-2 in an airflow dag, maybe 3 also but haven&amp;#39;t used airflow+snowflake yet so not sure&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z4dih5", "is_robot_indexable": true, "report_reasons": null, "author": "tea_horse", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z4dih5/best_practice_for_getting_data_from_a_to_b/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z4dih5/best_practice_for_getting_data_from_a_to_b/", "subreddit_subscribers": 80934, "created_utc": 1669383217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, currently I'm working as full stack developer at a company, but I've always shown interest in data engineer and how to apply data to help on decision making, so in the last couple of months my boss put me in charge of building a data platform for the company, we have a data warehouse on postgresql and some jobs running on lambda and AWS glue for ETL. I have an internship experience with data engineering and I kept studying it since then, but right now I feel that I have to search more knowledge on the steps to build a data platform and how to deal with the business side of with as well, how to talk to the stakeholders to understand their needs and how to approach data problems. So I'm search for some courses that could help me with that, do you guys have any recommendation?", "author_fullname": "t2_4fuoa0ic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Courses recommendation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z4ckz0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669380476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, currently I&amp;#39;m working as full stack developer at a company, but I&amp;#39;ve always shown interest in data engineer and how to apply data to help on decision making, so in the last couple of months my boss put me in charge of building a data platform for the company, we have a data warehouse on postgresql and some jobs running on lambda and AWS glue for ETL. I have an internship experience with data engineering and I kept studying it since then, but right now I feel that I have to search more knowledge on the steps to build a data platform and how to deal with the business side of with as well, how to talk to the stakeholders to understand their needs and how to approach data problems. So I&amp;#39;m search for some courses that could help me with that, do you guys have any recommendation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z4ckz0", "is_robot_indexable": true, "report_reasons": null, "author": "comediann", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z4ckz0/courses_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z4ckz0/courses_recommendation/", "subreddit_subscribers": 80934, "created_utc": 1669380476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello there, kind data engineering strangers! Various types of ELT tools are on the market these days. Maybe through a poll we can get a quantitative view on this community's preferences.\n\nThere is room for a more nuanced discussion about each approach in the comment section. E.g. 'under these and these circumstances I'd use approach X, but usually I prefer Y'.\n\nSo what approach do you think would work best for your team?\n\n[View Poll](https://www.reddit.com/poll/z4a102)", "author_fullname": "t2_714ba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What kind of ELT tool approach/paradigm would work best for your team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z4a102", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669371773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there, kind data engineering strangers! Various types of ELT tools are on the market these days. Maybe through a poll we can get a quantitative view on this community&amp;#39;s preferences.&lt;/p&gt;\n\n&lt;p&gt;There is room for a more nuanced discussion about each approach in the comment section. E.g. &amp;#39;under these and these circumstances I&amp;#39;d use approach X, but usually I prefer Y&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;So what approach do you think would work best for your team?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/z4a102\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z4a102", "is_robot_indexable": true, "report_reasons": null, "author": "ricklamers", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1669630973127, "options": [{"text": "Python decorators for DAGs, local development (e.g. VS Code) and execution, deploy serverless with CLI", "id": "20013940"}, {"text": "UI editor for DAGs, in-browser coding (e.g. Databricks notebooks), deploy serverless with UI clicks", "id": "20013941"}, {"text": "YAML based DAGs, local development, no local execution, deploy serverless with CLI", "id": "20013942"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 111, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z4a102/what_kind_of_elt_tool_approachparadigm_would_work/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/z4a102/what_kind_of_elt_tool_approachparadigm_would_work/", "subreddit_subscribers": 80934, "created_utc": 1669371773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Assuming you can do what you need to do in both, which is your go-to method?\n\n[View Poll](https://www.reddit.com/poll/z4662h)", "author_fullname": "t2_6ae9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is your preferred method of filtering/cleaning data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z4662h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669357895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Assuming you can do what you need to do in both, which is your go-to method?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/z4662h\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z4662h", "is_robot_indexable": true, "report_reasons": null, "author": "gerdes88", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1669530695874, "options": [{"text": "Built-in functions in spart, pandas etc.", "id": "20012155"}, {"text": "Regex", "id": "20012156"}, {"text": "Others", "id": "20012157"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 351, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z4662h/what_is_your_preferred_method_of/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/z4662h/what_is_your_preferred_method_of/", "subreddit_subscribers": 80934, "created_utc": 1669357895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nQuick question here. One of our vendors has an on-prem SQL database that they are using snapshot replication to keep my company's Azure SQL database updated.\n\nI need to set up ETL pipelines from our Azure SQL DB into a data warehouse, and I was unsure on if we enabled CDC on the Azure SQL database that it would keep track of the delta between the last two snapshot replications, or rather CDC would just have the full data insert each time. If it doesn't track the delta only, then our ETL pipelines would get quite costly.\n\nWould we need to use transactional replication here or am I misunderstanding something? Thanks.", "author_fullname": "t2_tql2kvxf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does CDC take the delta when using SSMS snapshot replication?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z47wtq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669363934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Quick question here. One of our vendors has an on-prem SQL database that they are using snapshot replication to keep my company&amp;#39;s Azure SQL database updated.&lt;/p&gt;\n\n&lt;p&gt;I need to set up ETL pipelines from our Azure SQL DB into a data warehouse, and I was unsure on if we enabled CDC on the Azure SQL database that it would keep track of the delta between the last two snapshot replications, or rather CDC would just have the full data insert each time. If it doesn&amp;#39;t track the delta only, then our ETL pipelines would get quite costly.&lt;/p&gt;\n\n&lt;p&gt;Would we need to use transactional replication here or am I misunderstanding something? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z47wtq", "is_robot_indexable": true, "report_reasons": null, "author": "AzureNoob1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z47wtq/does_cdc_take_the_delta_when_using_ssms_snapshot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z47wtq/does_cdc_take_the_delta_when_using_ssms_snapshot/", "subreddit_subscribers": 80934, "created_utc": 1669363934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let\u2019s say you have rows of aggregated data, so there are category fields and the a sum field. You want to reassign a portion of each record. Maybe you want 25% of each sum to be reassigned to another record. \n\nYou could split each record into two records and add a proration field which has 75% next to the first row and 25% next to the second row. Then you could change values in the 25% records however you wanted, or add fields for each proration.  Maybe you do this by joining the table to another table which has two records for each PK in the first table, and the proration, and then the information linked to each proration. \n\nI\u2019m sure there are many variations one could do, but I\u2019m wondering if this is so common it has a basic name I\u2019m not thinking of. \u201cSplit each row by proration.\u201d  Is that a thing, and is there a term for it?", "author_fullname": "t2_1nhgn5i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you call a process that splits/bifurcates records by percentage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z40rg8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669341231.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let\u2019s say you have rows of aggregated data, so there are category fields and the a sum field. You want to reassign a portion of each record. Maybe you want 25% of each sum to be reassigned to another record. &lt;/p&gt;\n\n&lt;p&gt;You could split each record into two records and add a proration field which has 75% next to the first row and 25% next to the second row. Then you could change values in the 25% records however you wanted, or add fields for each proration.  Maybe you do this by joining the table to another table which has two records for each PK in the first table, and the proration, and then the information linked to each proration. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m sure there are many variations one could do, but I\u2019m wondering if this is so common it has a basic name I\u2019m not thinking of. \u201cSplit each row by proration.\u201d  Is that a thing, and is there a term for it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z40rg8", "is_robot_indexable": true, "report_reasons": null, "author": "eerilyweird", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z40rg8/what_do_you_call_a_process_that_splitsbifurcates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z40rg8/what_do_you_call_a_process_that_splitsbifurcates/", "subreddit_subscribers": 80934, "created_utc": 1669341231.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it best practice to convert all datatype into strings during extraction and then convert back to its proper datatype during loading in data warehouse?", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Type after extraction", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z4jca1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669398242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it best practice to convert all datatype into strings during extraction and then convert back to its proper datatype during loading in data warehouse?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z4jca1", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z4jca1/data_type_after_extraction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z4jca1/data_type_after_extraction/", "subreddit_subscribers": 80934, "created_utc": 1669398242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \n\nAt the moment I was challenged to work with Palantir tools and I want to know if someone here has any experience with Palantir and if it is feasible to apply the Medallion Architecture (Bronze/Silver/Gold tier data) that Databricks uses.\n\nI searched online and didnt come to a conclusion regarding this. \nCan I use Palantir the same way as I use Databricks?\n\nI am trying to understand the Foundry and HyperAuto from Palantir but cant wrap this new information and make an analogy from Databricks.\n\nIf anyone can help I would be very grateful!", "author_fullname": "t2_6jg4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating to Palantir from Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z4izq4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669397380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;At the moment I was challenged to work with Palantir tools and I want to know if someone here has any experience with Palantir and if it is feasible to apply the Medallion Architecture (Bronze/Silver/Gold tier data) that Databricks uses.&lt;/p&gt;\n\n&lt;p&gt;I searched online and didnt come to a conclusion regarding this. \nCan I use Palantir the same way as I use Databricks?&lt;/p&gt;\n\n&lt;p&gt;I am trying to understand the Foundry and HyperAuto from Palantir but cant wrap this new information and make an analogy from Databricks.&lt;/p&gt;\n\n&lt;p&gt;If anyone can help I would be very grateful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z4izq4", "is_robot_indexable": true, "report_reasons": null, "author": "D1yzz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z4izq4/migrating_to_palantir_from_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z4izq4/migrating_to_palantir_from_databricks/", "subreddit_subscribers": 80934, "created_utc": 1669397380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If 1,000 records per second are streamed to parquet files in, let's say, S3, how would one approach handling small files and partitioning? Every second, when a write occurs, a small file of 1,000 records is generated, and these small files will accumulate very quickly. Is there some mechanism for periodically combining the small files into one once they add up to a block size, e.g. 128 MB? And then, let's say this data consists of simple user data and the most common queries against it are filtering on last name. What would a partitioning strategy look like, again in light of the small file sizes?", "author_fullname": "t2_1evp2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDFS Streaming - Small Files and Partitioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z4n1i5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669407400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If 1,000 records per second are streamed to parquet files in, let&amp;#39;s say, S3, how would one approach handling small files and partitioning? Every second, when a write occurs, a small file of 1,000 records is generated, and these small files will accumulate very quickly. Is there some mechanism for periodically combining the small files into one once they add up to a block size, e.g. 128 MB? And then, let&amp;#39;s say this data consists of simple user data and the most common queries against it are filtering on last name. What would a partitioning strategy look like, again in light of the small file sizes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z4n1i5", "is_robot_indexable": true, "report_reasons": null, "author": "LaminatedMisanthropy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z4n1i5/hdfs_streaming_small_files_and_partitioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z4n1i5/hdfs_streaming_small_files_and_partitioning/", "subreddit_subscribers": 80934, "created_utc": 1669407400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to switch from BI/DA role to DE roles.Since I have been working for small companies/consulting I never had DS or Algo or Systems design questions.\n\nI see that educative.io has the course Grokking system design interviews.\nEven though it is pricey , I am thinking of getting it.\n\nAlso I see some posts about LogicMojo course on DA,Algo.They do start from basics , so it sounds a good one too,but it is live class and it\u2019s IST (Indian Standard Time) time.\nI probably need some basics as well.\n\nDoes anyone have any experience with these courses.?\nAny suggestions are appreciated.", "author_fullname": "t2_25nt9ty6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Review on Educative.io or Logic Mojo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z4i75e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669395409.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to switch from BI/DA role to DE roles.Since I have been working for small companies/consulting I never had DS or Algo or Systems design questions.&lt;/p&gt;\n\n&lt;p&gt;I see that educative.io has the course Grokking system design interviews.\nEven though it is pricey , I am thinking of getting it.&lt;/p&gt;\n\n&lt;p&gt;Also I see some posts about LogicMojo course on DA,Algo.They do start from basics , so it sounds a good one too,but it is live class and it\u2019s IST (Indian Standard Time) time.\nI probably need some basics as well.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience with these courses.?\nAny suggestions are appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z4i75e", "is_robot_indexable": true, "report_reasons": null, "author": "srajeevan89", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z4i75e/review_on_educativeio_or_logic_mojo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z4i75e/review_on_educativeio_or_logic_mojo/", "subreddit_subscribers": 80934, "created_utc": 1669395409.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_12wi0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELT stack with CloudQuery Open source and Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_z4dvml", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4MrvokDxqONDiKpWWZG6kzWCoQ9X___AhlQ-HST-JZQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669384272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cloudquery.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.cloudquery.io/blog/announcing-cloudquery-snowflake-destination", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VQsxwDGeqfuwgBvirPTlAsCDrJrsKhWPnTesZ0Flsfg.jpg?auto=webp&amp;s=b28a1d4c4950ef46ecffff10b061597d62151958", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/VQsxwDGeqfuwgBvirPTlAsCDrJrsKhWPnTesZ0Flsfg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1a8d4bc5831bc452666569a36114ec22460af87", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/VQsxwDGeqfuwgBvirPTlAsCDrJrsKhWPnTesZ0Flsfg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b19df3eefb7cfa0942c103e355ea912b68ae77ad", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/VQsxwDGeqfuwgBvirPTlAsCDrJrsKhWPnTesZ0Flsfg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f860e48822cec60bbae1f186f58b8f64b40a5d92", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/VQsxwDGeqfuwgBvirPTlAsCDrJrsKhWPnTesZ0Flsfg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee9f2eda41a2fd4a44dc384f5999fb3899f881a8", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/VQsxwDGeqfuwgBvirPTlAsCDrJrsKhWPnTesZ0Flsfg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=93484552f4b92a74d03fcd4b6067ad3ac4fde88a", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/VQsxwDGeqfuwgBvirPTlAsCDrJrsKhWPnTesZ0Flsfg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=748d0794d2da2bb8641980c62485e45e602e5f22", "width": 1080, "height": 565}], "variants": {}, "id": "IDsPurG4RyrncX0rbFkvpbqLt_X2Fxu0CqFd-hxrbfM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z4dvml", "is_robot_indexable": true, "report_reasons": null, "author": "jekapats", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z4dvml/elt_stack_with_cloudquery_open_source_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.cloudquery.io/blog/announcing-cloudquery-snowflake-destination", "subreddit_subscribers": 80934, "created_utc": 1669384272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_b7f9ay9o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to create and use data flow in Azure Data Factory", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_z47d6k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ySvg0lTmdlY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How to create and use data flow in Azure Data Factory\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How to create and use data flow in Azure Data Factory", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ySvg0lTmdlY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How to create and use data flow in Azure Data Factory\"&gt;&lt;/iframe&gt;", "author_name": "SoftWiz Circle", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/ySvg0lTmdlY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@SoftWizCircle"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ySvg0lTmdlY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How to create and use data flow in Azure Data Factory\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/z47d6k", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_WOlxPTwuv_9J_q-AVtbuQ3nXkocWfPto9psf7EQ1EI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669362002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/ySvg0lTmdlY", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KADBGcB3aw1pZf9pWwzAOEQv5tyO_RM8g2Stfq1hZMw.jpg?auto=webp&amp;s=7ec13008b07996b213e054d66b192d55404cfff0", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/KADBGcB3aw1pZf9pWwzAOEQv5tyO_RM8g2Stfq1hZMw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=90ea33cfc69035f15ef8d7bbd53406387ac04e7b", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/KADBGcB3aw1pZf9pWwzAOEQv5tyO_RM8g2Stfq1hZMw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e1923da3362b92b2ef68cb146bacc48d49297d11", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/KADBGcB3aw1pZf9pWwzAOEQv5tyO_RM8g2Stfq1hZMw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=954c85d89f8339ef0910bc9af1c32bc1e3b049ba", "width": 320, "height": 240}], "variants": {}, "id": "2TIBerwoxwEaBrDmXYFtg-J4MbbOozCAELZh_OsMjfU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z47d6k", "is_robot_indexable": true, "report_reasons": null, "author": "balramprasad", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z47d6k/how_to_create_and_use_data_flow_in_azure_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/ySvg0lTmdlY", "subreddit_subscribers": 80934, "created_utc": 1669362002.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How to create and use data flow in Azure Data Factory", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ySvg0lTmdlY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How to create and use data flow in Azure Data Factory\"&gt;&lt;/iframe&gt;", "author_name": "SoftWiz Circle", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/ySvg0lTmdlY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@SoftWizCircle"}}, "is_video": false}}], "before": null}}