{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I did not think it would be possible for me to get a DE internship. It was always my dream to work on data engineering work ever since I started my education. I always thought the only way would for me to get a Master's in CS, and work in the data field as a data analyst for 5+ years before someone will let me in. Additional reasons why I thought I could never get a DE internship is because:\n\n1) I go to a no-name school, it used to be a community college\n\n2) My degree is not CS, its in data analytics\n\n3) I don't have any backend experience\n\n4) I have no experience with any big data processing or storage tools.\n\n5) I haven't even finished my undergrad\n\n6) I have never touched any ETL tools other than manually creating pipelines using python.\n\nEven with all these shortcomings, I got a DE intern offer with a Fortune 10 company. I have been lurking on this sub for nearly 2 years, but I never felt like I deserve to comment or post on this sub, let alone get a job in this field. It has always been my dream to work as a Data Engineer, and I just wanted to share the great news with a community that I would love to be a part of.", "author_fullname": "t2_2x32e84y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got a DE intern offer!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1zd6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 98, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 98, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669137986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did not think it would be possible for me to get a DE internship. It was always my dream to work on data engineering work ever since I started my education. I always thought the only way would for me to get a Master&amp;#39;s in CS, and work in the data field as a data analyst for 5+ years before someone will let me in. Additional reasons why I thought I could never get a DE internship is because:&lt;/p&gt;\n\n&lt;p&gt;1) I go to a no-name school, it used to be a community college&lt;/p&gt;\n\n&lt;p&gt;2) My degree is not CS, its in data analytics&lt;/p&gt;\n\n&lt;p&gt;3) I don&amp;#39;t have any backend experience&lt;/p&gt;\n\n&lt;p&gt;4) I have no experience with any big data processing or storage tools.&lt;/p&gt;\n\n&lt;p&gt;5) I haven&amp;#39;t even finished my undergrad&lt;/p&gt;\n\n&lt;p&gt;6) I have never touched any ETL tools other than manually creating pipelines using python.&lt;/p&gt;\n\n&lt;p&gt;Even with all these shortcomings, I got a DE intern offer with a Fortune 10 company. I have been lurking on this sub for nearly 2 years, but I never felt like I deserve to comment or post on this sub, let alone get a job in this field. It has always been my dream to work as a Data Engineer, and I just wanted to share the great news with a community that I would love to be a part of.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z1zd6t", "is_robot_indexable": true, "report_reasons": null, "author": "traderdrakor", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1zd6t/just_got_a_de_intern_offer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1zd6t/just_got_a_de_intern_offer/", "subreddit_subscribers": 80736, "created_utc": 1669137986.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently had an introductory interview for a Data Engineering position and was told that the next step of the process would be a take home project. I\u2019m usually ok with these as they\u2019re like 3-4 hours max of my time.\n\nI was pretty flabbergasted when I received the assignment and was told that it was expected it would take me up to 8 hours to complete. The assignment itself makes sense and I\u2019m sure it would take *some* time, but I\u2019m unwilling to spend 8 hours or more on something when there\u2019s a possibility of just being ghosted after. \n\nMy question is: is this common? What is the average amount of time that these take-home projects usually take for Data Engineering roles?\n\nEDIT: additional context is that I applied to this job out of curiosity/exploring DE as a potential career path, I definitely won't be moving forward with this place but just more curious if crazy requests like this are commonplace.\n\nEDIT 2: I will not be sharing the assignment or the data itself, please don't message me to ask :) what I will share is that the employer in question is a sports team and the assignment involves working with tabular and video data about specific elements of the game and its players.", "author_fullname": "t2_xi9z6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "8 hour take home project for interview\u2026is this normal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z2c5y9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669176296.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669168811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently had an introductory interview for a Data Engineering position and was told that the next step of the process would be a take home project. I\u2019m usually ok with these as they\u2019re like 3-4 hours max of my time.&lt;/p&gt;\n\n&lt;p&gt;I was pretty flabbergasted when I received the assignment and was told that it was expected it would take me up to 8 hours to complete. The assignment itself makes sense and I\u2019m sure it would take &lt;em&gt;some&lt;/em&gt; time, but I\u2019m unwilling to spend 8 hours or more on something when there\u2019s a possibility of just being ghosted after. &lt;/p&gt;\n\n&lt;p&gt;My question is: is this common? What is the average amount of time that these take-home projects usually take for Data Engineering roles?&lt;/p&gt;\n\n&lt;p&gt;EDIT: additional context is that I applied to this job out of curiosity/exploring DE as a potential career path, I definitely won&amp;#39;t be moving forward with this place but just more curious if crazy requests like this are commonplace.&lt;/p&gt;\n\n&lt;p&gt;EDIT 2: I will not be sharing the assignment or the data itself, please don&amp;#39;t message me to ask :) what I will share is that the employer in question is a sports team and the assignment involves working with tabular and video data about specific elements of the game and its players.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "z2c5y9", "is_robot_indexable": true, "report_reasons": null, "author": "don_draper97", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z2c5y9/8_hour_take_home_project_for_interviewis_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z2c5y9/8_hour_take_home_project_for_interviewis_this/", "subreddit_subscribers": 80736, "created_utc": 1669168811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, i started studyng data on July/August, and i just fell in love with it. Bought a good course, focused os D.E, where they teach all about Data Warehouses, Data Lakes, ETL. Tools like Oracle, Redshift, Spark, Kafka and lots of stuff, there's also some good projects using all of it. I'm just about to finish the first one where i build a DW with Oracle.\n\nBeside, i'm also learning SQL with Youtube material, and doing some Hackerrank questions, my goal is to do all medium level ones. Have some knowledge in Python/Pandas/etc.\n\nThing is, i'm graduated on Chemical Engineering and spent my whole life working in laboratories and quality control of chemicals, no data related at all. And most jobs i'm seeing requires experience (even the juniors). Also, i live in Brazil where there's like 14 million unemployed people, so even the data analysts jobs on Linkedin gets 500+ applications in less than a day. (and i've been told to get a data analysis job first, then migrate to D.E)\n\nI know it takes time, im okay where i am right now, but i'd like to get in the area at least until the end of 2023, how can i stand out in applications? should i apply in every job data related?\n\nAny tips will be appreciated, my english is kinda bad, sorry for the errors, and thanks!", "author_fullname": "t2_vordwax", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get in D.E?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1vx2v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669129659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, i started studyng data on July/August, and i just fell in love with it. Bought a good course, focused os D.E, where they teach all about Data Warehouses, Data Lakes, ETL. Tools like Oracle, Redshift, Spark, Kafka and lots of stuff, there&amp;#39;s also some good projects using all of it. I&amp;#39;m just about to finish the first one where i build a DW with Oracle.&lt;/p&gt;\n\n&lt;p&gt;Beside, i&amp;#39;m also learning SQL with Youtube material, and doing some Hackerrank questions, my goal is to do all medium level ones. Have some knowledge in Python/Pandas/etc.&lt;/p&gt;\n\n&lt;p&gt;Thing is, i&amp;#39;m graduated on Chemical Engineering and spent my whole life working in laboratories and quality control of chemicals, no data related at all. And most jobs i&amp;#39;m seeing requires experience (even the juniors). Also, i live in Brazil where there&amp;#39;s like 14 million unemployed people, so even the data analysts jobs on Linkedin gets 500+ applications in less than a day. (and i&amp;#39;ve been told to get a data analysis job first, then migrate to D.E)&lt;/p&gt;\n\n&lt;p&gt;I know it takes time, im okay where i am right now, but i&amp;#39;d like to get in the area at least until the end of 2023, how can i stand out in applications? should i apply in every job data related?&lt;/p&gt;\n\n&lt;p&gt;Any tips will be appreciated, my english is kinda bad, sorry for the errors, and thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z1vx2v", "is_robot_indexable": true, "report_reasons": null, "author": "Utopya96", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1vx2v/how_to_get_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1vx2v/how_to_get_in_de/", "subreddit_subscribers": 80736, "created_utc": 1669129659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I'm a data engineer (MS stack) with an emphasis on BI.\n\nI am just keen on Snowflake cloud and have decided to dive deeper into it. I know that opening your trial account for 40 days to play around with it is free.\n\nI ask for any study resources either YouTube lectures or full courses, that you could recommend.\n\nI would appreciate it if you shared your experience. What exactly stack did you use with Snowflake (data ingestion/ETL (ELT) + DWH + BI tool)", "author_fullname": "t2_ajo16sc3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake learning path", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z225cz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669144537.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I&amp;#39;m a data engineer (MS stack) with an emphasis on BI.&lt;/p&gt;\n\n&lt;p&gt;I am just keen on Snowflake cloud and have decided to dive deeper into it. I know that opening your trial account for 40 days to play around with it is free.&lt;/p&gt;\n\n&lt;p&gt;I ask for any study resources either YouTube lectures or full courses, that you could recommend.&lt;/p&gt;\n\n&lt;p&gt;I would appreciate it if you shared your experience. What exactly stack did you use with Snowflake (data ingestion/ETL (ELT) + DWH + BI tool)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z225cz", "is_robot_indexable": true, "report_reasons": null, "author": "echo_n1", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z225cz/snowflake_learning_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z225cz/snowflake_learning_path/", "subreddit_subscribers": 80736, "created_utc": 1669144537.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm still confused about the difference and use cases for a data warehouse and data lake. In my understanding what differs a database and data warehouse is OLTP and OLAP. While a database is more transaction and consitency focused, a data warehouse is optimized for big queries which makes it efficient for searching through big data. But why would I use a Data Warehouse like for example the Synapse Warehouse in Azure when I can create a Databricks solution with it's Lakehouse Architecture and Delta Tables that provide ACID? As far as I understand a Data Lake is just a dump for non relational data but you can still load from it since there a connector for Power BI also without the delta layer. So why not load directly from the data lake instead of putting the tables in a data warehouse as a intermediary step? Further, it is recommended to have around 3-4 stages (raw, curated, enriched), making the data lake also structured.  Another point is that a data Warehouse is very costy in Azure at least, while a data lake is quite cheap, so I don't really see the value. Can someone perhaps elaborate? Thanks!", "author_fullname": "t2_bgbrbly9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Difference between Data Warehouse and Data Lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z2jh8f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669191348.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m still confused about the difference and use cases for a data warehouse and data lake. In my understanding what differs a database and data warehouse is OLTP and OLAP. While a database is more transaction and consitency focused, a data warehouse is optimized for big queries which makes it efficient for searching through big data. But why would I use a Data Warehouse like for example the Synapse Warehouse in Azure when I can create a Databricks solution with it&amp;#39;s Lakehouse Architecture and Delta Tables that provide ACID? As far as I understand a Data Lake is just a dump for non relational data but you can still load from it since there a connector for Power BI also without the delta layer. So why not load directly from the data lake instead of putting the tables in a data warehouse as a intermediary step? Further, it is recommended to have around 3-4 stages (raw, curated, enriched), making the data lake also structured.  Another point is that a data Warehouse is very costy in Azure at least, while a data lake is quite cheap, so I don&amp;#39;t really see the value. Can someone perhaps elaborate? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z2jh8f", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Inspection3886", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z2jh8f/difference_between_data_warehouse_and_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z2jh8f/difference_between_data_warehouse_and_data_lake/", "subreddit_subscribers": 80736, "created_utc": 1669191348.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3yleu7rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Grupo Botic\u00e1rio Keep Their Pipelines Running With Automated Data Lineage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z21emd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1669142810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "alvin.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.alvin.ai/posts/how-grupo-boticario-keep-their-pipelines-running-with-automated-data-lineage", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z21emd", "is_robot_indexable": true, "report_reasons": null, "author": "gabsferreiradev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z21emd/how_grupo_botic\u00e1rio_keep_their_pipelines_running/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.alvin.ai/posts/how-grupo-boticario-keep-their-pipelines-running-with-automated-data-lineage", "subreddit_subscribers": 80736, "created_utc": 1669142810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnom3iyh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Superplate - An Open Source Next.js and React boilerplate with +30 plugins", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_z2jjct", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/HBha_Z9lFpIPvfXAaX1GdZi36Q4h81OGvDJu5RXY8v0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669191563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/pankod/superplate", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4l4FWhgt_JkqzkoHXyl3_IVeipa9svumEAy_NRr3yxM.jpg?auto=webp&amp;s=b9d6f504505828edd47463699ecaaf9833e8e1ac", "width": 492, "height": 276}, "resolutions": [{"url": "https://external-preview.redd.it/4l4FWhgt_JkqzkoHXyl3_IVeipa9svumEAy_NRr3yxM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4803fb65856f70202623a08aeb2279f4e09d1fc3", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/4l4FWhgt_JkqzkoHXyl3_IVeipa9svumEAy_NRr3yxM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=97b9d855b13d30df8af79768fb183ae8b50c1d6b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/4l4FWhgt_JkqzkoHXyl3_IVeipa9svumEAy_NRr3yxM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7fb3d366a6e09c7fc50029077e542ecfc7b45f34", "width": 320, "height": 179}], "variants": {}, "id": "rtV4RTFz5Z7E4_Kp3ToSBYhz7nqvPvaOajQnEjqGQKI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z2jjct", "is_robot_indexable": true, "report_reasons": null, "author": "kpeterfletcher", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z2jjct/superplate_an_open_source_nextjs_and_react/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/pankod/superplate", "subreddit_subscribers": 80736, "created_utc": 1669191563.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm evaluating airflow deployments options for simple personal data integration. \n\nAs the hypotheticals, we need to be able to hit REST APIs from personal health data services like Fitbit, MyFitnessPal, and similar and then do a simple storage load to S3. I estimate this can happen for all services within 1 hour of compute daily.\n\nSo far, my evaluation of the minimums looks to be something like this: \n\n1. GCP's Cloud Composer @ 730 hours/month w/ 3 workers (required), 1 scheduler, n1-standard vCPU, 10GB network, +persistent disk costs = $390/month\n\n2. AWS MWAA small @ 730 hours/month w/ 1 worker, 2 schedulers (required), 1GB database &amp; data storage = $359/month.\n\n3. AWS EC2 instance w/ airflow running as a local service on a t4g.large @ 730 hours/month w/ 2cpus &amp; 8gb memory, 32gb disk = $34/month\n\n4. AWS Fargate (example 1 on this page) @ 5 tasks daily, 60min duration, 2 vcpu, 8gb memory, 32gb disk, and x86 architecture = $18/month\n\nAre there other options I should be considering? So far it seems like the managed services are just way more expensive then they are worth for anything like a personal project. \n\nThanks.", "author_fullname": "t2_1icoacpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a place that I can run airflow cheap enough to use personally?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z24hf8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669149944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m evaluating airflow deployments options for simple personal data integration. &lt;/p&gt;\n\n&lt;p&gt;As the hypotheticals, we need to be able to hit REST APIs from personal health data services like Fitbit, MyFitnessPal, and similar and then do a simple storage load to S3. I estimate this can happen for all services within 1 hour of compute daily.&lt;/p&gt;\n\n&lt;p&gt;So far, my evaluation of the minimums looks to be something like this: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;GCP&amp;#39;s Cloud Composer @ 730 hours/month w/ 3 workers (required), 1 scheduler, n1-standard vCPU, 10GB network, +persistent disk costs = $390/month&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;AWS MWAA small @ 730 hours/month w/ 1 worker, 2 schedulers (required), 1GB database &amp;amp; data storage = $359/month.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;AWS EC2 instance w/ airflow running as a local service on a t4g.large @ 730 hours/month w/ 2cpus &amp;amp; 8gb memory, 32gb disk = $34/month&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;AWS Fargate (example 1 on this page) @ 5 tasks daily, 60min duration, 2 vcpu, 8gb memory, 32gb disk, and x86 architecture = $18/month&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Are there other options I should be considering? So far it seems like the managed services are just way more expensive then they are worth for anything like a personal project. &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z24hf8", "is_robot_indexable": true, "report_reasons": null, "author": "0_to_1", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z24hf8/is_there_a_place_that_i_can_run_airflow_cheap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z24hf8/is_there_a_place_that_i_can_run_airflow_cheap/", "subreddit_subscribers": 80736, "created_utc": 1669149944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nMy company has an ADF pipeline for bringing files from our network drive into our Snowflake enterprise data warehouse.  It works fine, is driven off a control table in snowflake that stores the parameters for each pipeline like retention time, file location, upsert keys, destination db. This pipeline additionally writes the data from snowflake to sql server to preserve some old analytic reporting, logs auditing info, and sends failure notifications\n\nThere has been talk of abandoning this pipeline and using snowpipe instead with schema inference and stages to take advantage of built in snowflake functionality (suggested by another data engineer). Can anyone speak to their experience with snowpipe vs adf? The transition to using snowpipe will take several months and likely deprecate a lot of the ADF work. I am most interested in what has worked well for other companies and less interested in utilizing built in snowflake features just for the sake of it. I have very little experience with snowpipe which is why I'm posting \n\nMany thanks", "author_fullname": "t2_7lyjqy22", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flatfile ingestion into Snowflake: ADF vs Snowpipe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z23w9h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669155170.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669148564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;My company has an ADF pipeline for bringing files from our network drive into our Snowflake enterprise data warehouse.  It works fine, is driven off a control table in snowflake that stores the parameters for each pipeline like retention time, file location, upsert keys, destination db. This pipeline additionally writes the data from snowflake to sql server to preserve some old analytic reporting, logs auditing info, and sends failure notifications&lt;/p&gt;\n\n&lt;p&gt;There has been talk of abandoning this pipeline and using snowpipe instead with schema inference and stages to take advantage of built in snowflake functionality (suggested by another data engineer). Can anyone speak to their experience with snowpipe vs adf? The transition to using snowpipe will take several months and likely deprecate a lot of the ADF work. I am most interested in what has worked well for other companies and less interested in utilizing built in snowflake features just for the sake of it. I have very little experience with snowpipe which is why I&amp;#39;m posting &lt;/p&gt;\n\n&lt;p&gt;Many thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z23w9h", "is_robot_indexable": true, "report_reasons": null, "author": "kitkatbar_2314", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z23w9h/flatfile_ingestion_into_snowflake_adf_vs_snowpipe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z23w9h/flatfile_ingestion_into_snowflake_adf_vs_snowpipe/", "subreddit_subscribers": 80736, "created_utc": 1669148564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi there,\n\nI have an Design/Architecture Interview scheduled for Senior S/W Engr - Data position.\n\nI am not sure how to prepare for this. Thanks for your help\n\nMR", "author_fullname": "t2_gp13ce3a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Prepare for Design/Architecture Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z21nrp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669143417.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I have an Design/Architecture Interview scheduled for Senior S/W Engr - Data position.&lt;/p&gt;\n\n&lt;p&gt;I am not sure how to prepare for this. Thanks for your help&lt;/p&gt;\n\n&lt;p&gt;MR&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "z21nrp", "is_robot_indexable": true, "report_reasons": null, "author": "meridian_12", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z21nrp/how_to_prepare_for_designarchitecture_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z21nrp/how_to_prepare_for_designarchitecture_interview/", "subreddit_subscribers": 80736, "created_utc": 1669143417.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to change my job to DE, I have some background in Python, SQL and Power Bi (this is not enough). What I can learn to increase my knowledge to become a junior? Can you recommend any worth money and time spending course/ classes ? What technology or topic is most important as a point of start?", "author_fullname": "t2_nqh56i4y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z2i04g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669186306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to change my job to DE, I have some background in Python, SQL and Power Bi (this is not enough). What I can learn to increase my knowledge to become a junior? Can you recommend any worth money and time spending course/ classes ? What technology or topic is most important as a point of start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z2i04g", "is_robot_indexable": true, "report_reasons": null, "author": "MagBro-sky", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z2i04g/how_to_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z2i04g/how_to_start/", "subreddit_subscribers": 80736, "created_utc": 1669186306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m looking at the source code for a Flink application and there are underscores everywhere in the Scala code for transformations. I like Scala but that stuff is super unintuitive. Can someone explain to me this example I saw on the Flink homepage (for the KeyedStream -&gt; DataStream reduction):\n\n`keyedStream.reduce { _ + _ } `", "author_fullname": "t2_7mdudcx1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can someone teach me what _ in Scala means?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z2fkrt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669178658.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking at the source code for a Flink application and there are underscores everywhere in the Scala code for transformations. I like Scala but that stuff is super unintuitive. Can someone explain to me this example I saw on the Flink homepage (for the KeyedStream -&amp;gt; DataStream reduction):&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;keyedStream.reduce { _ + _ }&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z2fkrt", "is_robot_indexable": true, "report_reasons": null, "author": "OldManWhoYellsAtX", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z2fkrt/can_someone_teach_me_what_in_scala_means/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z2fkrt/can_someone_teach_me_what_in_scala_means/", "subreddit_subscribers": 80736, "created_utc": 1669178658.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!  I have been recently working with a time series dataset that had at least 30% or more null values depending on the column. Assuming that  information existed prior to the date of the last null value we should be able to fill in those values using an average estimator. I was interested in filling these values, but was not sure what might be the best approach since the data was changing over time either in a positive or negative direction and thought that perhaps using methods that I had previously used for classification such as filling with median or mean would not produce accurate results. While examining the the data and looking for a possible solution to filling these values I remembered that by using Annual Growth Rate I could determine missing values by determining the rate of change between the given values then using the increase or decrease formula depending on the direction of the values given. Seeing as how I could not find this being done online I went ahead and attempted this in a Colab notebook and came up with the following proof of concept.\n\nI still have some questions about the usability of this method, but believe it may be a feasible solution for filling missing values in a time series.", "author_fullname": "t2_n2zasjis", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using AAGR to determine missing values in a time series", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_z2bkys", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/8uTZEPbnahwiqjDjwjz2f9SJj3cC5MBTGfuGcyftGB0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669167159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!  I have been recently working with a time series dataset that had at least 30% or more null values depending on the column. Assuming that  information existed prior to the date of the last null value we should be able to fill in those values using an average estimator. I was interested in filling these values, but was not sure what might be the best approach since the data was changing over time either in a positive or negative direction and thought that perhaps using methods that I had previously used for classification such as filling with median or mean would not produce accurate results. While examining the the data and looking for a possible solution to filling these values I remembered that by using Annual Growth Rate I could determine missing values by determining the rate of change between the given values then using the increase or decrease formula depending on the direction of the values given. Seeing as how I could not find this being done online I went ahead and attempted this in a Colab notebook and came up with the following proof of concept.&lt;/p&gt;\n\n&lt;p&gt;I still have some questions about the usability of this method, but believe it may be a feasible solution for filling missing values in a time series.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/lifeofbaka/US-Energy-Timeseries/blob/main/US_Energy.ipynb", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UPyObv-tEwtHImTjwqNe5-FOCiBkJedMRmTfKMiJK1A.jpg?auto=webp&amp;s=e62d90faeb8e0ddafb25db873e1ce956735eca11", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/UPyObv-tEwtHImTjwqNe5-FOCiBkJedMRmTfKMiJK1A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2e828a2420f69cf30b6b2ac76032a7a3dd1c62a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/UPyObv-tEwtHImTjwqNe5-FOCiBkJedMRmTfKMiJK1A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d33faa26bb4ec7e5a33a47c3cf2f337c7b5a01c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/UPyObv-tEwtHImTjwqNe5-FOCiBkJedMRmTfKMiJK1A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f4215d41a6a6a27a552c023ba602b9a02e348dae", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/UPyObv-tEwtHImTjwqNe5-FOCiBkJedMRmTfKMiJK1A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f558bed87a3edd11a005edd746a57474a8e93322", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/UPyObv-tEwtHImTjwqNe5-FOCiBkJedMRmTfKMiJK1A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=733b79d1d1eaf24da41797f6f6fc1d099ec3eafc", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/UPyObv-tEwtHImTjwqNe5-FOCiBkJedMRmTfKMiJK1A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2d2c6429e737a40044a69003be89df8c19cd163d", "width": 1080, "height": 540}], "variants": {}, "id": "KQI7jHpzUxS9sbS-aD-2Uj8ZJIoGfbxzoUtFCi4BwA0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "z2bkys", "is_robot_indexable": true, "report_reasons": null, "author": "DarthKermit-65", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z2bkys/using_aagr_to_determine_missing_values_in_a_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/lifeofbaka/US-Energy-Timeseries/blob/main/US_Energy.ipynb", "subreddit_subscribers": 80736, "created_utc": 1669167159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently we are set up in BQ with a pseudo relational model that doesn't really fit any standard architectures. I was advocating to refactor everything to a Star or Snowflake schema because that's what I'm familiar with. I have heard 0 things about it, good or bad.", "author_fullname": "t2_7wm26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BigQuery users, has anyone gone all in and converted your warehouse to the Dremel model and nested tables? What's your experience?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z2973o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669160790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently we are set up in BQ with a pseudo relational model that doesn&amp;#39;t really fit any standard architectures. I was advocating to refactor everything to a Star or Snowflake schema because that&amp;#39;s what I&amp;#39;m familiar with. I have heard 0 things about it, good or bad.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z2973o", "is_robot_indexable": true, "report_reasons": null, "author": "Landoperk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z2973o/bigquery_users_has_anyone_gone_all_in_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z2973o/bigquery_users_has_anyone_gone_all_in_and/", "subreddit_subscribers": 80736, "created_utc": 1669160790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\nHello, \n\nSuppose I have a dag with a few simple tasks. One of which  is it to run a python script that pulls data from an API and inserts into a sql db. A parameter for pulling the desired data from the API depends on an input for the week youd like to pull from. In this case theres 17 weeks and I want to catchup from the 1st week to the 10th week (weeks 11-17 havent happened yet). Im curious how I can manage the catchups/backfills to dynamically run such that airflow is able to understand we have missing weeks and be able to input the necessary weeks into the python script. Hopefully that makes sense", "author_fullname": "t2_qtssrdk7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow parameterized backfill", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z23t4k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669148362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;Suppose I have a dag with a few simple tasks. One of which  is it to run a python script that pulls data from an API and inserts into a sql db. A parameter for pulling the desired data from the API depends on an input for the week youd like to pull from. In this case theres 17 weeks and I want to catchup from the 1st week to the 10th week (weeks 11-17 havent happened yet). Im curious how I can manage the catchups/backfills to dynamically run such that airflow is able to understand we have missing weeks and be able to input the necessary weeks into the python script. Hopefully that makes sense&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z23t4k", "is_robot_indexable": true, "report_reasons": null, "author": "Primary-Self-6836", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z23t4k/airflow_parameterized_backfill/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z23t4k/airflow_parameterized_backfill/", "subreddit_subscribers": 80736, "created_utc": 1669148362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\n**TLDR:** Seeking valuable resources / advice for thinking bigger picture / designing effective and practical architecture.\n\nI have been practicing Data Engineering for a couple of years now, but I have a hard time contributing to team sessions where we brainstorm solutions / architectures to solve a specific problem. Perhaps it is imposter syndrome, but more often than not, solutions I propose never seem to gain traction with my team. I try to throw in my two cents whenever I can to hear whether or not it is a good or bad idea, and I justify my reasoning for everything I propose. \n\nI often ask for feedback, but I find my manager is so nice and I tend to receive comments like: \"You are doing great\", \"We are very impressed\". I am seeking some more critical feedback as to how I can improve, but having a hard time obtaining that with my current team. \n\nI do a lot of self assessment and I think I am too narrow focused. I tend to focus too much on what is right in front of me, and do not forecast longer term how my solutions could hurt or benefit the team. I know I am only a couple of years in, but I would like to push myself to grow faster since I started data engineering later in my career. \n\nMy questions to the data engineering community would be:\n\n1. What tools, tips or resources have you used to become a more effective engineer to address these problems ? Or put more simply, how have you become better at designing high level or bigger picture solutions ? \n2. I classify myself as an intermediate level engineer who is technically proficient. How can I challenge myself to become a better technical leader ? \n3. I am an avid learner and love to take courses or read books, so any recommendations are always appreciated! \n\nAdvice on any of the topics are much appreciated!", "author_fullname": "t2_1xdejvht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for more effective solution design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1zkho", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669138465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Seeking valuable resources / advice for thinking bigger picture / designing effective and practical architecture.&lt;/p&gt;\n\n&lt;p&gt;I have been practicing Data Engineering for a couple of years now, but I have a hard time contributing to team sessions where we brainstorm solutions / architectures to solve a specific problem. Perhaps it is imposter syndrome, but more often than not, solutions I propose never seem to gain traction with my team. I try to throw in my two cents whenever I can to hear whether or not it is a good or bad idea, and I justify my reasoning for everything I propose. &lt;/p&gt;\n\n&lt;p&gt;I often ask for feedback, but I find my manager is so nice and I tend to receive comments like: &amp;quot;You are doing great&amp;quot;, &amp;quot;We are very impressed&amp;quot;. I am seeking some more critical feedback as to how I can improve, but having a hard time obtaining that with my current team. &lt;/p&gt;\n\n&lt;p&gt;I do a lot of self assessment and I think I am too narrow focused. I tend to focus too much on what is right in front of me, and do not forecast longer term how my solutions could hurt or benefit the team. I know I am only a couple of years in, but I would like to push myself to grow faster since I started data engineering later in my career. &lt;/p&gt;\n\n&lt;p&gt;My questions to the data engineering community would be:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What tools, tips or resources have you used to become a more effective engineer to address these problems ? Or put more simply, how have you become better at designing high level or bigger picture solutions ? &lt;/li&gt;\n&lt;li&gt;I classify myself as an intermediate level engineer who is technically proficient. How can I challenge myself to become a better technical leader ? &lt;/li&gt;\n&lt;li&gt;I am an avid learner and love to take courses or read books, so any recommendations are always appreciated! &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Advice on any of the topics are much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z1zkho", "is_robot_indexable": true, "report_reasons": null, "author": "Danus123", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1zkho/tips_for_more_effective_solution_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1zkho/tips_for_more_effective_solution_design/", "subreddit_subscribers": 80736, "created_utc": 1669138465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://medium.com/memphis-dev/event-streams-are-nothing-without-action-f78823466fb4](https://medium.com/memphis-dev/event-streams-are-nothing-without-action-f78823466fb4)", "author_fullname": "t2_bqjrmnud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Event Streams Are Nothing Without Action", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1udt6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669125810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/memphis-dev/event-streams-are-nothing-without-action-f78823466fb4\"&gt;https://medium.com/memphis-dev/event-streams-are-nothing-without-action-f78823466fb4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?auto=webp&amp;s=e23c28db75a822ff58267da8591ce5c88e1ee6aa", "width": 1200, "height": 660}, "resolutions": [{"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=feee2c7c6c02270fc67e98bfd165ed7a82c33490", "width": 108, "height": 59}, {"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d2b9cbd4624e2a625ee1ab9a836e2d9149235144", "width": 216, "height": 118}, {"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fac6d6b272c8bb44e6f61413b429f8b48ea71f46", "width": 320, "height": 176}, {"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4acf139763986cdbbe1d26e289358774ebed85a7", "width": 640, "height": 352}, {"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=350827958b5de8caa11b291479485047c4d6a13b", "width": 960, "height": 528}, {"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=de8167cca3f782ae470e3a8296a8dae81c947a42", "width": 1080, "height": 594}], "variants": {}, "id": "PfhS002WvOJZACaqvBTP8UEQXWEYQz5EHFWT_zM36ls"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z1udt6", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Bug105", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1udt6/event_streams_are_nothing_without_action/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1udt6/event_streams_are_nothing_without_action/", "subreddit_subscribers": 80736, "created_utc": 1669125810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have two questions;\n\n1. How a data architecture set up in Azure will translate to GCP if we have the following components:\n\nData processing: **Databricks** notebooks with bronze, silver gold stages\n\nDatawarehouse: **Databricks** hive delta tables\n\nObject storage: **Azure** Storage\n\nData ingestion, and ETL (creation/monitoring): Azure **Data Factory**\n\nBI: PowerBI &gt; Databricks import with **Server Hostname** and **HTTP Path** connection. Ultra-slow as the pbi dataset grows. \n\n2. When or where could the typical tools like Airflow, Terraform, dbt, kubernetes, docker... be of any use or could help improve this architecture? \n\n&amp;#x200B;\n\nThe only thing that I know is missing here is **git** which seems to be a basic if you want to have a CI/CD setup. \n\n&amp;#x200B;\n\nThank you", "author_fullname": "t2_145y7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure architecture to GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1u7qy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669125405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have two questions;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How a data architecture set up in Azure will translate to GCP if we have the following components:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Data processing: &lt;strong&gt;Databricks&lt;/strong&gt; notebooks with bronze, silver gold stages&lt;/p&gt;\n\n&lt;p&gt;Datawarehouse: &lt;strong&gt;Databricks&lt;/strong&gt; hive delta tables&lt;/p&gt;\n\n&lt;p&gt;Object storage: &lt;strong&gt;Azure&lt;/strong&gt; Storage&lt;/p&gt;\n\n&lt;p&gt;Data ingestion, and ETL (creation/monitoring): Azure &lt;strong&gt;Data Factory&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;BI: PowerBI &amp;gt; Databricks import with &lt;strong&gt;Server Hostname&lt;/strong&gt; and &lt;strong&gt;HTTP Path&lt;/strong&gt; connection. Ultra-slow as the pbi dataset grows. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;When or where could the typical tools like Airflow, Terraform, dbt, kubernetes, docker... be of any use or could help improve this architecture? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The only thing that I know is missing here is &lt;strong&gt;git&lt;/strong&gt; which seems to be a basic if you want to have a CI/CD setup. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z1u7qy", "is_robot_indexable": true, "report_reasons": null, "author": "swaisdrais", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/z1u7qy/azure_architecture_to_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1u7qy/azure_architecture_to_gcp/", "subreddit_subscribers": 80736, "created_utc": 1669125405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently looking into building a data pipeline in GCP using google composer that moves data from an on prem oracle db to BQ. I\u2019m able to access the db in python if I were to run a script manually while connected to the vpn. However, when trying to run the script as a test in cloud functions while my PC is off it would fail since it cannot connect to vpn. Does anyone have experience connecting to company data programmatically that requires to be connected to vpn? Thanks !", "author_fullname": "t2_edr3jh4p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accessing on prem work data programmatically", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z2bw2f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669168036.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently looking into building a data pipeline in GCP using google composer that moves data from an on prem oracle db to BQ. I\u2019m able to access the db in python if I were to run a script manually while connected to the vpn. However, when trying to run the script as a test in cloud functions while my PC is off it would fail since it cannot connect to vpn. Does anyone have experience connecting to company data programmatically that requires to be connected to vpn? Thanks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z2bw2f", "is_robot_indexable": true, "report_reasons": null, "author": "babababooskio", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z2bw2f/accessing_on_prem_work_data_programmatically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z2bw2f/accessing_on_prem_work_data_programmatically/", "subreddit_subscribers": 80736, "created_utc": 1669168036.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "### Current pipeline is as follows:\n\n**Run Pipeline**\n\n1. Download data from Snowflake to Local (in parallel)\n2. Run Transformation Model \n3. Upload outputs of model to Snowflake\n\n### Issue:\n\nThere isn't a backfill proof system set up, and I'd like to implement that. What I'm looking at doing is capturing the input data in Azure Blob (cold storage), capturing the transformation model (either the folder system itself, or the git hash so it can be traced back to it's respective repo version), as well as capturing the output data in Azure, too.\n\n\n### My solution:\n\n**Run Pipeline**\n\n1. **CAPTURE Input Data** Snowflake &gt; External Stage &gt; Azure Blob\n2. Download data from Snowflake to Local (in parallel)\n3. **CAPTURE Model Image** Local &gt; Azure Blob (hash or folder)\n4. Run Transformation Model\n5. Upload outputs of model to Snowflake\n6. **CAPTURE Output Data** Snowflake &gt; External Stage &gt; Azure Blob\n\nWould this be a viable, efficient solution? The data will also be available locally for the AKS instance the model is running in, so another option would be to upload this data locally to Azure Blob before the pod winds down. Any ideas on a better way to do this before I proceed?", "author_fullname": "t2_8y4c8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best method to backfill this pipeline (Snowflake, Blob Storage)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z28rru", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669160683.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669159740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h3&gt;Current pipeline is as follows:&lt;/h3&gt;\n\n&lt;p&gt;&lt;strong&gt;Run Pipeline&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download data from Snowflake to Local (in parallel)&lt;/li&gt;\n&lt;li&gt;Run Transformation Model &lt;/li&gt;\n&lt;li&gt;Upload outputs of model to Snowflake&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h3&gt;Issue:&lt;/h3&gt;\n\n&lt;p&gt;There isn&amp;#39;t a backfill proof system set up, and I&amp;#39;d like to implement that. What I&amp;#39;m looking at doing is capturing the input data in Azure Blob (cold storage), capturing the transformation model (either the folder system itself, or the git hash so it can be traced back to it&amp;#39;s respective repo version), as well as capturing the output data in Azure, too.&lt;/p&gt;\n\n&lt;h3&gt;My solution:&lt;/h3&gt;\n\n&lt;p&gt;&lt;strong&gt;Run Pipeline&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;CAPTURE Input Data&lt;/strong&gt; Snowflake &amp;gt; External Stage &amp;gt; Azure Blob&lt;/li&gt;\n&lt;li&gt;Download data from Snowflake to Local (in parallel)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CAPTURE Model Image&lt;/strong&gt; Local &amp;gt; Azure Blob (hash or folder)&lt;/li&gt;\n&lt;li&gt;Run Transformation Model&lt;/li&gt;\n&lt;li&gt;Upload outputs of model to Snowflake&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CAPTURE Output Data&lt;/strong&gt; Snowflake &amp;gt; External Stage &amp;gt; Azure Blob&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Would this be a viable, efficient solution? The data will also be available locally for the AKS instance the model is running in, so another option would be to upload this data locally to Azure Blob before the pod winds down. Any ideas on a better way to do this before I proceed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z28rru", "is_robot_indexable": true, "report_reasons": null, "author": "azazazazaz3", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/z28rru/best_method_to_backfill_this_pipeline_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z28rru/best_method_to_backfill_this_pipeline_snowflake/", "subreddit_subscribers": 80736, "created_utc": 1669159740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Right now, we have a \"locations\" table that is only storing the `country_id` of each location.\n\nThis is a foreign key to a static `countries` table. \n\nRecently, it's become a requirement to also map each location to its state within that country, and city within the state.\n\n- Rather than redesigning a schema from scratch, we thought we should simply recycle something that is commonly used for this purpose. We found [this example](https://stackoverflow.com/questions/37366146/improve-relationship-design-relational-database-model-between-user-country-state) on stackoverflow - but it involves deprecating the `country_id` association atm.\n- When it comes to static data for a country/city/state hierarchy, is there generally a good source for static data that we can simply inject into our database, rather than manually scraping together those tables ourselves.", "author_fullname": "t2_k01zajj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Relational database structure for storing country, city, and state hierarchies.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1xcfh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669133094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Right now, we have a &amp;quot;locations&amp;quot; table that is only storing the &lt;code&gt;country_id&lt;/code&gt; of each location.&lt;/p&gt;\n\n&lt;p&gt;This is a foreign key to a static &lt;code&gt;countries&lt;/code&gt; table. &lt;/p&gt;\n\n&lt;p&gt;Recently, it&amp;#39;s become a requirement to also map each location to its state within that country, and city within the state.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Rather than redesigning a schema from scratch, we thought we should simply recycle something that is commonly used for this purpose. We found &lt;a href=\"https://stackoverflow.com/questions/37366146/improve-relationship-design-relational-database-model-between-user-country-state\"&gt;this example&lt;/a&gt; on stackoverflow - but it involves deprecating the &lt;code&gt;country_id&lt;/code&gt; association atm.&lt;/li&gt;\n&lt;li&gt;When it comes to static data for a country/city/state hierarchy, is there generally a good source for static data that we can simply inject into our database, rather than manually scraping together those tables ourselves.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z1xcfh", "is_robot_indexable": true, "report_reasons": null, "author": "Lostwhispers05", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1xcfh/relational_database_structure_for_storing_country/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1xcfh/relational_database_structure_for_storing_country/", "subreddit_subscribers": 80736, "created_utc": 1669133094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to know if it is possible to integrate flume and structured streaming. I know it is possible with spark streaming. \n\nCurrently I am trying to stream a log file which gets regular updates using flume. I used TAILDIR as source and file channel. Is there a good sink which can make it possible. Thanks in advance.", "author_fullname": "t2_cb7rpz4k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache flume and structured streaming integration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1th7f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669123353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to know if it is possible to integrate flume and structured streaming. I know it is possible with spark streaming. &lt;/p&gt;\n\n&lt;p&gt;Currently I am trying to stream a log file which gets regular updates using flume. I used TAILDIR as source and file channel. Is there a good sink which can make it possible. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z1th7f", "is_robot_indexable": true, "report_reasons": null, "author": "Mr_marshmellow19", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1th7f/apache_flume_and_structured_streaming_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1th7f/apache_flume_and_structured_streaming_integration/", "subreddit_subscribers": 80736, "created_utc": 1669123353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/z1tdb0)", "author_fullname": "t2_eajtr4nz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How many queries does your organization run on a typical day? (i.e. across scheduled dashboards / reports, and ad-hoc queries)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1tdb0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669123044.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/z1tdb0\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z1tdb0", "is_robot_indexable": true, "report_reasons": null, "author": "alneuman", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1669382244623, "options": [{"text": "less than 100", "id": "19960736"}, {"text": "100 - 500", "id": "19960737"}, {"text": "500 - 1,000", "id": "19960738"}, {"text": "1,000 - 5,000", "id": "19960739"}, {"text": "5,000 - 10,000", "id": "19960740"}, {"text": "&gt; 10,000", "id": "19960741"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 169, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1tdb0/how_many_queries_does_your_organization_run_on_a/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/z1tdb0/how_many_queries_does_your_organization_run_on_a/", "subreddit_subscribers": 80736, "created_utc": 1669123044.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Given all the vendors (AWS, GCP, Azure etc.) and cloud products, which cloud product should i go with to provide platform as a service? I am in the finance industry and we want to open up our platform for smaller companies to utilize.", "author_fullname": "t2_5zlk5o8b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which cloud platform to use in 2022\\2023 for your data product?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1re38", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669117097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given all the vendors (AWS, GCP, Azure etc.) and cloud products, which cloud product should i go with to provide platform as a service? I am in the finance industry and we want to open up our platform for smaller companies to utilize.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z1re38", "is_robot_indexable": true, "report_reasons": null, "author": "AggravatingWish1019", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1re38/which_cloud_platform_to_use_in_20222023_for_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1re38/which_cloud_platform_to_use_in_20222023_for_your/", "subreddit_subscribers": 80736, "created_utc": 1669117097.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}