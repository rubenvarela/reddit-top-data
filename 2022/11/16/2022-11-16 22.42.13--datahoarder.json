{"kind": "Listing", "data": {"after": "t3_ywfp5m", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_97qw4856", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\u20ac95/5TB Does it get any better for Europe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "name": "t3_ywqnac", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 278, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 278, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/SRy9YHX9uTCWVgALND56o7pcLuOEy-7tP0nCO2KED0Q.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668597069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1opx7t45qa0a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1opx7t45qa0a1.png?auto=webp&amp;s=36dc02229276e2912fe152709994fde3884127cd", "width": 1402, "height": 778}, "resolutions": [{"url": "https://preview.redd.it/1opx7t45qa0a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0742b145429d0c6e5fcedd4ad2d532050c61fbe9", "width": 108, "height": 59}, {"url": "https://preview.redd.it/1opx7t45qa0a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc5b6a6ad674993dad96ecd4f1f777a3245c6e32", "width": 216, "height": 119}, {"url": "https://preview.redd.it/1opx7t45qa0a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0862b4d16b3eb44017635cfcd704f43877f483e7", "width": 320, "height": 177}, {"url": "https://preview.redd.it/1opx7t45qa0a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=577446c10bd687e3bd9c3450884f81ce1c5ad51f", "width": 640, "height": 355}, {"url": "https://preview.redd.it/1opx7t45qa0a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dee6246a3894865346f1130c3c7afbd02027ea36", "width": 960, "height": 532}, {"url": "https://preview.redd.it/1opx7t45qa0a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7588f541211787846a55e425ac178d24cddfaafe", "width": 1080, "height": 599}], "variants": {}, "id": "jD438II8W_HxHdVc24kwpw5TKmru-JM8RvgSPQZovfo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ywqnac", "is_robot_indexable": true, "report_reasons": null, "author": "Sypermarket3", "discussion_type": null, "num_comments": 75, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywqnac/955tb_does_it_get_any_better_for_europe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1opx7t45qa0a1.png", "subreddit_subscribers": 654279, "created_utc": 1668597069.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Link to the sale](https://www.amazon.com/-/Seagate-IronWolf-HDD-16TB-almacenamiento/dp/B07SLPTK17/ref=sr_1_2?keywords=hard%2Bdrive&amp;qid=1668549728&amp;refinements=p_n_deal_type%3A23566065011&amp;rnid=23566063011&amp;sprefix=%2Caps%2C120&amp;sr=8-2&amp;th=1)  \n\n\nI'm fairly new to high end Hard Drives so I figured people here are much more experts than I. Seems that drive is made to be used as NAS with other drives, but my question is, can I just connect it to my pc even if it isn't made to function that way?", "author_fullname": "t2_17c802", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate 16TB Ironwolf Pro has 52% discount on Amazon. Is this HD good to use as internal storage for my PC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywbgom", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668550925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.amazon.com/-/Seagate-IronWolf-HDD-16TB-almacenamiento/dp/B07SLPTK17/ref=sr_1_2?keywords=hard%2Bdrive&amp;amp;qid=1668549728&amp;amp;refinements=p_n_deal_type%3A23566065011&amp;amp;rnid=23566063011&amp;amp;sprefix=%2Caps%2C120&amp;amp;sr=8-2&amp;amp;th=1\"&gt;Link to the sale&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m fairly new to high end Hard Drives so I figured people here are much more experts than I. Seems that drive is made to be used as NAS with other drives, but my question is, can I just connect it to my pc even if it isn&amp;#39;t made to function that way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywbgom", "is_robot_indexable": true, "report_reasons": null, "author": "Chinchillin09", "discussion_type": null, "num_comments": 80, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywbgom/seagate_16tb_ironwolf_pro_has_52_discount_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywbgom/seagate_16tb_ironwolf_pro_has_52_discount_on/", "subreddit_subscribers": 654279, "created_utc": 1668550925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all!\n\nSo I'm currently digitising all my grandma's photos from pictures from the 1950s and 1960s right through to the 2000s and 2010s. I was just wondering how you'd organise them all? (1000+). At the moment I've got a folder for each person, then within that I've got \"Group\" folder \"Solo\" folder and \"Documents\" folder. But when multiple people are in the group photos, having 10/15 copies of one photo seems overkill. There must be a better way to approach this? \n\nWould love to see any examples whether it's screenshots or links to a folder set up. Just any help would be much appreciated! Also naming conventions too. Initially I had \"Surname, First Name - Event\" however, my grandma has 11 siblings so I can't list everyone within that. \n\nI'll be using Google Drive to share the photos.", "author_fullname": "t2_redre", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you organise 1000+ family photos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywyf3m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668616528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all!&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m currently digitising all my grandma&amp;#39;s photos from pictures from the 1950s and 1960s right through to the 2000s and 2010s. I was just wondering how you&amp;#39;d organise them all? (1000+). At the moment I&amp;#39;ve got a folder for each person, then within that I&amp;#39;ve got &amp;quot;Group&amp;quot; folder &amp;quot;Solo&amp;quot; folder and &amp;quot;Documents&amp;quot; folder. But when multiple people are in the group photos, having 10/15 copies of one photo seems overkill. There must be a better way to approach this? &lt;/p&gt;\n\n&lt;p&gt;Would love to see any examples whether it&amp;#39;s screenshots or links to a folder set up. Just any help would be much appreciated! Also naming conventions too. Initially I had &amp;quot;Surname, First Name - Event&amp;quot; however, my grandma has 11 siblings so I can&amp;#39;t list everyone within that. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll be using Google Drive to share the photos.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywyf3m", "is_robot_indexable": true, "report_reasons": null, "author": "theferrolgamer", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywyf3m/how_would_you_organise_1000_family_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywyf3m/how_would_you_organise_1000_family_photos/", "subreddit_subscribers": 654279, "created_utc": 1668616528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi datahoarders,\n\nI hope this is the right place. Over the years my iPhone photos and videos have numbered 300,000 with a total of 1.8 terabytes of cloud space. \n\nI\u2019d like to back these up physically but iCloud Photos on PC has severe limitations. I can\u2019t physically download more than 999 photos at a time. \n\nIm currently trying to back up my iCloud to\nPhoto folder to google drive then use google drive to download everything on my eternal hard drive directly. Will this work or have I started a path to futility? \n\nI\u2019ve tried multiple phones calls with apple support and they are unable to advise on a clear path except for I need to get a Mac.\n\nI saw another thread somewhere where an individual had a python script to batch download photos. \n\nJust looking for the easiest solution. (I guess if I needed to buy a Mac to aid in downloading them all to a external hard drive and then return it) wouldn\u2019t be the end of the world for me as long as that would work for a fact. \n\nAll advice is welcome. Thank you", "author_fullname": "t2_1l239inu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have to download and physically back up 300,000 iCloud Photos. Is there a easy way?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywb83g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668550603.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668550361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi datahoarders,&lt;/p&gt;\n\n&lt;p&gt;I hope this is the right place. Over the years my iPhone photos and videos have numbered 300,000 with a total of 1.8 terabytes of cloud space. &lt;/p&gt;\n\n&lt;p&gt;I\u2019d like to back these up physically but iCloud Photos on PC has severe limitations. I can\u2019t physically download more than 999 photos at a time. &lt;/p&gt;\n\n&lt;p&gt;Im currently trying to back up my iCloud to\nPhoto folder to google drive then use google drive to download everything on my eternal hard drive directly. Will this work or have I started a path to futility? &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve tried multiple phones calls with apple support and they are unable to advise on a clear path except for I need to get a Mac.&lt;/p&gt;\n\n&lt;p&gt;I saw another thread somewhere where an individual had a python script to batch download photos. &lt;/p&gt;\n\n&lt;p&gt;Just looking for the easiest solution. (I guess if I needed to buy a Mac to aid in downloading them all to a external hard drive and then return it) wouldn\u2019t be the end of the world for me as long as that would work for a fact. &lt;/p&gt;\n\n&lt;p&gt;All advice is welcome. Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywb83g", "is_robot_indexable": true, "report_reasons": null, "author": "Xlr8in", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywb83g/i_have_to_download_and_physically_back_up_300000/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywb83g/i_have_to_download_and_physically_back_up_300000/", "subreddit_subscribers": 654279, "created_utc": 1668550361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My wife and I just picked up shiny new Pixel 7/Pro's and plan to take a whole bunch of videos in 4k of our family, kids, etc.  However after seeing what 5 minutes of a 4k video eats up, I'm starting to worry about the limits of my 2tb plan.  I saw there were some options, including buying a pixel 1... or paying $30/month for google enterprise that has either a 5tb or potentially unlimited space.  \n\nTwo questions - is the latter still a thing, or is it limited to 5tb?  If it's limited, wouldn't it just make sense for me to pay for 5tb plan on google one for $20/year (annual)?  \n\nAnd second, is there a better option out there?  Or am I worrying for nothing and probably won't hit even my 2tb limit anytime soon?", "author_fullname": "t2_o08jv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best option to future proof data hoarding on Google Photos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywu7vw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668607348.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My wife and I just picked up shiny new Pixel 7/Pro&amp;#39;s and plan to take a whole bunch of videos in 4k of our family, kids, etc.  However after seeing what 5 minutes of a 4k video eats up, I&amp;#39;m starting to worry about the limits of my 2tb plan.  I saw there were some options, including buying a pixel 1... or paying $30/month for google enterprise that has either a 5tb or potentially unlimited space.  &lt;/p&gt;\n\n&lt;p&gt;Two questions - is the latter still a thing, or is it limited to 5tb?  If it&amp;#39;s limited, wouldn&amp;#39;t it just make sense for me to pay for 5tb plan on google one for $20/year (annual)?  &lt;/p&gt;\n\n&lt;p&gt;And second, is there a better option out there?  Or am I worrying for nothing and probably won&amp;#39;t hit even my 2tb limit anytime soon?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywu7vw", "is_robot_indexable": true, "report_reasons": null, "author": "reezick", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywu7vw/best_option_to_future_proof_data_hoarding_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywu7vw/best_option_to_future_proof_data_hoarding_on/", "subreddit_subscribers": 654279, "created_utc": 1668607348.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As title, I am able to get either at $279.99 (with a coupon from WD). Which one would be better? From my research that the easystore may be HGST ultrastar HC550 inside, which maybe (?) is better than WD red? But it will be taking a chance there as there is no guarantee that's the exact drive. And warranty would be less, but I am not sure if the warranty is needed if the drive won't fail. Thoughts?", "author_fullname": "t2_yf3rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shucked Easystore 18TB vs WD Red Pro 18TB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywygfs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668616607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As title, I am able to get either at $279.99 (with a coupon from WD). Which one would be better? From my research that the easystore may be HGST ultrastar HC550 inside, which maybe (?) is better than WD red? But it will be taking a chance there as there is no guarantee that&amp;#39;s the exact drive. And warranty would be less, but I am not sure if the warranty is needed if the drive won&amp;#39;t fail. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywygfs", "is_robot_indexable": true, "report_reasons": null, "author": "arthurroos", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywygfs/shucked_easystore_18tb_vs_wd_red_pro_18tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywygfs/shucked_easystore_18tb_vs_wd_red_pro_18tb/", "subreddit_subscribers": 654279, "created_utc": 1668616607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to download video from a streaming website.\n\nHere's an [example](https://www.americastestkitchen.com/cookscountry/episode/829-low-country-party) of the link.\n\nI have tried using Inspect Element &gt; Network &gt; Media, but I don't see any link.\n\nI have also tried various softwares, but it doesn't work. Anyone can help?", "author_fullname": "t2_uciunzpl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to Download video from a streaming website.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywwzkq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668613653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to download video from a streaming website.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an &lt;a href=\"https://www.americastestkitchen.com/cookscountry/episode/829-low-country-party\"&gt;example&lt;/a&gt; of the link.&lt;/p&gt;\n\n&lt;p&gt;I have tried using Inspect Element &amp;gt; Network &amp;gt; Media, but I don&amp;#39;t see any link.&lt;/p&gt;\n\n&lt;p&gt;I have also tried various softwares, but it doesn&amp;#39;t work. Anyone can help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywwzkq", "is_robot_indexable": true, "report_reasons": null, "author": "oneminutetimemachine", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywwzkq/trying_to_download_video_from_a_streaming_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywwzkq/trying_to_download_video_from_a_streaming_website/", "subreddit_subscribers": 654279, "created_utc": 1668613653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Chinese companies will literally manufacture anything. Amazon is full of new cassette players recently manufactured but no one makes an affordable solutions to play those old video files. \n\nJust curious why that is? I recently digitized a lot of old Hi8, mini DV, and vhs tapes and I had to buy a 25 year old Hi8 player because my parents old one didn't power on when I started the project. It was like 100 dollars for a piece of 25 year old tech, lol. It just seems like SOME company out there has to make one, and I'm not looking in the right places online.", "author_fullname": "t2_528j4y3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why doesn't any company make affordable Hi8/Mini DV video players?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywujrj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668608097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Chinese companies will literally manufacture anything. Amazon is full of new cassette players recently manufactured but no one makes an affordable solutions to play those old video files. &lt;/p&gt;\n\n&lt;p&gt;Just curious why that is? I recently digitized a lot of old Hi8, mini DV, and vhs tapes and I had to buy a 25 year old Hi8 player because my parents old one didn&amp;#39;t power on when I started the project. It was like 100 dollars for a piece of 25 year old tech, lol. It just seems like SOME company out there has to make one, and I&amp;#39;m not looking in the right places online.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "ywujrj", "is_robot_indexable": true, "report_reasons": null, "author": "vaper1122", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywujrj/why_doesnt_any_company_make_affordable_hi8mini_dv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywujrj/why_doesnt_any_company_make_affordable_hi8mini_dv/", "subreddit_subscribers": 654279, "created_utc": 1668608097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Once upon a time I used to possess the ability to google search for directories of file servers such as apache that had no password protection just to poke around other peoples file for things i may also want to hoard. does anybody either have a link to a useful guide to these terms to use or would you like to share a personal favorite of yours? I've been only able to find webcam searching guides and clearly that's not what I'm looking for.", "author_fullname": "t2_4rrrxgqu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Dorking!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywi51u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668568531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Once upon a time I used to possess the ability to google search for directories of file servers such as apache that had no password protection just to poke around other peoples file for things i may also want to hoard. does anybody either have a link to a useful guide to these terms to use or would you like to share a personal favorite of yours? I&amp;#39;ve been only able to find webcam searching guides and clearly that&amp;#39;s not what I&amp;#39;m looking for.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywi51u", "is_robot_indexable": true, "report_reasons": null, "author": "wholesale_excuses", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywi51u/google_dorking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywi51u/google_dorking/", "subreddit_subscribers": 654279, "created_utc": 1668568531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Any recommendations for flash drives or thumb drives that should last forever? \n\nPrinting photo books for my kids, and I'd like to include a drive that holds all of the photos (beyond the ones printed).\n\nEx : Year One (drive holds all photos for the year)", "author_fullname": "t2_buinl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photo Archive / Flash Drive for Photo Books", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywyi8j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668616715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any recommendations for flash drives or thumb drives that should last forever? &lt;/p&gt;\n\n&lt;p&gt;Printing photo books for my kids, and I&amp;#39;d like to include a drive that holds all of the photos (beyond the ones printed).&lt;/p&gt;\n\n&lt;p&gt;Ex : Year One (drive holds all photos for the year)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywyi8j", "is_robot_indexable": true, "report_reasons": null, "author": "k0rtnie", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywyi8j/photo_archive_flash_drive_for_photo_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywyi8j/photo_archive_flash_drive_for_photo_books/", "subreddit_subscribers": 654279, "created_utc": 1668616715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking into an LTO drive and seeing if setting one up is even slightly reasonable... hardware wise I currently have an UNRAID server setup using SAS cards:\n\n\\- [LSI 6Gbps \u200bSAS HBA LS\u200bI 9211-8i](https://www.ebay.com/itm/163846248833)\n\n\\- IBM ServeRAID 16-Port 6Gbps SAS-2 SATA Expansion Adapter 46M0997 Firmware 634A\n\nIf I got an \"External SAS Tape Drive\" would I be able to just connect it to the existing cards or would I need some additional hardware?", "author_fullname": "t2_xo6fu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTO Drive Hardware Requirements", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywvk8p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668610434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking into an LTO drive and seeing if setting one up is even slightly reasonable... hardware wise I currently have an UNRAID server setup using SAS cards:&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://www.ebay.com/itm/163846248833\"&gt;LSI 6Gbps \u200bSAS HBA LS\u200bI 9211-8i&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;- IBM ServeRAID 16-Port 6Gbps SAS-2 SATA Expansion Adapter 46M0997 Firmware 634A&lt;/p&gt;\n\n&lt;p&gt;If I got an &amp;quot;External SAS Tape Drive&amp;quot; would I be able to just connect it to the existing cards or would I need some additional hardware?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aIxVIf65gdQa4Q52Am42_u1QDI2zWZrM5zd_7W3-RMg.jpg?auto=webp&amp;s=7332324fe116ca72801047496a80e8303ce6002d", "width": 400, "height": 306}, "resolutions": [{"url": "https://external-preview.redd.it/aIxVIf65gdQa4Q52Am42_u1QDI2zWZrM5zd_7W3-RMg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=58cb98cc3ebd95ddc2acbf09d1e13dcb2701c081", "width": 108, "height": 82}, {"url": "https://external-preview.redd.it/aIxVIf65gdQa4Q52Am42_u1QDI2zWZrM5zd_7W3-RMg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=69f736b1ec9c931ce9f71d4f934ec4d298899e04", "width": 216, "height": 165}, {"url": "https://external-preview.redd.it/aIxVIf65gdQa4Q52Am42_u1QDI2zWZrM5zd_7W3-RMg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=73d224d395cc4693fbf1f2c05bc0d0ab0b196a35", "width": 320, "height": 244}], "variants": {}, "id": "Gex7KEbK2g7DN-YtBgcB0bTCnsjcN0X6QcaLI0SldGo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywvk8p", "is_robot_indexable": true, "report_reasons": null, "author": "emotion_chip", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywvk8p/lto_drive_hardware_requirements/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywvk8p/lto_drive_hardware_requirements/", "subreddit_subscribers": 654279, "created_utc": 1668610434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If a LTO drive has a SAS connector on the back, does that mean it can be used with a normal SAS cable and card or does something else need to be done or is it a \"depends on the make and model of the drive\" thing?\n\nIt is supposedly from an old auto loader.", "author_fullname": "t2_16u0wi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTO Drive question, regarding connectivity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywcloj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668553680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If a LTO drive has a SAS connector on the back, does that mean it can be used with a normal SAS cable and card or does something else need to be done or is it a &amp;quot;depends on the make and model of the drive&amp;quot; thing?&lt;/p&gt;\n\n&lt;p&gt;It is supposedly from an old auto loader.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywcloj", "is_robot_indexable": true, "report_reasons": null, "author": "TheGleanerBaldwin", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywcloj/lto_drive_question_regarding_connectivity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywcloj/lto_drive_question_regarding_connectivity/", "subreddit_subscribers": 654279, "created_utc": 1668553680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey All,\n\nI'd like to add a cloud backup outside my normal routine. I would just like to find somewhere that would let me just upload via FTP. I need a decent amount of space 10tb or so. But I will be uploading about 6tb weekly (I can do bi-weekly if I have to.) This is for some VMs and is just another layer of backup. Obviously need something that can accept data at a good rate, I have 1250mbps up but fine with 500mbps up or so. I'd like to keep it around $100 a month. I'm ok paying something to download, if things go well I would never download. TIA", "author_fullname": "t2_17r9bt3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations for Cloud Storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yx7k33", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668635858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to add a cloud backup outside my normal routine. I would just like to find somewhere that would let me just upload via FTP. I need a decent amount of space 10tb or so. But I will be uploading about 6tb weekly (I can do bi-weekly if I have to.) This is for some VMs and is just another layer of backup. Obviously need something that can accept data at a good rate, I have 1250mbps up but fine with 500mbps up or so. I&amp;#39;d like to keep it around $100 a month. I&amp;#39;m ok paying something to download, if things go well I would never download. TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx7k33", "is_robot_indexable": true, "report_reasons": null, "author": "feudalle", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx7k33/recommendations_for_cloud_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx7k33/recommendations_for_cloud_storage/", "subreddit_subscribers": 654279, "created_utc": 1668635858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I downloaded dozens of .html files which represent my wesbite.  (Actually .mhtml)\n\nI want to browse this website offline.   Is there an efficient way of changing the links in those .mhtml files to look locally in the current directory for the next page. \n\nCurrently I am looking at Notepad++ find/replace in Files.  The links can be sometime a little complex, e.g.\n\n    http://www.mysitename.org/index.html?page=section2\n    (should translate into:   index.html?page=section2\n    or\n    http://www.mysitename.org/index.html#bookmark", "author_fullname": "t2_1cx04ck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I downloaded my website (a lot of .html files), is there a efficient way to change the links in those files to look locally ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yx5zl3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668632236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded dozens of .html files which represent my wesbite.  (Actually .mhtml)&lt;/p&gt;\n\n&lt;p&gt;I want to browse this website offline.   Is there an efficient way of changing the links in those .mhtml files to look locally in the current directory for the next page. &lt;/p&gt;\n\n&lt;p&gt;Currently I am looking at Notepad++ find/replace in Files.  The links can be sometime a little complex, e.g.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;http://www.mysitename.org/index.html?page=section2\n(should translate into:   index.html?page=section2\nor\nhttp://www.mysitename.org/index.html#bookmark\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx5zl3", "is_robot_indexable": true, "report_reasons": null, "author": "Ian_SAfc", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx5zl3/i_downloaded_my_website_a_lot_of_html_files_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx5zl3/i_downloaded_my_website_a_lot_of_html_files_is/", "subreddit_subscribers": 654279, "created_utc": 1668632236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Dying Drive](https://imgur.com/a/jChZ3mb)\n\nMy 2TB 5200RPM Drive its having a long death, so time to find its replacement. \n\nIm running a Xeoma Docker instance, connected to around 16 IP Cameras, Resolution 4MP, all H265, each has a data load of around .5 - 2MB/s with my current settings, so a total for all the cameras 30 - 40MB/s. \n\nMy current WD Golds data transfer average its 180MB/s, so that means a 7200RPM drive can handle a lot of cameras (64 Cameras)... Im thinking my numbers are probably way off. \n\nBut well, the latest Seagate 2TB its having a premature death, so maybe this setup its a little hard on the HDD's.... or not?. \n\nWould it be better to setup an small 512 / 1TB SSD as a cache of the recordings*?, and then send each day to the HDD?. Not sure if a lot of writes are better handled with a Enterprise SSD, or a spinnning one. \n\n*For the unfamiliar, Unraid can use SSD's as cache, and a little script called \"Mover\" can send the data to the HDD at certain intervals, generally each night. \n\n\n\n**Current Setup:**\n\n2TB Seagate, Single Pool Device, BTFRS partition. \n\nRequirements:\n* No need to have parity protection on this data.\n* Try to not kill the HDD / SSD too fast.\n* Able to handle future expansion, maybe 20 - 30 cameras in total\n\nIn other subject... -- Are the CCTV HHD Drives any different?. \n \n\nThanks!", "author_fullname": "t2_op35yhd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended Drive Setup for a \"Big\" CCTV Setup | SSD or HHD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yx5w9o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668632019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://imgur.com/a/jChZ3mb\"&gt;Dying Drive&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;My 2TB 5200RPM Drive its having a long death, so time to find its replacement. &lt;/p&gt;\n\n&lt;p&gt;Im running a Xeoma Docker instance, connected to around 16 IP Cameras, Resolution 4MP, all H265, each has a data load of around .5 - 2MB/s with my current settings, so a total for all the cameras 30 - 40MB/s. &lt;/p&gt;\n\n&lt;p&gt;My current WD Golds data transfer average its 180MB/s, so that means a 7200RPM drive can handle a lot of cameras (64 Cameras)... Im thinking my numbers are probably way off. &lt;/p&gt;\n\n&lt;p&gt;But well, the latest Seagate 2TB its having a premature death, so maybe this setup its a little hard on the HDD&amp;#39;s.... or not?. &lt;/p&gt;\n\n&lt;p&gt;Would it be better to setup an small 512 / 1TB SSD as a cache of the recordings*?, and then send each day to the HDD?. Not sure if a lot of writes are better handled with a Enterprise SSD, or a spinnning one. &lt;/p&gt;\n\n&lt;p&gt;*For the unfamiliar, Unraid can use SSD&amp;#39;s as cache, and a little script called &amp;quot;Mover&amp;quot; can send the data to the HDD at certain intervals, generally each night. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Current Setup:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;2TB Seagate, Single Pool Device, BTFRS partition. &lt;/p&gt;\n\n&lt;p&gt;Requirements:\n* No need to have parity protection on this data.\n* Try to not kill the HDD / SSD too fast.\n* Able to handle future expansion, maybe 20 - 30 cameras in total&lt;/p&gt;\n\n&lt;p&gt;In other subject... -- Are the CCTV HHD Drives any different?. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mR-ALxQ6H4sWfSfZCrVUdF9851aC9L_yMKomaozId5s.jpg?auto=webp&amp;s=5ecf8b530546b3ed5b9e604e3947786ed2130c0e", "width": 632, "height": 112}, "resolutions": [{"url": "https://external-preview.redd.it/mR-ALxQ6H4sWfSfZCrVUdF9851aC9L_yMKomaozId5s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=67641db16e987549fa687e2037826a60b0b1c9e3", "width": 108, "height": 19}, {"url": "https://external-preview.redd.it/mR-ALxQ6H4sWfSfZCrVUdF9851aC9L_yMKomaozId5s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d2dcf370d21430adf6ffdee95a3eddd0e3fe321f", "width": 216, "height": 38}, {"url": "https://external-preview.redd.it/mR-ALxQ6H4sWfSfZCrVUdF9851aC9L_yMKomaozId5s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1cb4c9cde1cd4540c0a81fff144cfb0d5c38ecf", "width": 320, "height": 56}], "variants": {}, "id": "Vqdg8PVo2JMnpNx40R_kHE4Yn9eH75yim4kKNekLkBA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx5w9o", "is_robot_indexable": true, "report_reasons": null, "author": "thinkyougotmewrong", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx5w9o/recommended_drive_setup_for_a_big_cctv_setup_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx5w9o/recommended_drive_setup_for_a_big_cctv_setup_ssd/", "subreddit_subscribers": 654279, "created_utc": 1668632019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was using Tiny Media, but discovered that it is rentalware.  I don't mind buying a tool, but abhor subscriptions.  Anyone have a good suggestion?\n\nThe hoard is on a Synology serving Nvidia Shields and Kodi.  I was using Tiny to scrape the metadata and had Kodi pulling it locally instead of searching the net (faster and organized/renamed the files).  \n\nI'm open to other ways of doing this that do not involve Plex.", "author_fullname": "t2_2ztygbcz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a Video Metadata Scraper", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx5hm0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668631131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was using Tiny Media, but discovered that it is rentalware.  I don&amp;#39;t mind buying a tool, but abhor subscriptions.  Anyone have a good suggestion?&lt;/p&gt;\n\n&lt;p&gt;The hoard is on a Synology serving Nvidia Shields and Kodi.  I was using Tiny to scrape the metadata and had Kodi pulling it locally instead of searching the net (faster and organized/renamed the files).  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m open to other ways of doing this that do not involve Plex.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx5hm0", "is_robot_indexable": true, "report_reasons": null, "author": "Bushpylot", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx5hm0/looking_for_a_video_metadata_scraper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx5hm0/looking_for_a_video_metadata_scraper/", "subreddit_subscribers": 654279, "created_utc": 1668631131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "# utzoo-mongo-db-converter\n\nI wrote some scripts for converting the UTZOO Usenet archive to a Mongo Database.\n\n## Background\n\nI recently found out about the [UTZOO Usenet Archive](https://archive.org/details/utzoo-wiseman-usenet-archive), a collection of something like two million [Usenet](https://en.wikipedia.org/wiki/Usenet) posts archived by [Henry Spencer](https://en.wikipedia.org/wiki/Henry_Spencer) from 1981 to 1991. I wanted to play around with the data, so I wrote some Node script to convert the files to a Mongo database. It's a little bit hacky, but it works.\n\n## Getting the data:\n\nThe data was available on the Internet Archive. You could download the data from the [UTZOO Usenet Archive](https://archive.org/details/utzoo-wiseman-usenet-archive) page. After legal demands were placed requesting the internet archive to remove the data, the data was removed and replaced with its checksum. Fortunately, some [friendly folks on Reddit](https://www.reddit.com/r/DataHoarder/comments/i2btuu/utzoo_archives_have_been_removed_from_archiveorg/) have preserved the data. You can download the UTZOO archive from a torrent linked in that post.\n\n## The files\n\nThe UTZOO archive has individual Usenet post saved individual files. This makes searching its contents pretty slow, especially running a search using in Windows. [GREP](https://man7.org/linux/man-pages/man1/grep.1.html) was faster, but still doesn't take advantage of search indexing. Seeking through the contents of files this way is always dead slow. I thought about a few ways of being able to quickly search the data. [Maybe a RAM disk?](https://en.wikipedia.org/wiki/RAM_drive). Faster storage? The drive I'm working from is already pretty fast (Gen 3 NVMe). Maybe I could use a database? I decided try using a database.\n\n## Why Mongo?\n\nI've been playing around with Mongo for a bit. Other implementations of database of the UTZOO archives are [SQL based](https://www.reddit.com/r/java/comments/bkm5h5/how_i_converted_utzoo_usenet_archive_from/) and I thought it might be interesting to try something different, NoSQL. What I would like to test is the ability to quickly search the data. I'm not sure if Mongo is the best choice for this, but I thought UTZOO could be a good test case for [Mongo's search indexing](https://www.mongodb.com/basics/search-index) features.\n\n## Decompressing the data:\n\nRather than decompress the data in Node, I decompressed it using 7zip, so once you retrieve the archive, you'll also need to do so.\n\n## How to run:\n\nYou must have the contents of the UTZOO Usenet archive extracted in a directory named UTZOO, in the same directory as the server and\n\n(If you are using [MongoDB.com](http://mongodb.com/) and) open the serverfolder and:\n\n* Create a file called credentials.js. In it, include:\n\n&amp;#x200B;\n\n    const clusterURL = `your_cluster_url`; \n    const credentials = {username : \"your_username\", password : \"your_password\" }  module.exports = {credentials: credentials, clusterURL: clusterURL }\n\n* Modify server.js:\n\n&amp;#x200B;\n\n    var { credentials, clusterURL } = require('./credentials'); \n    var dbURI = `mongodb+srv://${credentials.username}:${credentials.password}@${clusterURL}`;\n\n* Run \"node server\" to start the server.\n\nOr, if you are using a local database:\n\n* Install [MongoDB](https://www.mongodb.com/) and start a server\n* Open the serverfolder and modify server.js:\n\n&amp;#x200B;\n\n    var {clusterURL } = require('./credentials'); var dbURI = `${clusterURL}`;\n\n* Or, change clusterURLin credentials.jsto the URL of your local database, or replace the string with your database url, ex:\n\n&amp;#x200B;\n\n    var dbURI = \"mongodb://localhost:27017\";\n\nRun \"node server\" to start the server.\n\nImport:\n\nTo start adding data to the database, leave the server running and open the folder named \"importer\" and run \"node listworker.js\"\n\nThis will start inserting the data into the database. It will take a while.\n\n## Problems:\n\nI am still working on parsing the data, so the importer is not complete. I will update this when it is. There is a wide variety of header formats in the UTZOO archive. This includes a variety of different date codes. Later headers include a key to the header's value, early headers lack this. I still need to fix some of the date parsing. There's also a few headers that I haven't accounted for yet. I'm working on it.\n\n## Using the data:\n\nYou can use either [MongoDB.com](http://mongodb.com/)'s dashboard (if you host a remote database) or Mongo Compass to run queries on the data or you can modify the express middleware with your own queries. I'm still working on the API, so it's not very robust yet. I will update this when it is.\n\n## Performance\n\nThis script recursively crawls through the UTZOO directory and adds the files to the Mongo database using fetch calls to the expressbased API. Saving the data to the database took about 12 hours on my machine to a local MongoDB server.\n\nThere's definitely faster ways of doing this  \u00af\\_(\u30c4)\\_/\u00af. Ditching the middleware library (Express) would probably help.\n\n## Code:\n\nThe code is available on [Github](https://github.com/LiamOsler/utzoo-mongo-db-converter).\n\n## Going Forward:\n\nOnce I have the Database built up, I'm going to try and build a small front-end that will allow you to search the contents of the Usenet posts, something along the lines of [www.usenetarchives.com/](https://file+.vscode-resource.vscode-cdn.net/c:/Users/Magenta/Desktop/www.usenetarchives.com) but built with a MERN stack. Feel free to copy and modify the code.", "author_fullname": "t2_74ub1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I wrote some scripts for converting the UTZOO Usenet archive to a Mongo Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx44mx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668630289.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668628305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;utzoo-mongo-db-converter&lt;/h1&gt;\n\n&lt;p&gt;I wrote some scripts for converting the UTZOO Usenet archive to a Mongo Database.&lt;/p&gt;\n\n&lt;h2&gt;Background&lt;/h2&gt;\n\n&lt;p&gt;I recently found out about the &lt;a href=\"https://archive.org/details/utzoo-wiseman-usenet-archive\"&gt;UTZOO Usenet Archive&lt;/a&gt;, a collection of something like two million &lt;a href=\"https://en.wikipedia.org/wiki/Usenet\"&gt;Usenet&lt;/a&gt; posts archived by &lt;a href=\"https://en.wikipedia.org/wiki/Henry_Spencer\"&gt;Henry Spencer&lt;/a&gt; from 1981 to 1991. I wanted to play around with the data, so I wrote some Node script to convert the files to a Mongo database. It&amp;#39;s a little bit hacky, but it works.&lt;/p&gt;\n\n&lt;h2&gt;Getting the data:&lt;/h2&gt;\n\n&lt;p&gt;The data was available on the Internet Archive. You could download the data from the &lt;a href=\"https://archive.org/details/utzoo-wiseman-usenet-archive\"&gt;UTZOO Usenet Archive&lt;/a&gt; page. After legal demands were placed requesting the internet archive to remove the data, the data was removed and replaced with its checksum. Fortunately, some &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/i2btuu/utzoo_archives_have_been_removed_from_archiveorg/\"&gt;friendly folks on Reddit&lt;/a&gt; have preserved the data. You can download the UTZOO archive from a torrent linked in that post.&lt;/p&gt;\n\n&lt;h2&gt;The files&lt;/h2&gt;\n\n&lt;p&gt;The UTZOO archive has individual Usenet post saved individual files. This makes searching its contents pretty slow, especially running a search using in Windows. &lt;a href=\"https://man7.org/linux/man-pages/man1/grep.1.html\"&gt;GREP&lt;/a&gt; was faster, but still doesn&amp;#39;t take advantage of search indexing. Seeking through the contents of files this way is always dead slow. I thought about a few ways of being able to quickly search the data. &lt;a href=\"https://en.wikipedia.org/wiki/RAM_drive\"&gt;Maybe a RAM disk?&lt;/a&gt;. Faster storage? The drive I&amp;#39;m working from is already pretty fast (Gen 3 NVMe). Maybe I could use a database? I decided try using a database.&lt;/p&gt;\n\n&lt;h2&gt;Why Mongo?&lt;/h2&gt;\n\n&lt;p&gt;I&amp;#39;ve been playing around with Mongo for a bit. Other implementations of database of the UTZOO archives are &lt;a href=\"https://www.reddit.com/r/java/comments/bkm5h5/how_i_converted_utzoo_usenet_archive_from/\"&gt;SQL based&lt;/a&gt; and I thought it might be interesting to try something different, NoSQL. What I would like to test is the ability to quickly search the data. I&amp;#39;m not sure if Mongo is the best choice for this, but I thought UTZOO could be a good test case for &lt;a href=\"https://www.mongodb.com/basics/search-index\"&gt;Mongo&amp;#39;s search indexing&lt;/a&gt; features.&lt;/p&gt;\n\n&lt;h2&gt;Decompressing the data:&lt;/h2&gt;\n\n&lt;p&gt;Rather than decompress the data in Node, I decompressed it using 7zip, so once you retrieve the archive, you&amp;#39;ll also need to do so.&lt;/p&gt;\n\n&lt;h2&gt;How to run:&lt;/h2&gt;\n\n&lt;p&gt;You must have the contents of the UTZOO Usenet archive extracted in a directory named UTZOO, in the same directory as the server and&lt;/p&gt;\n\n&lt;p&gt;(If you are using &lt;a href=\"http://mongodb.com/\"&gt;MongoDB.com&lt;/a&gt; and) open the serverfolder and:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Create a file called credentials.js. In it, include:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;const clusterURL = `your_cluster_url`; \nconst credentials = {username : &amp;quot;your_username&amp;quot;, password : &amp;quot;your_password&amp;quot; }  module.exports = {credentials: credentials, clusterURL: clusterURL }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Modify server.js:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;var { credentials, clusterURL } = require(&amp;#39;./credentials&amp;#39;); \nvar dbURI = `mongodb+srv://${credentials.username}:${credentials.password}@${clusterURL}`;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Run &amp;quot;node server&amp;quot; to start the server.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Or, if you are using a local database:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Install &lt;a href=\"https://www.mongodb.com/\"&gt;MongoDB&lt;/a&gt; and start a server&lt;/li&gt;\n&lt;li&gt;Open the serverfolder and modify server.js:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;var {clusterURL } = require(&amp;#39;./credentials&amp;#39;); var dbURI = `${clusterURL}`;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Or, change clusterURLin credentials.jsto the URL of your local database, or replace the string with your database url, ex:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;var dbURI = &amp;quot;mongodb://localhost:27017&amp;quot;;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Run &amp;quot;node server&amp;quot; to start the server.&lt;/p&gt;\n\n&lt;p&gt;Import:&lt;/p&gt;\n\n&lt;p&gt;To start adding data to the database, leave the server running and open the folder named &amp;quot;importer&amp;quot; and run &amp;quot;node listworker.js&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;This will start inserting the data into the database. It will take a while.&lt;/p&gt;\n\n&lt;h2&gt;Problems:&lt;/h2&gt;\n\n&lt;p&gt;I am still working on parsing the data, so the importer is not complete. I will update this when it is. There is a wide variety of header formats in the UTZOO archive. This includes a variety of different date codes. Later headers include a key to the header&amp;#39;s value, early headers lack this. I still need to fix some of the date parsing. There&amp;#39;s also a few headers that I haven&amp;#39;t accounted for yet. I&amp;#39;m working on it.&lt;/p&gt;\n\n&lt;h2&gt;Using the data:&lt;/h2&gt;\n\n&lt;p&gt;You can use either &lt;a href=\"http://mongodb.com/\"&gt;MongoDB.com&lt;/a&gt;&amp;#39;s dashboard (if you host a remote database) or Mongo Compass to run queries on the data or you can modify the express middleware with your own queries. I&amp;#39;m still working on the API, so it&amp;#39;s not very robust yet. I will update this when it is.&lt;/p&gt;\n\n&lt;h2&gt;Performance&lt;/h2&gt;\n\n&lt;p&gt;This script recursively crawls through the UTZOO directory and adds the files to the Mongo database using fetch calls to the expressbased API. Saving the data to the database took about 12 hours on my machine to a local MongoDB server.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s definitely faster ways of doing this  \u00af_(\u30c4)_/\u00af. Ditching the middleware library (Express) would probably help.&lt;/p&gt;\n\n&lt;h2&gt;Code:&lt;/h2&gt;\n\n&lt;p&gt;The code is available on &lt;a href=\"https://github.com/LiamOsler/utzoo-mongo-db-converter\"&gt;Github&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h2&gt;Going Forward:&lt;/h2&gt;\n\n&lt;p&gt;Once I have the Database built up, I&amp;#39;m going to try and build a small front-end that will allow you to search the contents of the Usenet posts, something along the lines of &lt;a href=\"https://file+.vscode-resource.vscode-cdn.net/c:/Users/Magenta/Desktop/www.usenetarchives.com\"&gt;www.usenetarchives.com/&lt;/a&gt; but built with a MERN stack. Feel free to copy and modify the code.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EjEbcSRhInDbL38pGa6AqM8eZoVgo3z49ki35Fj7_zg.jpg?auto=webp&amp;s=e98b6b090958ed73451630b5d6218dab2dd87f96", "width": 180, "height": 144}, "resolutions": [{"url": "https://external-preview.redd.it/EjEbcSRhInDbL38pGa6AqM8eZoVgo3z49ki35Fj7_zg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=91ff2761db12355841a21002593f06f933362fe7", "width": 108, "height": 86}], "variants": {}, "id": "L618DVNfFyrYa22IPEcRYVxjn8GGPvNgDb1EWYQsaNo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx44mx", "is_robot_indexable": true, "report_reasons": null, "author": "Democedes", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx44mx/i_wrote_some_scripts_for_converting_the_utzoo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx44mx/i_wrote_some_scripts_for_converting_the_utzoo/", "subreddit_subscribers": 654279, "created_utc": 1668628305.0, "num_crossposts": 4, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "title explains most of it. i have ~1k+ songs liked on youtube music and even more videos liked on youtube. i want to download them just in case anything gets taken down or region banned (happened several times)\n\nis there a way to automate this? and i'm worried that if i do this i get flagged for suspicious activity and risk my account getting banned. so how can i minimize the risk of account issues? (e.g. batching them?)\n\nmainly interested in archiving my youtube music likes", "author_fullname": "t2_b0ac2pk3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to download my liked music and videos from youtube is there a way to do automate this and not risk getting my account banned?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx3t5k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668627616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title explains most of it. i have ~1k+ songs liked on youtube music and even more videos liked on youtube. i want to download them just in case anything gets taken down or region banned (happened several times)&lt;/p&gt;\n\n&lt;p&gt;is there a way to automate this? and i&amp;#39;m worried that if i do this i get flagged for suspicious activity and risk my account getting banned. so how can i minimize the risk of account issues? (e.g. batching them?)&lt;/p&gt;\n\n&lt;p&gt;mainly interested in archiving my youtube music likes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx3t5k", "is_robot_indexable": true, "report_reasons": null, "author": "ChaosFairyMagic", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx3t5k/i_want_to_download_my_liked_music_and_videos_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx3t5k/i_want_to_download_my_liked_music_and_videos_from/", "subreddit_subscribers": 654279, "created_utc": 1668627616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Wondering what's y'all's experience using SkyHawk and Purple surveillance drives in USB enclosures? I know some models are 5400/5900 RPM, but how do the 7200 RPM drives stack up in that kind of use?\n\n&amp;#x200B;\n\nOne of my 2.5 SMR drives is dead so I'm going to just go the 3.5 route, but there seems to be good pricing on certain surveillance drives, so I'm wondering about using one as an active drive (not just for backup mirror)", "author_fullname": "t2_lv0pd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Skyhawk/Purple drives as external", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx2txh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668625582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering what&amp;#39;s y&amp;#39;all&amp;#39;s experience using SkyHawk and Purple surveillance drives in USB enclosures? I know some models are 5400/5900 RPM, but how do the 7200 RPM drives stack up in that kind of use?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;One of my 2.5 SMR drives is dead so I&amp;#39;m going to just go the 3.5 route, but there seems to be good pricing on certain surveillance drives, so I&amp;#39;m wondering about using one as an active drive (not just for backup mirror)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx2txh", "is_robot_indexable": true, "report_reasons": null, "author": "121PB4Y2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx2txh/skyhawkpurple_drives_as_external/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx2txh/skyhawkpurple_drives_as_external/", "subreddit_subscribers": 654279, "created_utc": 1668625582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a tool to mirror dynamic websites that host theirs image/gif and videos on other domains. Im taking this [site](https://joedubs.com/) as starting point.  I tried with wget, but didn't find the satisfactory combination of arguments.  Any advice is welcome.", "author_fullname": "t2_59rlp3yo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimal tool for mirroring a dynamic website?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx2n89", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668625206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a tool to mirror dynamic websites that host theirs image/gif and videos on other domains. Im taking this &lt;a href=\"https://joedubs.com/\"&gt;site&lt;/a&gt; as starting point.  I tried with wget, but didn&amp;#39;t find the satisfactory combination of arguments.  Any advice is welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx2n89", "is_robot_indexable": true, "report_reasons": null, "author": "mataka54321", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx2n89/optimal_tool_for_mirroring_a_dynamic_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx2n89/optimal_tool_for_mirroring_a_dynamic_website/", "subreddit_subscribers": 654279, "created_utc": 1668625206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have been trying to download video from a streaming website, but nothing seems to be working.\n\nHere's an [example](https://www.americastestkitchen.com/cookscountry/episode/829-low-country-party) of the link.\n\nI have tried using Inspect Element &gt; Network &gt; Media, and then played the video, but I didn't see any link at all.\n\nI have also tried various softwares, but none of them worked.  Can anyone help please?", "author_fullname": "t2_7m3oj37l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to Download video from a streaming website.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywxlql", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668615347.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668614900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have been trying to download video from a streaming website, but nothing seems to be working.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an &lt;a href=\"https://www.americastestkitchen.com/cookscountry/episode/829-low-country-party\"&gt;example&lt;/a&gt; of the link.&lt;/p&gt;\n\n&lt;p&gt;I have tried using Inspect Element &amp;gt; Network &amp;gt; Media, and then played the video, but I didn&amp;#39;t see any link at all.&lt;/p&gt;\n\n&lt;p&gt;I have also tried various softwares, but none of them worked.  Can anyone help please?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywxlql", "is_robot_indexable": true, "report_reasons": null, "author": "Popular_Walk7", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywxlql/trying_to_download_video_from_a_streaming_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywxlql/trying_to_download_video_from_a_streaming_website/", "subreddit_subscribers": 654279, "created_utc": 1668614900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sharing my experience with others:\n\nAcronis True Image (WD) significantly reduced/impacted my Samsung EVO 970 Plus NVME's RND4K performance.\n\nUsing CrystalDiskMark 8\n\nFrom (original):  \n\\[Read\\] RND    4KiB (Q= 32, T= 1):   513 MB/s   \n\\[Write\\] RND    4KiB (Q= 32, T= 1):   378 MB/s\n\nTo:  \n\\[Read\\] RND    4KiB (Q= 32, T= 1):   310 MB/s   \n\\[Write\\] RND    4KiB (Q= 32, T= 1):   260 MB/s\n\n  \nAbout a 40% reduction.\n\nNo impact to sequential read/write (as far as I can tell)\n\nI'll now be seeking a different solution :)", "author_fullname": "t2_jbk6ie2s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA - Acronis True Image reduces NVME/SSD random read/write performance by 40%", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywuv79", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668608794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing my experience with others:&lt;/p&gt;\n\n&lt;p&gt;Acronis True Image (WD) significantly reduced/impacted my Samsung EVO 970 Plus NVME&amp;#39;s RND4K performance.&lt;/p&gt;\n\n&lt;p&gt;Using CrystalDiskMark 8&lt;/p&gt;\n\n&lt;p&gt;From (original):&lt;br/&gt;\n[Read] RND    4KiB (Q= 32, T= 1):   513 MB/s&lt;br/&gt;\n[Write] RND    4KiB (Q= 32, T= 1):   378 MB/s&lt;/p&gt;\n\n&lt;p&gt;To:&lt;br/&gt;\n[Read] RND    4KiB (Q= 32, T= 1):   310 MB/s&lt;br/&gt;\n[Write] RND    4KiB (Q= 32, T= 1):   260 MB/s&lt;/p&gt;\n\n&lt;p&gt;About a 40% reduction.&lt;/p&gt;\n\n&lt;p&gt;No impact to sequential read/write (as far as I can tell)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll now be seeking a different solution :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywuv79", "is_robot_indexable": true, "report_reasons": null, "author": "w8itquietly", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywuv79/psa_acronis_true_image_reduces_nvmessd_random/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywuv79/psa_acronis_true_image_reduces_nvmessd_random/", "subreddit_subscribers": 654279, "created_utc": 1668608794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_56is8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate claims Mach.2-enabled Exos 2X18 is the world's fastest hard disk", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_ywj044", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": "", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IscmVGaxM6C9vRbMvpTOBLRKUXsMkE19mJbjsa-4U9s.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668570938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "techspot.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.techspot.com/news/96662-seagate-claims-mach2-enabled-exos-2x18-world-fastest.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yb7-GI2Le7wjbze_nlLgjiDpRc6zs2XmLptp1n47-xc.jpg?auto=webp&amp;s=fb99809c25f22ddda70794c4dc07a71c7f253eca", "width": 2000, "height": 1125}, "resolutions": [{"url": "https://external-preview.redd.it/yb7-GI2Le7wjbze_nlLgjiDpRc6zs2XmLptp1n47-xc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=63ed53639cb4c2429cb61a56e880e894d96ad187", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/yb7-GI2Le7wjbze_nlLgjiDpRc6zs2XmLptp1n47-xc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=348f6aa3dc6c0aa0cbd5abc8af90026d040e28ef", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/yb7-GI2Le7wjbze_nlLgjiDpRc6zs2XmLptp1n47-xc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a351d75fc960052b9863b9ddce9b2d39bd65379a", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/yb7-GI2Le7wjbze_nlLgjiDpRc6zs2XmLptp1n47-xc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2b2486bea160062ba95000eac9105423af418f34", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/yb7-GI2Le7wjbze_nlLgjiDpRc6zs2XmLptp1n47-xc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e483721e3eafba34de112d8c56f4d92031e74de", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/yb7-GI2Le7wjbze_nlLgjiDpRc6zs2XmLptp1n47-xc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c73e752799463d19a858028cc0351b62e88ff0ae", "width": 1080, "height": 607}], "variants": {}, "id": "31pQkMrKzK-p7X9W8txqj4GUr_77eubTmMMRNbX8P7Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "17.58 TB of crap", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywj044", "is_robot_indexable": true, "report_reasons": null, "author": "wickedplayer494", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/ywj044/seagate_claims_mach2enabled_exos_2x18_is_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.techspot.com/news/96662-seagate-claims-mach2-enabled-exos-2x18-world-fastest.html", "subreddit_subscribers": 654279, "created_utc": 1668570938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, so I'd never usually buy a used HDD, but I'm low on money at the moment and need a new drive desperately, I know WD RE drives are datacenter drives made to generally take a lot of use, so is buying one used less risky than a run of the mill WD-Blue or equivalent? I say this because I've found a 2TB one for around \u00a320, date on it is from 2016 so it's already 8 years old, usually id steer clear but I've got no experience with this type of drive other than knowing they're generally tanks compared to more commercial/retail sold drives\n\nFor more information I'd be using it to store miscellaneous files and video games as I'm having to pull my existing 4tb drive and it's plex library into a USB enclosure and need a drive to put back into my pc for all the non-plex stuff that's on it", "author_fullname": "t2_xs46u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "buying a used WD-RE drive, bad idea?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywguip", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668565003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, so I&amp;#39;d never usually buy a used HDD, but I&amp;#39;m low on money at the moment and need a new drive desperately, I know WD RE drives are datacenter drives made to generally take a lot of use, so is buying one used less risky than a run of the mill WD-Blue or equivalent? I say this because I&amp;#39;ve found a 2TB one for around \u00a320, date on it is from 2016 so it&amp;#39;s already 8 years old, usually id steer clear but I&amp;#39;ve got no experience with this type of drive other than knowing they&amp;#39;re generally tanks compared to more commercial/retail sold drives&lt;/p&gt;\n\n&lt;p&gt;For more information I&amp;#39;d be using it to store miscellaneous files and video games as I&amp;#39;m having to pull my existing 4tb drive and it&amp;#39;s plex library into a USB enclosure and need a drive to put back into my pc for all the non-plex stuff that&amp;#39;s on it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywguip", "is_robot_indexable": true, "report_reasons": null, "author": "Georgerm97", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywguip/buying_a_used_wdre_drive_bad_idea/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywguip/buying_a_used_wdre_drive_bad_idea/", "subreddit_subscribers": 654279, "created_utc": 1668565003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm interested in the **best** way to sync **files** from OneDrive to Apple's iCloud Drive. I've read multiple articles on the issue but most deal with cost, which in my scenario is a non-issue because I'm already paying for OneDrive (professional reasons) and the family is on the icloud 2TB plan. I'm looking at about 100-300 GB of data. I have apple, windows and linux laptops, also proxmox servers. What I don't have, is a clear-cut solution to syncing OneDrive *to* iCloud Drive. Two-way sync would be nice, but it's not a must.\n\nMy preferred Apple laptop for this application is an old retina MacBook Pro 2013 that doesn't run Monterey, but I'm willing to buy some SH Mac Mini for this situation. Currently the solution is more important than the cost TBH bceasue I'm looking for a set and forget scenario.\n\nI'm hoping others can describe solutions if in a similar situation. I was thinking of the following:\n\n**Option 1:** Setup a macOS system to do the syncing. Setup OneDrive and iCloud Drive to \"keep all files locally\" and then rsync between the two folders. \n\nPros:\n\n* native syncing of iDrive\n\nCons:\n\n* my macOS laptop for this \"application\" is a bit old (2013 rMBP) which is currently unsupported.  I don't want to run unsupported OSes for security reasons and I don't want workarounds, especially when settings this up for the next few years;\n* dedicated Apple Hardware to do the task, whereas Windows VMs might do the trick;\n* Unable to check if iCloud Drive and OneDrive are signed in and healthy. When OneDrive might fail, it could desync, but worst case scenario I think it could completely wipeout \n\n**Option 2** \nRun a Windows VM, signed in to iCloud Drive and OneDrive, both setup to keep all files on device.\n\nPros:\n\n* cheapest and most portable\n\nIssues:\n\n* as above, unable to automcatically check and resolve if iCloud Drive and OneDrive are signed in and sync is healthy.", "author_fullname": "t2_13ctie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way, in the technical sense, to backup iCloud Drive to MS OneDrive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywfp5m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668561881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m interested in the &lt;strong&gt;best&lt;/strong&gt; way to sync &lt;strong&gt;files&lt;/strong&gt; from OneDrive to Apple&amp;#39;s iCloud Drive. I&amp;#39;ve read multiple articles on the issue but most deal with cost, which in my scenario is a non-issue because I&amp;#39;m already paying for OneDrive (professional reasons) and the family is on the icloud 2TB plan. I&amp;#39;m looking at about 100-300 GB of data. I have apple, windows and linux laptops, also proxmox servers. What I don&amp;#39;t have, is a clear-cut solution to syncing OneDrive &lt;em&gt;to&lt;/em&gt; iCloud Drive. Two-way sync would be nice, but it&amp;#39;s not a must.&lt;/p&gt;\n\n&lt;p&gt;My preferred Apple laptop for this application is an old retina MacBook Pro 2013 that doesn&amp;#39;t run Monterey, but I&amp;#39;m willing to buy some SH Mac Mini for this situation. Currently the solution is more important than the cost TBH bceasue I&amp;#39;m looking for a set and forget scenario.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping others can describe solutions if in a similar situation. I was thinking of the following:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Option 1:&lt;/strong&gt; Setup a macOS system to do the syncing. Setup OneDrive and iCloud Drive to &amp;quot;keep all files locally&amp;quot; and then rsync between the two folders. &lt;/p&gt;\n\n&lt;p&gt;Pros:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;native syncing of iDrive&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Cons:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;my macOS laptop for this &amp;quot;application&amp;quot; is a bit old (2013 rMBP) which is currently unsupported.  I don&amp;#39;t want to run unsupported OSes for security reasons and I don&amp;#39;t want workarounds, especially when settings this up for the next few years;&lt;/li&gt;\n&lt;li&gt;dedicated Apple Hardware to do the task, whereas Windows VMs might do the trick;&lt;/li&gt;\n&lt;li&gt;Unable to check if iCloud Drive and OneDrive are signed in and healthy. When OneDrive might fail, it could desync, but worst case scenario I think it could completely wipeout &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Option 2&lt;/strong&gt; \nRun a Windows VM, signed in to iCloud Drive and OneDrive, both setup to keep all files on device.&lt;/p&gt;\n\n&lt;p&gt;Pros:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;cheapest and most portable&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Issues:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;as above, unable to automcatically check and resolve if iCloud Drive and OneDrive are signed in and sync is healthy.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywfp5m", "is_robot_indexable": true, "report_reasons": null, "author": "blucafee80", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywfp5m/best_way_in_the_technical_sense_to_backup_icloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywfp5m/best_way_in_the_technical_sense_to_backup_icloud/", "subreddit_subscribers": 654279, "created_utc": 1668561881.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}