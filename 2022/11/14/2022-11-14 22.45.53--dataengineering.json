{"kind": "Listing", "data": {"after": "t3_yv1uee", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know the best way to find part time work (\\~20 hrs/wk as freelance/independent contractor) as a data engineer?\n\nRecruiters? Job sites? I\u2019ve thought about toptal/upwork but figured going out on my own is better", "author_fullname": "t2_gnj3pgpb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer Part Time Job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yui8kn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668381751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know the best way to find part time work (~20 hrs/wk as freelance/independent contractor) as a data engineer?&lt;/p&gt;\n\n&lt;p&gt;Recruiters? Job sites? I\u2019ve thought about toptal/upwork but figured going out on my own is better&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yui8kn", "is_robot_indexable": true, "report_reasons": null, "author": "Kini_J", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yui8kn/data_engineer_part_time_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yui8kn/data_engineer_part_time_job/", "subreddit_subscribers": 79968, "created_utc": 1668381751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the best practices to load large data into a cloud datawarehouse like snowflake? Lets say I have 10 tables (each table size is &gt; 50gb) in a source database and I am using s3 as a intermediate storage layer. What is the best approach to load this data? Should I split the data in each source table into smaller files and load it? If splitting the data into smaller files is recommended, please explain me the reason to follow this approach.", "author_fullname": "t2_sde7upnf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices to load large data file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yuudwe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668419371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the best practices to load large data into a cloud datawarehouse like snowflake? Lets say I have 10 tables (each table size is &amp;gt; 50gb) in a source database and I am using s3 as a intermediate storage layer. What is the best approach to load this data? Should I split the data in each source table into smaller files and load it? If splitting the data into smaller files is recommended, please explain me the reason to follow this approach.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yuudwe", "is_robot_indexable": true, "report_reasons": null, "author": "bharath-t", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yuudwe/best_practices_to_load_large_data_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yuudwe/best_practices_to_load_large_data_file/", "subreddit_subscribers": 79968, "created_utc": 1668419371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the best graph database usage scenario that you have encountered? I mean, something that made you say \"Wow, I didn't know that I could do that with graph databases\".", "author_fullname": "t2_r6aazfpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best graph database usage scenario that you have encountered?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yv1y0d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668438108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the best graph database usage scenario that you have encountered? I mean, something that made you say &amp;quot;Wow, I didn&amp;#39;t know that I could do that with graph databases&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yv1y0d", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Cap6526", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv1y0d/what_is_the_best_graph_database_usage_scenario/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yv1y0d/what_is_the_best_graph_database_usage_scenario/", "subreddit_subscribers": 79968, "created_utc": 1668438108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Not sure who will need this but I built a very simple and small tool to generate PySpark schema from JSON. I built it to solve one of my problem that I was facing. I had to handle complex JSON this way.\n\nThere are few bugs there but ya here it it. [Click here to try!!](https://preetranjan.github.io/pyspark-schema-generator/)\n\nThe github is here, [View on Github](https://github.com/PreetRanjan/pyspark-schema-generator). It has few simple features like JSON format and compare.\n\n&amp;#x200B;\n\n[Screenshot of the project](https://preview.redd.it/4jg90xz3dyz91.png?width=1171&amp;format=png&amp;auto=webp&amp;s=bd3be25f3f8f9f72d82b6d4db4a76baea4fa49b9)", "author_fullname": "t2_r1dyavvd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A PySpark Schema Generator from JSON", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": false, "media_metadata": {"4jg90xz3dyz91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c5e8651d1e80e2a9e8e31e19550398b3f17d62a"}, {"y": 125, "x": 216, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d093adb5b206e4437bf1c7ad4b428630da032fd4"}, {"y": 185, "x": 320, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c044774924e48d401e62232355565c446ade3195"}, {"y": 371, "x": 640, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6afe62b9602814b90f37c564477d61ffdcc122eb"}, {"y": 557, "x": 960, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bee8a6aab5182eb60d4fa806938c0555566422fe"}, {"y": 627, "x": 1080, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f08668259f36fe9a91b6152b19b504d66529d304"}], "s": {"y": 680, "x": 1171, "u": "https://preview.redd.it/4jg90xz3dyz91.png?width=1171&amp;format=png&amp;auto=webp&amp;s=bd3be25f3f8f9f72d82b6d4db4a76baea4fa49b9"}, "id": "4jg90xz3dyz91"}}, "name": "t3_yv6lon", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bsXl4bc9J1PhGXcXoYCif1ZVExGpFQ_wVelMWBMk4uw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668447464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure who will need this but I built a very simple and small tool to generate PySpark schema from JSON. I built it to solve one of my problem that I was facing. I had to handle complex JSON this way.&lt;/p&gt;\n\n&lt;p&gt;There are few bugs there but ya here it it. &lt;a href=\"https://preetranjan.github.io/pyspark-schema-generator/\"&gt;Click here to try!!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The github is here, &lt;a href=\"https://github.com/PreetRanjan/pyspark-schema-generator\"&gt;View on Github&lt;/a&gt;. It has few simple features like JSON format and compare.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4jg90xz3dyz91.png?width=1171&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd3be25f3f8f9f72d82b6d4db4a76baea4fa49b9\"&gt;Screenshot of the project&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yv6lon", "is_robot_indexable": true, "report_reasons": null, "author": "RegentSphinx", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv6lon/a_pyspark_schema_generator_from_json/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yv6lon/a_pyspark_schema_generator_from_json/", "subreddit_subscribers": 79968, "created_utc": 1668447464.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So i'm creating a pipeline that is ingesting from 2 sources, both API's and want to transform, and load them into a single table in an external database. The table does not exist.\n\nSo far I have taken both json responses and put them into seperate source .json files (should I? as a best practice?) and then also used pandas to create 2 additional csv files (for no particular reason?) from the json files.   \nDoes this even make sense?   \n\n\nNow I want to create a table and populate it with some of the fields (not all).\n\nIve learned about sqlalchemy to use stuff like declarative base, creating tables for both the raw and the clean models, where raw holds the current data types, and clean the new actual database types. But I'm a bit confused as to why I would create a raw model when I can just transform the data I need and put them straight into the clean model. What Am I missing here?   \n\n\nAlso, is this even the desired solution for what I want to accomplish?   \n\n\nTHank you!", "author_fullname": "t2_6kipsr88", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL question (noob)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yujevd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668384702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i&amp;#39;m creating a pipeline that is ingesting from 2 sources, both API&amp;#39;s and want to transform, and load them into a single table in an external database. The table does not exist.&lt;/p&gt;\n\n&lt;p&gt;So far I have taken both json responses and put them into seperate source .json files (should I? as a best practice?) and then also used pandas to create 2 additional csv files (for no particular reason?) from the json files.&lt;br/&gt;\nDoes this even make sense?   &lt;/p&gt;\n\n&lt;p&gt;Now I want to create a table and populate it with some of the fields (not all).&lt;/p&gt;\n\n&lt;p&gt;Ive learned about sqlalchemy to use stuff like declarative base, creating tables for both the raw and the clean models, where raw holds the current data types, and clean the new actual database types. But I&amp;#39;m a bit confused as to why I would create a raw model when I can just transform the data I need and put them straight into the clean model. What Am I missing here?   &lt;/p&gt;\n\n&lt;p&gt;Also, is this even the desired solution for what I want to accomplish?   &lt;/p&gt;\n\n&lt;p&gt;THank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yujevd", "is_robot_indexable": true, "report_reasons": null, "author": "Brontonomo", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yujevd/etl_question_noob/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yujevd/etl_question_noob/", "subreddit_subscribers": 79968, "created_utc": 1668384702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Compaction in Apache Iceberg: Fine-Tuning Your Iceberg Table\u2019s Data Files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 42, "top_awarded_type": null, "hide_score": false, "name": "t3_yv5yp3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rgDtRAuYhvrcxOof1JQwoWaI-fmBHaWJYZgrMpjPUcc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668446199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dremio.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dremio.com/subsurface/compaction-in-apache-iceberg-fine-tuning-your-iceberg-tables-data-files/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yaYNe53o4GMeUwzFdbE8thxQMn_6xNxvYLsd-zbgq2Y.jpg?auto=webp&amp;s=a2c26887f1d6d7a9997b7628f5677260b6b2fed6", "width": 1010, "height": 304}, "resolutions": [{"url": "https://external-preview.redd.it/yaYNe53o4GMeUwzFdbE8thxQMn_6xNxvYLsd-zbgq2Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=178f619f3812bd07fb2145e7d1e4de13e4871887", "width": 108, "height": 32}, {"url": "https://external-preview.redd.it/yaYNe53o4GMeUwzFdbE8thxQMn_6xNxvYLsd-zbgq2Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1530c337ccaf733a9bf6af78c7d40ebbd091973a", "width": 216, "height": 65}, {"url": "https://external-preview.redd.it/yaYNe53o4GMeUwzFdbE8thxQMn_6xNxvYLsd-zbgq2Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a22316ac4710a9fcee1c562fd23d9f4b0db2ba04", "width": 320, "height": 96}, {"url": "https://external-preview.redd.it/yaYNe53o4GMeUwzFdbE8thxQMn_6xNxvYLsd-zbgq2Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=838d3bed9b15b2f67e101dc3478066e7307e3aa9", "width": 640, "height": 192}, {"url": "https://external-preview.redd.it/yaYNe53o4GMeUwzFdbE8thxQMn_6xNxvYLsd-zbgq2Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e7a2fcedcba08b4fd2316420447e0df3cffa8ae", "width": 960, "height": 288}], "variants": {}, "id": "_3QLIej8-EtXQYfSFWVcRZq1EpXgFkyUELW9oAU7w9M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yv5yp3", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv5yp3/compaction_in_apache_iceberg_finetuning_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dremio.com/subsurface/compaction-in-apache-iceberg-fine-tuning-your-iceberg-tables-data-files/", "subreddit_subscribers": 79968, "created_utc": 1668446199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently I work daily with spark, aws and python in my current role and have 2years of DE experience. My employer has offered to pay for a certification of my choice and I was wondering if anyone had a recommend. \n\nBased upon questions like how valuable the certification feels/was the content interesting/does it help with employability. Thank you.", "author_fullname": "t2_a3m6qw38", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any recommend for valuable certifications? (aws/spark/python)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yuil1m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668382581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I work daily with spark, aws and python in my current role and have 2years of DE experience. My employer has offered to pay for a certification of my choice and I was wondering if anyone had a recommend. &lt;/p&gt;\n\n&lt;p&gt;Based upon questions like how valuable the certification feels/was the content interesting/does it help with employability. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yuil1m", "is_robot_indexable": true, "report_reasons": null, "author": "sk808mafia", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yuil1m/any_recommend_for_valuable_certifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yuil1m/any_recommend_for_valuable_certifications/", "subreddit_subscribers": 79968, "created_utc": 1668382581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been a web developer for 7 years and truly dread doing it now. I want to focus more on databases or cloud. It gets me excited when im working on databases. For some background, i never finished my college degree i got hired by a start up company before I could graduate I\u2019m now working for the government making 100k. I feel extremely lucky to land this job even without a degree but how hard would it be to transition to data engineering? Would certifications be enough? What path should i take? \n\nMy skills rn:\nNode.js typescript angular \nC# asp.net mvc\nBlazor \nSSMS SSRS SSIS \nPower Bi, dax\nAzure devops", "author_fullname": "t2_9bawc33i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Path to becoming Data Engineer 2022?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yv1dof", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668436916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been a web developer for 7 years and truly dread doing it now. I want to focus more on databases or cloud. It gets me excited when im working on databases. For some background, i never finished my college degree i got hired by a start up company before I could graduate I\u2019m now working for the government making 100k. I feel extremely lucky to land this job even without a degree but how hard would it be to transition to data engineering? Would certifications be enough? What path should i take? &lt;/p&gt;\n\n&lt;p&gt;My skills rn:\nNode.js typescript angular \nC# asp.net mvc\nBlazor \nSSMS SSRS SSIS \nPower Bi, dax\nAzure devops&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yv1dof", "is_robot_indexable": true, "report_reasons": null, "author": "Possible_Original326", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv1dof/path_to_becoming_data_engineer_2022/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yv1dof/path_to_becoming_data_engineer_2022/", "subreddit_subscribers": 79968, "created_utc": 1668436916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, I am interested to share with you my article about Terraform and its configuration on azure . I'll be happy if you share with me your thoughts and comments.\n https://link.medium.com/e6uKq8jUWub", "author_fullname": "t2_7ssutue8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "get started with Terraform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yv0u1t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668435739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, I am interested to share with you my article about Terraform and its configuration on azure . I&amp;#39;ll be happy if you share with me your thoughts and comments.\n &lt;a href=\"https://link.medium.com/e6uKq8jUWub\"&gt;https://link.medium.com/e6uKq8jUWub&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UMF5Mo2jN-tMdnLzKSBYHfLAEyxaX9SO40FjubO9e5M.jpg?auto=webp&amp;s=b794eed6f59ce00534ff32164c84d6cc6a71d519", "width": 1200, "height": 624}, "resolutions": [{"url": "https://external-preview.redd.it/UMF5Mo2jN-tMdnLzKSBYHfLAEyxaX9SO40FjubO9e5M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d986dd432f37582c06604fc3e452d3f024e380d7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/UMF5Mo2jN-tMdnLzKSBYHfLAEyxaX9SO40FjubO9e5M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=43b262d9de75a90e3d62fab593a2d2af57f8520d", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/UMF5Mo2jN-tMdnLzKSBYHfLAEyxaX9SO40FjubO9e5M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=59fe33412aa44ef6c1a6192990d54ea8e7dfef83", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/UMF5Mo2jN-tMdnLzKSBYHfLAEyxaX9SO40FjubO9e5M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ead6af24f208c6803f0710013c7c83768fa8efe1", "width": 640, "height": 332}, {"url": "https://external-preview.redd.it/UMF5Mo2jN-tMdnLzKSBYHfLAEyxaX9SO40FjubO9e5M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee6e5f62e1187b836ee558df3f7ae0d3c14587ff", "width": 960, "height": 499}, {"url": "https://external-preview.redd.it/UMF5Mo2jN-tMdnLzKSBYHfLAEyxaX9SO40FjubO9e5M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ba2875d61f55ebb2c49c97de6b14ac17e01c1f75", "width": 1080, "height": 561}], "variants": {}, "id": "cHfT9jHacbTASq0gkcdy0nSLJDpzTh9DUF8H5O6wT6M"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 125, "id": "award_5f123e3d-4f48-42f4-9c11-e98b566d5897", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "When you come across a feel-good thing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Wholesome", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yv0u1t", "is_robot_indexable": true, "report_reasons": null, "author": "Ansam93", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv0u1t/get_started_with_terraform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yv0u1t/get_started_with_terraform/", "subreddit_subscribers": 79968, "created_utc": 1668435739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I would like  to start a consultancy, to do some side projects on the side in the field of data analytics/ data engineering and maybe turn that into a full time job. I am currently working a senior data engineer/ consultant. \n\n\nHow do you approach new clients? \n\nHow would charge for the projects ? \n\nLike subscription model or one time off fees ?", "author_fullname": "t2_64nmnknh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Land clients to starting consultancy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yutl48", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668416989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I would like  to start a consultancy, to do some side projects on the side in the field of data analytics/ data engineering and maybe turn that into a full time job. I am currently working a senior data engineer/ consultant. &lt;/p&gt;\n\n&lt;p&gt;How do you approach new clients? &lt;/p&gt;\n\n&lt;p&gt;How would charge for the projects ? &lt;/p&gt;\n\n&lt;p&gt;Like subscription model or one time off fees ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yutl48", "is_robot_indexable": true, "report_reasons": null, "author": "SnooDoggos5883", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yutl48/how_land_clients_to_starting_consultancy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yutl48/how_land_clients_to_starting_consultancy/", "subreddit_subscribers": 79968, "created_utc": 1668416989.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have a good suggestion where to prepare for GCP professional data engineer? What is the best course to take and take on google provided content on the same.", "author_fullname": "t2_9navdxud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP professional data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yvair9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668455482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have a good suggestion where to prepare for GCP professional data engineer? What is the best course to take and take on google provided content on the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yvair9", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical-Grade2960", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvair9/gcp_professional_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvair9/gcp_professional_data_engineer/", "subreddit_subscribers": 79968, "created_utc": 1668455482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, im new to data engineering and stuff and currently working in project where we are trying to make own ETL framework like fivetran and etc. Currently im stuck with the problem of how can we detect any source database schema change after initial sync. Like if there new column added to the table or column datatype has been changed. Can anyone suggest how to approach this problem.\n\nPs. For now source db is postgresql", "author_fullname": "t2_bpdkk6n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to detect database schema changes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yv9kug", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668453883.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668453543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, im new to data engineering and stuff and currently working in project where we are trying to make own ETL framework like fivetran and etc. Currently im stuck with the problem of how can we detect any source database schema change after initial sync. Like if there new column added to the table or column datatype has been changed. Can anyone suggest how to approach this problem.&lt;/p&gt;\n\n&lt;p&gt;Ps. For now source db is postgresql&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yv9kug", "is_robot_indexable": true, "report_reasons": null, "author": "pakhira55", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv9kug/how_to_detect_database_schema_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yv9kug/how_to_detect_database_schema_changes/", "subreddit_subscribers": 79968, "created_utc": 1668453543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\n\nI'm currently dealing with a client that uses media files for ML (videos and high def pictures). Using ADLS gen2 the loading and training is pretty easy.\n\nOf course the drawbacks are mainly the costs associated, as we constantly load data into Azure and we move the models off-cloud and into my client' production environment. \n\nBecause of all the transfers in and out, our rising bandwidth costs are going to surpass the actual storage budget.\n\nDo you guys have any best practices we could implement to reduce associated costs or at least curb it ? \n\nThe first on our checklist was to verify that every services were on the same region which is pretty easy to manage. \nThank you guys \ud83e\udd16", "author_fullname": "t2_2jzoy43k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing bandwidth costs on Azure, any tips or best practices ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yut4jj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668415336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently dealing with a client that uses media files for ML (videos and high def pictures). Using ADLS gen2 the loading and training is pretty easy.&lt;/p&gt;\n\n&lt;p&gt;Of course the drawbacks are mainly the costs associated, as we constantly load data into Azure and we move the models off-cloud and into my client&amp;#39; production environment. &lt;/p&gt;\n\n&lt;p&gt;Because of all the transfers in and out, our rising bandwidth costs are going to surpass the actual storage budget.&lt;/p&gt;\n\n&lt;p&gt;Do you guys have any best practices we could implement to reduce associated costs or at least curb it ? &lt;/p&gt;\n\n&lt;p&gt;The first on our checklist was to verify that every services were on the same region which is pretty easy to manage. \nThank you guys \ud83e\udd16&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yut4jj", "is_robot_indexable": true, "report_reasons": null, "author": "FromageDangereux", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yut4jj/managing_bandwidth_costs_on_azure_any_tips_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yut4jj/managing_bandwidth_costs_on_azure_any_tips_or/", "subreddit_subscribers": 79968, "created_utc": 1668415336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dfcot0zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We are doing an on site team building activity for our tech department. Sure you can build robust data systems but can you engineer a structurally sound tower made out of spaghetti and marshmallows? Share your best team building stories.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_yvafx5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wQMEFYjgjz2PbAbtfMtMfFwWQji0KU5zL4LnU_5jQHY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668455320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/3qnba2u9i00a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?auto=webp&amp;s=ecea0a5a53b258fb24a3699beae6935584442fcf", "width": 2296, "height": 4080}, "resolutions": [{"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a021a716887160ebca3841d4244f77241275a51", "width": 108, "height": 191}, {"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da58ddec931053f7ae385ce79073ceb3a6164e0c", "width": 216, "height": 383}, {"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=53c202ab7c016deb6ffa2909caac13a663bf7546", "width": 320, "height": 568}, {"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce1aff6b77264b7813ace0833adfeb2cc4bbd1d9", "width": 640, "height": 1137}, {"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=965833813defd4538a6898920f331652eeaaa99c", "width": 960, "height": 1705}, {"url": "https://preview.redd.it/3qnba2u9i00a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=195fa553b27c3f95e4494347c03f8fde779aa53f", "width": 1080, "height": 1919}], "variants": {}, "id": "ld8n5uhR9PLh6A55Rbuy5G_BECXU2C6Dvtn0B-viV48"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yvafx5", "is_robot_indexable": true, "report_reasons": null, "author": "DrRedmondNYC", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvafx5/we_are_doing_an_on_site_team_building_activity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/3qnba2u9i00a1.jpg", "subreddit_subscribers": 79968, "created_utc": 1668455320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, i have been asked by my project manager to test the queries written by senior Devs acc to the usecases given to them to implement datamarts, currently I have very little knowledge about testing as I have never done it before and some advice from you guys will be really helpful", "author_fullname": "t2_fgjne05b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to test data Marts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yv9270", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668452460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, i have been asked by my project manager to test the queries written by senior Devs acc to the usecases given to them to implement datamarts, currently I have very little knowledge about testing as I have never done it before and some advice from you guys will be really helpful&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yv9270", "is_robot_indexable": true, "report_reasons": null, "author": "NoPreference7274", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv9270/how_to_test_data_marts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yv9270/how_to_test_data_marts/", "subreddit_subscribers": 79968, "created_utc": 1668452460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Greetings, \nI would like to hear your experiences with using Datavault and maybe suggestions on how to proceed. \nBackground infromation: I've been managing a 20+ internal application datavault in my organisation for 2+ years. The goal was to replace Dashboards directly connected to the applications with a Datawarehouse using the data vault model. \nSo, the problem: the longer the datavault is being used the further data quality is being degraded. Its easy to blame other's, duing the interviews they claimed that the business keys were unique, but after a while i observe that that is not the case. Or records being deleted while they specifically stated that that is not the case ( and the application even has a row closed column). The result is that dashboards based on the datawarehouse show incorrect numbers and we are to blame. \n\nExamples: \n1)Process explained to me: Sale records must be unique and have at least one product. If sale is annulled the record gets a sign that its annulled. \n1)Reality: If a product is no longer sold all sale records are purged from the system because reasons.... \n\n2)Process explained to me:An employee ID is unique and identifies a  single natural person\n2)Reality: A temp worker is replacing another temp worker, HR is lazy and just changes an existing ID persons name,age, ets. \n The challenge is that the data is not faulty at  any one point in time: Its just that processes are not adhered to and thus the only way to get correct data is to purge the data vault.", "author_fullname": "t2_ippbo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To those who have worked with Datavault, do you have any suggestions on how to proceed ( story inside)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yv0hcm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668434965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings, \nI would like to hear your experiences with using Datavault and maybe suggestions on how to proceed. \nBackground infromation: I&amp;#39;ve been managing a 20+ internal application datavault in my organisation for 2+ years. The goal was to replace Dashboards directly connected to the applications with a Datawarehouse using the data vault model. \nSo, the problem: the longer the datavault is being used the further data quality is being degraded. Its easy to blame other&amp;#39;s, duing the interviews they claimed that the business keys were unique, but after a while i observe that that is not the case. Or records being deleted while they specifically stated that that is not the case ( and the application even has a row closed column). The result is that dashboards based on the datawarehouse show incorrect numbers and we are to blame. &lt;/p&gt;\n\n&lt;p&gt;Examples: \n1)Process explained to me: Sale records must be unique and have at least one product. If sale is annulled the record gets a sign that its annulled. \n1)Reality: If a product is no longer sold all sale records are purged from the system because reasons.... &lt;/p&gt;\n\n&lt;p&gt;2)Process explained to me:An employee ID is unique and identifies a  single natural person\n2)Reality: A temp worker is replacing another temp worker, HR is lazy and just changes an existing ID persons name,age, ets. \n The challenge is that the data is not faulty at  any one point in time: Its just that processes are not adhered to and thus the only way to get correct data is to purge the data vault.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yv0hcm", "is_robot_indexable": true, "report_reasons": null, "author": "roadrussian", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv0hcm/to_those_who_have_worked_with_datavault_do_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yv0hcm/to_those_who_have_worked_with_datavault_do_you/", "subreddit_subscribers": 79968, "created_utc": 1668434965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ugjew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tulip: Schematizing Meta\u2019s data platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_yuxqor", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b8z_zdQvvuMArj5NFKjCmuYof4eSXcfNPz69X7FLX7g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668428550.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "engineering.fb.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://engineering.fb.com/2022/11/09/developer-tools/tulip-schematizing-metas-data-platform/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q33Xy0jiK981QN4H96G7QxWZDqytUJkU3KQs62JLlws.jpg?auto=webp&amp;s=985d9e86a60cd42c7c5961832c39f69ed8c48fe3", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/q33Xy0jiK981QN4H96G7QxWZDqytUJkU3KQs62JLlws.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=220d8d5f11385f3fe6570b772aeea4bbace3099a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/q33Xy0jiK981QN4H96G7QxWZDqytUJkU3KQs62JLlws.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=08170c62a860a68e2eecc7f37b0fea26567ddb35", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/q33Xy0jiK981QN4H96G7QxWZDqytUJkU3KQs62JLlws.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=15e9b80a4c44d3b89ab20b5985f6eec284ea0ee1", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/q33Xy0jiK981QN4H96G7QxWZDqytUJkU3KQs62JLlws.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aad16a28dc7cec8a06ac005a9fb6b186cb06ae21", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/q33Xy0jiK981QN4H96G7QxWZDqytUJkU3KQs62JLlws.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=60a7dd1ce23d82dd958b95473c659ff4a03785b5", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/q33Xy0jiK981QN4H96G7QxWZDqytUJkU3KQs62JLlws.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=79e4e12be1474061277a14f00e22094ed77d7a57", "width": 1080, "height": 607}], "variants": {}, "id": "8UfhOBudgsBvsTzSSr5lnkSwwiBO5L0NuKwtFuEAhjI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yuxqor", "is_robot_indexable": true, "report_reasons": null, "author": "marcosluis2186", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yuxqor/tulip_schematizing_metas_data_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://engineering.fb.com/2022/11/09/developer-tools/tulip-schematizing-metas-data-platform/", "subreddit_subscribers": 79968, "created_utc": 1668428550.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a data engineer having 12 years of experience in various tools and technologies ranging from Oracle, Teradata, bigdata, Hadoop, spark, python mostly in the financial industry, what's the way forward someone like me going forward should I pursue data architect role or consulting? Or keep doing what I am doing?", "author_fullname": "t2_76w8juxu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data engineering job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yutut4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668417785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a data engineer having 12 years of experience in various tools and technologies ranging from Oracle, Teradata, bigdata, Hadoop, spark, python mostly in the financial industry, what&amp;#39;s the way forward someone like me going forward should I pursue data architect role or consulting? Or keep doing what I am doing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yutut4", "is_robot_indexable": true, "report_reasons": null, "author": "Separate-Boat8195", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yutut4/data_engineering_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yutut4/data_engineering_job/", "subreddit_subscribers": 79968, "created_utc": 1668417785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have you ever heard of infrastructure as code? This will tell you what it is about and why it is so useful:\n\n[https://erwinschleier.medium.com/aws-cloudformation-introduction-e6d6f3fe89d2](https://erwinschleier.medium.com/aws-cloudformation-introduction-e6d6f3fe89d2)", "author_fullname": "t2_3di0zmcf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS CloudFormation Introduction", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yuths5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668416714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you ever heard of infrastructure as code? This will tell you what it is about and why it is so useful:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://erwinschleier.medium.com/aws-cloudformation-introduction-e6d6f3fe89d2\"&gt;https://erwinschleier.medium.com/aws-cloudformation-introduction-e6d6f3fe89d2&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/glHzZ_bfOaIdtR2Y4_oWR-mX9yTW_zRkAYOBIlmROTY.jpg?auto=webp&amp;s=76157e72d5fa325f969674de8d937b707e82a9c9", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/glHzZ_bfOaIdtR2Y4_oWR-mX9yTW_zRkAYOBIlmROTY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a81d0bbd7684d987ee985f2fd56ba142393be51c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/glHzZ_bfOaIdtR2Y4_oWR-mX9yTW_zRkAYOBIlmROTY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e98e7a07131f09ec2fa4d2ea5ab203c33b013c5", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/glHzZ_bfOaIdtR2Y4_oWR-mX9yTW_zRkAYOBIlmROTY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c57778b500a5ce0ced8df1fd086cc428aae77747", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/glHzZ_bfOaIdtR2Y4_oWR-mX9yTW_zRkAYOBIlmROTY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c21c14ab1c3ca06899bf28e0b3fd293d8dd689ab", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/glHzZ_bfOaIdtR2Y4_oWR-mX9yTW_zRkAYOBIlmROTY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6a8b34c649db8a185db2eb0ca6c02a272deed349", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/glHzZ_bfOaIdtR2Y4_oWR-mX9yTW_zRkAYOBIlmROTY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1eb2653c7e920f3261387f50c014eb834b27d4fb", "width": 1080, "height": 1080}], "variants": {}, "id": "14PTcHLZhsWzPYIRL1KFxfSWbZxysOVvupddjIWQQfw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yuths5", "is_robot_indexable": true, "report_reasons": null, "author": "EdgarHuber", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yuths5/aws_cloudformation_introduction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yuths5/aws_cloudformation_introduction/", "subreddit_subscribers": 79968, "created_utc": 1668416714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have collected thousands of unique files with slightly varying data structures in each file, most of them have a header but can be oddly shaped, is there any utility that can scan all of this in and suggest a data model a across all of them? I particular use case features a lot of financial data.", "author_fullname": "t2_116kc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Instant data model from 1000s of unique files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yufzlk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668377110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have collected thousands of unique files with slightly varying data structures in each file, most of them have a header but can be oddly shaped, is there any utility that can scan all of this in and suggest a data model a across all of them? I particular use case features a lot of financial data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yufzlk", "is_robot_indexable": true, "report_reasons": null, "author": "daeisfresh", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yufzlk/instant_data_model_from_1000s_of_unique_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yufzlk/instant_data_model_from_1000s_of_unique_files/", "subreddit_subscribers": 79968, "created_utc": 1668377110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, \n\nA regular ol' SWE making some steps into the data engineering world. I'm liking the lake house architecture, and I'm looking to implement it within my start-up org. For me the ACID compliance and governance model through technologies like Trino, delta lake etc are really appealing to me. \n\nHowever, since I'm really new to this space I'm missing some key factors that relate to my core experience of a regular SWE, and that is gitops. In my mind, nothing that is deployable is not done somehow through a pipeline or some automated action from a git repo. I can't even comprehend doing it any other way anymore. \n\nHowever, from what I've seen, this doesn't seem to be the case in the data lake house world that I've seen. For example something like data bricks just looks like a Jupyter notebook with some loaded delta lake house connectors. Do DE's and DS just write ad hoc scripts here that create tables in the data lake? What's the industry best practice to versioning data engineering work?", "author_fullname": "t2_nsflng", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gitops in a Data lakehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yvdydl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668463065.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, &lt;/p&gt;\n\n&lt;p&gt;A regular ol&amp;#39; SWE making some steps into the data engineering world. I&amp;#39;m liking the lake house architecture, and I&amp;#39;m looking to implement it within my start-up org. For me the ACID compliance and governance model through technologies like Trino, delta lake etc are really appealing to me. &lt;/p&gt;\n\n&lt;p&gt;However, since I&amp;#39;m really new to this space I&amp;#39;m missing some key factors that relate to my core experience of a regular SWE, and that is gitops. In my mind, nothing that is deployable is not done somehow through a pipeline or some automated action from a git repo. I can&amp;#39;t even comprehend doing it any other way anymore. &lt;/p&gt;\n\n&lt;p&gt;However, from what I&amp;#39;ve seen, this doesn&amp;#39;t seem to be the case in the data lake house world that I&amp;#39;ve seen. For example something like data bricks just looks like a Jupyter notebook with some loaded delta lake house connectors. Do DE&amp;#39;s and DS just write ad hoc scripts here that create tables in the data lake? What&amp;#39;s the industry best practice to versioning data engineering work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yvdydl", "is_robot_indexable": true, "report_reasons": null, "author": "CatabolicEdo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvdydl/gitops_in_a_data_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvdydl/gitops_in_a_data_lakehouse/", "subreddit_subscribers": 79968, "created_utc": 1668463065.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to request for some aws access/permssions from the tech team. What permissions/access can I request. \nI would like to use redshift, aurora mysql, lambda, airflow.\n\nI have always used okta and things are pretty set up in my old job.\n\nSo what should I request for?", "author_fullname": "t2_3w9dfvgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What access &amp; Permissions do I need on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yvcz5r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668460770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to request for some aws access/permssions from the tech team. What permissions/access can I request. \nI would like to use redshift, aurora mysql, lambda, airflow.&lt;/p&gt;\n\n&lt;p&gt;I have always used okta and things are pretty set up in my old job.&lt;/p&gt;\n\n&lt;p&gt;So what should I request for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yvcz5r", "is_robot_indexable": true, "report_reasons": null, "author": "mrmilata", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yvcz5r/what_access_permissions_do_i_need_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yvcz5r/what_access_permissions_do_i_need_on_aws/", "subreddit_subscribers": 79968, "created_utc": 1668460770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know if Delta Live Tables can stream data from a delta table? We currently have our source data for the DLT pipeline in delta format. I have successfully created a pipeline reading the data as parquet, but ideally we would want to read as delta. Does anyone have any insight on this?", "author_fullname": "t2_3mm882fo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data ingestion with Delta Live Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yv7imp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668449294.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know if Delta Live Tables can stream data from a delta table? We currently have our source data for the DLT pipeline in delta format. I have successfully created a pipeline reading the data as parquet, but ideally we would want to read as delta. Does anyone have any insight on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yv7imp", "is_robot_indexable": true, "report_reasons": null, "author": "mcqueg", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv7imp/data_ingestion_with_delta_live_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yv7imp/data_ingestion_with_delta_live_tables/", "subreddit_subscribers": 79968, "created_utc": 1668449294.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dremio Contributes the Arrow Flight SQL JDBC Driver to the Apache Arrow Community \u2013 The Latest of Many Contributions | Dremio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_yv4z3z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/VzybhJXLFA8l35ZZiFK_kRskUhsSQIe6puq46uqnwxk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668444257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dremio.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dremio.com/blog/dremio-contributes-the-arrow-flight-sql-jdbc-driver-to-the-apache-arrow-community-the-latest-of-many-contributions/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?auto=webp&amp;s=6e6cd51218e5daa44fda46e871254d67b4f0b028", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3ff4e8f3ca79122c80a3edca5e76f391dbe6bdfe", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=33960553c96829ccee2547f8ab406130247c7384", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df3787d2c0c0fd87c0515f59c003c36736a729ec", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f2078e40e00eed2276fcc88816b7ba1fc0b1e1f", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ce6f95c31b9473961006a3d143d79d7513af6c5", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/UjPgQ6jvcKO9etGms391DZ5QsStmI1Zik2a3kQwEJ5s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=511268e51ae4130135da26f6139fe780805c37b2", "width": 1080, "height": 565}], "variants": {}, "id": "dwgv-bjmtKMXlgmwTW_CSzpXGhwr6pO9hwIr945P_3g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yv4z3z", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv4z3z/dremio_contributes_the_arrow_flight_sql_jdbc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dremio.com/blog/dremio-contributes-the-arrow-flight-sql-jdbc-driver-to-the-apache-arrow-community-the-latest-of-many-contributions/", "subreddit_subscribers": 79968, "created_utc": 1668444257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm an Azure engineer, and a majority of my end users are data scientists/engineers.  One of them is looking at using Airflow to trigger Azure Databricks jobs, and I'm doing a bare bones POC of Airflow based on an [Azure quickstart template](https://azure.microsoft.com/en-us/blog/deploying-apache-airflow-in-azure-to-build-and-run-data-pipelines/) using [Puckel's dockerhub Airflow image](https://hub.docker.com/r/puckel/docker-airflow/) (yes, it's gone 3 years since updating; I'm only using it for a POC, not production)  The deployment is working fine, I'm just poking through Airflow's interface, and had a few questions that I can't quite seem to find answers to in my research so far:\n\n* Are the users created via Admin -&gt; Users created as local users on the Postgres DB?  If so, does creating them as a superuser grant them rights to the DB or is it just superuser for Airflow?\n* If an Airflow pool is defined, does it impact the number of worker nodes that Databricks spawns to run the jobs in that pool?  Or is there no relationship between them?\n* Where do I specify the parameters called out in [this site covering connections to Azure service principals](https://airflow.apache.org/docs/apache-airflow-providers-microsoft-azure/stable/connections/azure.html)?", "author_fullname": "t2_6syo2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got a few questions on Airflow administration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yv1uee", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668437919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an Azure engineer, and a majority of my end users are data scientists/engineers.  One of them is looking at using Airflow to trigger Azure Databricks jobs, and I&amp;#39;m doing a bare bones POC of Airflow based on an &lt;a href=\"https://azure.microsoft.com/en-us/blog/deploying-apache-airflow-in-azure-to-build-and-run-data-pipelines/\"&gt;Azure quickstart template&lt;/a&gt; using &lt;a href=\"https://hub.docker.com/r/puckel/docker-airflow/\"&gt;Puckel&amp;#39;s dockerhub Airflow image&lt;/a&gt; (yes, it&amp;#39;s gone 3 years since updating; I&amp;#39;m only using it for a POC, not production)  The deployment is working fine, I&amp;#39;m just poking through Airflow&amp;#39;s interface, and had a few questions that I can&amp;#39;t quite seem to find answers to in my research so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Are the users created via Admin -&amp;gt; Users created as local users on the Postgres DB?  If so, does creating them as a superuser grant them rights to the DB or is it just superuser for Airflow?&lt;/li&gt;\n&lt;li&gt;If an Airflow pool is defined, does it impact the number of worker nodes that Databricks spawns to run the jobs in that pool?  Or is there no relationship between them?&lt;/li&gt;\n&lt;li&gt;Where do I specify the parameters called out in &lt;a href=\"https://airflow.apache.org/docs/apache-airflow-providers-microsoft-azure/stable/connections/azure.html\"&gt;this site covering connections to Azure service principals&lt;/a&gt;?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ol9FZT8FcQfRfADTU8teH6pU_nlSqvd5CC7kJ40sCDk.jpg?auto=webp&amp;s=e3a3a19cf9b697e7dcb28ae5b14cc74cf9acd8c9", "width": 250, "height": 250}, "resolutions": [{"url": "https://external-preview.redd.it/ol9FZT8FcQfRfADTU8teH6pU_nlSqvd5CC7kJ40sCDk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=39e43cac4fbf358f8c0a5159c6c8bd2b60cc3cf0", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ol9FZT8FcQfRfADTU8teH6pU_nlSqvd5CC7kJ40sCDk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c452c2a15536677a4f16f14f69109d2fd7dc239a", "width": 216, "height": 216}], "variants": {}, "id": "Ln1lNDqLGYB-op_KKWkMJ3CIwoutiembrNZnyFULkZ4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yv1uee", "is_robot_indexable": true, "report_reasons": null, "author": "MohnJaddenPowers", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yv1uee/got_a_few_questions_on_airflow_administration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yv1uee/got_a_few_questions_on_airflow_administration/", "subreddit_subscribers": 79968, "created_utc": 1668437919.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}