{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wrote a high-level introduction about Apache Spark. Feel free to leave any feedback!\n\n*When choosing a compute engine, there is no way around Spark. But where does Spark come from and why is it so popular?....*\n\n[More on Medium &lt;:](https://medium.com/microsoft-data-platform-community-hamburg/apache-spark-for-dummies-b77384e33c91)\n\n&amp;#x200B;", "author_fullname": "t2_dyi4o2aq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Spark\u2122 for Dummies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1p5ny", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669108801.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote a high-level introduction about Apache Spark. Feel free to leave any feedback!&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;When choosing a compute engine, there is no way around Spark. But where does Spark come from and why is it so popular?....&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/microsoft-data-platform-community-hamburg/apache-spark-for-dummies-b77384e33c91\"&gt;More on Medium &amp;lt;:&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qEgdbT_h8xVBaBtVJeLRb-6uh1T0W2cbWCB6CkcxFsQ.jpg?auto=webp&amp;s=33ba1681eaac01398237d1adf8e1de74da12ca1a", "width": 873, "height": 292}, "resolutions": [{"url": "https://external-preview.redd.it/qEgdbT_h8xVBaBtVJeLRb-6uh1T0W2cbWCB6CkcxFsQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2fd299de29295b6a57d7fc07b74e6921207b0620", "width": 108, "height": 36}, {"url": "https://external-preview.redd.it/qEgdbT_h8xVBaBtVJeLRb-6uh1T0W2cbWCB6CkcxFsQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=582cbcc40c70d3bd02c240dd00a22f3ec581a949", "width": 216, "height": 72}, {"url": "https://external-preview.redd.it/qEgdbT_h8xVBaBtVJeLRb-6uh1T0W2cbWCB6CkcxFsQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a6d43de520ddb501dd66c789d3210b43b68e8a5", "width": 320, "height": 107}, {"url": "https://external-preview.redd.it/qEgdbT_h8xVBaBtVJeLRb-6uh1T0W2cbWCB6CkcxFsQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3ba62a0b2b92fcdbd08cffe04292fb80170983e5", "width": 640, "height": 214}], "variants": {}, "id": "r14g8lQwbCMwLihVfWU7_KwJRwCrO4xwATbFXKx8YcA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z1p5ny", "is_robot_indexable": true, "report_reasons": null, "author": "keevee94", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1p5ny/apache_spark_for_dummies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1p5ny/apache_spark_for_dummies/", "subreddit_subscribers": 80660, "created_utc": 1669108801.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking at a DE job description that I've been asked about. What might you expect from someone in an interview who wants \"very strong Python\"? They would do a code exercise. \n\nI'm an intermediate Python guy with long tech background. Been doing analytics for a few years and moving towards DE. I got some decent AWS experience. \n\nI'm not scared of it, half expect to fail but it would probably be a good experience. But obviously I would want to prepare.", "author_fullname": "t2_2o0q5m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to expect in interview that wants \"Very strong Python development experience\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1euyw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669075807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking at a DE job description that I&amp;#39;ve been asked about. What might you expect from someone in an interview who wants &amp;quot;very strong Python&amp;quot;? They would do a code exercise. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m an intermediate Python guy with long tech background. Been doing analytics for a few years and moving towards DE. I got some decent AWS experience. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not scared of it, half expect to fail but it would probably be a good experience. But obviously I would want to prepare.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z1euyw", "is_robot_indexable": true, "report_reasons": null, "author": "rotterdamn8", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1euyw/what_to_expect_in_interview_that_wants_very/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1euyw/what_to_expect_in_interview_that_wants_very/", "subreddit_subscribers": 80660, "created_utc": 1669075807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have worked on complex data pipelines and done analytics on existing tables in warehouse but never actually get to know who is responsible for designing the tables.", "author_fullname": "t2_rxkr4y1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who is ideally responsible for Data modelling in the project and as a data engineer it is expected from you to have strong knowledge and at what experience level ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z11anq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669043406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have worked on complex data pipelines and done analytics on existing tables in warehouse but never actually get to know who is responsible for designing the tables.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z11anq", "is_robot_indexable": true, "report_reasons": null, "author": "SoggyAbalone7392", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z11anq/who_is_ideally_responsible_for_data_modelling_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z11anq/who_is_ideally_responsible_for_data_modelling_in/", "subreddit_subscribers": 80660, "created_utc": 1669043406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\nI am in the process of learning spark and soon plan to interview. \nCould you please share some questions/challenges that you've encountered during the interviews?", "author_fullname": "t2_fadc9ofm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pyspark interview questions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1plgr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669110558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI am in the process of learning spark and soon plan to interview. \nCould you please share some questions/challenges that you&amp;#39;ve encountered during the interviews?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "z1plgr", "is_robot_indexable": true, "report_reasons": null, "author": "signacaste", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1plgr/pyspark_interview_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1plgr/pyspark_interview_questions/", "subreddit_subscribers": 80660, "created_utc": 1669110558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our use-case is as follows:\n\n1. \\[Extract &amp; Load\\] Every minute, query for new/modified records from \\~700 different customers and load new/modified records into raw tables as jsonb.\n2. \\[Transform\\] Near realtime (every minute or so), run a transform specific to each customer to transform raw data and load it into a normalized table that our application reads from frequently.\n\nRunning 700 distinct transforms and pushing data to the same table has me concerned. Each individual transform run could handle between zero and 50k records. I'm looking at options for using dbt for these near-realtime transforms, but I'm not convinced this is the best solution.\n\nHow does DBT perform for near realtime transforms to production tables seeing heavy read volume? If DBT isn't a good fit, what other options should I explore?", "author_fullname": "t2_gs0mp007", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solution for heavy volume, near realtime transforms to production tables seeing heavy read volume.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z15816", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669053125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our use-case is as follows:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;[Extract &amp;amp; Load] Every minute, query for new/modified records from ~700 different customers and load new/modified records into raw tables as jsonb.&lt;/li&gt;\n&lt;li&gt;[Transform] Near realtime (every minute or so), run a transform specific to each customer to transform raw data and load it into a normalized table that our application reads from frequently.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Running 700 distinct transforms and pushing data to the same table has me concerned. Each individual transform run could handle between zero and 50k records. I&amp;#39;m looking at options for using dbt for these near-realtime transforms, but I&amp;#39;m not convinced this is the best solution.&lt;/p&gt;\n\n&lt;p&gt;How does DBT perform for near realtime transforms to production tables seeing heavy read volume? If DBT isn&amp;#39;t a good fit, what other options should I explore?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z15816", "is_robot_indexable": true, "report_reasons": null, "author": "RandomWalk55", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z15816/solution_for_heavy_volume_near_realtime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z15816/solution_for_heavy_volume_near_realtime/", "subreddit_subscribers": 80660, "created_utc": 1669053125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Given recent debates on the necessity of certain tooling with small datasets, it would be interesting to know the distribution of workloads for users on this subreddit. Volume is obviously one of many potential proxies for the complexity of a workload, but it should suffice for a high level view.\n\n[View Poll](https://www.reddit.com/poll/z11xd8)", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much data do you process via your pipelines in a given day?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z11xd8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669044987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given recent debates on the necessity of certain tooling with small datasets, it would be interesting to know the distribution of workloads for users on this subreddit. Volume is obviously one of many potential proxies for the complexity of a workload, but it should suffice for a high level view.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/z11xd8\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z11xd8", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1669304187618, "options": [{"text": "Under 1GB", "id": "19944314"}, {"text": "1GB to 10GB", "id": "19944315"}, {"text": "10GB to 100GB", "id": "19944316"}, {"text": "100GB to 1TB", "id": "19944317"}, {"text": "1 TB+", "id": "19944318"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 313, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/z11xd8/how_much_data_do_you_process_via_your_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/z11xd8/how_much_data_do_you_process_via_your_pipelines/", "subreddit_subscribers": 80660, "created_utc": 1669044987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone here have stories or suggestions for making your own way as a DE, in a way that actually gives you enough daytime to go for walks, hikes, the freedom to work from wherever you want, set your own maybe project-by-project schedule, etc? So sick of being tied to the corporate calendar. WFH vs WAO both suck if it's the majority of your waking time, and you're too busy each day to take a bike ride or can't leave the city where your company is located to be closer to fresh air.", "author_fullname": "t2_i98ee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Breaking out into freelance DE or just leaving the rat race and managing to earn well?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1lfcp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669094975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone here have stories or suggestions for making your own way as a DE, in a way that actually gives you enough daytime to go for walks, hikes, the freedom to work from wherever you want, set your own maybe project-by-project schedule, etc? So sick of being tied to the corporate calendar. WFH vs WAO both suck if it&amp;#39;s the majority of your waking time, and you&amp;#39;re too busy each day to take a bike ride or can&amp;#39;t leave the city where your company is located to be closer to fresh air.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z1lfcp", "is_robot_indexable": true, "report_reasons": null, "author": "consiliac", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1lfcp/breaking_out_into_freelance_de_or_just_leaving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1lfcp/breaking_out_into_freelance_de_or_just_leaving/", "subreddit_subscribers": 80660, "created_utc": 1669094975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I stumbled across [this](https://informationscience.unt.edu/ms-information-systems) from the University of North Texas as I was searching for graduate level data modeling courses (I work as an analytics engineer with mainly dbt and GBQ, hardly any pipelines yet since we use stitch for now).. and it has me pretty interested due to the coursework.\n\nBy contrast and because of the popularity of computer science, I've been focusing on completing prerequisites for [Georgia Tech's OMSCS](https://omscs.gatech.edu/specialization-computing-systems), particularly in computing systems, through an online junior/community college.\n\nFor computing systems, I'm planning on learning about operating systems, networks, security, and computer architecture. Most of the prereqs have been in C and or C++, which is tangential to data engineering at best. There's numerous posts about SQL + Python being enough for most jobs (not considering jobs needing scala/java/BE knowledge). This all seems like a lot of preparation and knowledge acquisition for topics that I won't directly use in AE/DE. Have also read posts (from here and r/OMSCS) that say a masters isn't the best way to acquire knowledge needed to do one's job.\n\nMy impression is that most of DE is OJT and/or learned from personal projects and not really covered in academia at all. That being said, it seems the information science side of things worries about storage, organization, quality, etc. of the data. Isn't that primarily what we do, beyond moving the data?\n\nDoes it make more sense to stick with computing systems, or should I entertain something like the other masters, or something completely different? My end goal is to essentially stay in analytics or data engineering for the long term and make as much money as possible. No aspirations beyond that right now.", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why isn't information science a popular pathway into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z11r7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669044586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I stumbled across &lt;a href=\"https://informationscience.unt.edu/ms-information-systems\"&gt;this&lt;/a&gt; from the University of North Texas as I was searching for graduate level data modeling courses (I work as an analytics engineer with mainly dbt and GBQ, hardly any pipelines yet since we use stitch for now).. and it has me pretty interested due to the coursework.&lt;/p&gt;\n\n&lt;p&gt;By contrast and because of the popularity of computer science, I&amp;#39;ve been focusing on completing prerequisites for &lt;a href=\"https://omscs.gatech.edu/specialization-computing-systems\"&gt;Georgia Tech&amp;#39;s OMSCS&lt;/a&gt;, particularly in computing systems, through an online junior/community college.&lt;/p&gt;\n\n&lt;p&gt;For computing systems, I&amp;#39;m planning on learning about operating systems, networks, security, and computer architecture. Most of the prereqs have been in C and or C++, which is tangential to data engineering at best. There&amp;#39;s numerous posts about SQL + Python being enough for most jobs (not considering jobs needing scala/java/BE knowledge). This all seems like a lot of preparation and knowledge acquisition for topics that I won&amp;#39;t directly use in AE/DE. Have also read posts (from here and &lt;a href=\"/r/OMSCS\"&gt;r/OMSCS&lt;/a&gt;) that say a masters isn&amp;#39;t the best way to acquire knowledge needed to do one&amp;#39;s job.&lt;/p&gt;\n\n&lt;p&gt;My impression is that most of DE is OJT and/or learned from personal projects and not really covered in academia at all. That being said, it seems the information science side of things worries about storage, organization, quality, etc. of the data. Isn&amp;#39;t that primarily what we do, beyond moving the data?&lt;/p&gt;\n\n&lt;p&gt;Does it make more sense to stick with computing systems, or should I entertain something like the other masters, or something completely different? My end goal is to essentially stay in analytics or data engineering for the long term and make as much money as possible. No aspirations beyond that right now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z11r7y", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z11r7y/why_isnt_information_science_a_popular_pathway/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z11r7y/why_isnt_information_science_a_popular_pathway/", "subreddit_subscribers": 80660, "created_utc": 1669044586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5tz7z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Dodgy State of Stream Processing Delivery Guarantees", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_z12dql", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QmpBOCvY8mY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"The Dodgy State of Delivery Guarantees\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "The Dodgy State of Delivery Guarantees", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QmpBOCvY8mY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"The Dodgy State of Delivery Guarantees\"&gt;&lt;/iframe&gt;", "author_name": "Jeffail", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/QmpBOCvY8mY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Jeffail"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QmpBOCvY8mY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"The Dodgy State of Delivery Guarantees\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/z12dql", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/YRR3b8_BlHgeTA8P57ZTrl_GN3dmlqmby5c8IFL-yC0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669046090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=QmpBOCvY8mY", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rzwLrNWEOrQIfIOHmANPpQFl9N0oxXqSQD3swEEr2NI.jpg?auto=webp&amp;s=0305706187a2011f35c7c669268ee1e0888ea9ca", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/rzwLrNWEOrQIfIOHmANPpQFl9N0oxXqSQD3swEEr2NI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=92e39c5703310577ac56647bf0b7d49900e1d2e4", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/rzwLrNWEOrQIfIOHmANPpQFl9N0oxXqSQD3swEEr2NI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1031f8aae1c55d2db6e6ba587fd8ba729d31c03a", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/rzwLrNWEOrQIfIOHmANPpQFl9N0oxXqSQD3swEEr2NI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8b710bb5a738c4841e95e5a3507aca5e02a1be7", "width": 320, "height": 240}], "variants": {}, "id": "YBOTc6rsHRbN6AAvcd7IQ1EdOwNs2tixrFQmCC3PtM0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z12dql", "is_robot_indexable": true, "report_reasons": null, "author": "mihaitodor", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z12dql/the_dodgy_state_of_stream_processing_delivery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=QmpBOCvY8mY", "subreddit_subscribers": 80660, "created_utc": 1669046090.0, "num_crossposts": 1, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "The Dodgy State of Delivery Guarantees", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QmpBOCvY8mY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"The Dodgy State of Delivery Guarantees\"&gt;&lt;/iframe&gt;", "author_name": "Jeffail", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/QmpBOCvY8mY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Jeffail"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you there is a use case for it? From a DE standpoint to implement ETL?\n\nRight now Im on a project which the biggest table is 10M and we are using Databricks for a simple ETL process to load a dimensional model.\n\nLifting a cluster to deal with this volume sounds silly. This could be easily achieved with raw python or a DB.", "author_fullname": "t2_3174m8nl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks for small datasets? Worth it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z11ew5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669043711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you there is a use case for it? From a DE standpoint to implement ETL?&lt;/p&gt;\n\n&lt;p&gt;Right now Im on a project which the biggest table is 10M and we are using Databricks for a simple ETL process to load a dimensional model.&lt;/p&gt;\n\n&lt;p&gt;Lifting a cluster to deal with this volume sounds silly. This could be easily achieved with raw python or a DB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z11ew5", "is_robot_indexable": true, "report_reasons": null, "author": "PaleBass", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z11ew5/databricks_for_small_datasets_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z11ew5/databricks_for_small_datasets_worth_it/", "subreddit_subscribers": 80660, "created_utc": 1669043711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is any one using acloudguru or Cloudacademy to gain knowledge in Cloud/terraform? Looking to get some feedback on these , checking if we can buy one for this thanksgiving deals?\n\nI see there is 50% discount for Acloudguru personal edition and cloudacademy is offering 35% discount for yearly membership.", "author_fullname": "t2_9zb1pmhi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Acloudguru vs Cloudacademy - for cloud courses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1bnse", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669069443.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669068009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is any one using acloudguru or Cloudacademy to gain knowledge in Cloud/terraform? Looking to get some feedback on these , checking if we can buy one for this thanksgiving deals?&lt;/p&gt;\n\n&lt;p&gt;I see there is 50% discount for Acloudguru personal edition and cloudacademy is offering 35% discount for yearly membership.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z1bnse", "is_robot_indexable": true, "report_reasons": null, "author": "Foreign_Yam3729", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1bnse/acloudguru_vs_cloudacademy_for_cloud_courses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1bnse/acloudguru_vs_cloudacademy_for_cloud_courses/", "subreddit_subscribers": 80660, "created_utc": 1669068009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In university we are learning about NoSQL vs SQL databases. We learned that NoSQL databases sacrifice a degree of consistency in exchange for availability; guaranteeing eventual consistency instead.\n\nMy question - I struggle to see how this would ever be a good idea. When a service has millions of users and has to deal with a lot of queries / computation per (small) unit of time, I'm inclined to think that results propagating over an outdated version of a database of some sorts would be quite bad. It doesn't help if I have immediately available data that's... inaccurate right? Or is the delta between the actual/outdated values small enough to be negligible? Are there situations / use cases where one would want to absolutely guarantee consistency over availability?", "author_fullname": "t2_1j8v22cd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about prioritizing availability over consistency", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z14zoy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669052541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In university we are learning about NoSQL vs SQL databases. We learned that NoSQL databases sacrifice a degree of consistency in exchange for availability; guaranteeing eventual consistency instead.&lt;/p&gt;\n\n&lt;p&gt;My question - I struggle to see how this would ever be a good idea. When a service has millions of users and has to deal with a lot of queries / computation per (small) unit of time, I&amp;#39;m inclined to think that results propagating over an outdated version of a database of some sorts would be quite bad. It doesn&amp;#39;t help if I have immediately available data that&amp;#39;s... inaccurate right? Or is the delta between the actual/outdated values small enough to be negligible? Are there situations / use cases where one would want to absolutely guarantee consistency over availability?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z14zoy", "is_robot_indexable": true, "report_reasons": null, "author": "stuffingmybrain", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z14zoy/question_about_prioritizing_availability_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z14zoy/question_about_prioritizing_availability_over/", "subreddit_subscribers": 80660, "created_utc": 1669052541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started a full time lead role recently and I am worried that I might not be doing much active development as much I used to. Sure I can work on personal projects, but I\u2019d rather earn on the side. Has anyone tried toptal for data engineering, if so what is your experience and how was the interview process?", "author_fullname": "t2_244blh78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone tried freelancing whist working a full time job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1ncwq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669101754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started a full time lead role recently and I am worried that I might not be doing much active development as much I used to. Sure I can work on personal projects, but I\u2019d rather earn on the side. Has anyone tried toptal for data engineering, if so what is your experience and how was the interview process?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z1ncwq", "is_robot_indexable": true, "report_reasons": null, "author": "blahahaX", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1ncwq/has_anyone_tried_freelancing_whist_working_a_full/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1ncwq/has_anyone_tried_freelancing_whist_working_a_full/", "subreddit_subscribers": 80660, "created_utc": 1669101754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm not from a CSE background. So a lot of stuff for me is self taught. I understand at a high level the concept of RAM and Disk. I'd like to understand Arrow but that seems to be one layer deeper. \n\nWhat are some resources where I can learn more about concepts like:\n- Memory mapping\n- Serialization/Deserialization  \n- Buffers\n- Processes", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources to learn and understand Arrow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1ictz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669085481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not from a CSE background. So a lot of stuff for me is self taught. I understand at a high level the concept of RAM and Disk. I&amp;#39;d like to understand Arrow but that seems to be one layer deeper. &lt;/p&gt;\n\n&lt;p&gt;What are some resources where I can learn more about concepts like:\n- Memory mapping\n- Serialization/Deserialization&lt;br/&gt;\n- Buffers\n- Processes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z1ictz", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1ictz/resources_to_learn_and_understand_arrow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1ictz/resources_to_learn_and_understand_arrow/", "subreddit_subscribers": 80660, "created_utc": 1669085481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's say a service is supposed to get data from BigQuery tables and then return it to the caller. \n\nThe response is dynamic in that the caller can specify the columns as well as ordering, grouping, and filtering clauses.\n\nThe source tables in BigQuery are updating based on some cron-like setting so it could be anything from hours to days. \n\n    SELECT col1, col2 FROM (SELECT col1, col2, col3, RANK() OVER(PARTITION BY col1, col2 ORDER BY col3 DESC) AS rank) WHERE rank = 1\n\nIn the query above col3 is a timestamp-based column so the idea is to make sure that I only select rows with the most recent values of col3. What makes it more tricky though is the fact that the set of columns in such queries is not static (whereas col3 is always there). \n\nIf you have dealt with the same requirement, what was your approach? I was also thinking about ingestion-based table partitioning (hour-based) so that I could benefit from the \\_PARTITIONTIME meta field but even in this case it's not guaranteed that the services that gather the data do not export several times (for some reason) within one hour.", "author_fullname": "t2_zqr9y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deduplicating data from BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z164nf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669055194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say a service is supposed to get data from BigQuery tables and then return it to the caller. &lt;/p&gt;\n\n&lt;p&gt;The response is dynamic in that the caller can specify the columns as well as ordering, grouping, and filtering clauses.&lt;/p&gt;\n\n&lt;p&gt;The source tables in BigQuery are updating based on some cron-like setting so it could be anything from hours to days. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT col1, col2 FROM (SELECT col1, col2, col3, RANK() OVER(PARTITION BY col1, col2 ORDER BY col3 DESC) AS rank) WHERE rank = 1\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;In the query above col3 is a timestamp-based column so the idea is to make sure that I only select rows with the most recent values of col3. What makes it more tricky though is the fact that the set of columns in such queries is not static (whereas col3 is always there). &lt;/p&gt;\n\n&lt;p&gt;If you have dealt with the same requirement, what was your approach? I was also thinking about ingestion-based table partitioning (hour-based) so that I could benefit from the _PARTITIONTIME meta field but even in this case it&amp;#39;s not guaranteed that the services that gather the data do not export several times (for some reason) within one hour.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z164nf", "is_robot_indexable": true, "report_reasons": null, "author": "dondraper36", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z164nf/deduplicating_data_from_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z164nf/deduplicating_data_from_bigquery/", "subreddit_subscribers": 80660, "created_utc": 1669055194.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Good day,  \n\n\nI had a question regarding daily information (SDC): We are looking to create a table containing the daily team of our employees.  \nThis is then used when manipulating timelogs in our project manager (if employee was in this department on that day, count timelogs).  \n\n\nMy issue is: This means I am artificially generating **n rows** everyday, **n** being the number of employees.  \n\n\nTherefore, my question: What is the best way to take into account daily parameters without generating too many useless entries", "author_fullname": "t2_kinpz3uc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modeling best practices - HR daily department", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z18s7g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669061313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good day,  &lt;/p&gt;\n\n&lt;p&gt;I had a question regarding daily information (SDC): We are looking to create a table containing the daily team of our employees.&lt;br/&gt;\nThis is then used when manipulating timelogs in our project manager (if employee was in this department on that day, count timelogs).  &lt;/p&gt;\n\n&lt;p&gt;My issue is: This means I am artificially generating &lt;strong&gt;n rows&lt;/strong&gt; everyday, &lt;strong&gt;n&lt;/strong&gt; being the number of employees.  &lt;/p&gt;\n\n&lt;p&gt;Therefore, my question: What is the best way to take into account daily parameters without generating too many useless entries&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z18s7g", "is_robot_indexable": true, "report_reasons": null, "author": "Brewash_Beer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z18s7g/data_modeling_best_practices_hr_daily_department/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z18s7g/data_modeling_best_practices_hr_daily_department/", "subreddit_subscribers": 80660, "created_utc": 1669061313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, world, I need your help with with issues coming from my read replica.\n\n**Background**\n\nI am trying to replicate data between two databases both my source (aws rds Postgres read replica) and destination aws rds Postgres using #airbyte . The connection will sync and then fail with the following message:\n\n    Sync Failed\n    Last attempt:67.92 MB513,973 emitted recordsno records2m 44s\n    Failure Origin: source, Message: Something went wrong in the connector. See the logs for more details.\n    \n    2022-11-21 14:26:40 - Additional Failure Information: java.lang.RuntimeException: org.postgresql.util.PSQLException: FATAL: terminating connection due to conflict with recovery Detail: User query might have needed to see row versions that must be removed. Hint: In a moment you should be able to reconnect to the database and repeat your command.\n\n**Main issue**\n\nBased on this statement:\n\n&gt;terminating connection due to conflict with recovery Detail: User query might have needed to see row versions that must be removed\n\nI think this error is happening because there is some limitation on how much or how long I can read from the Postgres read replica. I don't know if this conclusion is correct or how to resolve it. Would appreciate any help.", "author_fullname": "t2_oi5sbpt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cannot replicate data from Postgres read replica (I use airbyte for replication)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z1v094", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669127405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, world, I need your help with with issues coming from my read replica.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I am trying to replicate data between two databases both my source (aws rds Postgres read replica) and destination aws rds Postgres using #airbyte . The connection will sync and then fail with the following message:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Sync Failed\nLast attempt:67.92 MB513,973 emitted recordsno records2m 44s\nFailure Origin: source, Message: Something went wrong in the connector. See the logs for more details.\n\n2022-11-21 14:26:40 - Additional Failure Information: java.lang.RuntimeException: org.postgresql.util.PSQLException: FATAL: terminating connection due to conflict with recovery Detail: User query might have needed to see row versions that must be removed. Hint: In a moment you should be able to reconnect to the database and repeat your command.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Main issue&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Based on this statement:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;terminating connection due to conflict with recovery Detail: User query might have needed to see row versions that must be removed&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I think this error is happening because there is some limitation on how much or how long I can read from the Postgres read replica. I don&amp;#39;t know if this conclusion is correct or how to resolve it. Would appreciate any help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z1v094", "is_robot_indexable": true, "report_reasons": null, "author": "FarisAi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1v094/cannot_replicate_data_from_postgres_read_replica/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1v094/cannot_replicate_data_from_postgres_read_replica/", "subreddit_subscribers": 80660, "created_utc": 1669127405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://medium.com/memphis-dev/event-streams-are-nothing-without-action-f78823466fb4](https://medium.com/memphis-dev/event-streams-are-nothing-without-action-f78823466fb4)", "author_fullname": "t2_bqjrmnud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Event Streams Are Nothing Without Action", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z1udt6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669125810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/memphis-dev/event-streams-are-nothing-without-action-f78823466fb4\"&gt;https://medium.com/memphis-dev/event-streams-are-nothing-without-action-f78823466fb4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?auto=webp&amp;s=e23c28db75a822ff58267da8591ce5c88e1ee6aa", "width": 1200, "height": 660}, "resolutions": [{"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=feee2c7c6c02270fc67e98bfd165ed7a82c33490", "width": 108, "height": 59}, {"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d2b9cbd4624e2a625ee1ab9a836e2d9149235144", "width": 216, "height": 118}, {"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fac6d6b272c8bb44e6f61413b429f8b48ea71f46", "width": 320, "height": 176}, {"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4acf139763986cdbbe1d26e289358774ebed85a7", "width": 640, "height": 352}, {"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=350827958b5de8caa11b291479485047c4d6a13b", "width": 960, "height": 528}, {"url": "https://external-preview.redd.it/Uctkyp9TO7LDEEgapMVg6FQTFjinzJ85V3iYiJPQIFU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=de8167cca3f782ae470e3a8296a8dae81c947a42", "width": 1080, "height": 594}], "variants": {}, "id": "PfhS002WvOJZACaqvBTP8UEQXWEYQz5EHFWT_zM36ls"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z1udt6", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Bug105", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1udt6/event_streams_are_nothing_without_action/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1udt6/event_streams_are_nothing_without_action/", "subreddit_subscribers": 80660, "created_utc": 1669125810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have two questions;\n\n1. How a data architecture set up in Azure will translate to GCP if we have the following components:\n\nData processing: **Databricks** notebooks with bronze, silver gold stages\n\nDatawarehouse: **Databricks** hive delta tables\n\nObject storage: **Azure** Storage\n\nData ingestion, and ETL (creation/monitoring): Azure **Data Factory**\n\nBI: PowerBI &gt; Databricks import with **Server Hostname** and **HTTP Path** connection. Ultra-slow as the pbi dataset grows. \n\n2. When or where could the typical tools like Airflow, Terraform, dbt, kubernetes, docker... be of any use or could help improve this architecture? \n\n&amp;#x200B;\n\nThe only thing that I know is missing here is **git** which seems to be a basic if you want to have a CI/CD setup. \n\n&amp;#x200B;\n\nThank you", "author_fullname": "t2_145y7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure architecture to GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z1u7qy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669125405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have two questions;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How a data architecture set up in Azure will translate to GCP if we have the following components:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Data processing: &lt;strong&gt;Databricks&lt;/strong&gt; notebooks with bronze, silver gold stages&lt;/p&gt;\n\n&lt;p&gt;Datawarehouse: &lt;strong&gt;Databricks&lt;/strong&gt; hive delta tables&lt;/p&gt;\n\n&lt;p&gt;Object storage: &lt;strong&gt;Azure&lt;/strong&gt; Storage&lt;/p&gt;\n\n&lt;p&gt;Data ingestion, and ETL (creation/monitoring): Azure &lt;strong&gt;Data Factory&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;BI: PowerBI &amp;gt; Databricks import with &lt;strong&gt;Server Hostname&lt;/strong&gt; and &lt;strong&gt;HTTP Path&lt;/strong&gt; connection. Ultra-slow as the pbi dataset grows. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;When or where could the typical tools like Airflow, Terraform, dbt, kubernetes, docker... be of any use or could help improve this architecture? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The only thing that I know is missing here is &lt;strong&gt;git&lt;/strong&gt; which seems to be a basic if you want to have a CI/CD setup. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z1u7qy", "is_robot_indexable": true, "report_reasons": null, "author": "swaisdrais", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/z1u7qy/azure_architecture_to_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1u7qy/azure_architecture_to_gcp/", "subreddit_subscribers": 80660, "created_utc": 1669125405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to know if it is possible to integrate flume and structured streaming. I know it is possible with spark streaming. \n\nCurrently I am trying to stream a log file which gets regular updates using flume. I used TAILDIR as source and file channel. Is there a good sink which can make it possible. Thanks in advance.", "author_fullname": "t2_cb7rpz4k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache flume and structured streaming integration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1th7f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669123353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to know if it is possible to integrate flume and structured streaming. I know it is possible with spark streaming. &lt;/p&gt;\n\n&lt;p&gt;Currently I am trying to stream a log file which gets regular updates using flume. I used TAILDIR as source and file channel. Is there a good sink which can make it possible. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z1th7f", "is_robot_indexable": true, "report_reasons": null, "author": "Mr_marshmellow19", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1th7f/apache_flume_and_structured_streaming_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1th7f/apache_flume_and_structured_streaming_integration/", "subreddit_subscribers": 80660, "created_utc": 1669123353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/z1tdb0)", "author_fullname": "t2_eajtr4nz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How many queries does your organization run on a typical day? (i.e. across scheduled dashboards / reports, and ad-hoc queries)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1tdb0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669123044.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/z1tdb0\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z1tdb0", "is_robot_indexable": true, "report_reasons": null, "author": "alneuman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1669382244623, "options": [{"text": "less than 100", "id": "19960736"}, {"text": "100 - 500", "id": "19960737"}, {"text": "500 - 1,000", "id": "19960738"}, {"text": "1,000 - 5,000", "id": "19960739"}, {"text": "5,000 - 10,000", "id": "19960740"}, {"text": "&gt; 10,000", "id": "19960741"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 26, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1tdb0/how_many_queries_does_your_organization_run_on_a/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/z1tdb0/how_many_queries_does_your_organization_run_on_a/", "subreddit_subscribers": 80660, "created_utc": 1669123044.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Given all the vendors (AWS, GCP, Azure etc.) and cloud products, which cloud product should i go with to provide platform as a service? I am in the finance industry and we want to open up our platform for smaller companies to utilize.", "author_fullname": "t2_5zlk5o8b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which cloud platform to use in 2022\\2023 for your data product?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1re38", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669117097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given all the vendors (AWS, GCP, Azure etc.) and cloud products, which cloud product should i go with to provide platform as a service? I am in the finance industry and we want to open up our platform for smaller companies to utilize.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z1re38", "is_robot_indexable": true, "report_reasons": null, "author": "AggravatingWish1019", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1re38/which_cloud_platform_to_use_in_20222023_for_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1re38/which_cloud_platform_to_use_in_20222023_for_your/", "subreddit_subscribers": 80660, "created_utc": 1669117097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m using FastAPI for Machine Learning prediction model deployment (classification case), In fact, I have 2 type of models that works the following way:\n\n1. if model got all features (**primary and secondary**) =&gt; apply **model\\_type1**\n2. if model got only **primary features** (missing secondary) =&gt; apply **model\\_type2**\n3. if model got missing primary features =&gt; display an error (invalid for prediction, we can't use any of the two models due to insufficient data)\n\nPS. model\\_type1 has features of model\\_type2 (called primary) + other features (called secondary), while model\\_type2 must have only primary features, if none of them is an option an error should be displayed.\n\nTo achieve this, I have the inputs of type \"array of JSON\" in the form below:\n\n    [ { \n    \"Id\":\"value\",\n    \"feature_primary1\":\"value\", \n    \"feature_primary2\":\"value\", \n    \"feature_primary3\":\"value\", \n    \"feature_secondary1\":\"value\", \n    \"feature_secondary2\":\"value\" \n    }, \n    { \n    \"Id\":\"value\", \n    \"feature_primary1\":\"value\", \n    \"feature_primary2\":\"value\", \n    \"feature_primary3\":\"value\", \n    \"feature_secondary1\":\"value\", \n    \"feature_secondary2\":\"value\" \n    }, \n    { \n    \"Id\":\"value\", \n    \"feature_primary1\":\"value\", \n    \"feature_primary2\":\"value\", \n    \"feature_primary3\":\"value\", \n    \"feature_secondary1\":\"value\", \n    \"feature_secondary2\":\"value\" \n    } \n    ... \n    ] \n\nAnd I need to filter the inputs before feeding them to the predictive model. Meaning, to filter each JSON  \n of the array.\n\nSo, I want to create 3 lists where:\n\n1. if inputs match **model\\_type1** append to list1\n2. if inputs doesn't match **model\\_type1** and match **model\\_type2** append to list2\n3. if inputs doesn't match **model\\_type1** nor **model\\_type2** append error detail to list3\n\nAnd then assign each list to its proper model and display the prediction results (Outputs). So if we send the following inputs:\n\n    [ \n    { \n    \"Id\": 1,\n    \"feature_primary1\": \"David\", \n    \"feature_primary2\": 15670.87, \n    \"feature_primary3\": \"Male\", \n    \"feature_secondary1\": \"Yes\", \n    \"feature_secondary2\": 45 \n    }, \n    { \n    \"Id\": 2, \n    \"feature_primary1\": \"Alice\", \n    \"feature_primary2\": 78995.65, \n    \"feature_primary3\": \"Female\", \n    \"feature_secondary1\": NaN, \n    \"feature_secondary2\": NaN   \n    }, \n    { \n    \"Id\": 3, \n    \"feature_primary1\": \"John\", \n    \"feature_primary2\": NaN, \n    \"feature_primary3\": NaN, \n    \"feature_secondary1\": NaN, \n    \"feature_secondary2\": 79 \n    } \n    ... \n    ] \n\nThe output (result of prediction) should look like this :\n\n    [ \n    { \n    \"Id\": 1, \n    \"type_model\": \"model_type1\", \n    \"prediction\": \"class1\" \n    }, \n    { \n    \"Id\": 2, \n    \"type_model\": \"model_type2\", \n    \"prediction\": \"class2\" \n    }, \n    { \"Id\": 3, \n    \"error_description\": \"missing values for feature_primary2,                          feature_primary3 and feature_secondary1\" \n    } \n    ... \n    ] \n\nHow can I achieve the filter process within the 3 lists, in an optimized way with FastAPI in Python?", "author_fullname": "t2_baqjslqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Filter inputs of FastAPI and assign each type to a specific list", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1312n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669047692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m using FastAPI for Machine Learning prediction model deployment (classification case), In fact, I have 2 type of models that works the following way:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;if model got all features (&lt;strong&gt;primary and secondary&lt;/strong&gt;) =&amp;gt; apply &lt;strong&gt;model_type1&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;if model got only &lt;strong&gt;primary features&lt;/strong&gt; (missing secondary) =&amp;gt; apply &lt;strong&gt;model_type2&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;if model got missing primary features =&amp;gt; display an error (invalid for prediction, we can&amp;#39;t use any of the two models due to insufficient data)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;PS. model_type1 has features of model_type2 (called primary) + other features (called secondary), while model_type2 must have only primary features, if none of them is an option an error should be displayed.&lt;/p&gt;\n\n&lt;p&gt;To achieve this, I have the inputs of type &amp;quot;array of JSON&amp;quot; in the form below:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[ { \n&amp;quot;Id&amp;quot;:&amp;quot;value&amp;quot;,\n&amp;quot;feature_primary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary3&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary2&amp;quot;:&amp;quot;value&amp;quot; \n}, \n{ \n&amp;quot;Id&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary3&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary2&amp;quot;:&amp;quot;value&amp;quot; \n}, \n{ \n&amp;quot;Id&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary3&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary2&amp;quot;:&amp;quot;value&amp;quot; \n} \n... \n] \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And I need to filter the inputs before feeding them to the predictive model. Meaning, to filter each JSON&lt;br/&gt;\n of the array.&lt;/p&gt;\n\n&lt;p&gt;So, I want to create 3 lists where:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;if inputs match &lt;strong&gt;model_type1&lt;/strong&gt; append to list1&lt;/li&gt;\n&lt;li&gt;if inputs doesn&amp;#39;t match &lt;strong&gt;model_type1&lt;/strong&gt; and match &lt;strong&gt;model_type2&lt;/strong&gt; append to list2&lt;/li&gt;\n&lt;li&gt;if inputs doesn&amp;#39;t match &lt;strong&gt;model_type1&lt;/strong&gt; nor &lt;strong&gt;model_type2&lt;/strong&gt; append error detail to list3&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;And then assign each list to its proper model and display the prediction results (Outputs). So if we send the following inputs:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[ \n{ \n&amp;quot;Id&amp;quot;: 1,\n&amp;quot;feature_primary1&amp;quot;: &amp;quot;David&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;: 15670.87, \n&amp;quot;feature_primary3&amp;quot;: &amp;quot;Male&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;: &amp;quot;Yes&amp;quot;, \n&amp;quot;feature_secondary2&amp;quot;: 45 \n}, \n{ \n&amp;quot;Id&amp;quot;: 2, \n&amp;quot;feature_primary1&amp;quot;: &amp;quot;Alice&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;: 78995.65, \n&amp;quot;feature_primary3&amp;quot;: &amp;quot;Female&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;: NaN, \n&amp;quot;feature_secondary2&amp;quot;: NaN   \n}, \n{ \n&amp;quot;Id&amp;quot;: 3, \n&amp;quot;feature_primary1&amp;quot;: &amp;quot;John&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;: NaN, \n&amp;quot;feature_primary3&amp;quot;: NaN, \n&amp;quot;feature_secondary1&amp;quot;: NaN, \n&amp;quot;feature_secondary2&amp;quot;: 79 \n} \n... \n] \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The output (result of prediction) should look like this :&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[ \n{ \n&amp;quot;Id&amp;quot;: 1, \n&amp;quot;type_model&amp;quot;: &amp;quot;model_type1&amp;quot;, \n&amp;quot;prediction&amp;quot;: &amp;quot;class1&amp;quot; \n}, \n{ \n&amp;quot;Id&amp;quot;: 2, \n&amp;quot;type_model&amp;quot;: &amp;quot;model_type2&amp;quot;, \n&amp;quot;prediction&amp;quot;: &amp;quot;class2&amp;quot; \n}, \n{ &amp;quot;Id&amp;quot;: 3, \n&amp;quot;error_description&amp;quot;: &amp;quot;missing values for feature_primary2,                          feature_primary3 and feature_secondary1&amp;quot; \n} \n... \n] \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;How can I achieve the filter process within the 3 lists, in an optimized way with FastAPI in Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z1312n", "is_robot_indexable": true, "report_reasons": null, "author": "According-Promise-23", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1312n/filter_inputs_of_fastapi_and_assign_each_type_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1312n/filter_inputs_of_fastapi_and_assign_each_type_to/", "subreddit_subscribers": 80660, "created_utc": 1669047692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Free online course\n\nIntroduction to Power BI\n\nIn this course, you\u2019ll go from zero to hero as you discover how to use this popular business intelligence platform through hands-on exercises.\n\n[https://formationgratuite.net/Introduction-to-Power-BI/](https://formationgratuite.net/Introduction-to-Power-BI/)", "author_fullname": "t2_ca2mv3e7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introduction to Power BI - Free online course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z13dx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669048588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Free online course&lt;/p&gt;\n\n&lt;p&gt;Introduction to Power BI&lt;/p&gt;\n\n&lt;p&gt;In this course, you\u2019ll go from zero to hero as you discover how to use this popular business intelligence platform through hands-on exercises.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://formationgratuite.net/Introduction-to-Power-BI/\"&gt;https://formationgratuite.net/Introduction-to-Power-BI/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gWillDPtz8S9fqu3z_2CJtKwn7K-4JGGc80WRivqakE.jpg?auto=webp&amp;s=fc5e8a60e405087ef02fc609376980b91e65677b", "width": 940, "height": 788}, "resolutions": [{"url": "https://external-preview.redd.it/gWillDPtz8S9fqu3z_2CJtKwn7K-4JGGc80WRivqakE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49d0755ed96a8f3a1d6c80e04df15501011f291a", "width": 108, "height": 90}, {"url": "https://external-preview.redd.it/gWillDPtz8S9fqu3z_2CJtKwn7K-4JGGc80WRivqakE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a28fecd117ae38bd5a6da1c47617f90631815313", "width": 216, "height": 181}, {"url": "https://external-preview.redd.it/gWillDPtz8S9fqu3z_2CJtKwn7K-4JGGc80WRivqakE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=80ad1e1f472ffb2f5c94147a6c1d06fd076822d8", "width": 320, "height": 268}, {"url": "https://external-preview.redd.it/gWillDPtz8S9fqu3z_2CJtKwn7K-4JGGc80WRivqakE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=44e0a26db070b4ef5a13a22521a0956c8c13e942", "width": 640, "height": 536}], "variants": {}, "id": "wHqW370wf9afWH-dOw8frn8SzXpisyKcQLd9kwOoMK4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z13dx8", "is_robot_indexable": true, "report_reasons": null, "author": "MDLearning", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z13dx8/introduction_to_power_bi_free_online_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z13dx8/introduction_to_power_bi_free_online_course/", "subreddit_subscribers": 80660, "created_utc": 1669048588.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}