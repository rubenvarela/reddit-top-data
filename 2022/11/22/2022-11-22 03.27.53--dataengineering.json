{"kind": "Listing", "data": {"after": "t3_z0wzng", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can anybody please explain to me why people are obsessed with working for FAANG+? These companies are notorious for being morally dubious, having shitty working environments and high burnout churn. Plenty of more attractive options out there. Just curious as I see a lot of posts on here and other subs with people asking for FAANG+ specific interview prep advice and it just seems odd. You wouldn't want to work for SPECTRE would you, very pass\u00e9.", "author_fullname": "t2_8p2u7cmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why FAANG+", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0srs8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 84, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 84, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669016067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anybody please explain to me why people are obsessed with working for FAANG+? These companies are notorious for being morally dubious, having shitty working environments and high burnout churn. Plenty of more attractive options out there. Just curious as I see a lot of posts on here and other subs with people asking for FAANG+ specific interview prep advice and it just seems odd. You wouldn&amp;#39;t want to work for SPECTRE would you, very pass\u00e9.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z0srs8", "is_robot_indexable": true, "report_reasons": null, "author": "Jamese0", "discussion_type": null, "num_comments": 65, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0srs8/why_faang/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0srs8/why_faang/", "subreddit_subscribers": 80601, "created_utc": 1669016067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dfcot0zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Made a post about weird hybrid titles coming out. Who can guess the job duties without reading the full posting (which can be Easily found on LinkedIN for anyone interested)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 66, "top_awarded_type": null, "hide_score": false, "name": "t3_z0wzb0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 69, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 69, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/wI0LS_wQTFa0N8oq6Rx5xiXYpYQ4CIknIAYOvv4yEA0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669031156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9lc0okovka1a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9lc0okovka1a1.png?auto=webp&amp;s=9150a5487c6a0fe1c4b10435678b9361caafa34b", "width": 908, "height": 432}, "resolutions": [{"url": "https://preview.redd.it/9lc0okovka1a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=db074733d799d3e20c43e18f43584ecf97e7195b", "width": 108, "height": 51}, {"url": "https://preview.redd.it/9lc0okovka1a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=339386dd677d010b9cb024f9bfe9d3b6445ab396", "width": 216, "height": 102}, {"url": "https://preview.redd.it/9lc0okovka1a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=74a5b4b4a9956b1501d41bc683afafe712073550", "width": 320, "height": 152}, {"url": "https://preview.redd.it/9lc0okovka1a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=61fabfcfab0682c4f5cca3744d64fdef0600f5d2", "width": 640, "height": 304}], "variants": {}, "id": "1nk3YUgpBJpISmiABbzlD9Co0XakzXnBCry9eCzOy-A"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z0wzb0", "is_robot_indexable": true, "report_reasons": null, "author": "DrRedmondNYC", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0wzb0/made_a_post_about_weird_hybrid_titles_coming_out/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9lc0okovka1a1.png", "subreddit_subscribers": 80601, "created_utc": 1669031156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi readers, I made this post to share my journey as a Junior Data Engineer in a tech startup that initially has no data infrastructure. How it all began, what I've learnt, where I've failed, and where I am now.\n\nBefore I begin telling you about my journey, let me explain a bit about my background, ie: education and how I applied for this job post-covid. I graduated with a Bachelor of Computer Science in Data Science, and with that, it would have been natural for me to apply for a job as a Data Scientist. I applied for that position for around a month and sadly, did not get any response. This ofcourse mace me feel depressed and frustrated thinking that I would never be hired as a Data Scientist. Then, I decided to shift my strategy to another method, in which I approached a professional recruiter to assist me in finding a suitable company who would value learning and taking a chance with me. Thankfully, within a week, I got a response from a company which eventually became the workplace I am currently in.\n\nSo let\u2019s talk about the interview process. I was interviewed for a Data Scientist position, that consists of 3 seperate sessions. In the first session, I was given an assignment to solve \"The Birthday\" problem along with a probability problem (which I kind of forgot the actual name of it). To be completely honest, I had no idea what \"The Birthday\" problem even is, so I did what everyone who was given an interview assignment does and searched for it online. I then copied and pasted the code/solution for the assignment. It only took about 30 minutes to understand the code and prepare it for the second session. As the second session was about to get started, I bluntly told the interviewer that I just sort've cheated and pasted the online code/solution. Regardless, the interviewer brushed it off, and began another interview process. A few questions of the solution I copied were thrown at me, and I was able to answer a few of them with relative confidence. Though after this session, I immediately lost hope due to the factors in how I did the assignment and answered the questions, which again I remind you thar I literally copied and pasted the code to make it run. To my surprise, about a couple weeks later, I was asked to attend a third interview! I of course went in expecting some feedback on what I've done so far(maybe some \"DO NOT CHEAT!\" advice), but instead they congratulated me and told me I got the post. I was left shocked and confused but ultimately happy at this turn of events. \n\nFast forward to my first day in office, it was also during a Program Increment event, and I had no idea what anyone was doing. I kept hearing my manager go on about SAFe (Scaled Agile Framework) and Scrum events. To be frank, I was so overloaded with information and was pretty shocked(scared more like) when my manager told me that I was to be split in half, figuratively of course, to be a member of both the Data and the Integration teams. This was due to the data team being new and small, only consisting of myself and my manager(whom by now I should mention, was the one who interviewed me). So during the first few days, I was trying to understand what the company is trying to acheive as well as my job scope within these two teams. I spent almost a month doing practically nothing and instead kept going around and bugging my colleages(specifically the Software Testing Lead, in my defense, they told me to ask him anything) to show me the ropes and understand what the system architecture is and so on and so on. \n\nIn the first few months in the company, I was temporarily assigned as a software tester in the Integration team and an analyst in the Data team.\u00a0 For the testing scope, most of the work is clear and easy to perform as it was just testing API callbacks, validating the documents' structure, and checking the acceptance of the API. I was a bit more relaxed in the Integration team but when it came to the Data side of things, I felt overwhelmed with stress and confusion from time to time. \n\nIn the data team, I was experimenting with a lot of stuff, such as analysing the company communication platform, coming up with word cloud and scraping information online. I spent most of my time cleaning the data and I felt it was pretty much wasted as it never reached production. While I was studyinh in University, the process of making an ML/AI model is straightforward, as the data has been cleaned and ready to use, but in real life, we need a data infrastructure which provides clean data for the downstream user to do analytics or ML/AI model training. That\u2019s when I began to learn about data engineering, this led to me realised that to create a fancy ML/AI, first we need to have a good dataset or else it will just be garbage in and garbage out. \n\nWith this newfound knowledge, I consulted with my manager and volunteered myself to explore the data engineering field and him being himself gave me the green light to do so. When I was exploring I discovered a lot of terms like data warehousing, data lake, ETL, ELT, reserver ETL and etc. I was so confused and overwhelmed by it so I searched how to start in DE(Data Engineering), and came to find that different people share different DE roadmaps with the similarities being the fundamentals, the usage of Python and SQL. At first, I never understood why we even need SQL in DE, so for the next few months, I kept following The Seattle Data Guys aka Ben Rogojan. I learn a lot of stuff from him such as explaining the different tools from the website.\n\nAfter a while, my company FINALLY appointed someone else as a Data Lead(this person is not the manager who interviewed me because he was busy being the Release Train Engineer for SAFe). This is good because most of the time I have no one else to consult to about data matters and I end-up YOLO-ing the job, leading to me messing up or it being so bad that it doesn\u2019t get the pushed to the production stage. At last, I finally have a team but it still only consists of two people hahaha. So then, we start performing proper work like researching ETL tools, data warehouse and data visualisation tools. \n\nI spent the following month trying out those ETL tools and Data Warehousing. I learnt a lot of stuff like the best practices, ETL vs ELT the use cases and so on. A website I'm fond of, [g2.com](https://g2.com), is a good place to compare different tools. I was tempted at first to try out a famous ETL tool, Fivetran. However,it isn't that beginner-friendly when compared to HevoData(which my company currently uses). That is way more user-friendly, where I can straight away query the data from HevoData without needing to query the data from the destination GUI, ie: AWS Redshift. I also learnt about DBT labs, which I still struggle with because I need to learn how to host them on GitHub and link it to the destination. \n\nSo, where did I fail? I failed in quite a few things such as understanding the use and importance of SQL, especially for the transformation part. I also made some mistakes such as I need to make a schema of raw data and a schema of transforming data in the data warehouse/ data lakehouse because I use the ELT method. I've also messed up by learning theorical concepts without an inkling of real-world use cases, as I'm afraid to try it out at first because in reality, depending on a companys' data maturity, there will have different ways to carry out building the data infrastructure. I still have other stuff to learn like database denormalisation, advanced SQL query, and etc. \n\nMy take is if you truly want to survive in the DE field, don\u2019t be afraid to fail, learn from the it and improve upon it. That\u2019s how I was able to build the companys' first data pipeline and Data warehouse/data lakehouse. I'm still way beyond from being expert let alone covering everything in this field but I will continue to keep on learning as I proceed. Thanks for reading this and P.S: This is my first time writing a blog, so if you have any suggestions for me feel free to tell me in the comments. Please excuse my English, even with I had someone to proofread for me and he isn't that great in English either.", "author_fullname": "t2_k7g9mhou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Journey as a Junior Data Engineer in a tech startup that initially has no data infrastructure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0u75v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669021348.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi readers, I made this post to share my journey as a Junior Data Engineer in a tech startup that initially has no data infrastructure. How it all began, what I&amp;#39;ve learnt, where I&amp;#39;ve failed, and where I am now.&lt;/p&gt;\n\n&lt;p&gt;Before I begin telling you about my journey, let me explain a bit about my background, ie: education and how I applied for this job post-covid. I graduated with a Bachelor of Computer Science in Data Science, and with that, it would have been natural for me to apply for a job as a Data Scientist. I applied for that position for around a month and sadly, did not get any response. This ofcourse mace me feel depressed and frustrated thinking that I would never be hired as a Data Scientist. Then, I decided to shift my strategy to another method, in which I approached a professional recruiter to assist me in finding a suitable company who would value learning and taking a chance with me. Thankfully, within a week, I got a response from a company which eventually became the workplace I am currently in.&lt;/p&gt;\n\n&lt;p&gt;So let\u2019s talk about the interview process. I was interviewed for a Data Scientist position, that consists of 3 seperate sessions. In the first session, I was given an assignment to solve &amp;quot;The Birthday&amp;quot; problem along with a probability problem (which I kind of forgot the actual name of it). To be completely honest, I had no idea what &amp;quot;The Birthday&amp;quot; problem even is, so I did what everyone who was given an interview assignment does and searched for it online. I then copied and pasted the code/solution for the assignment. It only took about 30 minutes to understand the code and prepare it for the second session. As the second session was about to get started, I bluntly told the interviewer that I just sort&amp;#39;ve cheated and pasted the online code/solution. Regardless, the interviewer brushed it off, and began another interview process. A few questions of the solution I copied were thrown at me, and I was able to answer a few of them with relative confidence. Though after this session, I immediately lost hope due to the factors in how I did the assignment and answered the questions, which again I remind you thar I literally copied and pasted the code to make it run. To my surprise, about a couple weeks later, I was asked to attend a third interview! I of course went in expecting some feedback on what I&amp;#39;ve done so far(maybe some &amp;quot;DO NOT CHEAT!&amp;quot; advice), but instead they congratulated me and told me I got the post. I was left shocked and confused but ultimately happy at this turn of events. &lt;/p&gt;\n\n&lt;p&gt;Fast forward to my first day in office, it was also during a Program Increment event, and I had no idea what anyone was doing. I kept hearing my manager go on about SAFe (Scaled Agile Framework) and Scrum events. To be frank, I was so overloaded with information and was pretty shocked(scared more like) when my manager told me that I was to be split in half, figuratively of course, to be a member of both the Data and the Integration teams. This was due to the data team being new and small, only consisting of myself and my manager(whom by now I should mention, was the one who interviewed me). So during the first few days, I was trying to understand what the company is trying to acheive as well as my job scope within these two teams. I spent almost a month doing practically nothing and instead kept going around and bugging my colleages(specifically the Software Testing Lead, in my defense, they told me to ask him anything) to show me the ropes and understand what the system architecture is and so on and so on. &lt;/p&gt;\n\n&lt;p&gt;In the first few months in the company, I was temporarily assigned as a software tester in the Integration team and an analyst in the Data team.\u00a0 For the testing scope, most of the work is clear and easy to perform as it was just testing API callbacks, validating the documents&amp;#39; structure, and checking the acceptance of the API. I was a bit more relaxed in the Integration team but when it came to the Data side of things, I felt overwhelmed with stress and confusion from time to time. &lt;/p&gt;\n\n&lt;p&gt;In the data team, I was experimenting with a lot of stuff, such as analysing the company communication platform, coming up with word cloud and scraping information online. I spent most of my time cleaning the data and I felt it was pretty much wasted as it never reached production. While I was studyinh in University, the process of making an ML/AI model is straightforward, as the data has been cleaned and ready to use, but in real life, we need a data infrastructure which provides clean data for the downstream user to do analytics or ML/AI model training. That\u2019s when I began to learn about data engineering, this led to me realised that to create a fancy ML/AI, first we need to have a good dataset or else it will just be garbage in and garbage out. &lt;/p&gt;\n\n&lt;p&gt;With this newfound knowledge, I consulted with my manager and volunteered myself to explore the data engineering field and him being himself gave me the green light to do so. When I was exploring I discovered a lot of terms like data warehousing, data lake, ETL, ELT, reserver ETL and etc. I was so confused and overwhelmed by it so I searched how to start in DE(Data Engineering), and came to find that different people share different DE roadmaps with the similarities being the fundamentals, the usage of Python and SQL. At first, I never understood why we even need SQL in DE, so for the next few months, I kept following The Seattle Data Guys aka Ben Rogojan. I learn a lot of stuff from him such as explaining the different tools from the website.&lt;/p&gt;\n\n&lt;p&gt;After a while, my company FINALLY appointed someone else as a Data Lead(this person is not the manager who interviewed me because he was busy being the Release Train Engineer for SAFe). This is good because most of the time I have no one else to consult to about data matters and I end-up YOLO-ing the job, leading to me messing up or it being so bad that it doesn\u2019t get the pushed to the production stage. At last, I finally have a team but it still only consists of two people hahaha. So then, we start performing proper work like researching ETL tools, data warehouse and data visualisation tools. &lt;/p&gt;\n\n&lt;p&gt;I spent the following month trying out those ETL tools and Data Warehousing. I learnt a lot of stuff like the best practices, ETL vs ELT the use cases and so on. A website I&amp;#39;m fond of, &lt;a href=\"https://g2.com\"&gt;g2.com&lt;/a&gt;, is a good place to compare different tools. I was tempted at first to try out a famous ETL tool, Fivetran. However,it isn&amp;#39;t that beginner-friendly when compared to HevoData(which my company currently uses). That is way more user-friendly, where I can straight away query the data from HevoData without needing to query the data from the destination GUI, ie: AWS Redshift. I also learnt about DBT labs, which I still struggle with because I need to learn how to host them on GitHub and link it to the destination. &lt;/p&gt;\n\n&lt;p&gt;So, where did I fail? I failed in quite a few things such as understanding the use and importance of SQL, especially for the transformation part. I also made some mistakes such as I need to make a schema of raw data and a schema of transforming data in the data warehouse/ data lakehouse because I use the ELT method. I&amp;#39;ve also messed up by learning theorical concepts without an inkling of real-world use cases, as I&amp;#39;m afraid to try it out at first because in reality, depending on a companys&amp;#39; data maturity, there will have different ways to carry out building the data infrastructure. I still have other stuff to learn like database denormalisation, advanced SQL query, and etc. &lt;/p&gt;\n\n&lt;p&gt;My take is if you truly want to survive in the DE field, don\u2019t be afraid to fail, learn from the it and improve upon it. That\u2019s how I was able to build the companys&amp;#39; first data pipeline and Data warehouse/data lakehouse. I&amp;#39;m still way beyond from being expert let alone covering everything in this field but I will continue to keep on learning as I proceed. Thanks for reading this and P.S: This is my first time writing a blog, so if you have any suggestions for me feel free to tell me in the comments. Please excuse my English, even with I had someone to proofread for me and he isn&amp;#39;t that great in English either.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 125, "id": "award_5f123e3d-4f48-42f4-9c11-e98b566d5897", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "When you come across a feel-good thing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Wholesome", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z0u75v", "is_robot_indexable": true, "report_reasons": null, "author": "FlorexOng_a1", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0u75v/journey_as_a_junior_data_engineer_in_a_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0u75v/journey_as_a_junior_data_engineer_in_a_tech/", "subreddit_subscribers": 80601, "created_utc": 1669021348.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to know how DE(Data Engineering) community make use of Airflow on AWS. \n\nMWAA(Amazon Managed Workflows for Apache Airflow) is ruled out due to cost considerations as we are a very small team.\n\nCan I install Airflow on EC2 instance and explore? Thanks.\n\nWant to learn form the community about how they use Airflow in their projects on AWS. Please through some examples.", "author_fullname": "t2_eozceps7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0pa71", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669004580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to know how DE(Data Engineering) community make use of Airflow on AWS. &lt;/p&gt;\n\n&lt;p&gt;MWAA(Amazon Managed Workflows for Apache Airflow) is ruled out due to cost considerations as we are a very small team.&lt;/p&gt;\n\n&lt;p&gt;Can I install Airflow on EC2 instance and explore? Thanks.&lt;/p&gt;\n\n&lt;p&gt;Want to learn form the community about how they use Airflow in their projects on AWS. Please through some examples.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z0pa71", "is_robot_indexable": true, "report_reasons": null, "author": "Liily_07", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0pa71/airflow_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0pa71/airflow_on_aws/", "subreddit_subscribers": 80601, "created_utc": 1669004580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve recently moved into the field of data engineering (6 months) after two years as a Business Analyst in NZ. Was wondering what the data engineering salaries were in NZ so that I can a plan on what to ask for our upcoming salary review. Thanks heaps", "author_fullname": "t2_if7fad4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are Data Engineer salaries in New Zealand?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0qw22", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669009601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve recently moved into the field of data engineering (6 months) after two years as a Business Analyst in NZ. Was wondering what the data engineering salaries were in NZ so that I can a plan on what to ask for our upcoming salary review. Thanks heaps&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z0qw22", "is_robot_indexable": true, "report_reasons": null, "author": "Fickle-Diet-3060", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0qw22/how_are_data_engineer_salaries_in_new_zealand/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0qw22/how_are_data_engineer_salaries_in_new_zealand/", "subreddit_subscribers": 80601, "created_utc": 1669009601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have worked on complex data pipelines and done analytics on existing tables in warehouse but never actually get to know who is responsible for designing the tables.", "author_fullname": "t2_rxkr4y1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who is ideally responsible for Data modelling in the project and as a data engineer it is expected from you to have strong knowledge and at what experience level ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z11anq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669043406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have worked on complex data pipelines and done analytics on existing tables in warehouse but never actually get to know who is responsible for designing the tables.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z11anq", "is_robot_indexable": true, "report_reasons": null, "author": "SoggyAbalone7392", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z11anq/who_is_ideally_responsible_for_data_modelling_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z11anq/who_is_ideally_responsible_for_data_modelling_in/", "subreddit_subscribers": 80601, "created_utc": 1669043406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Given recent debates on the necessity of certain tooling with small datasets, it would be interesting to know the distribution of workloads for users on this subreddit. Volume is obviously one of many potential proxies for the complexity of a workload, but it should suffice for a high level view.\n\n[View Poll](https://www.reddit.com/poll/z11xd8)", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much data do you process via your pipelines in a given day?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z11xd8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669044987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given recent debates on the necessity of certain tooling with small datasets, it would be interesting to know the distribution of workloads for users on this subreddit. Volume is obviously one of many potential proxies for the complexity of a workload, but it should suffice for a high level view.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/z11xd8\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z11xd8", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1669304187618, "options": [{"text": "Under 1GB", "id": "19944314"}, {"text": "1GB to 10GB", "id": "19944315"}, {"text": "10GB to 100GB", "id": "19944316"}, {"text": "100GB to 1TB", "id": "19944317"}, {"text": "1 TB+", "id": "19944318"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 230, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/z11xd8/how_much_data_do_you_process_via_your_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/z11xd8/how_much_data_do_you_process_via_your_pipelines/", "subreddit_subscribers": 80601, "created_utc": 1669044987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking at a DE job description that I've been asked about. What might you expect from someone in an interview who wants \"very strong Python\"? They would do a code exercise. \n\nI'm an intermediate Python guy with long tech background. Been doing analytics for a few years and moving towards DE. I got some decent AWS experience. \n\nI'm not scared of it, half expect to fail but it would probably be a good experience. But obviously I would want to prepare.", "author_fullname": "t2_2o0q5m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to expect in interview that wants \"Very strong Python development experience\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1euyw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669075807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking at a DE job description that I&amp;#39;ve been asked about. What might you expect from someone in an interview who wants &amp;quot;very strong Python&amp;quot;? They would do a code exercise. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m an intermediate Python guy with long tech background. Been doing analytics for a few years and moving towards DE. I got some decent AWS experience. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not scared of it, half expect to fail but it would probably be a good experience. But obviously I would want to prepare.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z1euyw", "is_robot_indexable": true, "report_reasons": null, "author": "rotterdamn8", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1euyw/what_to_expect_in_interview_that_wants_very/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1euyw/what_to_expect_in_interview_that_wants_very/", "subreddit_subscribers": 80601, "created_utc": 1669075807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our use-case is as follows:\n\n1. \\[Extract &amp; Load\\] Every minute, query for new/modified records from \\~700 different customers and load new/modified records into raw tables as jsonb.\n2. \\[Transform\\] Near realtime (every minute or so), run a transform specific to each customer to transform raw data and load it into a normalized table that our application reads from frequently.\n\nRunning 700 distinct transforms and pushing data to the same table has me concerned. Each individual transform run could handle between zero and 50k records. I'm looking at options for using dbt for these near-realtime transforms, but I'm not convinced this is the best solution.\n\nHow does DBT perform for near realtime transforms to production tables seeing heavy read volume? If DBT isn't a good fit, what other options should I explore?", "author_fullname": "t2_gs0mp007", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solution for heavy volume, near realtime transforms to production tables seeing heavy read volume.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z15816", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669053125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our use-case is as follows:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;[Extract &amp;amp; Load] Every minute, query for new/modified records from ~700 different customers and load new/modified records into raw tables as jsonb.&lt;/li&gt;\n&lt;li&gt;[Transform] Near realtime (every minute or so), run a transform specific to each customer to transform raw data and load it into a normalized table that our application reads from frequently.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Running 700 distinct transforms and pushing data to the same table has me concerned. Each individual transform run could handle between zero and 50k records. I&amp;#39;m looking at options for using dbt for these near-realtime transforms, but I&amp;#39;m not convinced this is the best solution.&lt;/p&gt;\n\n&lt;p&gt;How does DBT perform for near realtime transforms to production tables seeing heavy read volume? If DBT isn&amp;#39;t a good fit, what other options should I explore?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z15816", "is_robot_indexable": true, "report_reasons": null, "author": "RandomWalk55", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z15816/solution_for_heavy_volume_near_realtime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z15816/solution_for_heavy_volume_near_realtime/", "subreddit_subscribers": 80601, "created_utc": 1669053125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I stumbled across [this](https://informationscience.unt.edu/ms-information-systems) from the University of North Texas as I was searching for graduate level data modeling courses (I work as an analytics engineer with mainly dbt and GBQ, hardly any pipelines yet since we use stitch for now).. and it has me pretty interested due to the coursework.\n\nBy contrast and because of the popularity of computer science, I've been focusing on completing prerequisites for [Georgia Tech's OMSCS](https://omscs.gatech.edu/specialization-computing-systems), particularly in computing systems, through an online junior/community college.\n\nFor computing systems, I'm planning on learning about operating systems, networks, security, and computer architecture. Most of the prereqs have been in C and or C++, which is tangential to data engineering at best. There's numerous posts about SQL + Python being enough for most jobs (not considering jobs needing scala/java/BE knowledge). This all seems like a lot of preparation and knowledge acquisition for topics that I won't directly use in AE/DE. Have also read posts (from here and r/OMSCS) that say a masters isn't the best way to acquire knowledge needed to do one's job.\n\nMy impression is that most of DE is OJT and/or learned from personal projects and not really covered in academia at all. That being said, it seems the information science side of things worries about storage, organization, quality, etc. of the data. Isn't that primarily what we do, beyond moving the data?\n\nDoes it make more sense to stick with computing systems, or should I entertain something like the other masters, or something completely different? My end goal is to essentially stay in analytics or data engineering for the long term and make as much money as possible. No aspirations beyond that right now.", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why isn't information science a popular pathway into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z11r7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669044586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I stumbled across &lt;a href=\"https://informationscience.unt.edu/ms-information-systems\"&gt;this&lt;/a&gt; from the University of North Texas as I was searching for graduate level data modeling courses (I work as an analytics engineer with mainly dbt and GBQ, hardly any pipelines yet since we use stitch for now).. and it has me pretty interested due to the coursework.&lt;/p&gt;\n\n&lt;p&gt;By contrast and because of the popularity of computer science, I&amp;#39;ve been focusing on completing prerequisites for &lt;a href=\"https://omscs.gatech.edu/specialization-computing-systems\"&gt;Georgia Tech&amp;#39;s OMSCS&lt;/a&gt;, particularly in computing systems, through an online junior/community college.&lt;/p&gt;\n\n&lt;p&gt;For computing systems, I&amp;#39;m planning on learning about operating systems, networks, security, and computer architecture. Most of the prereqs have been in C and or C++, which is tangential to data engineering at best. There&amp;#39;s numerous posts about SQL + Python being enough for most jobs (not considering jobs needing scala/java/BE knowledge). This all seems like a lot of preparation and knowledge acquisition for topics that I won&amp;#39;t directly use in AE/DE. Have also read posts (from here and &lt;a href=\"/r/OMSCS\"&gt;r/OMSCS&lt;/a&gt;) that say a masters isn&amp;#39;t the best way to acquire knowledge needed to do one&amp;#39;s job.&lt;/p&gt;\n\n&lt;p&gt;My impression is that most of DE is OJT and/or learned from personal projects and not really covered in academia at all. That being said, it seems the information science side of things worries about storage, organization, quality, etc. of the data. Isn&amp;#39;t that primarily what we do, beyond moving the data?&lt;/p&gt;\n\n&lt;p&gt;Does it make more sense to stick with computing systems, or should I entertain something like the other masters, or something completely different? My end goal is to essentially stay in analytics or data engineering for the long term and make as much money as possible. No aspirations beyond that right now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z11r7y", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z11r7y/why_isnt_information_science_a_popular_pathway/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z11r7y/why_isnt_information_science_a_popular_pathway/", "subreddit_subscribers": 80601, "created_utc": 1669044586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is any one using acloudguru or Cloudacademy to gain knowledge in Cloud/terraform? Looking to get some feedback on these , checking if we can buy one for this thanksgiving deals?\n\nI see there is 50% discount for Acloudguru personal edition and cloudacademy is offering 35% discount for yearly membership.", "author_fullname": "t2_9zb1pmhi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Acloudguru vs Cloudacademy - for cloud courses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1bnse", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669069443.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669068009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is any one using acloudguru or Cloudacademy to gain knowledge in Cloud/terraform? Looking to get some feedback on these , checking if we can buy one for this thanksgiving deals?&lt;/p&gt;\n\n&lt;p&gt;I see there is 50% discount for Acloudguru personal edition and cloudacademy is offering 35% discount for yearly membership.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z1bnse", "is_robot_indexable": true, "report_reasons": null, "author": "Foreign_Yam3729", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1bnse/acloudguru_vs_cloudacademy_for_cloud_courses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1bnse/acloudguru_vs_cloudacademy_for_cloud_courses/", "subreddit_subscribers": 80601, "created_utc": 1669068009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5tz7z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Dodgy State of Stream Processing Delivery Guarantees", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_z12dql", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QmpBOCvY8mY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"The Dodgy State of Delivery Guarantees\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "The Dodgy State of Delivery Guarantees", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QmpBOCvY8mY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"The Dodgy State of Delivery Guarantees\"&gt;&lt;/iframe&gt;", "author_name": "Jeffail", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/QmpBOCvY8mY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Jeffail"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QmpBOCvY8mY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"The Dodgy State of Delivery Guarantees\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/z12dql", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/YRR3b8_BlHgeTA8P57ZTrl_GN3dmlqmby5c8IFL-yC0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669046090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=QmpBOCvY8mY", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rzwLrNWEOrQIfIOHmANPpQFl9N0oxXqSQD3swEEr2NI.jpg?auto=webp&amp;s=0305706187a2011f35c7c669268ee1e0888ea9ca", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/rzwLrNWEOrQIfIOHmANPpQFl9N0oxXqSQD3swEEr2NI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=92e39c5703310577ac56647bf0b7d49900e1d2e4", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/rzwLrNWEOrQIfIOHmANPpQFl9N0oxXqSQD3swEEr2NI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1031f8aae1c55d2db6e6ba587fd8ba729d31c03a", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/rzwLrNWEOrQIfIOHmANPpQFl9N0oxXqSQD3swEEr2NI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8b710bb5a738c4841e95e5a3507aca5e02a1be7", "width": 320, "height": 240}], "variants": {}, "id": "YBOTc6rsHRbN6AAvcd7IQ1EdOwNs2tixrFQmCC3PtM0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z12dql", "is_robot_indexable": true, "report_reasons": null, "author": "mihaitodor", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z12dql/the_dodgy_state_of_stream_processing_delivery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=QmpBOCvY8mY", "subreddit_subscribers": 80601, "created_utc": 1669046090.0, "num_crossposts": 1, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "The Dodgy State of Delivery Guarantees", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QmpBOCvY8mY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"The Dodgy State of Delivery Guarantees\"&gt;&lt;/iframe&gt;", "author_name": "Jeffail", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/QmpBOCvY8mY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Jeffail"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In university we are learning about NoSQL vs SQL databases. We learned that NoSQL databases sacrifice a degree of consistency in exchange for availability; guaranteeing eventual consistency instead.\n\nMy question - I struggle to see how this would ever be a good idea. When a service has millions of users and has to deal with a lot of queries / computation per (small) unit of time, I'm inclined to think that results propagating over an outdated version of a database of some sorts would be quite bad. It doesn't help if I have immediately available data that's... inaccurate right? Or is the delta between the actual/outdated values small enough to be negligible? Are there situations / use cases where one would want to absolutely guarantee consistency over availability?", "author_fullname": "t2_1j8v22cd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about prioritizing availability over consistency", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z14zoy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669052541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In university we are learning about NoSQL vs SQL databases. We learned that NoSQL databases sacrifice a degree of consistency in exchange for availability; guaranteeing eventual consistency instead.&lt;/p&gt;\n\n&lt;p&gt;My question - I struggle to see how this would ever be a good idea. When a service has millions of users and has to deal with a lot of queries / computation per (small) unit of time, I&amp;#39;m inclined to think that results propagating over an outdated version of a database of some sorts would be quite bad. It doesn&amp;#39;t help if I have immediately available data that&amp;#39;s... inaccurate right? Or is the delta between the actual/outdated values small enough to be negligible? Are there situations / use cases where one would want to absolutely guarantee consistency over availability?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z14zoy", "is_robot_indexable": true, "report_reasons": null, "author": "stuffingmybrain", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z14zoy/question_about_prioritizing_availability_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z14zoy/question_about_prioritizing_availability_over/", "subreddit_subscribers": 80601, "created_utc": 1669052541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you there is a use case for it? From a DE standpoint to implement ETL?\n\nRight now Im on a project which the biggest table is 10M and we are using Databricks for a simple ETL process to load a dimensional model.\n\nLifting a cluster to deal with this volume sounds silly. This could be easily achieved with raw python or a DB.", "author_fullname": "t2_3174m8nl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks for small datasets? Worth it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z11ew5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669043711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you there is a use case for it? From a DE standpoint to implement ETL?&lt;/p&gt;\n\n&lt;p&gt;Right now Im on a project which the biggest table is 10M and we are using Databricks for a simple ETL process to load a dimensional model.&lt;/p&gt;\n\n&lt;p&gt;Lifting a cluster to deal with this volume sounds silly. This could be easily achieved with raw python or a DB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z11ew5", "is_robot_indexable": true, "report_reasons": null, "author": "PaleBass", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z11ew5/databricks_for_small_datasets_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z11ew5/databricks_for_small_datasets_worth_it/", "subreddit_subscribers": 80601, "created_utc": 1669043711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working with a large undocumented Oracle database that has over 1900 tables and 900 views. I know approximately which are the main tables and how the system works, but I'm wondering if there are any visualization or other type of tools that you use in your daily life to navigate such large complex databases and do exploratory work (see relationships between tables, column comments where such exist, etc.).\n\nPerhaps you have any other tips and tricks on what to do when approaching such a problem?\n\nThe goal is to extract some recurring data once a day, but first I have to unfortunately find it.", "author_fullname": "t2_elso2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tools that help to explore and navigate database tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z10fxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669041207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working with a large undocumented Oracle database that has over 1900 tables and 900 views. I know approximately which are the main tables and how the system works, but I&amp;#39;m wondering if there are any visualization or other type of tools that you use in your daily life to navigate such large complex databases and do exploratory work (see relationships between tables, column comments where such exist, etc.).&lt;/p&gt;\n\n&lt;p&gt;Perhaps you have any other tips and tricks on what to do when approaching such a problem?&lt;/p&gt;\n\n&lt;p&gt;The goal is to extract some recurring data once a day, but first I have to unfortunately find it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z10fxk", "is_robot_indexable": true, "report_reasons": null, "author": "Kardinals", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z10fxk/tools_that_help_to_explore_and_navigate_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z10fxk/tools_that_help_to_explore_and_navigate_database/", "subreddit_subscribers": 80601, "created_utc": 1669041207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Simple question, curious about experiences and how much of an operational pain it is / isn't. I intend to experiment with it when I get some time but, in the meantime, I thought I would ask the hivemind.", "author_fullname": "t2_dxegl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone using Prefect Orion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0zfhb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669038505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Simple question, curious about experiences and how much of an operational pain it is / isn&amp;#39;t. I intend to experiment with it when I get some time but, in the meantime, I thought I would ask the hivemind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z0zfhb", "is_robot_indexable": true, "report_reasons": null, "author": "nutso_muzz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0zfhb/anyone_using_prefect_orion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0zfhb/anyone_using_prefect_orion/", "subreddit_subscribers": 80601, "created_utc": 1669038505.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy,\n\nConsidering the architecture for implementing something to ingest a stream from Twitter API, I'd like it to be distributable.\n\nSo far I've considered:\n\nCreating ingestion via Python (tweepy probably), containerised and deployed into K8s with scalable replicas. \n\nSeparate pod for Postgres DB for data persistence. \n\n Without overcomplicating it is there an easier way to avoid dealing with duplicates from the stream? (I thought about maybe using Kafka or some message bus)", "author_fullname": "t2_11ua04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parallelised twitter stream - how to avoid duplicates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0uiky", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669022523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy,&lt;/p&gt;\n\n&lt;p&gt;Considering the architecture for implementing something to ingest a stream from Twitter API, I&amp;#39;d like it to be distributable.&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve considered:&lt;/p&gt;\n\n&lt;p&gt;Creating ingestion via Python (tweepy probably), containerised and deployed into K8s with scalable replicas. &lt;/p&gt;\n\n&lt;p&gt;Separate pod for Postgres DB for data persistence. &lt;/p&gt;\n\n&lt;p&gt;Without overcomplicating it is there an easier way to avoid dealing with duplicates from the stream? (I thought about maybe using Kafka or some message bus)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z0uiky", "is_robot_indexable": true, "report_reasons": null, "author": "7007001", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0uiky/parallelised_twitter_stream_how_to_avoid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0uiky/parallelised_twitter_stream_how_to_avoid/", "subreddit_subscribers": 80601, "created_utc": 1669022523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, I am just starting on my way to learn DE. I have CS and engineering degree and I am now working as a BI engineer and would like someone to give me some mentoring and tell me if I am going on the right path.", "author_fullname": "t2_ozdnflqp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice and mentoring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0u41u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669021058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, I am just starting on my way to learn DE. I have CS and engineering degree and I am now working as a BI engineer and would like someone to give me some mentoring and tell me if I am going on the right path.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z0u41u", "is_robot_indexable": true, "report_reasons": null, "author": "Various_Bandicoot977", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0u41u/advice_and_mentoring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0u41u/advice_and_mentoring/", "subreddit_subscribers": 80601, "created_utc": 1669021058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to know if I can use apache flume to stream postgres log to a pyspark program. If yes can you tell me how, thanks", "author_fullname": "t2_cb7rpz4k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use apache flume to stream postgres log file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0sx8t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669016624.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to know if I can use apache flume to stream postgres log to a pyspark program. If yes can you tell me how, thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z0sx8t", "is_robot_indexable": true, "report_reasons": null, "author": "Mr_marshmellow19", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0sx8t/use_apache_flume_to_stream_postgres_log_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0sx8t/use_apache_flume_to_stream_postgres_log_file/", "subreddit_subscribers": 80601, "created_utc": 1669016624.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm not from a CSE background. So a lot of stuff for me is self taught. I understand at a high level the concept of RAM and Disk. I'd like to understand Arrow but that seems to be one layer deeper. \n\nWhat are some resources where I can learn more about concepts like:\n- Memory mapping\n- Serialization/Deserialization  \n- Buffers\n- Processes", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources to learn and understand Arrow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z1ictz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669085481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not from a CSE background. So a lot of stuff for me is self taught. I understand at a high level the concept of RAM and Disk. I&amp;#39;d like to understand Arrow but that seems to be one layer deeper. &lt;/p&gt;\n\n&lt;p&gt;What are some resources where I can learn more about concepts like:\n- Memory mapping\n- Serialization/Deserialization&lt;br/&gt;\n- Buffers\n- Processes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z1ictz", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1ictz/resources_to_learn_and_understand_arrow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1ictz/resources_to_learn_and_understand_arrow/", "subreddit_subscribers": 80601, "created_utc": 1669085481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Good day,  \n\n\nI had a question regarding daily information (SDC): We are looking to create a table containing the daily team of our employees.  \nThis is then used when manipulating timelogs in our project manager (if employee was in this department on that day, count timelogs).  \n\n\nMy issue is: This means I am artificially generating **n rows** everyday, **n** being the number of employees.  \n\n\nTherefore, my question: What is the best way to take into account daily parameters without generating too many useless entries", "author_fullname": "t2_kinpz3uc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modeling best practices - HR daily department", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z18s7g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669061313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good day,  &lt;/p&gt;\n\n&lt;p&gt;I had a question regarding daily information (SDC): We are looking to create a table containing the daily team of our employees.&lt;br/&gt;\nThis is then used when manipulating timelogs in our project manager (if employee was in this department on that day, count timelogs).  &lt;/p&gt;\n\n&lt;p&gt;My issue is: This means I am artificially generating &lt;strong&gt;n rows&lt;/strong&gt; everyday, &lt;strong&gt;n&lt;/strong&gt; being the number of employees.  &lt;/p&gt;\n\n&lt;p&gt;Therefore, my question: What is the best way to take into account daily parameters without generating too many useless entries&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z18s7g", "is_robot_indexable": true, "report_reasons": null, "author": "Brewash_Beer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z18s7g/data_modeling_best_practices_hr_daily_department/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z18s7g/data_modeling_best_practices_hr_daily_department/", "subreddit_subscribers": 80601, "created_utc": 1669061313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's say a service is supposed to get data from BigQuery tables and then return it to the caller. \n\nThe response is dynamic in that the caller can specify the columns as well as ordering, grouping, and filtering clauses.\n\nThe source tables in BigQuery are updating based on some cron-like setting so it could be anything from hours to days. \n\n    SELECT col1, col2 FROM (SELECT col1, col2, col3, RANK() OVER(PARTITION BY col1, col2 ORDER BY col3 DESC) AS rank) WHERE rank = 1\n\nIn the query above col3 is a timestamp-based column so the idea is to make sure that I only select rows with the most recent values of col3. What makes it more tricky though is the fact that the set of columns in such queries is not static (whereas col3 is always there). \n\nIf you have dealt with the same requirement, what was your approach? I was also thinking about ingestion-based table partitioning (hour-based) so that I could benefit from the \\_PARTITIONTIME meta field but even in this case it's not guaranteed that the services that gather the data do not export several times (for some reason) within one hour.", "author_fullname": "t2_zqr9y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deduplicating data from BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z164nf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669055194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say a service is supposed to get data from BigQuery tables and then return it to the caller. &lt;/p&gt;\n\n&lt;p&gt;The response is dynamic in that the caller can specify the columns as well as ordering, grouping, and filtering clauses.&lt;/p&gt;\n\n&lt;p&gt;The source tables in BigQuery are updating based on some cron-like setting so it could be anything from hours to days. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT col1, col2 FROM (SELECT col1, col2, col3, RANK() OVER(PARTITION BY col1, col2 ORDER BY col3 DESC) AS rank) WHERE rank = 1\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;In the query above col3 is a timestamp-based column so the idea is to make sure that I only select rows with the most recent values of col3. What makes it more tricky though is the fact that the set of columns in such queries is not static (whereas col3 is always there). &lt;/p&gt;\n\n&lt;p&gt;If you have dealt with the same requirement, what was your approach? I was also thinking about ingestion-based table partitioning (hour-based) so that I could benefit from the _PARTITIONTIME meta field but even in this case it&amp;#39;s not guaranteed that the services that gather the data do not export several times (for some reason) within one hour.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z164nf", "is_robot_indexable": true, "report_reasons": null, "author": "dondraper36", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z164nf/deduplicating_data_from_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z164nf/deduplicating_data_from_bigquery/", "subreddit_subscribers": 80601, "created_utc": 1669055194.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m using FastAPI for Machine Learning prediction model deployment (classification case), In fact, I have 2 type of models that works the following way:\n\n1. if model got all features (**primary and secondary**) =&gt; apply **model\\_type1**\n2. if model got only **primary features** (missing secondary) =&gt; apply **model\\_type2**\n3. if model got missing primary features =&gt; display an error (invalid for prediction, we can't use any of the two models due to insufficient data)\n\nPS. model\\_type1 has features of model\\_type2 (called primary) + other features (called secondary), while model\\_type2 must have only primary features, if none of them is an option an error should be displayed.\n\nTo achieve this, I have the inputs of type \"array of JSON\" in the form below:\n\n    [ { \n    \"Id\":\"value\",\n    \"feature_primary1\":\"value\", \n    \"feature_primary2\":\"value\", \n    \"feature_primary3\":\"value\", \n    \"feature_secondary1\":\"value\", \n    \"feature_secondary2\":\"value\" \n    }, \n    { \n    \"Id\":\"value\", \n    \"feature_primary1\":\"value\", \n    \"feature_primary2\":\"value\", \n    \"feature_primary3\":\"value\", \n    \"feature_secondary1\":\"value\", \n    \"feature_secondary2\":\"value\" \n    }, \n    { \n    \"Id\":\"value\", \n    \"feature_primary1\":\"value\", \n    \"feature_primary2\":\"value\", \n    \"feature_primary3\":\"value\", \n    \"feature_secondary1\":\"value\", \n    \"feature_secondary2\":\"value\" \n    } \n    ... \n    ] \n\nAnd I need to filter the inputs before feeding them to the predictive model. Meaning, to filter each JSON  \n of the array.\n\nSo, I want to create 3 lists where:\n\n1. if inputs match **model\\_type1** append to list1\n2. if inputs doesn't match **model\\_type1** and match **model\\_type2** append to list2\n3. if inputs doesn't match **model\\_type1** nor **model\\_type2** append error detail to list3\n\nAnd then assign each list to its proper model and display the prediction results (Outputs). So if we send the following inputs:\n\n    [ \n    { \n    \"Id\": 1,\n    \"feature_primary1\": \"David\", \n    \"feature_primary2\": 15670.87, \n    \"feature_primary3\": \"Male\", \n    \"feature_secondary1\": \"Yes\", \n    \"feature_secondary2\": 45 \n    }, \n    { \n    \"Id\": 2, \n    \"feature_primary1\": \"Alice\", \n    \"feature_primary2\": 78995.65, \n    \"feature_primary3\": \"Female\", \n    \"feature_secondary1\": NaN, \n    \"feature_secondary2\": NaN   \n    }, \n    { \n    \"Id\": 3, \n    \"feature_primary1\": \"John\", \n    \"feature_primary2\": NaN, \n    \"feature_primary3\": NaN, \n    \"feature_secondary1\": NaN, \n    \"feature_secondary2\": 79 \n    } \n    ... \n    ] \n\nThe output (result of prediction) should look like this :\n\n    [ \n    { \n    \"Id\": 1, \n    \"type_model\": \"model_type1\", \n    \"prediction\": \"class1\" \n    }, \n    { \n    \"Id\": 2, \n    \"type_model\": \"model_type2\", \n    \"prediction\": \"class2\" \n    }, \n    { \"Id\": 3, \n    \"error_description\": \"missing values for feature_primary2,                          feature_primary3 and feature_secondary1\" \n    } \n    ... \n    ] \n\nHow can I achieve the filter process within the 3 lists, in an optimized way with FastAPI in Python?", "author_fullname": "t2_baqjslqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Filter inputs of FastAPI and assign each type to a specific list", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z1312n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669047692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m using FastAPI for Machine Learning prediction model deployment (classification case), In fact, I have 2 type of models that works the following way:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;if model got all features (&lt;strong&gt;primary and secondary&lt;/strong&gt;) =&amp;gt; apply &lt;strong&gt;model_type1&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;if model got only &lt;strong&gt;primary features&lt;/strong&gt; (missing secondary) =&amp;gt; apply &lt;strong&gt;model_type2&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;if model got missing primary features =&amp;gt; display an error (invalid for prediction, we can&amp;#39;t use any of the two models due to insufficient data)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;PS. model_type1 has features of model_type2 (called primary) + other features (called secondary), while model_type2 must have only primary features, if none of them is an option an error should be displayed.&lt;/p&gt;\n\n&lt;p&gt;To achieve this, I have the inputs of type &amp;quot;array of JSON&amp;quot; in the form below:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[ { \n&amp;quot;Id&amp;quot;:&amp;quot;value&amp;quot;,\n&amp;quot;feature_primary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary3&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary2&amp;quot;:&amp;quot;value&amp;quot; \n}, \n{ \n&amp;quot;Id&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary3&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary2&amp;quot;:&amp;quot;value&amp;quot; \n}, \n{ \n&amp;quot;Id&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary3&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary2&amp;quot;:&amp;quot;value&amp;quot; \n} \n... \n] \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And I need to filter the inputs before feeding them to the predictive model. Meaning, to filter each JSON&lt;br/&gt;\n of the array.&lt;/p&gt;\n\n&lt;p&gt;So, I want to create 3 lists where:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;if inputs match &lt;strong&gt;model_type1&lt;/strong&gt; append to list1&lt;/li&gt;\n&lt;li&gt;if inputs doesn&amp;#39;t match &lt;strong&gt;model_type1&lt;/strong&gt; and match &lt;strong&gt;model_type2&lt;/strong&gt; append to list2&lt;/li&gt;\n&lt;li&gt;if inputs doesn&amp;#39;t match &lt;strong&gt;model_type1&lt;/strong&gt; nor &lt;strong&gt;model_type2&lt;/strong&gt; append error detail to list3&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;And then assign each list to its proper model and display the prediction results (Outputs). So if we send the following inputs:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[ \n{ \n&amp;quot;Id&amp;quot;: 1,\n&amp;quot;feature_primary1&amp;quot;: &amp;quot;David&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;: 15670.87, \n&amp;quot;feature_primary3&amp;quot;: &amp;quot;Male&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;: &amp;quot;Yes&amp;quot;, \n&amp;quot;feature_secondary2&amp;quot;: 45 \n}, \n{ \n&amp;quot;Id&amp;quot;: 2, \n&amp;quot;feature_primary1&amp;quot;: &amp;quot;Alice&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;: 78995.65, \n&amp;quot;feature_primary3&amp;quot;: &amp;quot;Female&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;: NaN, \n&amp;quot;feature_secondary2&amp;quot;: NaN   \n}, \n{ \n&amp;quot;Id&amp;quot;: 3, \n&amp;quot;feature_primary1&amp;quot;: &amp;quot;John&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;: NaN, \n&amp;quot;feature_primary3&amp;quot;: NaN, \n&amp;quot;feature_secondary1&amp;quot;: NaN, \n&amp;quot;feature_secondary2&amp;quot;: 79 \n} \n... \n] \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The output (result of prediction) should look like this :&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[ \n{ \n&amp;quot;Id&amp;quot;: 1, \n&amp;quot;type_model&amp;quot;: &amp;quot;model_type1&amp;quot;, \n&amp;quot;prediction&amp;quot;: &amp;quot;class1&amp;quot; \n}, \n{ \n&amp;quot;Id&amp;quot;: 2, \n&amp;quot;type_model&amp;quot;: &amp;quot;model_type2&amp;quot;, \n&amp;quot;prediction&amp;quot;: &amp;quot;class2&amp;quot; \n}, \n{ &amp;quot;Id&amp;quot;: 3, \n&amp;quot;error_description&amp;quot;: &amp;quot;missing values for feature_primary2,                          feature_primary3 and feature_secondary1&amp;quot; \n} \n... \n] \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;How can I achieve the filter process within the 3 lists, in an optimized way with FastAPI in Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z1312n", "is_robot_indexable": true, "report_reasons": null, "author": "According-Promise-23", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1312n/filter_inputs_of_fastapi_and_assign_each_type_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1312n/filter_inputs_of_fastapi_and_assign_each_type_to/", "subreddit_subscribers": 80601, "created_utc": 1669047692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI'm on my way to creating a new pipeline on our Azure stack that is supposed to read Excel sheets located at Google Drive partitions.\n\nWe're using Data Factory to read those files, but they don't have a connection directly with Google Drive.\n\nSo, I'm assuming I have basically two options\n\n1. Transfer all those files to SharePoint manually or ask clients to use it instead of Gdrive (not nice)\n2. Create an automation or pipeline at GCP that extract files from GDrive and sends to Google Cloud Storage (data factory has built-in connection with it)\n\nHow would you guys handle this scenario? There are more options available?\n\nThanks in advance!\n\nRegards,\n\nDouglas.", "author_fullname": "t2_30mkkbap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory reading flat files from Google Drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0yqpc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669036536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on my way to creating a new pipeline on our Azure stack that is supposed to read Excel sheets located at Google Drive partitions.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re using Data Factory to read those files, but they don&amp;#39;t have a connection directly with Google Drive.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m assuming I have basically two options&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Transfer all those files to SharePoint manually or ask clients to use it instead of Gdrive (not nice)&lt;/li&gt;\n&lt;li&gt;Create an automation or pipeline at GCP that extract files from GDrive and sends to Google Cloud Storage (data factory has built-in connection with it)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;How would you guys handle this scenario? There are more options available?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;Regards,&lt;/p&gt;\n\n&lt;p&gt;Douglas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z0yqpc", "is_robot_indexable": true, "report_reasons": null, "author": "ddddddkkk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0yqpc/azure_data_factory_reading_flat_files_from_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0yqpc/azure_data_factory_reading_flat_files_from_google/", "subreddit_subscribers": 80601, "created_utc": 1669036536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://medium.com/dev-genius/data-processing-and-analysis-using-spark-spark-project-1-3f52516272a7](https://medium.com/dev-genius/data-processing-and-analysis-using-spark-spark-project-1-3f52516272a7)", "author_fullname": "t2_7oampu1h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Processing and Analysis using Spark | Spark Project-1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0wzng", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669031188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/dev-genius/data-processing-and-analysis-using-spark-spark-project-1-3f52516272a7\"&gt;https://medium.com/dev-genius/data-processing-and-analysis-using-spark-spark-project-1-3f52516272a7&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?auto=webp&amp;s=20f5c598da0f726c0dc984526647eae6fe4c97f2", "width": 1200, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8f4cbfbb3100edaa4d8de9fbbfd2c55e88c4ddd", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac3e01858cd638a208b4ba634dcbf44b20f34e0c", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e13905cadd8cdc93192b539f75027342f4fec511", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea93eebc2223084e23b32d12900b165ba5a28bbc", "width": 640, "height": 346}, {"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e4018604129d965e0fb3564c52cce3b4b60273c3", "width": 960, "height": 520}, {"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1141bfabf3a6010c851f306ca5d51776b9a03293", "width": 1080, "height": 585}], "variants": {}, "id": "G745Y6CwlB7JbdlpVSGhxXc4G8tAb-IPbzGzjIM7fiY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z0wzng", "is_robot_indexable": true, "report_reasons": null, "author": "Sidharth_r", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/z0wzng/data_processing_and_analysis_using_spark_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0wzng/data_processing_and_analysis_using_spark_spark/", "subreddit_subscribers": 80601, "created_utc": 1669031188.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}