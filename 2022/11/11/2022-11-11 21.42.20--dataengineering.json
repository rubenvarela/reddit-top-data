{"kind": "Listing", "data": {"after": null, "dist": 20, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The incredible [Syl Taylor](https://github.com/syl-taylor-aws) just released two new workshops related to [AWS Graviton](https://aws.amazon.com/ec2/graviton/) (the processors developed by the company with the best price/performance combination) and Machine Learning:\n\n* [An example solution for running ML workloads on AWS Graviton using AWS Nitro Enclaves](https://github.com/aws-samples/aws-graviton-run-confidential-ml-workloads-using-nitro-enclaves):\n\n&gt;Customers from diverse industries collaborate with other parties to exchange sensitive information, such as code and data. For artificial intelligence (AI), machine learning (ML), and data science (DS) practitioners, the ability to experiment with externally-provided algorithms, models, and datasets is key to improving business outcomes.  \nWe will demonstrate how you can share your sensitive AI/ML files in a manner that safeguards application and data confidentiality. To present you with a familiar environment, we included the ability to do seamless data transfers to accelerate ML and DS workloads, as well as run software downloaded at runtime to process that data conveniently.  \n[AWS Nitro Enclaves](https://aws.amazon.com/ec2/nitro/nitro-enclaves/) enables customers to create isolated compute environments to maintain the confidentiality of applications and data. The sample provided uses Nitro enclaves to enable sensitive file sharing and usage for ML workloads.\n\n* [How to run ML inference on EC2 (Graviton | arm64) using Apache TVM (TVMC)](https://github.com/aws-samples/aws-graviton-ml-inference-apache-tvm-example):\n\n&gt;This sample provides steps to deploy Apache TVM (TVMC Python) on a Graviton (arm64) EC2 instance to do ML inference using a ResNet50 model (ONNX).\n\nIf you want to get the last news/ use cases/articles/videos and more related to AWS Graviton, feel free to subscribe to our [weekly newsletter here](https://awsgravitonweekly.com/newsletter).", "author_fullname": "t2_ugjew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2 new workshops related to AWS Graviton and Machine Learning from the AWS team itself", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys558u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668155973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The incredible &lt;a href=\"https://github.com/syl-taylor-aws\"&gt;Syl Taylor&lt;/a&gt; just released two new workshops related to &lt;a href=\"https://aws.amazon.com/ec2/graviton/\"&gt;AWS Graviton&lt;/a&gt; (the processors developed by the company with the best price/performance combination) and Machine Learning:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/aws-samples/aws-graviton-run-confidential-ml-workloads-using-nitro-enclaves\"&gt;An example solution for running ML workloads on AWS Graviton using AWS Nitro Enclaves&lt;/a&gt;:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Customers from diverse industries collaborate with other parties to exchange sensitive information, such as code and data. For artificial intelligence (AI), machine learning (ML), and data science (DS) practitioners, the ability to experiment with externally-provided algorithms, models, and datasets is key to improving business outcomes.&lt;br/&gt;\nWe will demonstrate how you can share your sensitive AI/ML files in a manner that safeguards application and data confidentiality. To present you with a familiar environment, we included the ability to do seamless data transfers to accelerate ML and DS workloads, as well as run software downloaded at runtime to process that data conveniently.&lt;br/&gt;\n&lt;a href=\"https://aws.amazon.com/ec2/nitro/nitro-enclaves/\"&gt;AWS Nitro Enclaves&lt;/a&gt; enables customers to create isolated compute environments to maintain the confidentiality of applications and data. The sample provided uses Nitro enclaves to enable sensitive file sharing and usage for ML workloads.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/aws-samples/aws-graviton-ml-inference-apache-tvm-example\"&gt;How to run ML inference on EC2 (Graviton | arm64) using Apache TVM (TVMC)&lt;/a&gt;:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;This sample provides steps to deploy Apache TVM (TVMC Python) on a Graviton (arm64) EC2 instance to do ML inference using a ResNet50 model (ONNX).&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;If you want to get the last news/ use cases/articles/videos and more related to AWS Graviton, feel free to subscribe to our &lt;a href=\"https://awsgravitonweekly.com/newsletter\"&gt;weekly newsletter here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/I0L4Wu97BZiegQJjJZmMRpS7maypMXUt-AfBoH52WY8.jpg?auto=webp&amp;s=600910a80f5c15459a15412eba8f709a88dbaa49", "width": 420, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/I0L4Wu97BZiegQJjJZmMRpS7maypMXUt-AfBoH52WY8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=339bb2aeaa127d1b31e2f48ec359d3972aa6d2e3", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/I0L4Wu97BZiegQJjJZmMRpS7maypMXUt-AfBoH52WY8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=966a5e2fda08db3d7d995e50e145b78c80f5db27", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/I0L4Wu97BZiegQJjJZmMRpS7maypMXUt-AfBoH52WY8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=032f634d8a58e313073dfca21e7c54113f9009ad", "width": 320, "height": 320}], "variants": {}, "id": "9QiuiOa2bSL0giWPjv6q515sBbK6pyY5pmXWvryl38s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "ys558u", "is_robot_indexable": true, "report_reasons": null, "author": "marcosluis2186", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ys558u/2_new_workshops_related_to_aws_graviton_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ys558u/2_new_workshops_related_to_aws_graviton_and/", "subreddit_subscribers": 79650, "created_utc": 1668155973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Question regarding union. I know how it works but i am struggling with coming up with a practical use of it. Also it seems you can get the same results with just doing joins. \n\nWhen would you do a union function on a data set instead of a join(s).", "author_fullname": "t2_iafvx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Union / Union All operator", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysdmch", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668180497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Question regarding union. I know how it works but i am struggling with coming up with a practical use of it. Also it seems you can get the same results with just doing joins. &lt;/p&gt;\n\n&lt;p&gt;When would you do a union function on a data set instead of a join(s).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ysdmch", "is_robot_indexable": true, "report_reasons": null, "author": "duckvirus", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysdmch/union_union_all_operator/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysdmch/union_union_all_operator/", "subreddit_subscribers": 79650, "created_utc": 1668180497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just wondering if anyone have some good resources on how to choose cluster size for spark jobs. \n\nRecently i am faced with a task to extract data from a hive table than run some very simple processes on it before storing it in blob storage. These are huge tables with 30-40 Mil rows per year \n\ni am not sure how to pick my spark cluster size. Like # of executors, core and memory for thr executor and drivers.\n\nAnyone has some tips or resources on that. \n\nLike given the size of the data and thr partitions . How big should my cluster be?", "author_fullname": "t2_c3yqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Cluster Sizing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysgftu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668186442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wondering if anyone have some good resources on how to choose cluster size for spark jobs. &lt;/p&gt;\n\n&lt;p&gt;Recently i am faced with a task to extract data from a hive table than run some very simple processes on it before storing it in blob storage. These are huge tables with 30-40 Mil rows per year &lt;/p&gt;\n\n&lt;p&gt;i am not sure how to pick my spark cluster size. Like # of executors, core and memory for thr executor and drivers.&lt;/p&gt;\n\n&lt;p&gt;Anyone has some tips or resources on that. &lt;/p&gt;\n\n&lt;p&gt;Like given the size of the data and thr partitions . How big should my cluster be?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ysgftu", "is_robot_indexable": true, "report_reasons": null, "author": "543254447", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysgftu/spark_cluster_sizing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysgftu/spark_cluster_sizing/", "subreddit_subscribers": 79650, "created_utc": 1668186442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm ingesting JSON data and as part of the my data pipeline, I'm hoping to clean the data to match a schema that can only be known at runtime.\n\nWhich tools should I be looking into? So far from my research, I've found that it's common to use schemas for validating data, but not so much for cleaning it. What I mean by cleaning is:\n\n1. Drop additional fields that are not preset in the input\n2. Set missing fields to null\n3. Perform basic type conversion, e.g. int -&gt; string, or string -&gt; date (using customizable logic)\n\nI'm still also working on the schema layer - currently writing them in GraphQL but considering moving to TypeScript or any other format/language, since it's fairly easy to write converters between formats these days, I should be able to transform the authored format into whatever is required by the tooling for the above 3 features\n\nCheers", "author_fullname": "t2_vep5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cleaning data using a schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrxwf9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668130889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m ingesting JSON data and as part of the my data pipeline, I&amp;#39;m hoping to clean the data to match a schema that can only be known at runtime.&lt;/p&gt;\n\n&lt;p&gt;Which tools should I be looking into? So far from my research, I&amp;#39;ve found that it&amp;#39;s common to use schemas for validating data, but not so much for cleaning it. What I mean by cleaning is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Drop additional fields that are not preset in the input&lt;/li&gt;\n&lt;li&gt;Set missing fields to null&lt;/li&gt;\n&lt;li&gt;Perform basic type conversion, e.g. int -&amp;gt; string, or string -&amp;gt; date (using customizable logic)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m still also working on the schema layer - currently writing them in GraphQL but considering moving to TypeScript or any other format/language, since it&amp;#39;s fairly easy to write converters between formats these days, I should be able to transform the authored format into whatever is required by the tooling for the above 3 features&lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrxwf9", "is_robot_indexable": true, "report_reasons": null, "author": "matty_fu", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrxwf9/cleaning_data_using_a_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrxwf9/cleaning_data_using_a_schema/", "subreddit_subscribers": 79650, "created_utc": 1668130889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I have a use case with an architecture \"recommended\" by my client where they want to analyze different system logs with Log Analytics and then save the results of these queries and later be imported in PowerBI.\n\nThe use-case the client is thinking of is saving/uploading any event logs from any system to Log Analytics and query those logs.\n\nI have close to 0 experience with Azure Log Analytics and from the Udemy courses I've seen so far and the documentation I've read, the default way of working is to use either an agent to upload logs to a storage account or to the Log Analytics workspace directly and then query them there.\n\nBut the documentation shows only a predetermined set of types of logs, like IIS logs, event logs (configured to be saved from Application Service directly to a storage account), but no \"custom logs\".\n\nFrom what I've read about custom logs, it seems that there's no way to plug-and-play with these, you still need an agent that captures the logs and a schema provided that can process the logs and save them to Log Analytics workspace. So from what I see, custom logs are **always linked to an agent**. But in my use-case an agent wouldn't exist, because logs are copied on-demand.\n\nIn this case is Log Analytics a good tool to use? Or should I pushback on my client to use an agent on his devices (if the security team allows it) to send log data to Log Analytics?\n\nOr, is there a better technology to use to analyze different types of log text information?", "author_fullname": "t2_pnduh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some advice regarding Azure Log Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys60ca", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668159589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have a use case with an architecture &amp;quot;recommended&amp;quot; by my client where they want to analyze different system logs with Log Analytics and then save the results of these queries and later be imported in PowerBI.&lt;/p&gt;\n\n&lt;p&gt;The use-case the client is thinking of is saving/uploading any event logs from any system to Log Analytics and query those logs.&lt;/p&gt;\n\n&lt;p&gt;I have close to 0 experience with Azure Log Analytics and from the Udemy courses I&amp;#39;ve seen so far and the documentation I&amp;#39;ve read, the default way of working is to use either an agent to upload logs to a storage account or to the Log Analytics workspace directly and then query them there.&lt;/p&gt;\n\n&lt;p&gt;But the documentation shows only a predetermined set of types of logs, like IIS logs, event logs (configured to be saved from Application Service directly to a storage account), but no &amp;quot;custom logs&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;From what I&amp;#39;ve read about custom logs, it seems that there&amp;#39;s no way to plug-and-play with these, you still need an agent that captures the logs and a schema provided that can process the logs and save them to Log Analytics workspace. So from what I see, custom logs are &lt;strong&gt;always linked to an agent&lt;/strong&gt;. But in my use-case an agent wouldn&amp;#39;t exist, because logs are copied on-demand.&lt;/p&gt;\n\n&lt;p&gt;In this case is Log Analytics a good tool to use? Or should I pushback on my client to use an agent on his devices (if the security team allows it) to send log data to Log Analytics?&lt;/p&gt;\n\n&lt;p&gt;Or, is there a better technology to use to analyze different types of log text information?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ys60ca", "is_robot_indexable": true, "report_reasons": null, "author": "raduqq", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ys60ca/need_some_advice_regarding_azure_log_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ys60ca/need_some_advice_regarding_azure_log_analytics/", "subreddit_subscribers": 79650, "created_utc": 1668159589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone know which companies don\u2019t ask leetcode?", "author_fullname": "t2_ecofamsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a job but tired of doing leetcode.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrs7h8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668115391.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone know which companies don\u2019t ask leetcode?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrs7h8", "is_robot_indexable": true, "report_reasons": null, "author": "Flat_Selection1105", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrs7h8/looking_for_a_job_but_tired_of_doing_leetcode/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrs7h8/looking_for_a_job_but_tired_of_doing_leetcode/", "subreddit_subscribers": 79650, "created_utc": 1668115391.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Based on what should i choose my number of repartitions for example when writing dataframe to cluster, most of the internet says 4 times number of cores, but noone says why?What are the most important criteria here?Spark(pyspark)", "author_fullname": "t2_sx1wry60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "repartitions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysij46", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668191061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based on what should i choose my number of repartitions for example when writing dataframe to cluster, most of the internet says 4 times number of cores, but noone says why?What are the most important criteria here?Spark(pyspark)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ysij46", "is_robot_indexable": true, "report_reasons": null, "author": "AcceptableProcess772", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysij46/repartitions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysij46/repartitions/", "subreddit_subscribers": 79650, "created_utc": 1668191061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone. I have been a data engineer for 4-5 years. I am curious if I should be looking at a different career path/title based on my experience. \n\nI am in a situation where every job I've had, has not used OOP, meaning that I have no python/scala experience in a production environment.\n\nMy first job was creating complex ad hoc analytical sql queries and and also developing etl pipelienes in talend/kettle (which is an older stack -- and tbh, it's not a stack I would work on again) This position gave me a bit of DE and DA background.\n\nMy second job was primarily using a data integration management tool called timextender, where I was building a data warehouse and migrating data from on prem SAP to the cloud.  I used python here a little, but only for analysis using numpy/pandas. I actually have a decent background with python as a data analysis tool.\n\nFinally, my current position is 100% building integrations in Azure -- my title is Data Engineer. The only programming I do here is functional programming for infrastructure to code--so again, no OOP.\n\nI have been talking to recruiters recently, and my lack of production level experience with OOP disqualifies me from most DE roles. I would love for my next position to have some python I involved so I can learn, but I know that I am not a good fit for a DE role that is primarily using python/scala for integrations, especially at point of career and salary expectations I am at now. \n\nMy question is where I go from here? I'm also not super enthusiastic about programming a side project outside of work at this point in my life. I think I could pivot into cloud integration role, but is this even considered a DE position anymore? Based on my job searches, it seems like DE is doesn't encompass the stack I'm using right now, which is 100% cloud native.  \n\nI could for sure learn python on my own and pass leetcode questions, but that is not the same as having the experience as developing in python in a job. Anyhow, cheers everyone, thanks for reading.", "author_fullname": "t2_47vwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer or something else?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys66qn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668160323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. I have been a data engineer for 4-5 years. I am curious if I should be looking at a different career path/title based on my experience. &lt;/p&gt;\n\n&lt;p&gt;I am in a situation where every job I&amp;#39;ve had, has not used OOP, meaning that I have no python/scala experience in a production environment.&lt;/p&gt;\n\n&lt;p&gt;My first job was creating complex ad hoc analytical sql queries and and also developing etl pipelienes in talend/kettle (which is an older stack -- and tbh, it&amp;#39;s not a stack I would work on again) This position gave me a bit of DE and DA background.&lt;/p&gt;\n\n&lt;p&gt;My second job was primarily using a data integration management tool called timextender, where I was building a data warehouse and migrating data from on prem SAP to the cloud.  I used python here a little, but only for analysis using numpy/pandas. I actually have a decent background with python as a data analysis tool.&lt;/p&gt;\n\n&lt;p&gt;Finally, my current position is 100% building integrations in Azure -- my title is Data Engineer. The only programming I do here is functional programming for infrastructure to code--so again, no OOP.&lt;/p&gt;\n\n&lt;p&gt;I have been talking to recruiters recently, and my lack of production level experience with OOP disqualifies me from most DE roles. I would love for my next position to have some python I involved so I can learn, but I know that I am not a good fit for a DE role that is primarily using python/scala for integrations, especially at point of career and salary expectations I am at now. &lt;/p&gt;\n\n&lt;p&gt;My question is where I go from here? I&amp;#39;m also not super enthusiastic about programming a side project outside of work at this point in my life. I think I could pivot into cloud integration role, but is this even considered a DE position anymore? Based on my job searches, it seems like DE is doesn&amp;#39;t encompass the stack I&amp;#39;m using right now, which is 100% cloud native.  &lt;/p&gt;\n\n&lt;p&gt;I could for sure learn python on my own and pass leetcode questions, but that is not the same as having the experience as developing in python in a job. Anyhow, cheers everyone, thanks for reading.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "ys66qn", "is_robot_indexable": true, "report_reasons": null, "author": "Owmyeye", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ys66qn/data_engineer_or_something_else/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ys66qn/data_engineer_or_something_else/", "subreddit_subscribers": 79650, "created_utc": 1668160323.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We had a table which was storing data daily.\nThe size of the table started growing to 20+ milion rows and we were tasked to reduce its size.\n\nSenior came up with \"for each month, we'll remove the days that are not the last of month data. The calculation and the table will not change\"\n\nI've asked him how by doing that the table would not change and he looked at me puzzled.\n\nI still can't understand how this can be a solution. Can someone explain in easier words?\n\nEdit : modified and wrote better", "author_fullname": "t2_fsoatxql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reducing dimension of History Table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys5llm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668158413.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668157887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We had a table which was storing data daily.\nThe size of the table started growing to 20+ milion rows and we were tasked to reduce its size.&lt;/p&gt;\n\n&lt;p&gt;Senior came up with &amp;quot;for each month, we&amp;#39;ll remove the days that are not the last of month data. The calculation and the table will not change&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve asked him how by doing that the table would not change and he looked at me puzzled.&lt;/p&gt;\n\n&lt;p&gt;I still can&amp;#39;t understand how this can be a solution. Can someone explain in easier words?&lt;/p&gt;\n\n&lt;p&gt;Edit : modified and wrote better&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ys5llm", "is_robot_indexable": true, "report_reasons": null, "author": "CauliflowerJolly4599", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ys5llm/reducing_dimension_of_history_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ys5llm/reducing_dimension_of_history_table/", "subreddit_subscribers": 79650, "created_utc": 1668157887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m working in a data quality project, using pandera lib, it\u2019s like pydantic, but it\u2019s no way to return a complete name of the basics classes.\nI\u2019m mean if the data is int, i want the type name \u201cINTEGER\u201d not &lt;class \u2018int\u2019&gt;, to upload the schema to bigquery.\nAnyone knows a good way to implement this or knows a good lib that do this?", "author_fullname": "t2_nlcs880z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating a schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrrwze", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668114618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m working in a data quality project, using pandera lib, it\u2019s like pydantic, but it\u2019s no way to return a complete name of the basics classes.\nI\u2019m mean if the data is int, i want the type name \u201cINTEGER\u201d not &amp;lt;class \u2018int\u2019&amp;gt;, to upload the schema to bigquery.\nAnyone knows a good way to implement this or knows a good lib that do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrrwze", "is_robot_indexable": true, "report_reasons": null, "author": "Unhappy_SoftwareEng", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrrwze/creating_a_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrrwze/creating_a_schema/", "subreddit_subscribers": 79650, "created_utc": 1668114618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had some luck getting an external data pipeline up with Dataiku. Seems pretty similar to Alteryx but a bit more user friendly and I love that it can run python scripts. \n\nMy org is looking at moving a bunch of flows over to either Airflow or Dataiku since Databricks is getting a bit congested with hundreds of notebooks where it\u2019s hard to keep track of.\n\nMy question to you all is: is there a catch? Any major bugs or issues anyone has run into? Is it worth it when compared to open sourced Airflow? My only gripe so far is that debugging the python scripts sucks, and they basically need to be ready to go before you plug them in to Dataiku since if it breaks GOOD LUCK figuring out why.", "author_fullname": "t2_8rmuclrh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone use Dataiku for ETL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrzc0b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668135217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had some luck getting an external data pipeline up with Dataiku. Seems pretty similar to Alteryx but a bit more user friendly and I love that it can run python scripts. &lt;/p&gt;\n\n&lt;p&gt;My org is looking at moving a bunch of flows over to either Airflow or Dataiku since Databricks is getting a bit congested with hundreds of notebooks where it\u2019s hard to keep track of.&lt;/p&gt;\n\n&lt;p&gt;My question to you all is: is there a catch? Any major bugs or issues anyone has run into? Is it worth it when compared to open sourced Airflow? My only gripe so far is that debugging the python scripts sucks, and they basically need to be ready to go before you plug them in to Dataiku since if it breaks GOOD LUCK figuring out why.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yrzc0b", "is_robot_indexable": true, "report_reasons": null, "author": "Own_Neighborhood_231", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrzc0b/anyone_use_dataiku_for_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrzc0b/anyone_use_dataiku_for_etl/", "subreddit_subscribers": 79650, "created_utc": 1668135217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I interviewed for a position as a Software Engineer II, although given the job description the position seems more like a data engineering position. For my interview, I presented two projects I finished and one project I was working on. The project I was working on was a machine-learning algorithm for 3D image segmentation. When they asked me questions about the project I finished, I answered well, but when asked questions about my ML algorithm, there were a few questions I couldn't answer (for instance, the accuracy of my algorithm compared to other algorithms) because I haven't got around to answering those questions. \n\nTwo days after the interview, however, I developed an algorithm to measure the accuracy of my ML algorithm. I got 98% accuracy. I'm going to calculate the accuracy of other, competing algorithms. I expect that they'll be a lot less accurate. Should I send this information to the hiring manager as a follow-up? I felt that the weakest part of my interview was the moments when I had to answer questions about my ML algorithm.\n\nThank you!", "author_fullname": "t2_2ve4vyvh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I follow up on an interview with information on a current project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ysmqxu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668199917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I interviewed for a position as a Software Engineer II, although given the job description the position seems more like a data engineering position. For my interview, I presented two projects I finished and one project I was working on. The project I was working on was a machine-learning algorithm for 3D image segmentation. When they asked me questions about the project I finished, I answered well, but when asked questions about my ML algorithm, there were a few questions I couldn&amp;#39;t answer (for instance, the accuracy of my algorithm compared to other algorithms) because I haven&amp;#39;t got around to answering those questions. &lt;/p&gt;\n\n&lt;p&gt;Two days after the interview, however, I developed an algorithm to measure the accuracy of my ML algorithm. I got 98% accuracy. I&amp;#39;m going to calculate the accuracy of other, competing algorithms. I expect that they&amp;#39;ll be a lot less accurate. Should I send this information to the hiring manager as a follow-up? I felt that the weakest part of my interview was the moments when I had to answer questions about my ML algorithm.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "ysmqxu", "is_robot_indexable": true, "report_reasons": null, "author": "statius9", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysmqxu/should_i_follow_up_on_an_interview_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysmqxu/should_i_follow_up_on_an_interview_with/", "subreddit_subscribers": 79650, "created_utc": 1668199917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm new to the data engineering world and I want to learn more about *good* ways to do this. I feel like the approach my team uses is a little convoluted, but I don't know a better way to do it just yet.\n\n\nMy team owns several data products and one in particular I think could be better. It consists of ~10 data sources in delta format that have foreign key values to join them in a relational model. There are also a lot of transformations happening on the datasets before &amp; after the joins. The joins are basically being done randomly throughout the code and it feels very inconsistent and hard to navigate as it is.\n\n\nIf these data sources were tables in a SQL database, I'd say to just use some kind of ORM and have all the joins in place when we load the tables as objects in the app. Is there an equivalent to that in the data engineering world? Or maybe a better way to do it? For context, I'm coming from a backend Java point of view so I guess I'm trying to relate it to that, but I also don't want to tunnel-vision towards something like that as an end goal if that's not the industry standard approach.\n\n\nAny advice on where to start? I'm looking for best practices and ideally code/project examples that are creating similar data products to the one I described.", "author_fullname": "t2_229opcl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for joining multiple datasets in a data pipeline (Pyspark)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ysmdk9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668199124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to the data engineering world and I want to learn more about &lt;em&gt;good&lt;/em&gt; ways to do this. I feel like the approach my team uses is a little convoluted, but I don&amp;#39;t know a better way to do it just yet.&lt;/p&gt;\n\n&lt;p&gt;My team owns several data products and one in particular I think could be better. It consists of ~10 data sources in delta format that have foreign key values to join them in a relational model. There are also a lot of transformations happening on the datasets before &amp;amp; after the joins. The joins are basically being done randomly throughout the code and it feels very inconsistent and hard to navigate as it is.&lt;/p&gt;\n\n&lt;p&gt;If these data sources were tables in a SQL database, I&amp;#39;d say to just use some kind of ORM and have all the joins in place when we load the tables as objects in the app. Is there an equivalent to that in the data engineering world? Or maybe a better way to do it? For context, I&amp;#39;m coming from a backend Java point of view so I guess I&amp;#39;m trying to relate it to that, but I also don&amp;#39;t want to tunnel-vision towards something like that as an end goal if that&amp;#39;s not the industry standard approach.&lt;/p&gt;\n\n&lt;p&gt;Any advice on where to start? I&amp;#39;m looking for best practices and ideally code/project examples that are creating similar data products to the one I described.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ysmdk9", "is_robot_indexable": true, "report_reasons": null, "author": "y8MAC", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysmdk9/best_practices_for_joining_multiple_datasets_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysmdk9/best_practices_for_joining_multiple_datasets_in_a/", "subreddit_subscribers": 79650, "created_utc": 1668199124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AWS just announced that now [Amazon RDS supports gp3 volumes](https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-rds-general-purpose-gp3-storage-volumes/). This is very exciting for a single reason: you could save up to 20% on costs when you move from gp2 to gp3 volumes. You can read more here: [https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/](https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/)\n\nIn order to see how much money you could save, you could use this calculator: [https://aws.amazon.com/ebs/resources/](https://aws.amazon.com/ebs/resources/)", "author_fullname": "t2_ugjew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon RDS now supports new General Purpose gp3 storage volumes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yshbtk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668188327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AWS just announced that now &lt;a href=\"https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-rds-general-purpose-gp3-storage-volumes/\"&gt;Amazon RDS supports gp3 volumes&lt;/a&gt;. This is very exciting for a single reason: you could save up to 20% on costs when you move from gp2 to gp3 volumes. You can read more here: &lt;a href=\"https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/\"&gt;https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In order to see how much money you could save, you could use this calculator: &lt;a href=\"https://aws.amazon.com/ebs/resources/\"&gt;https://aws.amazon.com/ebs/resources/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?auto=webp&amp;s=8e3eb77ba905bb641af80fcf3efe1de0190ac8c2", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b015da4f990706696f7d06ac19bc75b807d90200", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44756ae9c6e1724356ccaef8214086d7d0cc95da", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a5696e6599c7d56b3770650b416341ba2102fd8", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4610f9bb7893259c61ba4fda892295f0da1a05ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9979842391099359ecaa7d0ce4c8c31f1e3bead7", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fe7c4eb58196f897578137b50f669a4707c9902", "width": 1080, "height": 567}], "variants": {}, "id": "RUqh18uQTwuGJocqdUcC-6UfvfWS63SRDdr8AQqU3uM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yshbtk", "is_robot_indexable": true, "report_reasons": null, "author": "marcosluis2186", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yshbtk/amazon_rds_now_supports_new_general_purpose_gp3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yshbtk/amazon_rds_now_supports_new_general_purpose_gp3/", "subreddit_subscribers": 79650, "created_utc": 1668188327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI am new to pyspark and writing simple programs to read and print a data. Every time I need to debug Spark it load entire pyspark shell which takes a long time(10-20 sec) and displays output. is this normal? Whereas I see some oneusing jyputer which clicked on each cell, it returns results", "author_fullname": "t2_ix7cgn3y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "pySpark Startup takes long time to run print statement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys0dbq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668138520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I am new to pyspark and writing simple programs to read and print a data. Every time I need to debug Spark it load entire pyspark shell which takes a long time(10-20 sec) and displays output. is this normal? Whereas I see some oneusing jyputer which clicked on each cell, it returns results&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ys0dbq", "is_robot_indexable": true, "report_reasons": null, "author": "bommu99", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ys0dbq/pyspark_startup_takes_long_time_to_run_print/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ys0dbq/pyspark_startup_takes_long_time_to_run_print/", "subreddit_subscribers": 79650, "created_utc": 1668138520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an airflow dag that runs every ten minutes and gets all the rows from Dynamodb in that period, creates a csv in S3, loads it onto redshift and then deletes the rows from Dynamodb.\n\nLast Friday, the dag stopped working when it said it couldn't connect to Redshift. \n\nAfter much looking around, we assumed it might just be a one off error as multiple other dags in production were able to work with Redshift. \n\nNow, it also depends on the past runs, so just triggering it to run didn't make it run normally. \n\nWe changed the start date of the dag to where the dag failed and loaded it into staging and it caught up to the present time after a few runs and was working fine.\n\nWe deployed this into production, it just gets timed out in the stage where it is extracting the rows from Dynamodb.\n\nAre there any suggestions on what might be the issue?", "author_fullname": "t2_yagno", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Code that works in staging, doesn't seem to be working in production.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrvo9f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668124445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an airflow dag that runs every ten minutes and gets all the rows from Dynamodb in that period, creates a csv in S3, loads it onto redshift and then deletes the rows from Dynamodb.&lt;/p&gt;\n\n&lt;p&gt;Last Friday, the dag stopped working when it said it couldn&amp;#39;t connect to Redshift. &lt;/p&gt;\n\n&lt;p&gt;After much looking around, we assumed it might just be a one off error as multiple other dags in production were able to work with Redshift. &lt;/p&gt;\n\n&lt;p&gt;Now, it also depends on the past runs, so just triggering it to run didn&amp;#39;t make it run normally. &lt;/p&gt;\n\n&lt;p&gt;We changed the start date of the dag to where the dag failed and loaded it into staging and it caught up to the present time after a few runs and was working fine.&lt;/p&gt;\n\n&lt;p&gt;We deployed this into production, it just gets timed out in the stage where it is extracting the rows from Dynamodb.&lt;/p&gt;\n\n&lt;p&gt;Are there any suggestions on what might be the issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrvo9f", "is_robot_indexable": true, "report_reasons": null, "author": "noNSFWcontent", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrvo9f/code_that_works_in_staging_doesnt_seem_to_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrvo9f/code_that_works_in_staging_doesnt_seem_to_be/", "subreddit_subscribers": 79650, "created_utc": 1668124445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title. I have a simple python script that moves some data and part of it loads/unloads files through our onprem network drive.  Script is run multiple times per day and I want to migrate it to an azure based service but I'm having trouble with accessing the onprem and not sure where to go/look for an azure service to execute the script and connect to the on prem directory.  \n\nI tried googling but I get so much info back. If anyone can provide some quick tips/guidance I'd appreciate it!", "author_fullname": "t2_szv0ygic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which cloud service do I need for a python script to access an on-prem file directory?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrur7l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668121920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title. I have a simple python script that moves some data and part of it loads/unloads files through our onprem network drive.  Script is run multiple times per day and I want to migrate it to an azure based service but I&amp;#39;m having trouble with accessing the onprem and not sure where to go/look for an azure service to execute the script and connect to the on prem directory.  &lt;/p&gt;\n\n&lt;p&gt;I tried googling but I get so much info back. If anyone can provide some quick tips/guidance I&amp;#39;d appreciate it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrur7l", "is_robot_indexable": true, "report_reasons": null, "author": "Hippodick666420", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrur7l/which_cloud_service_do_i_need_for_a_python_script/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrur7l/which_cloud_service_do_i_need_for_a_python_script/", "subreddit_subscribers": 79650, "created_utc": 1668121920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a free certification exam voucher from Microsoft for Exam PL-300: Microsoft Power BI Data Analyst, which expires in December 31 2022. I'd just finished some DE courses and was about to start building projects. \n\nI'd like to know if having a Power BI cert counts as something towards becoming a DE.\n\nSo, I'm a little confused here, should I purse my DE project and study for the Power BI exam or I should just put my focus on my projects?", "author_fullname": "t2_jvuhrmb0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Power BI certification, yay or nay?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ysm1tt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668198435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a free certification exam voucher from Microsoft for Exam PL-300: Microsoft Power BI Data Analyst, which expires in December 31 2022. I&amp;#39;d just finished some DE courses and was about to start building projects. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to know if having a Power BI cert counts as something towards becoming a DE.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m a little confused here, should I purse my DE project and study for the Power BI exam or I should just put my focus on my projects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ysm1tt", "is_robot_indexable": true, "report_reasons": null, "author": "Necessary-Factor8861", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysm1tt/power_bi_certification_yay_or_nay/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysm1tt/power_bi_certification_yay_or_nay/", "subreddit_subscribers": 79650, "created_utc": 1668198435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Besides being a more senior data engineer or engineering manager, what other types of positions can a data engineer move into?\n\n\n\nI'm particularly interested in devops.  Is that type of career change possible?  I'm also interested in data engineering that is less SQL and more spark development.", "author_fullname": "t2_3v6ob2bf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the different types of career progression available to data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysim7f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668191234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Besides being a more senior data engineer or engineering manager, what other types of positions can a data engineer move into?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m particularly interested in devops.  Is that type of career change possible?  I&amp;#39;m also interested in data engineering that is less SQL and more spark development.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ysim7f", "is_robot_indexable": true, "report_reasons": null, "author": "data_preprocessing", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysim7f/what_are_the_different_types_of_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysim7f/what_are_the_different_types_of_career/", "subreddit_subscribers": 79650, "created_utc": 1668191234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to do a data engineering project on aws. I tried aws glue before using tutorials but that pretty much just get data from s3 transform it and load into some target like s3 or redshift.\nBut this time I want to create an automated ETL pipeline on aws.\nBut I don't know how to get with all the steps.\nMy main source of data would be an api like reddit or Twitter. Should I use lambda or glue to load data into s3.\nI want to know the steps from getting data from api to generating some dashboards.\nAnd at the end how to automate all that stuff so I can run it daily or weekly.\nIf someone can breakdown the process for me that would be very helpful.\nThanks", "author_fullname": "t2_r509bej6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "need help with aws project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yseqms", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668182872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to do a data engineering project on aws. I tried aws glue before using tutorials but that pretty much just get data from s3 transform it and load into some target like s3 or redshift.\nBut this time I want to create an automated ETL pipeline on aws.\nBut I don&amp;#39;t know how to get with all the steps.\nMy main source of data would be an api like reddit or Twitter. Should I use lambda or glue to load data into s3.\nI want to know the steps from getting data from api to generating some dashboards.\nAnd at the end how to automate all that stuff so I can run it daily or weekly.\nIf someone can breakdown the process for me that would be very helpful.\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yseqms", "is_robot_indexable": true, "report_reasons": null, "author": "mediocrX", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yseqms/need_help_with_aws_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yseqms/need_help_with_aws_project/", "subreddit_subscribers": 79650, "created_utc": 1668182872.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}