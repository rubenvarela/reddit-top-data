{"kind": "Listing", "data": {"after": "t3_yrjs1m", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_anyz9dbz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is data integration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_yro0xc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 244, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/d2wrn8sd56z91/DASH_720.mp4?source=fallback", "height": 720, "width": 960, "scrubber_media_url": "https://v.redd.it/d2wrn8sd56z91/DASH_96.mp4", "dash_url": "https://v.redd.it/d2wrn8sd56z91/DASHPlaylist.mpd?a=1670769755%2CMTg4OTQ3NGM5NTJjYjg2YmYyYTE3YjkwNDdlZTNmNjFlZDJiNjI4NWQ0OTE5OTEzNmFmY2E5ZjBhNjYxY2E4MQ%3D%3D&amp;v=1&amp;f=sd", "duration": 14, "hls_url": "https://v.redd.it/d2wrn8sd56z91/HLSPlaylist.m3u8?a=1670769755%2CMmVhYjcxMmQ1NjI0MDEzYzg1YWFjZGI2MDFmNTJlNTM1NDZkYWJjYmQxMTczNTFmMTkzMTI1NTg3MDMyOTE4OA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 244, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vaCZzGJb6B08W8qE_YT3O5rb9Kjtsg7VSH2Uyk1plsQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668105985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/d2wrn8sd56z91", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B11CykTr3JEGxipI9NQMaPWu_vsb9nN5UNdcWkDUMQk.png?format=pjpg&amp;auto=webp&amp;s=620f4098a0fb06d26d56c39fd0f284b59136b78a", "width": 960, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/B11CykTr3JEGxipI9NQMaPWu_vsb9nN5UNdcWkDUMQk.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f7f70543a8dd30cda2598b78626444b64f61b72d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/B11CykTr3JEGxipI9NQMaPWu_vsb9nN5UNdcWkDUMQk.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e15aa8b56698a3650b97dcb4e035dc192b33e306", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/B11CykTr3JEGxipI9NQMaPWu_vsb9nN5UNdcWkDUMQk.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=cd1602f999b97dad61d21b4f8f4389ab31ce58fd", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/B11CykTr3JEGxipI9NQMaPWu_vsb9nN5UNdcWkDUMQk.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=31eab3e31afcebdf4ccecb53205ca5cfa34b9786", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/B11CykTr3JEGxipI9NQMaPWu_vsb9nN5UNdcWkDUMQk.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c5cf9fef3184f0a15100e1f4d2dcc694fc88f9d1", "width": 960, "height": 720}], "variants": {}, "id": "7SbytvMxmbgKSUEfhxA5yVQngeMlOUAUFfH3LSGntIA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yro0xc", "is_robot_indexable": true, "report_reasons": null, "author": "tchungry", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yro0xc/what_is_data_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/d2wrn8sd56z91", "subreddit_subscribers": 79606, "created_utc": 1668105985.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/d2wrn8sd56z91/DASH_720.mp4?source=fallback", "height": 720, "width": 960, "scrubber_media_url": "https://v.redd.it/d2wrn8sd56z91/DASH_96.mp4", "dash_url": "https://v.redd.it/d2wrn8sd56z91/DASHPlaylist.mpd?a=1670769755%2CMTg4OTQ3NGM5NTJjYjg2YmYyYTE3YjkwNDdlZTNmNjFlZDJiNjI4NWQ0OTE5OTEzNmFmY2E5ZjBhNjYxY2E4MQ%3D%3D&amp;v=1&amp;f=sd", "duration": 14, "hls_url": "https://v.redd.it/d2wrn8sd56z91/HLSPlaylist.m3u8?a=1670769755%2CMmVhYjcxMmQ1NjI0MDEzYzg1YWFjZGI2MDFmNTJlNTM1NDZkYWJjYmQxMTczNTFmMTkzMTI1NTg3MDMyOTE4OA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_gaa9o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Out of work for 8 months, trying something new with my resume. I have to imagine this is easier for hiring managers to look at. And much easier to have in front of anyone conducting your interview. But then again, I'm not a hiring manager. What are people's thoughts on this format?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_yrjwcf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 127, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 127, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9AIliIqy2bl1nhpDSa2BKUsEI3mW2BctRZxWO2oIM9I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668097222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/2h61ob5wf5z91.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/2h61ob5wf5z91.png?auto=webp&amp;s=1331671f03bd4cac3df0e7df3badbfb45445d013", "width": 720, "height": 960}, "resolutions": [{"url": "https://preview.redd.it/2h61ob5wf5z91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9db4a71dc787e88e91580aada97b47e8a92de7be", "width": 108, "height": 144}, {"url": "https://preview.redd.it/2h61ob5wf5z91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a38c5e7c680b219c1bd23e5473acbbccf784d6f", "width": 216, "height": 288}, {"url": "https://preview.redd.it/2h61ob5wf5z91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ea90ac385be8ed51d4f5d0a4191d1afc40afc647", "width": 320, "height": 426}, {"url": "https://preview.redd.it/2h61ob5wf5z91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=80ce1be9273ae35c1de4c8467b0f79d6574ae941", "width": 640, "height": 853}], "variants": {}, "id": "zxvMfvAXhxX7uEDSPnqulEliSf0tOx43KgXT_YJpswQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yrjwcf", "is_robot_indexable": true, "report_reasons": null, "author": "Cli4ordtheBRD", "discussion_type": null, "num_comments": 59, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrjwcf/out_of_work_for_8_months_trying_something_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/2h61ob5wf5z91.png", "subreddit_subscribers": 79606, "created_utc": 1668097222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The incredible [Syl Taylor](https://github.com/syl-taylor-aws) just released two new workshops related to [AWS Graviton](https://aws.amazon.com/ec2/graviton/) (the processors developed by the company with the best price/performance combination) and Machine Learning:\n\n* [An example solution for running ML workloads on AWS Graviton using AWS Nitro Enclaves](https://github.com/aws-samples/aws-graviton-run-confidential-ml-workloads-using-nitro-enclaves):\n\n&gt;Customers from diverse industries collaborate with other parties to exchange sensitive information, such as code and data. For artificial intelligence (AI), machine learning (ML), and data science (DS) practitioners, the ability to experiment with externally-provided algorithms, models, and datasets is key to improving business outcomes.  \nWe will demonstrate how you can share your sensitive AI/ML files in a manner that safeguards application and data confidentiality. To present you with a familiar environment, we included the ability to do seamless data transfers to accelerate ML and DS workloads, as well as run software downloaded at runtime to process that data conveniently.  \n[AWS Nitro Enclaves](https://aws.amazon.com/ec2/nitro/nitro-enclaves/) enables customers to create isolated compute environments to maintain the confidentiality of applications and data. The sample provided uses Nitro enclaves to enable sensitive file sharing and usage for ML workloads.\n\n* [How to run ML inference on EC2 (Graviton | arm64) using Apache TVM (TVMC)](https://github.com/aws-samples/aws-graviton-ml-inference-apache-tvm-example):\n\n&gt;This sample provides steps to deploy Apache TVM (TVMC Python) on a Graviton (arm64) EC2 instance to do ML inference using a ResNet50 model (ONNX).\n\nIf you want to get the last news/ use cases/articles/videos and more related to AWS Graviton, feel free to subscribe to our [weekly newsletter here](https://awsgravitonweekly.com/newsletter).", "author_fullname": "t2_ugjew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2 new workshops related to AWS Graviton and Machine Learning from the AWS team itself", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys558u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668155973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The incredible &lt;a href=\"https://github.com/syl-taylor-aws\"&gt;Syl Taylor&lt;/a&gt; just released two new workshops related to &lt;a href=\"https://aws.amazon.com/ec2/graviton/\"&gt;AWS Graviton&lt;/a&gt; (the processors developed by the company with the best price/performance combination) and Machine Learning:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/aws-samples/aws-graviton-run-confidential-ml-workloads-using-nitro-enclaves\"&gt;An example solution for running ML workloads on AWS Graviton using AWS Nitro Enclaves&lt;/a&gt;:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Customers from diverse industries collaborate with other parties to exchange sensitive information, such as code and data. For artificial intelligence (AI), machine learning (ML), and data science (DS) practitioners, the ability to experiment with externally-provided algorithms, models, and datasets is key to improving business outcomes.&lt;br/&gt;\nWe will demonstrate how you can share your sensitive AI/ML files in a manner that safeguards application and data confidentiality. To present you with a familiar environment, we included the ability to do seamless data transfers to accelerate ML and DS workloads, as well as run software downloaded at runtime to process that data conveniently.&lt;br/&gt;\n&lt;a href=\"https://aws.amazon.com/ec2/nitro/nitro-enclaves/\"&gt;AWS Nitro Enclaves&lt;/a&gt; enables customers to create isolated compute environments to maintain the confidentiality of applications and data. The sample provided uses Nitro enclaves to enable sensitive file sharing and usage for ML workloads.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/aws-samples/aws-graviton-ml-inference-apache-tvm-example\"&gt;How to run ML inference on EC2 (Graviton | arm64) using Apache TVM (TVMC)&lt;/a&gt;:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;This sample provides steps to deploy Apache TVM (TVMC Python) on a Graviton (arm64) EC2 instance to do ML inference using a ResNet50 model (ONNX).&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;If you want to get the last news/ use cases/articles/videos and more related to AWS Graviton, feel free to subscribe to our &lt;a href=\"https://awsgravitonweekly.com/newsletter\"&gt;weekly newsletter here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/I0L4Wu97BZiegQJjJZmMRpS7maypMXUt-AfBoH52WY8.jpg?auto=webp&amp;s=600910a80f5c15459a15412eba8f709a88dbaa49", "width": 420, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/I0L4Wu97BZiegQJjJZmMRpS7maypMXUt-AfBoH52WY8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=339bb2aeaa127d1b31e2f48ec359d3972aa6d2e3", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/I0L4Wu97BZiegQJjJZmMRpS7maypMXUt-AfBoH52WY8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=966a5e2fda08db3d7d995e50e145b78c80f5db27", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/I0L4Wu97BZiegQJjJZmMRpS7maypMXUt-AfBoH52WY8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=032f634d8a58e313073dfca21e7c54113f9009ad", "width": 320, "height": 320}], "variants": {}, "id": "9QiuiOa2bSL0giWPjv6q515sBbK6pyY5pmXWvryl38s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "ys558u", "is_robot_indexable": true, "report_reasons": null, "author": "marcosluis2186", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ys558u/2_new_workshops_related_to_aws_graviton_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ys558u/2_new_workshops_related_to_aws_graviton_and/", "subreddit_subscribers": 79606, "created_utc": 1668155973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is best practice?  I  believe each source should be ingested into a separate source\\_input table before any transformation occurs. When the  transformation is  ok, then a source\\_input table is merged to a main table containing many sources of data.", "author_fullname": "t2_50oyomi2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When using ETL for ingesting data from multiple sources should each source have a separate destination table in the database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrqa0w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668110717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is best practice?  I  believe each source should be ingested into a separate source_input table before any transformation occurs. When the  transformation is  ok, then a source_input table is merged to a main table containing many sources of data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yrqa0w", "is_robot_indexable": true, "report_reasons": null, "author": "Eorpoch", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrqa0w/when_using_etl_for_ingesting_data_from_multiple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrqa0w/when_using_etl_for_ingesting_data_from_multiple/", "subreddit_subscribers": 79606, "created_utc": 1668110717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm ingesting JSON data and as part of the my data pipeline, I'm hoping to clean the data to match a schema that can only be known at runtime.\n\nWhich tools should I be looking into? So far from my research, I've found that it's common to use schemas for validating data, but not so much for cleaning it. What I mean by cleaning is:\n\n1. Drop additional fields that are not preset in the input\n2. Set missing fields to null\n3. Perform basic type conversion, e.g. int -&gt; string, or string -&gt; date (using customizable logic)\n\nI'm still also working on the schema layer - currently writing them in GraphQL but considering moving to TypeScript or any other format/language, since it's fairly easy to write converters between formats these days, I should be able to transform the authored format into whatever is required by the tooling for the above 3 features\n\nCheers", "author_fullname": "t2_vep5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cleaning data using a schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrxwf9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668130889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m ingesting JSON data and as part of the my data pipeline, I&amp;#39;m hoping to clean the data to match a schema that can only be known at runtime.&lt;/p&gt;\n\n&lt;p&gt;Which tools should I be looking into? So far from my research, I&amp;#39;ve found that it&amp;#39;s common to use schemas for validating data, but not so much for cleaning it. What I mean by cleaning is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Drop additional fields that are not preset in the input&lt;/li&gt;\n&lt;li&gt;Set missing fields to null&lt;/li&gt;\n&lt;li&gt;Perform basic type conversion, e.g. int -&amp;gt; string, or string -&amp;gt; date (using customizable logic)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m still also working on the schema layer - currently writing them in GraphQL but considering moving to TypeScript or any other format/language, since it&amp;#39;s fairly easy to write converters between formats these days, I should be able to transform the authored format into whatever is required by the tooling for the above 3 features&lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrxwf9", "is_robot_indexable": true, "report_reasons": null, "author": "matty_fu", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrxwf9/cleaning_data_using_a_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrxwf9/cleaning_data_using_a_schema/", "subreddit_subscribers": 79606, "created_utc": 1668130889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title says, I am now an Associate Data Engineer from being a Junior Data Analyst. I don\u2019t have any software engineering background. All the knowledge I have came from Udemy courses, YouTube videos and books. \n\nI am proficient in Python and SQL. I mostly use BigQuery for my work and dbt along with it. Python comes into play, if I need to do some kind of automation. I can write R codes also. I maintain data pipelines using xplenty (intergrate.io).\n\nI want to learn the necessary knowledge so I can add value to the role I have. I was looking into some LinkedIn profiles of Data Engineers and what they do. Based on those, I thought I should start learning something. So, I found a video on YouTube which is about [API Development using Python](https://youtu.be/0sOvCWFmrtA). Can an expert in this field, let me know if this is a good video to get started on something? If you have any other resources or even paid Udemy courses. Can you link them here? \n\nThanks.", "author_fullname": "t2_5p87574b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got my designation changed to an Associate Data Engineer.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrlg4a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668100359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says, I am now an Associate Data Engineer from being a Junior Data Analyst. I don\u2019t have any software engineering background. All the knowledge I have came from Udemy courses, YouTube videos and books. &lt;/p&gt;\n\n&lt;p&gt;I am proficient in Python and SQL. I mostly use BigQuery for my work and dbt along with it. Python comes into play, if I need to do some kind of automation. I can write R codes also. I maintain data pipelines using xplenty (intergrate.io).&lt;/p&gt;\n\n&lt;p&gt;I want to learn the necessary knowledge so I can add value to the role I have. I was looking into some LinkedIn profiles of Data Engineers and what they do. Based on those, I thought I should start learning something. So, I found a video on YouTube which is about &lt;a href=\"https://youtu.be/0sOvCWFmrtA\"&gt;API Development using Python&lt;/a&gt;. Can an expert in this field, let me know if this is a good video to get started on something? If you have any other resources or even paid Udemy courses. Can you link them here? &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_PrmREx15utaLnBCrarZR2mpSIxGwoDz6IVBxu1NfuU.jpg?auto=webp&amp;s=f750dd9eb7aa95554349b6c9f71cd4eb513a2043", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/_PrmREx15utaLnBCrarZR2mpSIxGwoDz6IVBxu1NfuU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e867ef3a0bf08341d69fbb3da90ca34b0f891766", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/_PrmREx15utaLnBCrarZR2mpSIxGwoDz6IVBxu1NfuU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d12cb5d208ba87d0abcd89a1c784f085036267fe", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/_PrmREx15utaLnBCrarZR2mpSIxGwoDz6IVBxu1NfuU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8d3458227803fdcd5c97cc18434e97d9ef07f1b", "width": 320, "height": 240}], "variants": {}, "id": "Yw2QqL1DkXn49PDHR7XOSwNY4640ATzyKH0GwEHUmSI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrlg4a", "is_robot_indexable": true, "report_reasons": null, "author": "Affectionate-Pride19", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrlg4a/got_my_designation_changed_to_an_associate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrlg4a/got_my_designation_changed_to_an_associate_data/", "subreddit_subscribers": 79606, "created_utc": 1668100359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I have a use case with an architecture \"recommended\" by my client where they want to analyze different system logs with Log Analytics and then save the results of these queries and later be imported in PowerBI.\n\nThe use-case the client is thinking of is saving/uploading any event logs from any system to Log Analytics and query those logs.\n\nI have close to 0 experience with Azure Log Analytics and from the Udemy courses I've seen so far and the documentation I've read, the default way of working is to use either an agent to upload logs to a storage account or to the Log Analytics workspace directly and then query them there.\n\nBut the documentation shows only a predetermined set of types of logs, like IIS logs, event logs (configured to be saved from Application Service directly to a storage account), but no \"custom logs\".\n\nFrom what I've read about custom logs, it seems that there's no way to plug-and-play with these, you still need an agent that captures the logs and a schema provided that can process the logs and save them to Log Analytics workspace. So from what I see, custom logs are **always linked to an agent**. But in my use-case an agent wouldn't exist, because logs are copied on-demand.\n\nIn this case is Log Analytics a good tool to use? Or should I pushback on my client to use an agent on his devices (if the security team allows it) to send log data to Log Analytics?\n\nOr, is there a better technology to use to analyze different types of log text information?", "author_fullname": "t2_pnduh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some advice regarding Azure Log Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys60ca", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668159589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have a use case with an architecture &amp;quot;recommended&amp;quot; by my client where they want to analyze different system logs with Log Analytics and then save the results of these queries and later be imported in PowerBI.&lt;/p&gt;\n\n&lt;p&gt;The use-case the client is thinking of is saving/uploading any event logs from any system to Log Analytics and query those logs.&lt;/p&gt;\n\n&lt;p&gt;I have close to 0 experience with Azure Log Analytics and from the Udemy courses I&amp;#39;ve seen so far and the documentation I&amp;#39;ve read, the default way of working is to use either an agent to upload logs to a storage account or to the Log Analytics workspace directly and then query them there.&lt;/p&gt;\n\n&lt;p&gt;But the documentation shows only a predetermined set of types of logs, like IIS logs, event logs (configured to be saved from Application Service directly to a storage account), but no &amp;quot;custom logs&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;From what I&amp;#39;ve read about custom logs, it seems that there&amp;#39;s no way to plug-and-play with these, you still need an agent that captures the logs and a schema provided that can process the logs and save them to Log Analytics workspace. So from what I see, custom logs are &lt;strong&gt;always linked to an agent&lt;/strong&gt;. But in my use-case an agent wouldn&amp;#39;t exist, because logs are copied on-demand.&lt;/p&gt;\n\n&lt;p&gt;In this case is Log Analytics a good tool to use? Or should I pushback on my client to use an agent on his devices (if the security team allows it) to send log data to Log Analytics?&lt;/p&gt;\n\n&lt;p&gt;Or, is there a better technology to use to analyze different types of log text information?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ys60ca", "is_robot_indexable": true, "report_reasons": null, "author": "raduqq", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ys60ca/need_some_advice_regarding_azure_log_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ys60ca/need_some_advice_regarding_azure_log_analytics/", "subreddit_subscribers": 79606, "created_utc": 1668159589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We had a table which was storing data daily.\nThe size of the table started growing to 20+ milion rows and we were tasked to reduce its size.\n\nSenior came up with \"for each month, we'll remove the days that are not the last of month data. The calculation and the table will not change\"\n\nI've asked him how by doing that the table would not change and he looked at me puzzled.\n\nI still can't understand how this can be a solution. Can someone explain in easier words?\n\nEdit : modified and wrote better", "author_fullname": "t2_fsoatxql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reducing dimension of History Table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys5llm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668158413.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668157887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We had a table which was storing data daily.\nThe size of the table started growing to 20+ milion rows and we were tasked to reduce its size.&lt;/p&gt;\n\n&lt;p&gt;Senior came up with &amp;quot;for each month, we&amp;#39;ll remove the days that are not the last of month data. The calculation and the table will not change&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve asked him how by doing that the table would not change and he looked at me puzzled.&lt;/p&gt;\n\n&lt;p&gt;I still can&amp;#39;t understand how this can be a solution. Can someone explain in easier words?&lt;/p&gt;\n\n&lt;p&gt;Edit : modified and wrote better&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ys5llm", "is_robot_indexable": true, "report_reasons": null, "author": "CauliflowerJolly4599", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ys5llm/reducing_dimension_of_history_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ys5llm/reducing_dimension_of_history_table/", "subreddit_subscribers": 79606, "created_utc": 1668157887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m working in a data quality project, using pandera lib, it\u2019s like pydantic, but it\u2019s no way to return a complete name of the basics classes.\nI\u2019m mean if the data is int, i want the type name \u201cINTEGER\u201d not &lt;class \u2018int\u2019&gt;, to upload the schema to bigquery.\nAnyone knows a good way to implement this or knows a good lib that do this?", "author_fullname": "t2_nlcs880z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating a schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrrwze", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668114618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m working in a data quality project, using pandera lib, it\u2019s like pydantic, but it\u2019s no way to return a complete name of the basics classes.\nI\u2019m mean if the data is int, i want the type name \u201cINTEGER\u201d not &amp;lt;class \u2018int\u2019&amp;gt;, to upload the schema to bigquery.\nAnyone knows a good way to implement this or knows a good lib that do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrrwze", "is_robot_indexable": true, "report_reasons": null, "author": "Unhappy_SoftwareEng", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrrwze/creating_a_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrrwze/creating_a_schema/", "subreddit_subscribers": 79606, "created_utc": 1668114618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to build ETLs that read 3500 MYSQL DBs daily, with about 80 tables in each DB, and then upload all the data in BQ.  \nActually, I read every table and overwrite it daily, which takes about 10 hours using a python script on Databricks.\n\nConsidering:  \n\\- I have a limit to the queries I can run at the same time in BigQuery  \n\\- I have already parallelised read and write  \n\\- I overwrite all the tables every day because I have tried to update only the fields that change but it takes ages  \n\\- Some tables are empty, I take them empty  \n\\- The largest table is 3GB, often the run fail when arrived to the biggest tables\n\nWhat is a better way to do that in less time?  \nEven when changing tools, I'm considering using Airflow + Dataflow.", "author_fullname": "t2_febeq0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data from MYSQL to BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrnizl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668104769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to build ETLs that read 3500 MYSQL DBs daily, with about 80 tables in each DB, and then upload all the data in BQ.&lt;br/&gt;\nActually, I read every table and overwrite it daily, which takes about 10 hours using a python script on Databricks.&lt;/p&gt;\n\n&lt;p&gt;Considering:&lt;br/&gt;\n- I have a limit to the queries I can run at the same time in BigQuery&lt;br/&gt;\n- I have already parallelised read and write&lt;br/&gt;\n- I overwrite all the tables every day because I have tried to update only the fields that change but it takes ages&lt;br/&gt;\n- Some tables are empty, I take them empty&lt;br/&gt;\n- The largest table is 3GB, often the run fail when arrived to the biggest tables&lt;/p&gt;\n\n&lt;p&gt;What is a better way to do that in less time?&lt;br/&gt;\nEven when changing tools, I&amp;#39;m considering using Airflow + Dataflow.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrnizl", "is_robot_indexable": true, "report_reasons": null, "author": "Domy__", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/yrnizl/data_from_mysql_to_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrnizl/data_from_mysql_to_bigquery/", "subreddit_subscribers": 79606, "created_utc": 1668104769.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anybody got a good PHI obfuscation tool for text files?  Even better if it will retain relationships between other file types.  What you got?", "author_fullname": "t2_4270ek0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tools for obfuscating PHI in .csv files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrg5fu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668089488.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anybody got a good PHI obfuscation tool for text files?  Even better if it will retain relationships between other file types.  What you got?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrg5fu", "is_robot_indexable": true, "report_reasons": null, "author": "mr_electric_wizard", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrg5fu/tools_for_obfuscating_phi_in_csv_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrg5fu/tools_for_obfuscating_phi_in_csv_files/", "subreddit_subscribers": 79606, "created_utc": 1668089488.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI am new to pyspark and writing simple programs to read and print a data. Every time I need to debug Spark it load entire pyspark shell which takes a long time(10-20 sec) and displays output. is this normal? Whereas I see some oneusing jyputer which clicked on each cell, it returns results", "author_fullname": "t2_ix7cgn3y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "pySpark Startup takes long time to run print statement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys0dbq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668138520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I am new to pyspark and writing simple programs to read and print a data. Every time I need to debug Spark it load entire pyspark shell which takes a long time(10-20 sec) and displays output. is this normal? Whereas I see some oneusing jyputer which clicked on each cell, it returns results&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ys0dbq", "is_robot_indexable": true, "report_reasons": null, "author": "bommu99", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ys0dbq/pyspark_startup_takes_long_time_to_run_print/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ys0dbq/pyspark_startup_takes_long_time_to_run_print/", "subreddit_subscribers": 79606, "created_utc": 1668138520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone. I have been a data engineer for 4-5 years. I am curious if I should be looking at a different career path/title based on my experience. \n\nI am in a situation where every job I've had, has not used OOP, meaning that I have no python/scala experience in a production environment.\n\nMy first job was creating complex ad hoc analytical sql queries and and also developing etl pipelienes in talend/kettle (which is an older stack -- and tbh, it's not a stack I would work on again) This position gave me a bit of DE and DA background.\n\nMy second job was primarily using a data integration management tool called timextender, where I was building a data warehouse and migrating data from on prem SAP to the cloud.  I used python here a little, but only for analysis using numpy/pandas. I actually have a decent background with python as a data analysis tool.\n\nFinally, my current position is 100% building integrations in Azure -- my title is Data Engineer. The only programming I do here is functional programming for infrastructure to code--so again, no OOP.\n\nI have been talking to recruiters recently, and my lack of production level experience with OOP disqualifies me from most DE roles. I would love for my next position to have some python I involved so I can learn, but I know that I am not a good fit for a DE role that is primarily using python/scala for integrations, especially at point of career and salary expectations I am at now. \n\nMy question is where I go from here? I'm also not super enthusiastic about programming a side project outside of work at this point in my life. I think I could pivot into cloud integration role, but is this even considered a DE position anymore? Based on my job searches, it seems like DE is doesn't encompass the stack I'm using right now, which is 100% cloud native.  \n\nI could for sure learn python on my own and pass leetcode questions, but that is not the same as having the experience as developing in python in a job. Anyhow, cheers everyone, thanks for reading.", "author_fullname": "t2_47vwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer or something else?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys66qn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668160323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. I have been a data engineer for 4-5 years. I am curious if I should be looking at a different career path/title based on my experience. &lt;/p&gt;\n\n&lt;p&gt;I am in a situation where every job I&amp;#39;ve had, has not used OOP, meaning that I have no python/scala experience in a production environment.&lt;/p&gt;\n\n&lt;p&gt;My first job was creating complex ad hoc analytical sql queries and and also developing etl pipelienes in talend/kettle (which is an older stack -- and tbh, it&amp;#39;s not a stack I would work on again) This position gave me a bit of DE and DA background.&lt;/p&gt;\n\n&lt;p&gt;My second job was primarily using a data integration management tool called timextender, where I was building a data warehouse and migrating data from on prem SAP to the cloud.  I used python here a little, but only for analysis using numpy/pandas. I actually have a decent background with python as a data analysis tool.&lt;/p&gt;\n\n&lt;p&gt;Finally, my current position is 100% building integrations in Azure -- my title is Data Engineer. The only programming I do here is functional programming for infrastructure to code--so again, no OOP.&lt;/p&gt;\n\n&lt;p&gt;I have been talking to recruiters recently, and my lack of production level experience with OOP disqualifies me from most DE roles. I would love for my next position to have some python I involved so I can learn, but I know that I am not a good fit for a DE role that is primarily using python/scala for integrations, especially at point of career and salary expectations I am at now. &lt;/p&gt;\n\n&lt;p&gt;My question is where I go from here? I&amp;#39;m also not super enthusiastic about programming a side project outside of work at this point in my life. I think I could pivot into cloud integration role, but is this even considered a DE position anymore? Based on my job searches, it seems like DE is doesn&amp;#39;t encompass the stack I&amp;#39;m using right now, which is 100% cloud native.  &lt;/p&gt;\n\n&lt;p&gt;I could for sure learn python on my own and pass leetcode questions, but that is not the same as having the experience as developing in python in a job. Anyhow, cheers everyone, thanks for reading.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "ys66qn", "is_robot_indexable": true, "report_reasons": null, "author": "Owmyeye", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ys66qn/data_engineer_or_something_else/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ys66qn/data_engineer_or_something_else/", "subreddit_subscribers": 79606, "created_utc": 1668160323.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had some luck getting an external data pipeline up with Dataiku. Seems pretty similar to Alteryx but a bit more user friendly and I love that it can run python scripts. \n\nMy org is looking at moving a bunch of flows over to either Airflow or Dataiku since Databricks is getting a bit congested with hundreds of notebooks where it\u2019s hard to keep track of.\n\nMy question to you all is: is there a catch? Any major bugs or issues anyone has run into? Is it worth it when compared to open sourced Airflow? My only gripe so far is that debugging the python scripts sucks, and they basically need to be ready to go before you plug them in to Dataiku since if it breaks GOOD LUCK figuring out why.", "author_fullname": "t2_8rmuclrh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone use Dataiku for ETL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrzc0b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668135217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had some luck getting an external data pipeline up with Dataiku. Seems pretty similar to Alteryx but a bit more user friendly and I love that it can run python scripts. &lt;/p&gt;\n\n&lt;p&gt;My org is looking at moving a bunch of flows over to either Airflow or Dataiku since Databricks is getting a bit congested with hundreds of notebooks where it\u2019s hard to keep track of.&lt;/p&gt;\n\n&lt;p&gt;My question to you all is: is there a catch? Any major bugs or issues anyone has run into? Is it worth it when compared to open sourced Airflow? My only gripe so far is that debugging the python scripts sucks, and they basically need to be ready to go before you plug them in to Dataiku since if it breaks GOOD LUCK figuring out why.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yrzc0b", "is_robot_indexable": true, "report_reasons": null, "author": "Own_Neighborhood_231", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrzc0b/anyone_use_dataiku_for_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrzc0b/anyone_use_dataiku_for_etl/", "subreddit_subscribers": 79606, "created_utc": 1668135217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an airflow dag that runs every ten minutes and gets all the rows from Dynamodb in that period, creates a csv in S3, loads it onto redshift and then deletes the rows from Dynamodb.\n\nLast Friday, the dag stopped working when it said it couldn't connect to Redshift. \n\nAfter much looking around, we assumed it might just be a one off error as multiple other dags in production were able to work with Redshift. \n\nNow, it also depends on the past runs, so just triggering it to run didn't make it run normally. \n\nWe changed the start date of the dag to where the dag failed and loaded it into staging and it caught up to the present time after a few runs and was working fine.\n\nWe deployed this into production, it just gets timed out in the stage where it is extracting the rows from Dynamodb.\n\nAre there any suggestions on what might be the issue?", "author_fullname": "t2_yagno", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Code that works in staging, doesn't seem to be working in production.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrvo9f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668124445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an airflow dag that runs every ten minutes and gets all the rows from Dynamodb in that period, creates a csv in S3, loads it onto redshift and then deletes the rows from Dynamodb.&lt;/p&gt;\n\n&lt;p&gt;Last Friday, the dag stopped working when it said it couldn&amp;#39;t connect to Redshift. &lt;/p&gt;\n\n&lt;p&gt;After much looking around, we assumed it might just be a one off error as multiple other dags in production were able to work with Redshift. &lt;/p&gt;\n\n&lt;p&gt;Now, it also depends on the past runs, so just triggering it to run didn&amp;#39;t make it run normally. &lt;/p&gt;\n\n&lt;p&gt;We changed the start date of the dag to where the dag failed and loaded it into staging and it caught up to the present time after a few runs and was working fine.&lt;/p&gt;\n\n&lt;p&gt;We deployed this into production, it just gets timed out in the stage where it is extracting the rows from Dynamodb.&lt;/p&gt;\n\n&lt;p&gt;Are there any suggestions on what might be the issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrvo9f", "is_robot_indexable": true, "report_reasons": null, "author": "noNSFWcontent", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrvo9f/code_that_works_in_staging_doesnt_seem_to_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrvo9f/code_that_works_in_staging_doesnt_seem_to_be/", "subreddit_subscribers": 79606, "created_utc": 1668124445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title. I have a simple python script that moves some data and part of it loads/unloads files through our onprem network drive.  Script is run multiple times per day and I want to migrate it to an azure based service but I'm having trouble with accessing the onprem and not sure where to go/look for an azure service to execute the script and connect to the on prem directory.  \n\nI tried googling but I get so much info back. If anyone can provide some quick tips/guidance I'd appreciate it!", "author_fullname": "t2_szv0ygic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which cloud service do I need for a python script to access an on-prem file directory?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrur7l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668121920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title. I have a simple python script that moves some data and part of it loads/unloads files through our onprem network drive.  Script is run multiple times per day and I want to migrate it to an azure based service but I&amp;#39;m having trouble with accessing the onprem and not sure where to go/look for an azure service to execute the script and connect to the on prem directory.  &lt;/p&gt;\n\n&lt;p&gt;I tried googling but I get so much info back. If anyone can provide some quick tips/guidance I&amp;#39;d appreciate it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrur7l", "is_robot_indexable": true, "report_reasons": null, "author": "Hippodick666420", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrur7l/which_cloud_service_do_i_need_for_a_python_script/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrur7l/which_cloud_service_do_i_need_for_a_python_script/", "subreddit_subscribers": 79606, "created_utc": 1668121920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long story short we have a hadoop environment at my current company and we wanted to start doing some data testing. \n\nWhat would be some recommendation for tools to use. I would really like to use some open source, so apache or hadoop, mapreduce stuff I might not know about, more then Dbt or other companies. But if there is no way or its just better to do it that way it is more then okay.\n\nLet me know what is the best way to go about this any help is more then appreciated :) \n\nThis can also help to discuss what is the best solution for data testing in your opinion as well :P", "author_fullname": "t2_728guva3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data testing in hadoop", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrqol1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668111582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short we have a hadoop environment at my current company and we wanted to start doing some data testing. &lt;/p&gt;\n\n&lt;p&gt;What would be some recommendation for tools to use. I would really like to use some open source, so apache or hadoop, mapreduce stuff I might not know about, more then Dbt or other companies. But if there is no way or its just better to do it that way it is more then okay.&lt;/p&gt;\n\n&lt;p&gt;Let me know what is the best way to go about this any help is more then appreciated :) &lt;/p&gt;\n\n&lt;p&gt;This can also help to discuss what is the best solution for data testing in your opinion as well :P&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yrqol1", "is_robot_indexable": true, "report_reasons": null, "author": "_Ishdhoggur_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrqol1/data_testing_in_hadoop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrqol1/data_testing_in_hadoop/", "subreddit_subscribers": 79606, "created_utc": 1668111582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This may actually be more of a DevOps question, even if it's ultimately ETL-related. Anyways, if so, let me know and I will go bother them.\n\nWe need to push our client's data into SalesForce. For now, we want them to be able to log in to an environment (probably some flavor of windows) with a database application that is already configured and ready to go, with no configuration to do on their part.\n\nI don't know that much about dockers or virtual machines - can they be configured in such a way? I like docker because it has AWS and Azure options available. But I'm open to whatever works. Thanks!", "author_fullname": "t2_1663zr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need a prefab environment for a client to log in to (Docker? VM?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrpo5m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668109438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This may actually be more of a DevOps question, even if it&amp;#39;s ultimately ETL-related. Anyways, if so, let me know and I will go bother them.&lt;/p&gt;\n\n&lt;p&gt;We need to push our client&amp;#39;s data into SalesForce. For now, we want them to be able to log in to an environment (probably some flavor of windows) with a database application that is already configured and ready to go, with no configuration to do on their part.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know that much about dockers or virtual machines - can they be configured in such a way? I like docker because it has AWS and Azure options available. But I&amp;#39;m open to whatever works. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrpo5m", "is_robot_indexable": true, "report_reasons": null, "author": "lengthy_preamble", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrpo5m/i_need_a_prefab_environment_for_a_client_to_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrpo5m/i_need_a_prefab_environment_for_a_client_to_log/", "subreddit_subscribers": 79606, "created_utc": 1668109438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dfcot0zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Big Book of Data Engineering - Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_yrp7n0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.56, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/R9tWjrW09PyAQrUtpANd1R-d_nRfCv7fpTT5PV-mq4g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668108469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "databricks.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.databricks.com/p/ebook/the-big-book-of-data-engineering", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KkhZgNXiIjLwTHALVBEqV1l2xBpr4GwE9T-5pGl0H9k.jpg?auto=webp&amp;s=74cca8cdf98daec365850611309731c9599b36d1", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/KkhZgNXiIjLwTHALVBEqV1l2xBpr4GwE9T-5pGl0H9k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb0632009907196f1c8e74a2ae093520fdbaa66e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/KkhZgNXiIjLwTHALVBEqV1l2xBpr4GwE9T-5pGl0H9k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f9b73ee6aad734d52c6ec5d0b266f31513443976", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/KkhZgNXiIjLwTHALVBEqV1l2xBpr4GwE9T-5pGl0H9k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=626c715ff0eb27a87e4a1d00d210ffd1ab67d03c", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/KkhZgNXiIjLwTHALVBEqV1l2xBpr4GwE9T-5pGl0H9k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=46af51f77f5c7a9718291eaab2d018c3ab60f4cb", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/KkhZgNXiIjLwTHALVBEqV1l2xBpr4GwE9T-5pGl0H9k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=10f4300c986d87bd0dabf463f53c38c4472519ed", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/KkhZgNXiIjLwTHALVBEqV1l2xBpr4GwE9T-5pGl0H9k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9952fb37853176ffdcb5b6dad22f81884c2e9395", "width": 1080, "height": 565}], "variants": {}, "id": "TN8kXtKzOEWeBJNGeTcvWYP4sqxmO5jmr9P4zHYSj_Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yrp7n0", "is_robot_indexable": true, "report_reasons": null, "author": "DrRedmondNYC", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrp7n0/the_big_book_of_data_engineering_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.databricks.com/p/ebook/the-big-book-of-data-engineering", "subreddit_subscribers": 79606, "created_utc": 1668108469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is probably a big question and I want to preface this by saying I have more of a background in data analysis/cleaning and visualization using R, SQL queries, Pandas and PBI.\n\nWhat would best practice be for getting live streaming data from industrial PLC into some kind of usable application that can be used by the business daily. We currently have a SQL database that houses the data from the PLCs but I would like to make an application that can be viewed with real time data. PowerBI is not currently an option due to the refresh limits.\n\nAny help of what direction to go into would be great. Thanks!", "author_fullname": "t2_o62o7qef", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for getting live data from PLCs into visualization/application", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrnv0h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668105604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is probably a big question and I want to preface this by saying I have more of a background in data analysis/cleaning and visualization using R, SQL queries, Pandas and PBI.&lt;/p&gt;\n\n&lt;p&gt;What would best practice be for getting live streaming data from industrial PLC into some kind of usable application that can be used by the business daily. We currently have a SQL database that houses the data from the PLCs but I would like to make an application that can be viewed with real time data. PowerBI is not currently an option due to the refresh limits.&lt;/p&gt;\n\n&lt;p&gt;Any help of what direction to go into would be great. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrnv0h", "is_robot_indexable": true, "report_reasons": null, "author": "Stewdioo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrnv0h/tips_for_getting_live_data_from_plcs_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrnv0h/tips_for_getting_live_data_from_plcs_into/", "subreddit_subscribers": 79606, "created_utc": 1668105604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to help software devs answer some questions about query optimization for their ruby on rails web app. They are also using instant queries. Any tips for where to start? (I\u2019m pretty much a noob on this to be honest).", "author_fullname": "t2_dbjonqs9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best resources for database and query optimization for web based apps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrjwn3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668097239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to help software devs answer some questions about query optimization for their ruby on rails web app. They are also using instant queries. Any tips for where to start? (I\u2019m pretty much a noob on this to be honest).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrjwn3", "is_robot_indexable": true, "report_reasons": null, "author": "homeybody", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrjwn3/best_resources_for_database_and_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrjwn3/best_resources_for_database_and_query/", "subreddit_subscribers": 79606, "created_utc": 1668097239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started learning cloud with Dp-900 . Now.i am lost since after giving the exam in coming week i have no idea where to go on to . I want work as data engineer or analyst. Please advise.\n\nAlso please mention free resources", "author_fullname": "t2_f836ym4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ysa4yt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668172428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started learning cloud with Dp-900 . Now.i am lost since after giving the exam in coming week i have no idea where to go on to . I want work as data engineer or analyst. Please advise.&lt;/p&gt;\n\n&lt;p&gt;Also please mention free resources&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "ysa4yt", "is_robot_indexable": true, "report_reasons": null, "author": "Aggravating_Wind8365", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysa4yt/need_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysa4yt/need_advice/", "subreddit_subscribers": 79606, "created_utc": 1668172428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If anyone needs GCP Voucher please ping me, Price is negotiable!", "author_fullname": "t2_fcj7lu74", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have an Extra GCP Exam voucher", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrzk6m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668135910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If anyone needs GCP Voucher please ping me, Price is negotiable!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yrzk6m", "is_robot_indexable": true, "report_reasons": null, "author": "Hackeit", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrzk6m/i_have_an_extra_gcp_exam_voucher/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrzk6m/i_have_an_extra_gcp_exam_voucher/", "subreddit_subscribers": 79606, "created_utc": 1668135910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Friends, we built a bronze-silver-gold datastack, where silver is dbt running on databricks. Our gold is Postgres and Elastic so we are looking for out of the box solutions for moving data from delta to pg and elastic. I wonder if anybody can share experience with Airbyte, Arcion or Rivery? Especially in terms of merge strategies and schema translation.So far we are doing this ingestion with spark.", "author_fullname": "t2_bgjdjmxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cross warehouse ETLs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrjswf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668097028.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Friends, we built a bronze-silver-gold datastack, where silver is dbt running on databricks. Our gold is Postgres and Elastic so we are looking for out of the box solutions for moving data from delta to pg and elastic. I wonder if anybody can share experience with Airbyte, Arcion or Rivery? Especially in terms of merge strategies and schema translation.So far we are doing this ingestion with spark.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yrjswf", "is_robot_indexable": true, "report_reasons": null, "author": "No_Sorbet_4345", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrjswf/cross_warehouse_etls/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrjswf/cross_warehouse_etls/", "subreddit_subscribers": 79606, "created_utc": 1668097028.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen it recommended heaps, essentially anytime someone asks about getting jobs, esp FANNY jobs\n\nI've been more interested project based learning, but after someone mentioned they used it and it helped a lot with SQL query optimisation I decided to have a look\n\nSo far, it seems pretty trask imo. The SQL questions I've looked at so far are incredibly basic, even the medium ones. Majority of  the hard ones seem to be on premium, hence I'm wondering if you people (leetcode sales reps/advocates) tend to pay for these?", "author_fullname": "t2_1w1o79i7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "do you ppl pay for leetcode or nah?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrjs1m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668096980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen it recommended heaps, essentially anytime someone asks about getting jobs, esp FANNY jobs&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been more interested project based learning, but after someone mentioned they used it and it helped a lot with SQL query optimisation I decided to have a look&lt;/p&gt;\n\n&lt;p&gt;So far, it seems pretty trask imo. The SQL questions I&amp;#39;ve looked at so far are incredibly basic, even the medium ones. Majority of  the hard ones seem to be on premium, hence I&amp;#39;m wondering if you people (leetcode sales reps/advocates) tend to pay for these?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yrjs1m", "is_robot_indexable": true, "report_reasons": null, "author": "tea_horse", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yrjs1m/do_you_ppl_pay_for_leetcode_or_nah/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yrjs1m/do_you_ppl_pay_for_leetcode_or_nah/", "subreddit_subscribers": 79606, "created_utc": 1668096980.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}