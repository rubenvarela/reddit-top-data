{"kind": "Listing", "data": {"after": "t3_ys8stc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I live off grid, living completely from solar power. I've done so for the past five years, and everything is pretty much sorted out at this point. I am here to ask for advice related to my data hoard though, please hear my explanation, it may be a unique situation. I've done my own research &amp; am pretty knowledgeable, but it's been two years and I still haven't found an ideal solution, so that's why I'm coming here for advice. I appreciate your patience.\n\nAmong the usual life's worth of photos, movies, music, ancient software, media etc, my hoard is a complete collection of video game ROMs and disc images (ISOs) for every console. Near-complete libraries, and many rare and custom curated ROMs as well. I use this for hobbies and my side hustle, where I repair and mod game systems, often backing up data for customers to my 6TB SSD.\n\nRight now this data is on a 6TB Samsung 2.5\" SATA SSD which is inside my windows 10 laptop. It is not the OS boot drive.\n\nI have three full size 3.5\" SATA HDDs that are 6TB each. Two WD Blue and one WD Enterprise which I manually copy the SSD to with (awesome) teracopy software, using a two bay SATA USB3.1 dock which also has an offline cloning feature that I stopped using. This is not automated at all, and takes several days to copy &amp; verify checksum with teracopy. Because I'm on solar, I also suspend the Teracopy overnight and start it again where it left off every morning. This requires that I take time to deliberately update copies, and there doesn't appear to be an easy way with teracopy to \"update\" the copy to a mirror of the original drive. I have to either format and do it all again, or copy all the new data to the drive, no way to automatically remove data from the backup that is no longer on the original. (like if I were to prune out several redundant folders, for example)\n\nSo I'm looking for an automated solution. My goal is just to not have to worry about losing all this data, or over a month of work, and having to remember to manually copy &amp; monitor the process.\n\nI've tried many other methods, but haven't found anything ideal for my situation and lifestyle. Ideally I would have two of these HDDs connected in the usb dock, one being copied from the SSD weekly and the other being copied daily, whenever I have the dock powered on and connected.\n\nHere's some methods I've tried\n\n\\- mirrored volumes in windows 10 diskmgmt will not work because the HDDs I'm mirroring to are connected via USB\n\n\\- Windows 10 storage pools seems like it requires the drives to be empty before starting a RAID1 setup, and I don't think there's an option to have one drive copied to two external drives at the same time. Also I don't think it's possible to begin this process without losing data on the original drive to be copied, meaning I'd have to wipe everything, then move data from my sole remaining copy.\n\n\\- Easus and similar backup software trials I've tried create disk images that seems to always become corrupted, though the scheduling features seem pretty good.\n\n\\- Online backup is *not* an option. Most of the time I'm working with very slow internet speeds.\n\n\\- ~~NAS is not an option, there is no network that is always connected for me to create a NAS. I suppose that means I'm using DAS.~~  (edit: a NAS would be nice down the line but for now I'm restricted by budget)\n\n\\- It's not feasible nor is it possible for me to have *any* device that is always on 24/7. Like I said above, at night the devices are asleep, but often need to be fully powered off. Any devices drawing power could become a burden, even during the day, if it's the cloudy season. (I have enough power for another device, but it would need to make efficient use of power, and the more low-powered, the better.)\n\nOver the past couple of years doing manual backups, I've had some close calls &amp; hardware failures where I've gone to a month old backup and almost lost it all. Manually backing up this way is not reliable.\n\nSo with all that said, bottom line (tldr) is I'm looking for software to run on my windows 10 PC. Adding more devices is not ideal for me for many reasons.\n\n**(EDIT: An important portion of my OP was accidentally deleted while I was making final edits... The part describing the media I copy the SSD to. I've added it back in.)**", "author_fullname": "t2_15pjfcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for Automated Copying of my Off Grid 6TB Media Hoard :)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrrhwt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 129, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 129, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668192422.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668113559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I live off grid, living completely from solar power. I&amp;#39;ve done so for the past five years, and everything is pretty much sorted out at this point. I am here to ask for advice related to my data hoard though, please hear my explanation, it may be a unique situation. I&amp;#39;ve done my own research &amp;amp; am pretty knowledgeable, but it&amp;#39;s been two years and I still haven&amp;#39;t found an ideal solution, so that&amp;#39;s why I&amp;#39;m coming here for advice. I appreciate your patience.&lt;/p&gt;\n\n&lt;p&gt;Among the usual life&amp;#39;s worth of photos, movies, music, ancient software, media etc, my hoard is a complete collection of video game ROMs and disc images (ISOs) for every console. Near-complete libraries, and many rare and custom curated ROMs as well. I use this for hobbies and my side hustle, where I repair and mod game systems, often backing up data for customers to my 6TB SSD.&lt;/p&gt;\n\n&lt;p&gt;Right now this data is on a 6TB Samsung 2.5&amp;quot; SATA SSD which is inside my windows 10 laptop. It is not the OS boot drive.&lt;/p&gt;\n\n&lt;p&gt;I have three full size 3.5&amp;quot; SATA HDDs that are 6TB each. Two WD Blue and one WD Enterprise which I manually copy the SSD to with (awesome) teracopy software, using a two bay SATA USB3.1 dock which also has an offline cloning feature that I stopped using. This is not automated at all, and takes several days to copy &amp;amp; verify checksum with teracopy. Because I&amp;#39;m on solar, I also suspend the Teracopy overnight and start it again where it left off every morning. This requires that I take time to deliberately update copies, and there doesn&amp;#39;t appear to be an easy way with teracopy to &amp;quot;update&amp;quot; the copy to a mirror of the original drive. I have to either format and do it all again, or copy all the new data to the drive, no way to automatically remove data from the backup that is no longer on the original. (like if I were to prune out several redundant folders, for example)&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m looking for an automated solution. My goal is just to not have to worry about losing all this data, or over a month of work, and having to remember to manually copy &amp;amp; monitor the process.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried many other methods, but haven&amp;#39;t found anything ideal for my situation and lifestyle. Ideally I would have two of these HDDs connected in the usb dock, one being copied from the SSD weekly and the other being copied daily, whenever I have the dock powered on and connected.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s some methods I&amp;#39;ve tried&lt;/p&gt;\n\n&lt;p&gt;- mirrored volumes in windows 10 diskmgmt will not work because the HDDs I&amp;#39;m mirroring to are connected via USB&lt;/p&gt;\n\n&lt;p&gt;- Windows 10 storage pools seems like it requires the drives to be empty before starting a RAID1 setup, and I don&amp;#39;t think there&amp;#39;s an option to have one drive copied to two external drives at the same time. Also I don&amp;#39;t think it&amp;#39;s possible to begin this process without losing data on the original drive to be copied, meaning I&amp;#39;d have to wipe everything, then move data from my sole remaining copy.&lt;/p&gt;\n\n&lt;p&gt;- Easus and similar backup software trials I&amp;#39;ve tried create disk images that seems to always become corrupted, though the scheduling features seem pretty good.&lt;/p&gt;\n\n&lt;p&gt;- Online backup is &lt;em&gt;not&lt;/em&gt; an option. Most of the time I&amp;#39;m working with very slow internet speeds.&lt;/p&gt;\n\n&lt;p&gt;- &lt;del&gt;NAS is not an option, there is no network that is always connected for me to create a NAS. I suppose that means I&amp;#39;m using DAS.&lt;/del&gt;  (edit: a NAS would be nice down the line but for now I&amp;#39;m restricted by budget)&lt;/p&gt;\n\n&lt;p&gt;- It&amp;#39;s not feasible nor is it possible for me to have &lt;em&gt;any&lt;/em&gt; device that is always on 24/7. Like I said above, at night the devices are asleep, but often need to be fully powered off. Any devices drawing power could become a burden, even during the day, if it&amp;#39;s the cloudy season. (I have enough power for another device, but it would need to make efficient use of power, and the more low-powered, the better.)&lt;/p&gt;\n\n&lt;p&gt;Over the past couple of years doing manual backups, I&amp;#39;ve had some close calls &amp;amp; hardware failures where I&amp;#39;ve gone to a month old backup and almost lost it all. Manually backing up this way is not reliable.&lt;/p&gt;\n\n&lt;p&gt;So with all that said, bottom line (tldr) is I&amp;#39;m looking for software to run on my windows 10 PC. Adding more devices is not ideal for me for many reasons.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;(EDIT: An important portion of my OP was accidentally deleted while I was making final edits... The part describing the media I copy the SSD to. I&amp;#39;ve added it back in.)&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yrrhwt", "is_robot_indexable": true, "report_reasons": null, "author": "nessinreallife", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yrrhwt/advice_for_automated_copying_of_my_off_grid_6tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yrrhwt/advice_for_automated_copying_of_my_off_grid_6tb/", "subreddit_subscribers": 653013, "created_utc": 1668113559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Humble Bundle Downloader (bulk mass downloader) which \u2728 new feature \u2728 would you like me to add ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 118, "top_awarded_type": null, "hide_score": false, "name": "t3_ys6cbb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 70, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_4r40mj7", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 70, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dJ0s6KU1TCVT5fo-K4cU6vaS621fayGDGPve-Q0ymcY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "humblebundles", "selftext": "Hi everyone ! I am the developer of the Firefox (yes only Firefox \ud83e\udd2a that's how I support them) add-on `Humble Bundle Downloader` (you can find [here](https://addons.mozilla.org/firefox/addon/humble-bundle-downloader/)), that allows you to download all the ebooks, properly renamed of your purchased bundle by file extension (you select in the popup).\n\nFirst of all thanks \ud83e\udd70 for your support for your 4/5 stars  \ud83d\ude4f \n\nNow that I finally stabilized the code and cleaned-up the file naming, I was wondering **if you needed 1 more feature, what would it be** (please give details) ?   \nPer example I had a few users asking me for a way to customize the download path that is now :  \n`Your_own_download_folder/HumbleBundle/{bundleName}/{fileExtension}/{ebookTitle} - {publisher}.{fileExtension})`\n\n&amp;#x200B;\n\n[Humble Bundle Downloader bulk mass](https://preview.redd.it/kzqbwp3nmaz91.png?width=1724&amp;format=png&amp;auto=webp&amp;s=9092ab23c377d63360d3419d1bb6cd6832dcca87)", "author_fullname": "t2_4r40mj7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Humble Bundle Downloader (bulk mass downloader) which \u2728 new feature \u2728 would you like me to add ?", "link_flair_richtext": [{"e": "text", "t": "Book Bundle"}], "subreddit_name_prefixed": "r/humblebundles", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 118, "top_awarded_type": null, "hide_score": false, "media_metadata": {"kzqbwp3nmaz91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 91, "x": 108, "u": "https://preview.redd.it/kzqbwp3nmaz91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=daa9f122ba2ceaa9a31c4e1062d4971bccd6a22e"}, {"y": 182, "x": 216, "u": "https://preview.redd.it/kzqbwp3nmaz91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bbd53e5c19f678b40dcb5afe45f84c5ef5820fbd"}, {"y": 269, "x": 320, "u": "https://preview.redd.it/kzqbwp3nmaz91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0f5b7c68594c8a9b5cad2d6703a6906146b58a4"}, {"y": 539, "x": 640, "u": "https://preview.redd.it/kzqbwp3nmaz91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e73f507f5002c134ba01b230c19347f2eff1ee5c"}, {"y": 809, "x": 960, "u": "https://preview.redd.it/kzqbwp3nmaz91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f53ea45b71a024bc8e02143ed9a64f31b6468767"}, {"y": 910, "x": 1080, "u": "https://preview.redd.it/kzqbwp3nmaz91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7edba3be1d9d094e5bc813da94aa3dcc996c87b6"}], "s": {"y": 1454, "x": 1724, "u": "https://preview.redd.it/kzqbwp3nmaz91.png?width=1724&amp;format=png&amp;auto=webp&amp;s=9092ab23c377d63360d3419d1bb6cd6832dcca87"}, "id": "kzqbwp3nmaz91"}}, "name": "t3_ys677z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Book Bundle", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dJ0s6KU1TCVT5fo-K4cU6vaS621fayGDGPve-Q0ymcY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1668160382.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.humblebundles", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone ! I am the developer of the Firefox (yes only Firefox \ud83e\udd2a that&amp;#39;s how I support them) add-on &lt;code&gt;Humble Bundle Downloader&lt;/code&gt; (you can find &lt;a href=\"https://addons.mozilla.org/firefox/addon/humble-bundle-downloader/\"&gt;here&lt;/a&gt;), that allows you to download all the ebooks, properly renamed of your purchased bundle by file extension (you select in the popup).&lt;/p&gt;\n\n&lt;p&gt;First of all thanks \ud83e\udd70 for your support for your 4/5 stars  \ud83d\ude4f &lt;/p&gt;\n\n&lt;p&gt;Now that I finally stabilized the code and cleaned-up the file naming, I was wondering &lt;strong&gt;if you needed 1 more feature, what would it be&lt;/strong&gt; (please give details) ?&lt;br/&gt;\nPer example I had a few users asking me for a way to customize the download path that is now :&lt;br/&gt;\n&lt;code&gt;Your_own_download_folder/HumbleBundle/{bundleName}/{fileExtension}/{ebookTitle} - {publisher}.{fileExtension})&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kzqbwp3nmaz91.png?width=1724&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9092ab23c377d63360d3419d1bb6cd6832dcca87\"&gt;Humble Bundle Downloader bulk mass&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?auto=webp&amp;s=f0cf82bd3af87b1244149a748e5aff70dc88d3ae", "width": 1724, "height": 1454}, "resolutions": [{"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=791610911a0dcb296d7de58f9c4bed61dc945719", "width": 108, "height": 91}, {"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=edc501713279e0fb5a4d45232955d7b6b0cfdf11", "width": 216, "height": 182}, {"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b802a8d0aeefc61e09b5f30e44b2d77deba9854", "width": 320, "height": 269}, {"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8631311b8de79b974266e4468d969cc0050a6889", "width": 640, "height": 539}, {"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a7ca6e537f71977382427d217a4e9b2b5bf1c9f", "width": 960, "height": 809}, {"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9a0e137821fe0b91c0466f3640534d5afaf64f8a", "width": 1080, "height": 910}], "variants": {}, "id": "qYjOo1UrfPE8KGIqHVo7udv785SPYk47-w36tJNpJ3s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1f5f7e48-24a2-11e8-b660-0ea31c109b4c", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2xbzi", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dc4031", "id": "ys677z", "is_robot_indexable": true, "report_reasons": null, "author": "behrouze", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/humblebundles/comments/ys677z/humble_bundle_downloader_bulk_mass_downloader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/humblebundles/comments/ys677z/humble_bundle_downloader_bulk_mass_downloader/", "subreddit_subscribers": 85903, "created_utc": 1668160382.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1668160923.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.humblebundles", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/humblebundles/comments/ys677z/humble_bundle_downloader_bulk_mass_downloader/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?auto=webp&amp;s=f0cf82bd3af87b1244149a748e5aff70dc88d3ae", "width": 1724, "height": 1454}, "resolutions": [{"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=791610911a0dcb296d7de58f9c4bed61dc945719", "width": 108, "height": 91}, {"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=edc501713279e0fb5a4d45232955d7b6b0cfdf11", "width": 216, "height": 182}, {"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b802a8d0aeefc61e09b5f30e44b2d77deba9854", "width": 320, "height": 269}, {"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8631311b8de79b974266e4468d969cc0050a6889", "width": 640, "height": 539}, {"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a7ca6e537f71977382427d217a4e9b2b5bf1c9f", "width": 960, "height": 809}, {"url": "https://external-preview.redd.it/UdNQyKh-hojUDjfKJPrRKijdVzUuBjldp6w8nRZV9Yg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9a0e137821fe0b91c0466f3640534d5afaf64f8a", "width": 1080, "height": 910}], "variants": {}, "id": "qYjOo1UrfPE8KGIqHVo7udv785SPYk47-w36tJNpJ3s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ys6cbb", "is_robot_indexable": true, "report_reasons": null, "author": "behrouze", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_ys677z", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ys6cbb/humble_bundle_downloader_bulk_mass_downloader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/humblebundles/comments/ys677z/humble_bundle_downloader_bulk_mass_downloader/", "subreddit_subscribers": 653013, "created_utc": 1668160923.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My FIL is a movie lover.  I have terabytes of media, I'd love to share with him but he really can't manage navigating a Plex share and I can't afford the time to burn DVDs for him.  I'm wondering if there is some kind of media player that has storage that will plug directly into his TV, can manage H.265 compression, and has an easy to navigate menu system.  In this way I can just do a \"sneakernet\" to this player from my NAS to keep him up to date on new releases.  Any suggestions welcome!  Thanks!", "author_fullname": "t2_czhff6xi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Regarding media storage ...some advice please", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrt6nn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Advice", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668117884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My FIL is a movie lover.  I have terabytes of media, I&amp;#39;d love to share with him but he really can&amp;#39;t manage navigating a Plex share and I can&amp;#39;t afford the time to burn DVDs for him.  I&amp;#39;m wondering if there is some kind of media player that has storage that will plug directly into his TV, can manage H.265 compression, and has an easy to navigate menu system.  In this way I can just do a &amp;quot;sneakernet&amp;quot; to this player from my NAS to keep him up to date on new releases.  Any suggestions welcome!  Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yrt6nn", "is_robot_indexable": true, "report_reasons": null, "author": "NewsboyHank", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yrt6nn/regarding_media_storage_some_advice_please/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yrt6nn/regarding_media_storage_some_advice_please/", "subreddit_subscribers": 653013, "created_utc": 1668117884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The storage space on my Acer Aspire 5 laptop is soon going to run out. It has a 1TB m.2 NVME, which I really like, but there is no second m.2 slot. However, there is expansion space for a 2.5 inch SATA drive. I'm planning to get a 2TB secondary SSD that I will be using for games, photos and most other things that don't necessarily benefit from the high NVME speeds. \n\nFurthermore, I'm trying to get as much out of this drive in terms of longevity as I will be able to. My laptop is 2,5 years old and I'll likely be using it for another 2-4 years. After that I will either build the SSD into a PC, or if I'm not going for a PC build and buying another laptop instead I'm guessing that it will probably not come with expansion space for a 2.5 inch drive. My plan then is to buy an enclosure from Amazon to turn it into a USB-C external drive that I will be using as a backup drive for photos, important files and also some less important files. It therefore won't require a ton of reads and writes anymore at that point and will likely just sit around a lot with the data. (Unless this is for some reason a bad idea?).\n\nSo my question is should I go for the MX500, which seems to be the S-tier SATA SSD in terms of price/performance or is that overkill in my case, since I will not be using it as a primary drive? It's clear to me that I need a drive with TLC and not QLC, but does it need DRAM? For example there are SSDs like the Silicon Power Ace A55 which are TLC but without DRAM for 2/3 of the price. Will the difference in build quality matter in terms of longevity? or is there another one that you can recommend me for my situation?\n\nEDIT: Thanks to everyone for the replies! I really appreciate how reddit can help me connect with and ask questions to enthusiasts in one specific field. \n\nYou guys convinced me to go for an SSD with DRAM.\n \nI'll get one of these three:\n\n\n\n-Crucial MX500\n\n-Sandisk Ultra 3D/Western Digital Blue 3D\n\n-Samsung 870 EVO\n\nWill be continuously checking for price drops, especially around black friday, but if it hasn't dropped by December i'll pull the trigger anyway.", "author_fullname": "t2_vlkrm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I need a high quality SSD like the MX500 as a secondary drive in my laptop?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrug3h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668162302.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668121119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The storage space on my Acer Aspire 5 laptop is soon going to run out. It has a 1TB m.2 NVME, which I really like, but there is no second m.2 slot. However, there is expansion space for a 2.5 inch SATA drive. I&amp;#39;m planning to get a 2TB secondary SSD that I will be using for games, photos and most other things that don&amp;#39;t necessarily benefit from the high NVME speeds. &lt;/p&gt;\n\n&lt;p&gt;Furthermore, I&amp;#39;m trying to get as much out of this drive in terms of longevity as I will be able to. My laptop is 2,5 years old and I&amp;#39;ll likely be using it for another 2-4 years. After that I will either build the SSD into a PC, or if I&amp;#39;m not going for a PC build and buying another laptop instead I&amp;#39;m guessing that it will probably not come with expansion space for a 2.5 inch drive. My plan then is to buy an enclosure from Amazon to turn it into a USB-C external drive that I will be using as a backup drive for photos, important files and also some less important files. It therefore won&amp;#39;t require a ton of reads and writes anymore at that point and will likely just sit around a lot with the data. (Unless this is for some reason a bad idea?).&lt;/p&gt;\n\n&lt;p&gt;So my question is should I go for the MX500, which seems to be the S-tier SATA SSD in terms of price/performance or is that overkill in my case, since I will not be using it as a primary drive? It&amp;#39;s clear to me that I need a drive with TLC and not QLC, but does it need DRAM? For example there are SSDs like the Silicon Power Ace A55 which are TLC but without DRAM for 2/3 of the price. Will the difference in build quality matter in terms of longevity? or is there another one that you can recommend me for my situation?&lt;/p&gt;\n\n&lt;p&gt;EDIT: Thanks to everyone for the replies! I really appreciate how reddit can help me connect with and ask questions to enthusiasts in one specific field. &lt;/p&gt;\n\n&lt;p&gt;You guys convinced me to go for an SSD with DRAM.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll get one of these three:&lt;/p&gt;\n\n&lt;p&gt;-Crucial MX500&lt;/p&gt;\n\n&lt;p&gt;-Sandisk Ultra 3D/Western Digital Blue 3D&lt;/p&gt;\n\n&lt;p&gt;-Samsung 870 EVO&lt;/p&gt;\n\n&lt;p&gt;Will be continuously checking for price drops, especially around black friday, but if it hasn&amp;#39;t dropped by December i&amp;#39;ll pull the trigger anyway.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yrug3h", "is_robot_indexable": true, "report_reasons": null, "author": "TheBirdOfFire", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yrug3h/do_i_need_a_high_quality_ssd_like_the_mx500_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yrug3h/do_i_need_a_high_quality_ssd_like_the_mx500_as_a/", "subreddit_subscribers": 653013, "created_utc": 1668121119.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm an IT engineer and keep lots of my notes in OneNote. Its quick and easy. But recently want to transfer some to my website only to discover there is no right click save image in OneNote.\n\nSo the question is, doe anyone know of any way of exporting all the images? I once came across paid for software that claimed to do it. But don't want to pay for it if it ends up the GUI app is just running some powershell commands in the background.\n\nAnyone know?", "author_fullname": "t2_3xqpnc72", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting images from OneNote", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrv4hu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668122899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an IT engineer and keep lots of my notes in OneNote. Its quick and easy. But recently want to transfer some to my website only to discover there is no right click save image in OneNote.&lt;/p&gt;\n\n&lt;p&gt;So the question is, doe anyone know of any way of exporting all the images? I once came across paid for software that claimed to do it. But don&amp;#39;t want to pay for it if it ends up the GUI app is just running some powershell commands in the background.&lt;/p&gt;\n\n&lt;p&gt;Anyone know?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yrv4hu", "is_robot_indexable": true, "report_reasons": null, "author": "steviefaux", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yrv4hu/extracting_images_from_onenote/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yrv4hu/extracting_images_from_onenote/", "subreddit_subscribers": 653013, "created_utc": 1668122899.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Essentially I'm looking for a script or program that would automate the process of downloading each of my bookmarks and saving them as in individual .html files (or something similar that accomplishes the same thing) in the event that twitter blows up for good and goes offline - or I get locked out by a paywall that I will never, under any circumstance, pay to use the bird app. \n\nI have several years worth of linux tips, paper recommendations, advice threads, weird memes, etc piled up in my twitter bookmarks that I don't want to lose access to.", "author_fullname": "t2_rv5zodom", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to automate downloading copies of all of my twitter bookmarks / likes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys1hg9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668142205.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Essentially I&amp;#39;m looking for a script or program that would automate the process of downloading each of my bookmarks and saving them as in individual .html files (or something similar that accomplishes the same thing) in the event that twitter blows up for good and goes offline - or I get locked out by a paywall that I will never, under any circumstance, pay to use the bird app. &lt;/p&gt;\n\n&lt;p&gt;I have several years worth of linux tips, paper recommendations, advice threads, weird memes, etc piled up in my twitter bookmarks that I don&amp;#39;t want to lose access to.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ys1hg9", "is_robot_indexable": true, "report_reasons": null, "author": "threads_of_measure", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ys1hg9/is_there_a_way_to_automate_downloading_copies_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ys1hg9/is_there_a_way_to_automate_downloading_copies_of/", "subreddit_subscribers": 653013, "created_utc": 1668142205.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I used to open Minecraft servers using my old pc but now I don't anymore. I want to upgrade this pc for NAS, media servers (Kodi, Jellyfin, Plex), and a faster Minecraft server. What should parts should I upgrade it?  \nMainboard: ASUS P8H61-M LE  \nCPU: i3-3220  \nRAM : 8Gb DDR3 ram (2 x SAMSUNG 4GB DDR3 BUS 1600Mhz)  \nOr is it a bad idea?\n\nI also have a Buffalo LinkStation Pro Quad(No HDD). What should I do with it?  \nThank you so much for your help!! (Sorry for the bad English)", "author_fullname": "t2_4gb3ngwx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on upgrading my old computer for DIY NAS/Minecraft/Media Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys7bf5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668164700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to open Minecraft servers using my old pc but now I don&amp;#39;t anymore. I want to upgrade this pc for NAS, media servers (Kodi, Jellyfin, Plex), and a faster Minecraft server. What should parts should I upgrade it?&lt;br/&gt;\nMainboard: ASUS P8H61-M LE&lt;br/&gt;\nCPU: i3-3220&lt;br/&gt;\nRAM : 8Gb DDR3 ram (2 x SAMSUNG 4GB DDR3 BUS 1600Mhz)&lt;br/&gt;\nOr is it a bad idea?&lt;/p&gt;\n\n&lt;p&gt;I also have a Buffalo LinkStation Pro Quad(No HDD). What should I do with it?&lt;br/&gt;\nThank you so much for your help!! (Sorry for the bad English)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ys7bf5", "is_robot_indexable": true, "report_reasons": null, "author": "NukerDucker", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ys7bf5/need_advice_on_upgrading_my_old_computer_for_diy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ys7bf5/need_advice_on_upgrading_my_old_computer_for_diy/", "subreddit_subscribers": 653013, "created_utc": 1668164700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "To speed up time machine operations you use this: `sudo sysctl debug.lowpri_throttle_enabled=0`\n\nI wonder if there's a similar trick to fasten metadata operations for external disks in macOS\n\nCurrently the external disk is not used by anything... other than  RdMeta[AP], WrMeta[AP], RdMeta[ST3], WrMeta[ST3] according to `sudo fs_usage -f diskio -t ...`", "author_fullname": "t2_tler9zon", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to speed up metadata writing/reading process in macOS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysdcq5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668179900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To speed up time machine operations you use this: &lt;code&gt;sudo sysctl debug.lowpri_throttle_enabled=0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I wonder if there&amp;#39;s a similar trick to fasten metadata operations for external disks in macOS&lt;/p&gt;\n\n&lt;p&gt;Currently the external disk is not used by anything... other than  RdMeta[AP], WrMeta[AP], RdMeta[ST3], WrMeta[ST3] according to &lt;code&gt;sudo fs_usage -f diskio -t ...&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ysdcq5", "is_robot_indexable": true, "report_reasons": null, "author": "nonononononoou", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ysdcq5/how_to_speed_up_metadata_writingreading_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ysdcq5/how_to_speed_up_metadata_writingreading_process/", "subreddit_subscribers": 653013, "created_utc": 1668179900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know the direct record link that I can paste into total recorder and record on a timer?\n\naudacy stupidly moved all their signals in house, so places like stream the world stopped working.  I need to record a show for the next few months until my guys leaves the station. \n\n&amp;#x200B;\n\nthanks :)", "author_fullname": "t2_759rirfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "wcbs fm direct record link?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys79z9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668164538.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know the direct record link that I can paste into total recorder and record on a timer?&lt;/p&gt;\n\n&lt;p&gt;audacy stupidly moved all their signals in house, so places like stream the world stopped working.  I need to record a show for the next few months until my guys leaves the station. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ys79z9", "is_robot_indexable": true, "report_reasons": null, "author": "HorribleEmulator", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ys79z9/wcbs_fm_direct_record_link/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ys79z9/wcbs_fm_direct_record_link/", "subreddit_subscribers": 653013, "created_utc": 1668164538.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to scrape tweets in a visual manner, I used a tool  github/shot-scraper (which is based on Chrome web driver &amp; ffmpeg) to generate pngs successfully, but I am facing few problems.\n\n**Issues:**\n\n\\- Screenshots are processed with twitter login-to-view popup, which blocks the rendered screen and ruins the screenshots too.  \n\\- I know these popups can be avoided by using json authentication and excluding elements using javascript , (I don't have programming skills, don't have twitter API as well)\n\n**Goals:**\n\nI want to scrape a public twitter account using a bot that crawls for new tweets and screenshots them as soon they are posted.  (Media content is not necessary, just screenshot of tweet is fine).\n\nI know there are other tools available that save tweets as CSV, JSON but I am looking for Visual/screenshots. \n\nSuggestions are welcome.", "author_fullname": "t2_mcxnwm4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scraping twitter...and some problems.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrw6px", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668125820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to scrape tweets in a visual manner, I used a tool  github/shot-scraper (which is based on Chrome web driver &amp;amp; ffmpeg) to generate pngs successfully, but I am facing few problems.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Issues:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Screenshots are processed with twitter login-to-view popup, which blocks the rendered screen and ruins the screenshots too.&lt;br/&gt;\n- I know these popups can be avoided by using json authentication and excluding elements using javascript , (I don&amp;#39;t have programming skills, don&amp;#39;t have twitter API as well)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Goals:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I want to scrape a public twitter account using a bot that crawls for new tweets and screenshots them as soon they are posted.  (Media content is not necessary, just screenshot of tweet is fine).&lt;/p&gt;\n\n&lt;p&gt;I know there are other tools available that save tweets as CSV, JSON but I am looking for Visual/screenshots. &lt;/p&gt;\n\n&lt;p&gt;Suggestions are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yrw6px", "is_robot_indexable": true, "report_reasons": null, "author": "nikkytor", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yrw6px/scraping_twitterand_some_problems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yrw6px/scraping_twitterand_some_problems/", "subreddit_subscribers": 653013, "created_utc": 1668125820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone!\n\nI have a question for you who are surely more experienced than me about collecting data: I would need to download all the images on this site ([www.antweb.org](https://www.antweb.org)) for a project I am working on (as well as for data hoarding, of course ;D). Could you illustrate a way to do this automatically (or at least semi-automatically)?\n\nIdeally, I would like to be able to download all the images related to a single ant genus (e.g., all those on this page \"[https://www.antweb.org/browse.do?subfamily=myrmicinae&amp;genus=messor&amp;rank=genus](https://www.antweb.org/browse.do?subfamily=myrmicinae&amp;genus=messor&amp;rank=genus)\") into a single folder, and then do the same with the other genera I need.\n\nCan you tell me if there is a program or something similar that can help me in this endeavor? Or am I doomed to download all the images I need one by one, manually?\n\nThank you for your help!", "author_fullname": "t2_i64b9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to automatically download images from antweb.org", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrvf9c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668123740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I have a question for you who are surely more experienced than me about collecting data: I would need to download all the images on this site (&lt;a href=\"https://www.antweb.org\"&gt;www.antweb.org&lt;/a&gt;) for a project I am working on (as well as for data hoarding, of course ;D). Could you illustrate a way to do this automatically (or at least semi-automatically)?&lt;/p&gt;\n\n&lt;p&gt;Ideally, I would like to be able to download all the images related to a single ant genus (e.g., all those on this page &amp;quot;&lt;a href=\"https://www.antweb.org/browse.do?subfamily=myrmicinae&amp;amp;genus=messor&amp;amp;rank=genus\"&gt;https://www.antweb.org/browse.do?subfamily=myrmicinae&amp;amp;genus=messor&amp;amp;rank=genus&lt;/a&gt;&amp;quot;) into a single folder, and then do the same with the other genera I need.&lt;/p&gt;\n\n&lt;p&gt;Can you tell me if there is a program or something similar that can help me in this endeavor? Or am I doomed to download all the images I need one by one, manually?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bhZkXmPwLo_9h3t_HSC4PBw5ttlfkOwNrEr3FEz7_YA.jpg?auto=webp&amp;s=95547d9469c2b971745c5f077ae581f0d7f67dba", "width": 125, "height": 25}, "resolutions": [{"url": "https://external-preview.redd.it/bhZkXmPwLo_9h3t_HSC4PBw5ttlfkOwNrEr3FEz7_YA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=008af2bbafaeefd04bd3eaee0e0d203b1a8f866a", "width": 108, "height": 21}], "variants": {}, "id": "mwFSSdBg0Vqoedk2tuxciRdpdLWWTRyB2Gpgcw9J598"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yrvf9c", "is_robot_indexable": true, "report_reasons": null, "author": "Franz91", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yrvf9c/how_to_automatically_download_images_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yrvf9c/how_to_automatically_download_images_from/", "subreddit_subscribers": 653013, "created_utc": 1668123740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Can't seem to find anything anywhere doing anything with readly lately.", "author_fullname": "t2_c3cht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone having any luck pulling magazines off of Readly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yru176", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668120018.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can&amp;#39;t seem to find anything anywhere doing anything with readly lately.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yru176", "is_robot_indexable": true, "report_reasons": null, "author": "lady_azkadelia", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yru176/anyone_having_any_luck_pulling_magazines_off_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yru176/anyone_having_any_luck_pulling_magazines_off_of/", "subreddit_subscribers": 653013, "created_utc": 1668120018.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have an external hard drive that's probably about four or five years old now, I don't have the box for it anymore but I'm pretty sure it's a SeaGate. Based on the fact that it's only like 1 inch thin I'm certain it's an SSD external drive.\n\nHow reliable are SSD external hard drives? Should I expect it to fail soon? Like an old conventional magnetic disk drive could? It's kept on a desk at room temperature, never moves and just collects a bit of dust that I periodically clean off. Is this a safe device to keep pictures, music, files, etc. on? Or should I begin making plans to replace it?", "author_fullname": "t2_6i5a2b13", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are SSD External Hard Drives Reliable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrq4hx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668110412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an external hard drive that&amp;#39;s probably about four or five years old now, I don&amp;#39;t have the box for it anymore but I&amp;#39;m pretty sure it&amp;#39;s a SeaGate. Based on the fact that it&amp;#39;s only like 1 inch thin I&amp;#39;m certain it&amp;#39;s an SSD external drive.&lt;/p&gt;\n\n&lt;p&gt;How reliable are SSD external hard drives? Should I expect it to fail soon? Like an old conventional magnetic disk drive could? It&amp;#39;s kept on a desk at room temperature, never moves and just collects a bit of dust that I periodically clean off. Is this a safe device to keep pictures, music, files, etc. on? Or should I begin making plans to replace it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yrq4hx", "is_robot_indexable": true, "report_reasons": null, "author": "Daily__Reminder", "discussion_type": null, "num_comments": 13, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yrq4hx/are_ssd_external_hard_drives_reliable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yrq4hx/are_ssd_external_hard_drives_reliable/", "subreddit_subscribers": 653013, "created_utc": 1668110412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello guys,\n\nI\u2019m planning to buy an external hard drive to use sometimes, not to be always plugged to PC.\n\nI\u2019ve read some reviews here on Reddit about WD and it seems people don\u2019t quite like them, saying something about the USB connection being problematic if I understand correctly..\n\nIt\u2019s just that they are cheaper than the Seagate drives from example, and I wonder if it should be fine to the \u201caverage\u201d user - I mean it would suffice if it will just work as intended without failing suddenly.\n\nThanks in advance.", "author_fullname": "t2_16vs44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I buy a WD Portable 4/5tb?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys4jxg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668153548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m planning to buy an external hard drive to use sometimes, not to be always plugged to PC.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve read some reviews here on Reddit about WD and it seems people don\u2019t quite like them, saying something about the USB connection being problematic if I understand correctly..&lt;/p&gt;\n\n&lt;p&gt;It\u2019s just that they are cheaper than the Seagate drives from example, and I wonder if it should be fine to the \u201caverage\u201d user - I mean it would suffice if it will just work as intended without failing suddenly.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ys4jxg", "is_robot_indexable": true, "report_reasons": null, "author": "toktok159", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ys4jxg/should_i_buy_a_wd_portable_45tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ys4jxg/should_i_buy_a_wd_portable_45tb/", "subreddit_subscribers": 653013, "created_utc": 1668153548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Using MiniTool Tool Partition Wizard ver 10.1.\n\nI've got Win10 installed on a 120GB SSD and XP installed on a 500 GB SSD. I created a new 120GB partition on the 500GB drive so I could clone Win10 to the new 120GB partition and make a dual boot system off one drive .\n\nI can't find an option to do this. Does anybody know of a way?", "author_fullname": "t2_16ljkt33", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help Please - How to clone an entire disk to a partition and not a whole disk?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrr8mv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668112893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Using MiniTool Tool Partition Wizard ver 10.1.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got Win10 installed on a 120GB SSD and XP installed on a 500 GB SSD. I created a new 120GB partition on the 500GB drive so I could clone Win10 to the new 120GB partition and make a dual boot system off one drive .&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t find an option to do this. Does anybody know of a way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yrr8mv", "is_robot_indexable": true, "report_reasons": null, "author": "ddrfraser1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yrr8mv/help_please_how_to_clone_an_entire_disk_to_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yrr8mv/help_please_how_to_clone_an_entire_disk_to_a/", "subreddit_subscribers": 653013, "created_utc": 1668112893.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The case I have for my NAS has 12 HDD bays so instead of using the 8 SATA capable LSI controller, I opted for the 9201-16i. It's a great card, but I'm looking to upgrade my machine and I'm wondering if there is a newer card out there that is taking advantage of a newer version of PCIe in order to require fewer lanes. Granted with 12x HDDs I'm probably not even using 4 PCIe lanes, it's proven to be very difficult to find a motherboard that will support x16 and x8 at the same time, you definitely can do x16 and x4 at the same time though.\n\nMy end goal is to have a pretty beefy NAS that can use Intel quicksync for Plex transcodes, passthrough a GPU to a VM, have a 10Gb NIC, and can use at least 15 (probably 16) drives. I know it won't be cheap, but obviously I'm looking to do this as reasonably affordable as possible.", "author_fullname": "t2_5dnja", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Updated LSI 9201-16i?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yrqosf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668111595.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The case I have for my NAS has 12 HDD bays so instead of using the 8 SATA capable LSI controller, I opted for the 9201-16i. It&amp;#39;s a great card, but I&amp;#39;m looking to upgrade my machine and I&amp;#39;m wondering if there is a newer card out there that is taking advantage of a newer version of PCIe in order to require fewer lanes. Granted with 12x HDDs I&amp;#39;m probably not even using 4 PCIe lanes, it&amp;#39;s proven to be very difficult to find a motherboard that will support x16 and x8 at the same time, you definitely can do x16 and x4 at the same time though.&lt;/p&gt;\n\n&lt;p&gt;My end goal is to have a pretty beefy NAS that can use Intel quicksync for Plex transcodes, passthrough a GPU to a VM, have a 10Gb NIC, and can use at least 15 (probably 16) drives. I know it won&amp;#39;t be cheap, but obviously I&amp;#39;m looking to do this as reasonably affordable as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yrqosf", "is_robot_indexable": true, "report_reasons": null, "author": "FlexibleToast", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yrqosf/updated_lsi_920116i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yrqosf/updated_lsi_920116i/", "subreddit_subscribers": 653013, "created_utc": 1668111595.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Aside from the regular varieties of disc, I have seen Verbatim offer another kind of BD-R for data storage called DataLifePlus but I assume it's not good for archiving data for decades. Is M-disc the reliable gold standard?", "author_fullname": "t2_tlwbyabt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is M-disc the gold standard for backing up files on Blu-Ray discs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysfdgf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668184209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Aside from the regular varieties of disc, I have seen Verbatim offer another kind of BD-R for data storage called DataLifePlus but I assume it&amp;#39;s not good for archiving data for decades. Is M-disc the reliable gold standard?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ysfdgf", "is_robot_indexable": true, "report_reasons": null, "author": "nightdonut", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ysfdgf/is_mdisc_the_gold_standard_for_backing_up_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ysfdgf/is_mdisc_the_gold_standard_for_backing_up_files/", "subreddit_subscribers": 653013, "created_utc": 1668184209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_lxpfqyl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft on LinkedIn: After a devastating cyberattack, the Eastern Band of Cherokee Indians | 13 comments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_ys57kr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/E2eShB2GTShSC0eyIxBdaI-vBFP_f1c4EDmyoGzLmvA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668156271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/posts/microsoft_after-a-devastating-cyberattack-the-eastern-activity-6996248429471412224-XsqS?utm_source=share&amp;utm_medium=member_android", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aexyPX3SlxpLv2Qv8LISY2LVHz0S1JQ4qKni3EDOd_c.jpg?auto=webp&amp;s=e1b5bda7239c9defa0441dd7f3d04cc9175465fd", "width": 1199, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/aexyPX3SlxpLv2Qv8LISY2LVHz0S1JQ4qKni3EDOd_c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d4f15911ba7b781eda6ffa52c0722d21f0d61826", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/aexyPX3SlxpLv2Qv8LISY2LVHz0S1JQ4qKni3EDOd_c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c12a8eb94f4114c64c70b11918534f112fb84f0e", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/aexyPX3SlxpLv2Qv8LISY2LVHz0S1JQ4qKni3EDOd_c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=21558249daa4a8baa63941280f4edafaf4512f46", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/aexyPX3SlxpLv2Qv8LISY2LVHz0S1JQ4qKni3EDOd_c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=37fe01ed7f9cc8f29f902f048e0a3d649309765f", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/aexyPX3SlxpLv2Qv8LISY2LVHz0S1JQ4qKni3EDOd_c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=725b164fe5f2a793d9d7a04e3c5b704870bf35aa", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/aexyPX3SlxpLv2Qv8LISY2LVHz0S1JQ4qKni3EDOd_c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=537164dfdb35966df601cd69ae21afb9a257db25", "width": 1080, "height": 720}], "variants": {}, "id": "WlWKc8rAmciLZ6SC641cT3h_oB9lVTuojvsr5FV3MhQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ys57kr", "is_robot_indexable": true, "report_reasons": null, "author": "MOHdennisNL", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ys57kr/microsoft_on_linkedin_after_a_devastating/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/posts/microsoft_after-a-devastating-cyberattack-the-eastern-activity-6996248429471412224-XsqS?utm_source=share&amp;utm_medium=member_android", "subreddit_subscribers": 653013, "created_utc": 1668156271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So wondering if there is any real advantage to newer disk shelves?\n\nMostly power efficiency?\n\nI'm expecting to use pretty slow 5x00 rpm drives.", "author_fullname": "t2_13ju6y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wondering disk shelves old vs new?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys4xht", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": "", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668155045.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So wondering if there is any real advantage to newer disk shelves?&lt;/p&gt;\n\n&lt;p&gt;Mostly power efficiency?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m expecting to use pretty slow 5x00 rpm drives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "4TB WD Red", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "ys4xht", "is_robot_indexable": true, "report_reasons": null, "author": "corruptboomerang", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/ys4xht/wondering_disk_shelves_old_vs_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ys4xht/wondering_disk_shelves_old_vs_new/", "subreddit_subscribers": 653013, "created_utc": 1668155045.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I noticed for quite a while (even with another account) the Internet Archive is always giving 503 Slow down errors which prevent NEW uploads from going there. It's odd, because you can perfectly upload to a page that already exists, but you can't create a new one, otherwise 503 SLOW DOWN again.\n\nDid they just close for good and no new pages are allowed?\n\nCan someone check if the 503 Slow Down error is still there?", "author_fullname": "t2_1utoiwm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Internet Archive uploads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys1xjp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668143695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I noticed for quite a while (even with another account) the Internet Archive is always giving 503 Slow down errors which prevent NEW uploads from going there. It&amp;#39;s odd, because you can perfectly upload to a page that already exists, but you can&amp;#39;t create a new one, otherwise 503 SLOW DOWN again.&lt;/p&gt;\n\n&lt;p&gt;Did they just close for good and no new pages are allowed?&lt;/p&gt;\n\n&lt;p&gt;Can someone check if the 503 Slow Down error is still there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ys1xjp", "is_robot_indexable": true, "report_reasons": null, "author": "Maratocarde", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ys1xjp/internet_archive_uploads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ys1xjp/internet_archive_uploads/", "subreddit_subscribers": 653013, "created_utc": 1668143695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, this is my first post in here and im very new to this subreddit \n\nIm currently having a 1tb hdd but it's full. Im hoping to add storage but due to some problem i can't buy physical products like hdd, sdd, external hdd.,, etc. (Money is not at all a problem)but i want a alternative storage program or something from which i can mount it as a local disk\n And offline acess to every file uploaded here.\n\nThe Best alternative i have encountered is cloud storage but offline acess is a problem  because if i put a file to make available offline  it will be dowloaded to my local storage and i dont want that. \n\nmore specifically, i have a automated setup with seedbox, so when the files uploaded to my local storage. The files is completely acessible even when offline. I want mainly this feature without having to buy for a physical storage that is..(again money is not a problem)\n\n\nIf you think  this is a wrong subreddit to ask this question, pls tell where can i.", "author_fullname": "t2_qpydm8ri", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An alternative to cloud storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys4vqu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668154835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, this is my first post in here and im very new to this subreddit &lt;/p&gt;\n\n&lt;p&gt;Im currently having a 1tb hdd but it&amp;#39;s full. Im hoping to add storage but due to some problem i can&amp;#39;t buy physical products like hdd, sdd, external hdd.,, etc. (Money is not at all a problem)but i want a alternative storage program or something from which i can mount it as a local disk\n And offline acess to every file uploaded here.&lt;/p&gt;\n\n&lt;p&gt;The Best alternative i have encountered is cloud storage but offline acess is a problem  because if i put a file to make available offline  it will be dowloaded to my local storage and i dont want that. &lt;/p&gt;\n\n&lt;p&gt;more specifically, i have a automated setup with seedbox, so when the files uploaded to my local storage. The files is completely acessible even when offline. I want mainly this feature without having to buy for a physical storage that is..(again money is not a problem)&lt;/p&gt;\n\n&lt;p&gt;If you think  this is a wrong subreddit to ask this question, pls tell where can i.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ys4vqu", "is_robot_indexable": true, "report_reasons": null, "author": "animeguytamillife", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ys4vqu/an_alternative_to_cloud_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ys4vqu/an_alternative_to_cloud_storage/", "subreddit_subscribers": 653013, "created_utc": 1668154835.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Have a 5TB HDD that I no longer need after upgrading my Plex server to a 14TB HDD. However, I don't want to just use it for nothing, and I'm sure there's some good stuff I can preserve here and there that's between 4.5-5TB. If you had this space, and could choose stuff to put on it that's *not* media, what would you put?", "author_fullname": "t2_7to0ng6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Useful Things Could I Preserve The Entirety Of With A Drive With ~4.5TB Of Usable Space?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ysgplc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668187009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have a 5TB HDD that I no longer need after upgrading my Plex server to a 14TB HDD. However, I don&amp;#39;t want to just use it for nothing, and I&amp;#39;m sure there&amp;#39;s some good stuff I can preserve here and there that&amp;#39;s between 4.5-5TB. If you had this space, and could choose stuff to put on it that&amp;#39;s &lt;em&gt;not&lt;/em&gt; media, what would you put?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ysgplc", "is_robot_indexable": true, "report_reasons": null, "author": "Aside_Dish", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ysgplc/what_useful_things_could_i_preserve_the_entirety/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ysgplc/what_useful_things_could_i_preserve_the_entirety/", "subreddit_subscribers": 653013, "created_utc": 1668187009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm worried about Raw\\_Read\\_Error\\_Rate and Seek\\_Error\\_Rate. I just got it, so I could send it back.\n\n`SMART Attributes Data Structure revision number: 10`\n\n`Vendor Specific SMART Attributes with Thresholds:`\n\n`ID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE`\n\n  `1 Raw_Read_Error_Rate     0x000f   100   100   044    Pre-fail  Always       -       11473`\n\n  `3 Spin_Up_Time            0x0003   099   099   000    Pre-fail  Always       -       0`\n\n  `4 Start_Stop_Count        0x0032   100   100   020    Old_age   Always       -       1`\n\n  `5 Reallocated_Sector_Ct   0x0033   100   100   010    Pre-fail  Always       -       0`\n\n  `7 Seek_Error_Rate         0x000f   100   253   045    Pre-fail  Always       -       45116`\n\n  `9 Power_On_Hours          0x0032   100   100   000    Old_age   Always       -       0`\n\n `10 Spin_Retry_Count        0x0013   100   100   097    Pre-fail  Always       -       0`\n\n `12 Power_Cycle_Count       0x0032   100   100   020    Old_age   Always       -       1`\n\n `18 Unknown_Attribute       0x000b   100   100   050    Pre-fail  Always       -       0`\n\n`187 Reported_Uncorrect      0x0032   100   100   000    Old_age   Always       -       0`\n\n`188 Command_Timeout         0x0032   100   253   000    Old_age   Always       -       0`\n\n`190 Airflow_Temperature_Cel 0x0022   065   065   000    Old_age   Always       -       35 (Min/Max 25/35)`\n\n`192 Power-Off_Retract_Count 0x0032   100   100   000    Old_age   Always       -       1`\n\n`193 Load_Cycle_Count        0x0032   100   100   000    Old_age   Always       -       2`\n\n`194 Temperature_Celsius     0x0022   035   040   000    Old_age   Always       -       35 (0 25 0 0 0)`\n\n`197 Current_Pending_Sector  0x0012   100   100   000    Old_age   Always       -       0`\n\n`198 Offline_Uncorrectable   0x0010   100   100   000    Old_age   Offline      -       0`\n\n`199 UDMA_CRC_Error_Count    0x003e   200   200   000    Old_age   Always       -       0`\n\n`200 Multi_Zone_Error_Rate   0x0023   100   100   001    Pre-fail  Always       -       0`\n\n`240 Head_Flying_Hours       0x0000   100   253   000    Old_age   Offline      -       0 (10 6 0)`\n\n`241 Total_LBAs_Written      0x0000   100   253   000    Old_age   Offline      -       0`\n\n`242 Total_LBAs_Read         0x0000   100   253   000    Old_age   Offline      -       11473`\n\n&amp;#x200B;\n\n`SMART Error Log Version: 1`\n\n`No Errors Logged`", "author_fullname": "t2_rlzoh69c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this new Seagate X18 fine? I'm a bit worried about the SMART results.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysaooy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668173763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m worried about Raw_Read_Error_Rate and Seek_Error_Rate. I just got it, so I could send it back.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SMART Attributes Data Structure revision number: 10&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Vendor Specific SMART Attributes with Thresholds:&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;1 Raw_Read_Error_Rate     0x000f   100   100   044    Pre-fail  Always       -       11473&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;3 Spin_Up_Time            0x0003   099   099   000    Pre-fail  Always       -       0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;4 Start_Stop_Count        0x0032   100   100   020    Old_age   Always       -       1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;5 Reallocated_Sector_Ct   0x0033   100   100   010    Pre-fail  Always       -       0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;7 Seek_Error_Rate         0x000f   100   253   045    Pre-fail  Always       -       45116&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;9 Power_On_Hours          0x0032   100   100   000    Old_age   Always       -       0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;10 Spin_Retry_Count        0x0013   100   100   097    Pre-fail  Always       -       0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;12 Power_Cycle_Count       0x0032   100   100   020    Old_age   Always       -       1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;18 Unknown_Attribute       0x000b   100   100   050    Pre-fail  Always       -       0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;187 Reported_Uncorrect      0x0032   100   100   000    Old_age   Always       -       0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;188 Command_Timeout         0x0032   100   253   000    Old_age   Always       -       0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;190 Airflow_Temperature_Cel 0x0022   065   065   000    Old_age   Always       -       35 (Min/Max 25/35)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;192 Power-Off_Retract_Count 0x0032   100   100   000    Old_age   Always       -       1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;193 Load_Cycle_Count        0x0032   100   100   000    Old_age   Always       -       2&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;194 Temperature_Celsius     0x0022   035   040   000    Old_age   Always       -       35 (0 25 0 0 0)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;197 Current_Pending_Sector  0x0012   100   100   000    Old_age   Always       -       0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;198 Offline_Uncorrectable   0x0010   100   100   000    Old_age   Offline      -       0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;199 UDMA_CRC_Error_Count    0x003e   200   200   000    Old_age   Always       -       0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;200 Multi_Zone_Error_Rate   0x0023   100   100   001    Pre-fail  Always       -       0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;240 Head_Flying_Hours       0x0000   100   253   000    Old_age   Offline      -       0 (10 6 0)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;241 Total_LBAs_Written      0x0000   100   253   000    Old_age   Offline      -       0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;242 Total_LBAs_Read         0x0000   100   253   000    Old_age   Offline      -       11473&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SMART Error Log Version: 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;No Errors Logged&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ysaooy", "is_robot_indexable": true, "report_reasons": null, "author": "SirLordTheThird", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ysaooy/is_this_new_seagate_x18_fine_im_a_bit_worried/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ysaooy/is_this_new_seagate_x18_fine_im_a_bit_worried/", "subreddit_subscribers": 653013, "created_utc": 1668173763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Have a bunch of blu ray data discs and need to duplicate them. I have two blu ray burner drives. Is it safer to create an image from each disc and burn the images, or is it reliable to copy directly between two drives to save time?", "author_fullname": "t2_tlwbyabt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to copy data directly between two Blu-Ray burner drives and is this reliable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys8v83", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668169270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have a bunch of blu ray data discs and need to duplicate them. I have two blu ray burner drives. Is it safer to create an image from each disc and burn the images, or is it reliable to copy directly between two drives to save time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ys8v83", "is_robot_indexable": true, "report_reasons": null, "author": "nightdonut", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ys8v83/is_it_possible_to_copy_data_directly_between_two/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ys8v83/is_it_possible_to_copy_data_directly_between_two/", "subreddit_subscribers": 653013, "created_utc": 1668169270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I have a small mini PC (specifically a Acer Revo One RL85) that I am planning to use as a music server PC.\n\nThe problem I'm having is that whilst it takes 2.5\" drives, the value of these compared to a normal 3.5\" internal drive is just not there (e.g. a 2tb 2.5\" drive is $129 AUD, where a 3.5\" internal drive is $125)\n\nSo what I was wondering is if anyone has had luck shucking a standard external USB drive to use the internal 2.5\" drive.\n\nIf you have had luck shucking one, what brands and other things would you give advice on doing?\n\nAlternatively, even though I'm looking for something real small form factor, am I better off just plugging in an external drive into it running all my media off that.\n\nWould love to hear your experiences or thoughts on the matter. Thanks in advance.", "author_fullname": "t2_14x33n57", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shucking an external hard drive to use as an internal drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ys8stc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668169103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I have a small mini PC (specifically a Acer Revo One RL85) that I am planning to use as a music server PC.&lt;/p&gt;\n\n&lt;p&gt;The problem I&amp;#39;m having is that whilst it takes 2.5&amp;quot; drives, the value of these compared to a normal 3.5&amp;quot; internal drive is just not there (e.g. a 2tb 2.5&amp;quot; drive is $129 AUD, where a 3.5&amp;quot; internal drive is $125)&lt;/p&gt;\n\n&lt;p&gt;So what I was wondering is if anyone has had luck shucking a standard external USB drive to use the internal 2.5&amp;quot; drive.&lt;/p&gt;\n\n&lt;p&gt;If you have had luck shucking one, what brands and other things would you give advice on doing?&lt;/p&gt;\n\n&lt;p&gt;Alternatively, even though I&amp;#39;m looking for something real small form factor, am I better off just plugging in an external drive into it running all my media off that.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your experiences or thoughts on the matter. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ys8stc", "is_robot_indexable": true, "report_reasons": null, "author": "rcapi94", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ys8stc/shucking_an_external_hard_drive_to_use_as_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ys8stc/shucking_an_external_hard_drive_to_use_as_an/", "subreddit_subscribers": 653013, "created_utc": 1668169103.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}