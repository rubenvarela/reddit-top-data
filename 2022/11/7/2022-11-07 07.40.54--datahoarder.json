{"kind": "Listing", "data": {"after": "t3_yo3ius", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need to store about 500GB online, for backup (so never likely to be accessed).\n\nI am aware of:\n\n* iDrive e2: around $4 per month for 1TB (excluding offer for the first year, at $0.33 per month)\n* Backblaze B2: around $2.5 per month (for 500GB)\n* Hetzner Storage Box: around $3.8 for 1TB\n* Hetzner Storage Share (NextCloud): around $5 for 1TB (but with NextCloud web functionality, and 28 backups not taken from storage allowance)\n* Wasabi: around $5 per month per 1TB\n* OVH Cloud Archive: around $3 (but $14 for the initial load)\n\nAnything else cheaper to consider, that I can use with rclone?\n\n=== Edited ===\n\nOther people have mentioned:\n\n* pCloud, at around \u00a34 a month for 500GB\n* Scaleaway Glacier. About 0.85 EUR a month for 500GB. Seems to be possible to store directly to Glacier (without a normal storage step first)\n* AWS Glacier Deep Archive. About $0.5 a month for 500GB. However, seems quite complicated to use. Overall cost is not clear, and minimum 180 day storage\n\nFor the time being, I will use iDrive e2 (since only paying $4 for a year), but if/when they increase their price (I am sure they will), it looks like Backblaze B2 would be the easiest and cheapest option, or Scaleaway Glacier (to be tested). Of course, all may be different in 12 month's time.\n\n&amp;#x200B;", "author_fullname": "t2_lwumyho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the cheapest Cloud storage providers currently? S3 compatible or usable with rclone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo3bgh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 118, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 118, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667806205.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667769926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to store about 500GB online, for backup (so never likely to be accessed).&lt;/p&gt;\n\n&lt;p&gt;I am aware of:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;iDrive e2: around $4 per month for 1TB (excluding offer for the first year, at $0.33 per month)&lt;/li&gt;\n&lt;li&gt;Backblaze B2: around $2.5 per month (for 500GB)&lt;/li&gt;\n&lt;li&gt;Hetzner Storage Box: around $3.8 for 1TB&lt;/li&gt;\n&lt;li&gt;Hetzner Storage Share (NextCloud): around $5 for 1TB (but with NextCloud web functionality, and 28 backups not taken from storage allowance)&lt;/li&gt;\n&lt;li&gt;Wasabi: around $5 per month per 1TB&lt;/li&gt;\n&lt;li&gt;OVH Cloud Archive: around $3 (but $14 for the initial load)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anything else cheaper to consider, that I can use with rclone?&lt;/p&gt;\n\n&lt;p&gt;=== Edited ===&lt;/p&gt;\n\n&lt;p&gt;Other people have mentioned:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;pCloud, at around \u00a34 a month for 500GB&lt;/li&gt;\n&lt;li&gt;Scaleaway Glacier. About 0.85 EUR a month for 500GB. Seems to be possible to store directly to Glacier (without a normal storage step first)&lt;/li&gt;\n&lt;li&gt;AWS Glacier Deep Archive. About $0.5 a month for 500GB. However, seems quite complicated to use. Overall cost is not clear, and minimum 180 day storage&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For the time being, I will use iDrive e2 (since only paying $4 for a year), but if/when they increase their price (I am sure they will), it looks like Backblaze B2 would be the easiest and cheapest option, or Scaleaway Glacier (to be tested). Of course, all may be different in 12 month&amp;#39;s time.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo3bgh", "is_robot_indexable": true, "report_reasons": null, "author": "TedBob99", "discussion_type": null, "num_comments": 43, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo3bgh/what_are_the_cheapest_cloud_storage_providers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo3bgh/what_are_the_cheapest_cloud_storage_providers/", "subreddit_subscribers": 652076, "created_utc": 1667769926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Libgen is down. Looks like the same fate as Z-Library.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yocd7g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "author_fullname": "t2_jxsk4", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "libgen", "selftext": "Hopefully this is only temporary, for both.\n\nEDIT: I am glad to be wrong--Libgen appears to be back up! Knock on wood. I'll be over here walking on eggshells...", "author_fullname": "t2_52b6g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Libgen is down. Looks like the same fate as Z-Library.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/libgen", "hidden": false, "pwls": 7, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yobm50", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 78, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 78, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667801726.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667792176.0, "link_flair_type": "text", "wls": 7, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.libgen", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hopefully this is only temporary, for both.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I am glad to be wrong--Libgen appears to be back up! Knock on wood. I&amp;#39;ll be over here walking on eggshells...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_31p7i", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yobm50", "is_robot_indexable": true, "report_reasons": null, "author": "soulcaptain", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "some_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/libgen/comments/yobm50/libgen_is_down_looks_like_the_same_fate_as/", "parent_whitelist_status": "some_ads", "stickied": false, "url": "https://old.reddit.com/r/libgen/comments/yobm50/libgen_is_down_looks_like_the_same_fate_as/", "subreddit_subscribers": 43053, "created_utc": 1667792176.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1667794469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.libgen", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/libgen/comments/yobm50/libgen_is_down_looks_like_the_same_fate_as/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "100 Zettabytes zfs", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yocd7g", "is_robot_indexable": true, "report_reasons": null, "author": "Yekab0f", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_yobm50", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yocd7g/libgen_is_down_looks_like_the_same_fate_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/libgen/comments/yobm50/libgen_is_down_looks_like_the_same_fate_as/", "subreddit_subscribers": 652076, "created_utc": 1667794469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just showing off but I thought this might be useful to people who own several hundred GB of comics. I use it to repack high quality manga on my Kobo, it more than doubled the amount I can store at once.\n\nI know such repacking software already exists because people post on manga sites all the time, but I couldn't find any scripts online, much less ones that were easy to use. This tries to fill that niche, letting you do bulk operations on entire folders.\n\nObvious disclaimer that lossy compression is always, well, lossy, but based on my testing the tradeoff is very favorable (though lossless WebPs are basically a free improvement over PNG, as long as you're okay with using WebP). While this \\*can\\* be used to make images look like a potato, the defaults produce files which are indistinguishable from the source, unless you zoom into individual pixels.\n\nThe thing with CBZ files (which are essentially just a ZIP image folder by a different name), is that many publishers use the JPEG format with parts of the compression algorithm disabled, which while good for distribution, provides little benefit to end users, and by re-enabling those parts you can get free space savings without degrading image quality at all.\n\nThis applies to JPEG images broadly, and this utility can be used for that too (bulk conversion of images), although there are probably better options available.\n\n[https://github.com/avalonv/reCBZ](https://github.com/avalonv/reCBZ)", "author_fullname": "t2_3e6048wu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created a utility that repacks manga and comic books to save disk space", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo3urd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667791450.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667771100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just showing off but I thought this might be useful to people who own several hundred GB of comics. I use it to repack high quality manga on my Kobo, it more than doubled the amount I can store at once.&lt;/p&gt;\n\n&lt;p&gt;I know such repacking software already exists because people post on manga sites all the time, but I couldn&amp;#39;t find any scripts online, much less ones that were easy to use. This tries to fill that niche, letting you do bulk operations on entire folders.&lt;/p&gt;\n\n&lt;p&gt;Obvious disclaimer that lossy compression is always, well, lossy, but based on my testing the tradeoff is very favorable (though lossless WebPs are basically a free improvement over PNG, as long as you&amp;#39;re okay with using WebP). While this *can* be used to make images look like a potato, the defaults produce files which are indistinguishable from the source, unless you zoom into individual pixels.&lt;/p&gt;\n\n&lt;p&gt;The thing with CBZ files (which are essentially just a ZIP image folder by a different name), is that many publishers use the JPEG format with parts of the compression algorithm disabled, which while good for distribution, provides little benefit to end users, and by re-enabling those parts you can get free space savings without degrading image quality at all.&lt;/p&gt;\n\n&lt;p&gt;This applies to JPEG images broadly, and this utility can be used for that too (bulk conversion of images), although there are probably better options available.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/avalonv/reCBZ\"&gt;https://github.com/avalonv/reCBZ&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YqoArXeLJ1Qi373lei88vrP_rFj3Z6MuyEasqZfUaNs.jpg?auto=webp&amp;s=84ff58ace6f502718c4a73d11bd5e34ced6f0bfb", "width": 1028, "height": 505}, "resolutions": [{"url": "https://external-preview.redd.it/YqoArXeLJ1Qi373lei88vrP_rFj3Z6MuyEasqZfUaNs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f454e5cd52ce6c6b4e1409addd7a6c13a092f430", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/YqoArXeLJ1Qi373lei88vrP_rFj3Z6MuyEasqZfUaNs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f98b22a0b3642b8d978f1e4cfb1e693582fa3b9", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/YqoArXeLJ1Qi373lei88vrP_rFj3Z6MuyEasqZfUaNs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e8c20ff787eab1de990309a1014ebdf63321995", "width": 320, "height": 157}, {"url": "https://external-preview.redd.it/YqoArXeLJ1Qi373lei88vrP_rFj3Z6MuyEasqZfUaNs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2bc3ab3f52d88b9052f6540564d6f6e6731f6721", "width": 640, "height": 314}, {"url": "https://external-preview.redd.it/YqoArXeLJ1Qi373lei88vrP_rFj3Z6MuyEasqZfUaNs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b962b767143761b2ef2d6a95dea4d0710fd81a65", "width": 960, "height": 471}], "variants": {}, "id": "PyYxW-cFjbnslNZDJI9-oeGnfzWDwVTI_S8I_ihNSzE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo3urd", "is_robot_indexable": true, "report_reasons": null, "author": "reallyfuckingay", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo3urd/i_created_a_utility_that_repacks_manga_and_comic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo3urd/i_created_a_utility_that_repacks_manga_and_comic/", "subreddit_subscribers": 652076, "created_utc": 1667771100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, I do apologize if this is not the right subreddit, but since this is an issue I came across while organizing my NAS contents, I thought someone here may help, and I do believe after all, this is effectively about archival of files.\n\nI'm currently running a script in a directory structure containing various video files and encodes everything in an x265 MKV file. I thought that the only file present would be mov or mp4, but it turns out there were two jpg files too.  \nThe funny thing, is that HandBrake CLI actually crated an MKV file for those without complaining, and reduced the file size from 3MB to 400KB. Opening it with VLC just flashes the image for a second, while other media players keep the frame, I didn't do any quality analysis, but at first glance the quality seem similar (consider that the picture quality wasn't great to begin with, and I'm with the size being reduced even if lossy)...\n\nNow, aside the fact that for sure there is a way to make a much better JPG and shrink the file size while still being a JPG, has anybody ever actually used x265 for image compression? Seems pretty ridiculous, but I'm kind of curious about it.\n\nClearly thought, an image cannot just stay inside an MKV container, it's too much of a pain.\n\nSorry for the weird question and thanks in advance!", "author_fullname": "t2_7vil65ud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HEVC for image compression?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo6wh7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667778903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I do apologize if this is not the right subreddit, but since this is an issue I came across while organizing my NAS contents, I thought someone here may help, and I do believe after all, this is effectively about archival of files.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently running a script in a directory structure containing various video files and encodes everything in an x265 MKV file. I thought that the only file present would be mov or mp4, but it turns out there were two jpg files too.&lt;br/&gt;\nThe funny thing, is that HandBrake CLI actually crated an MKV file for those without complaining, and reduced the file size from 3MB to 400KB. Opening it with VLC just flashes the image for a second, while other media players keep the frame, I didn&amp;#39;t do any quality analysis, but at first glance the quality seem similar (consider that the picture quality wasn&amp;#39;t great to begin with, and I&amp;#39;m with the size being reduced even if lossy)...&lt;/p&gt;\n\n&lt;p&gt;Now, aside the fact that for sure there is a way to make a much better JPG and shrink the file size while still being a JPG, has anybody ever actually used x265 for image compression? Seems pretty ridiculous, but I&amp;#39;m kind of curious about it.&lt;/p&gt;\n\n&lt;p&gt;Clearly thought, an image cannot just stay inside an MKV container, it&amp;#39;s too much of a pain.&lt;/p&gt;\n\n&lt;p&gt;Sorry for the weird question and thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo6wh7", "is_robot_indexable": true, "report_reasons": null, "author": "LynxesExe", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo6wh7/hevc_for_image_compression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo6wh7/hevc_for_image_compression/", "subreddit_subscribers": 652076, "created_utc": 1667778903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Tons of tools out there that can create hashes for files, but I cannot find enough to verify the files with that hash as well. Kleopetra does this (gnupgp) but for some reason, it fails for files above 2 Gigabytes. \nSimply creating checksum files is useless if I cannot use them to verify data.\n\n\nEdit: Found a solution Thanks to u/xlltt\n\nhttps://github.com/namazso/OpenHashTab is exactly what I was looking for. Although I haven't tested larger files (512GB+) with it, it works nicely with my current setup.", "author_fullname": "t2_p02cy80h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An open source file Hasher AND Verifier?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynkzyb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667735818.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667724677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tons of tools out there that can create hashes for files, but I cannot find enough to verify the files with that hash as well. Kleopetra does this (gnupgp) but for some reason, it fails for files above 2 Gigabytes. \nSimply creating checksum files is useless if I cannot use them to verify data.&lt;/p&gt;\n\n&lt;p&gt;Edit: Found a solution Thanks to &lt;a href=\"/u/xlltt\"&gt;u/xlltt&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/namazso/OpenHashTab\"&gt;https://github.com/namazso/OpenHashTab&lt;/a&gt; is exactly what I was looking for. Although I haven&amp;#39;t tested larger files (512GB+) with it, it works nicely with my current setup.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FYOCCFC4GzfSBiHgcZC_rP24w-Egfr8R0afkYgCmqvQ.jpg?auto=webp&amp;s=2d16cddc6b994a0ea638c5a066987306d075597c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/FYOCCFC4GzfSBiHgcZC_rP24w-Egfr8R0afkYgCmqvQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0e1f1c87ba336c6f3e0c2bbf10518b5d9870d52", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/FYOCCFC4GzfSBiHgcZC_rP24w-Egfr8R0afkYgCmqvQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a7ab9235332c4e3a82e40b0490f208a3cb25911", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/FYOCCFC4GzfSBiHgcZC_rP24w-Egfr8R0afkYgCmqvQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b85bd0d5fcdb052ce22f875a9acd7fe1b06c7581", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/FYOCCFC4GzfSBiHgcZC_rP24w-Egfr8R0afkYgCmqvQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e971a88a45736525140eb64b4ba888a977d51e63", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/FYOCCFC4GzfSBiHgcZC_rP24w-Egfr8R0afkYgCmqvQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ade7874d41f165831d969efa4f527275ace55f05", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/FYOCCFC4GzfSBiHgcZC_rP24w-Egfr8R0afkYgCmqvQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2c8e7f00c303711af71a9e8b4a14a7ca31d6e663", "width": 1080, "height": 540}], "variants": {}, "id": "VCA9gA-JsLfxe4ihzHZ9gsjUdMx21bSFwcYUTZ8-BKA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ynkzyb", "is_robot_indexable": true, "report_reasons": null, "author": "Frosty-Influence988", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ynkzyb/an_open_source_file_hasher_and_verifier/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ynkzyb/an_open_source_file_hasher_and_verifier/", "subreddit_subscribers": 652076, "created_utc": 1667724677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to download a copy of this site rpgtinker.com for offline use. I'm in the navy and the nature of carrier life prevents me from being able access it on a regular basis.\n\n I run a couple d&amp;d campaigns on the ship, and the ability to generate these statblocks for npcs on the fly is incredibly useful.\n\nI've tried things like Httrack and the like, and the critical function of actually generating the statblocks doesn't seem to work, and I'm way out of my depth here. I'm straight up at the point where I'd be willing to throw money at the issue to get this done. I've dug into any and all alternatives, and there are none that really do what this site does. Sites, apps, I've looked all over. The only alternative is foundry vtt, and that would cost me over 50 bucks and would pale in comparison to the options this website provides for free", "author_fullname": "t2_7lreihth", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need a hand here.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo3w3o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667771198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to download a copy of this site rpgtinker.com for offline use. I&amp;#39;m in the navy and the nature of carrier life prevents me from being able access it on a regular basis.&lt;/p&gt;\n\n&lt;p&gt;I run a couple d&amp;amp;d campaigns on the ship, and the ability to generate these statblocks for npcs on the fly is incredibly useful.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried things like Httrack and the like, and the critical function of actually generating the statblocks doesn&amp;#39;t seem to work, and I&amp;#39;m way out of my depth here. I&amp;#39;m straight up at the point where I&amp;#39;d be willing to throw money at the issue to get this done. I&amp;#39;ve dug into any and all alternatives, and there are none that really do what this site does. Sites, apps, I&amp;#39;ve looked all over. The only alternative is foundry vtt, and that would cost me over 50 bucks and would pale in comparison to the options this website provides for free&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo3w3o", "is_robot_indexable": true, "report_reasons": null, "author": "Detrand", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo3w3o/i_need_a_hand_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo3w3o/i_need_a_hand_here/", "subreddit_subscribers": 652076, "created_utc": 1667771198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyone have a recommendation for a decent one that they use?  I'm looking on Amazon and there seems to multitudes with very high ratings.  \n\nAlso they all seem to be USB is there a type that I can connect through my network? \n\nI just want to use it for quick copying or transferring of unimportant files to free up space from my normal backups.", "author_fullname": "t2_11ghpp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hard Drive Docking Station.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynyw1l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667760633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have a recommendation for a decent one that they use?  I&amp;#39;m looking on Amazon and there seems to multitudes with very high ratings.  &lt;/p&gt;\n\n&lt;p&gt;Also they all seem to be USB is there a type that I can connect through my network? &lt;/p&gt;\n\n&lt;p&gt;I just want to use it for quick copying or transferring of unimportant files to free up space from my normal backups.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ynyw1l", "is_robot_indexable": true, "report_reasons": null, "author": "_King_pin_", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ynyw1l/hard_drive_docking_station/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ynyw1l/hard_drive_docking_station/", "subreddit_subscribers": 652076, "created_utc": 1667760633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Lets say I have a 100 folder with different names.  How do I batch rename all the files in each folder to have the name of the folder that they are in? I am running Windows 11. Thanks.", "author_fullname": "t2_ah6cj5rd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I rename files in different folders to have the name of the folder?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynjwzo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667720531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say I have a 100 folder with different names.  How do I batch rename all the files in each folder to have the name of the folder that they are in? I am running Windows 11. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "ynjwzo", "is_robot_indexable": true, "report_reasons": null, "author": "ONEto10dollars", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ynjwzo/how_do_i_rename_files_in_different_folders_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ynjwzo/how_do_i_rename_files_in_different_folders_to/", "subreddit_subscribers": 652076, "created_utc": 1667720531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Easystore 14 TB (Example) - 5400 RPM, 2 year warranty $199.99 on sale...\n\nWestern Digital 14TB WD Red Pro NAS - 7200 RPM, 5 year warranty $260.00 on sale...\n\nYou guys really think the better speeds and 3 years extra warranty isn't worth it?", "author_fullname": "t2_iver8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "easystore drives compared to PRO red drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yodojy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667799532.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667798613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Easystore 14 TB (Example) - 5400 RPM, 2 year warranty $199.99 on sale...&lt;/p&gt;\n\n&lt;p&gt;Western Digital 14TB WD Red Pro NAS - 7200 RPM, 5 year warranty $260.00 on sale...&lt;/p&gt;\n\n&lt;p&gt;You guys really think the better speeds and 3 years extra warranty isn&amp;#39;t worth it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yodojy", "is_robot_indexable": true, "report_reasons": null, "author": "SickestGuy", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yodojy/easystore_drives_compared_to_pro_red_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yodojy/easystore_drives_compared_to_pro_red_drives/", "subreddit_subscribers": 652076, "created_utc": 1667798613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a bit fed up with Google's storage issues (my photos go to my Gmail account that shares 15gb with email) and I recently realized that Amazon claims to offer unlimited photo storage.\n\nHas anybody taken advantage of this? Are there any scripts or libraries i can use to upload to it from my workstation? Not all my pictures are from a phone but i do sync everything to my home server so that's the ideal place to sync from.\n\nAlso, do the \"family\" accounts get the same? I would like to add wife and mom as well if possible.\n\nI'm only considering this as an a quick and easy way for us to have a Google photos like experience with minimal effort on my part (vs self hosting something)", "author_fullname": "t2_1ijfzrf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon photos anyone?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yoddzl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667797670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a bit fed up with Google&amp;#39;s storage issues (my photos go to my Gmail account that shares 15gb with email) and I recently realized that Amazon claims to offer unlimited photo storage.&lt;/p&gt;\n\n&lt;p&gt;Has anybody taken advantage of this? Are there any scripts or libraries i can use to upload to it from my workstation? Not all my pictures are from a phone but i do sync everything to my home server so that&amp;#39;s the ideal place to sync from.&lt;/p&gt;\n\n&lt;p&gt;Also, do the &amp;quot;family&amp;quot; accounts get the same? I would like to add wife and mom as well if possible.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m only considering this as an a quick and easy way for us to have a Google photos like experience with minimal effort on my part (vs self hosting something)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yoddzl", "is_robot_indexable": true, "report_reasons": null, "author": "mdeanda", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yoddzl/amazon_photos_anyone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yoddzl/amazon_photos_anyone/", "subreddit_subscribers": 652076, "created_utc": 1667797670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So my grandfather was a computer engineer, he passed away and now i have a bunch of old legit CD's most with keys of windows, microsoft programs and others. (pre 2010)\n\nany suggestions what i should do with them, i know there's a case use out there for them, but (due to need of downgrading storage after a divorce) I'm to the point of just chucking them.  what would be the best way of giving people a chance to grab them.  or is there a digital repository i can upload an ISO?", "author_fullname": "t2_bg78e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "looking to unload physical cd's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo1fl8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667765974.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So my grandfather was a computer engineer, he passed away and now i have a bunch of old legit CD&amp;#39;s most with keys of windows, microsoft programs and others. (pre 2010)&lt;/p&gt;\n\n&lt;p&gt;any suggestions what i should do with them, i know there&amp;#39;s a case use out there for them, but (due to need of downgrading storage after a divorce) I&amp;#39;m to the point of just chucking them.  what would be the best way of giving people a chance to grab them.  or is there a digital repository i can upload an ISO?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo1fl8", "is_robot_indexable": true, "report_reasons": null, "author": "cyten23", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo1fl8/looking_to_unload_physical_cds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo1fl8/looking_to_unload_physical_cds/", "subreddit_subscribers": 652076, "created_utc": 1667765974.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Maybe most here have a different idea of data hoarding but in essence I think the idea is similar.  Basically I don't want my knowledge lost.  I hate having to reresearch and rediscover things over and over.  So I started my own wiki where I deposit all the fruits of my research.  \n\nI picked PmWiki over MediaWiki because it uses simple flat files to store data so no database is needed to run it, just php.  It can also easily be run offline.  My site is https://naturevault.org and has an auto github archive every 8 hours at https://github.com/GiverofMemory/NatureVault\n\nIt is cool having a flat file wiki cause it can be backed up via git and I just do a 'git pull' on my pc to back up the github archive.  Right now it is 1500 pages and only about 10 mb and it probably contains more data than my brain is holding lol.\n\nSo my best thought is that ideally \"everyone\" would have their own wiki like this and then we sync our wikis so that each of our wikis contain copies of each other's wikis.  It is organized in groups so my wiki group is naturevault and your wikigroup would be whatever you wanted to call it.  It is the original 'wiki federation' idea by the creator of the wiki.\n\nDoes this sort of thing appeal to anyone, or is the idea of just personal private data storage all that this group really cares about?  For me I am into the \"library of alexandria\" idea.\n\nAnother idea is I could release a blank version of my wiki with no pages so you can use it as a template for your own personal private data storage, if there is interest I could maintain a github archive of that.  I have put hundreds+ of hours into tuning PmWiki to be great.\n\nEDIT: I did this for kicks, see https://github.com/GiverofMemory/Vaults (hasn't been tested yet, probably still some bugs to work out).  License is Unlicensed.\n\nAnyway glad to have stumbled upon this group!  Excited to learn more of how people are backing up their life.", "author_fullname": "t2_a6pa0v1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wiki as medium for datahoarding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynvu6f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667792226.0, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667754118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Maybe most here have a different idea of data hoarding but in essence I think the idea is similar.  Basically I don&amp;#39;t want my knowledge lost.  I hate having to reresearch and rediscover things over and over.  So I started my own wiki where I deposit all the fruits of my research.  &lt;/p&gt;\n\n&lt;p&gt;I picked PmWiki over MediaWiki because it uses simple flat files to store data so no database is needed to run it, just php.  It can also easily be run offline.  My site is &lt;a href=\"https://naturevault.org\"&gt;https://naturevault.org&lt;/a&gt; and has an auto github archive every 8 hours at &lt;a href=\"https://github.com/GiverofMemory/NatureVault\"&gt;https://github.com/GiverofMemory/NatureVault&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It is cool having a flat file wiki cause it can be backed up via git and I just do a &amp;#39;git pull&amp;#39; on my pc to back up the github archive.  Right now it is 1500 pages and only about 10 mb and it probably contains more data than my brain is holding lol.&lt;/p&gt;\n\n&lt;p&gt;So my best thought is that ideally &amp;quot;everyone&amp;quot; would have their own wiki like this and then we sync our wikis so that each of our wikis contain copies of each other&amp;#39;s wikis.  It is organized in groups so my wiki group is naturevault and your wikigroup would be whatever you wanted to call it.  It is the original &amp;#39;wiki federation&amp;#39; idea by the creator of the wiki.&lt;/p&gt;\n\n&lt;p&gt;Does this sort of thing appeal to anyone, or is the idea of just personal private data storage all that this group really cares about?  For me I am into the &amp;quot;library of alexandria&amp;quot; idea.&lt;/p&gt;\n\n&lt;p&gt;Another idea is I could release a blank version of my wiki with no pages so you can use it as a template for your own personal private data storage, if there is interest I could maintain a github archive of that.  I have put hundreds+ of hours into tuning PmWiki to be great.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I did this for kicks, see &lt;a href=\"https://github.com/GiverofMemory/Vaults\"&gt;https://github.com/GiverofMemory/Vaults&lt;/a&gt; (hasn&amp;#39;t been tested yet, probably still some bugs to work out).  License is Unlicensed.&lt;/p&gt;\n\n&lt;p&gt;Anyway glad to have stumbled upon this group!  Excited to learn more of how people are backing up their life.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "10MB (wiki)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ynvu6f", "is_robot_indexable": true, "report_reasons": null, "author": "NatureVault", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/ynvu6f/wiki_as_medium_for_datahoarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ynvu6f/wiki_as_medium_for_datahoarding/", "subreddit_subscribers": 652076, "created_utc": 1667754118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_xqgmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nice xbox repo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynmtli", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1667731590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "archive.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://archive.org/details/xboxcds?sort=titleSorter", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ynmtli", "is_robot_indexable": true, "report_reasons": null, "author": "we4donald", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ynmtli/nice_xbox_repo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://archive.org/details/xboxcds?sort=titleSorter", "subreddit_subscribers": 652076, "created_utc": 1667731590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not so recently google took down a link to an extremely detailed beautifully formatted spreadsheet of all the AM4 motherboards and for a while I thought it was gone forever. I recently however found a mirror link to a copy of it but it's still on Google Sheets and also does not have any of the export options enabled.\n\nI was wondering if there's still any way to download this file given copying, downloading, and printing have all been disabled (I tried copying it to a new sheet and then downloading that but they've fixed that trick). I don't trust google to keep the mirror up forever and even then the owner could still delete it.\n\nHere is a link to the spreadsheet for those that want to look at it.  \n[https://docs.google.com/spreadsheets/d/1-cw7A2MDHPvA-oB3OKXivdUo9BbTcsss1Rzy3J4hRyA/edit#gid=2112472504](https://docs.google.com/spreadsheets/d/1-cw7A2MDHPvA-oB3OKXivdUo9BbTcsss1Rzy3J4hRyA/edit#gid=2112472504)", "author_fullname": "t2_3d6imjg5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating Copy From Google Sheets When Export Is Disabled", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yoa7tz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667788081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not so recently google took down a link to an extremely detailed beautifully formatted spreadsheet of all the AM4 motherboards and for a while I thought it was gone forever. I recently however found a mirror link to a copy of it but it&amp;#39;s still on Google Sheets and also does not have any of the export options enabled.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if there&amp;#39;s still any way to download this file given copying, downloading, and printing have all been disabled (I tried copying it to a new sheet and then downloading that but they&amp;#39;ve fixed that trick). I don&amp;#39;t trust google to keep the mirror up forever and even then the owner could still delete it.&lt;/p&gt;\n\n&lt;p&gt;Here is a link to the spreadsheet for those that want to look at it.&lt;br/&gt;\n&lt;a href=\"https://docs.google.com/spreadsheets/d/1-cw7A2MDHPvA-oB3OKXivdUo9BbTcsss1Rzy3J4hRyA/edit#gid=2112472504\"&gt;https://docs.google.com/spreadsheets/d/1-cw7A2MDHPvA-oB3OKXivdUo9BbTcsss1Rzy3J4hRyA/edit#gid=2112472504&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?auto=webp&amp;s=74ee7b7f273686d3a6d63132dead8fd1764cf746", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c19110c068d76ab200f1245f86280073ea0f58a6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44c18606b62aece853ea487bd43c828325a0bdce", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=209ec2a10a56fe45fe6cb9d261c1ab221b6e9ac6", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d4eeeb63a94a849ea3cf810b90f2da22c354be9", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=be1258b942082c46071af527419ef9e0a648cf73", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0a5b3638d48be6c610b283e28d05c05ed93c981", "width": 1080, "height": 567}], "variants": {}, "id": "sJTk-8u5yRf8kptjtc2UKTd4Hk25h5aqolxUj_u1Y2k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yoa7tz", "is_robot_indexable": true, "report_reasons": null, "author": "Awkward_Elf", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yoa7tz/creating_copy_from_google_sheets_when_export_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yoa7tz/creating_copy_from_google_sheets_when_export_is/", "subreddit_subscribers": 652076, "created_utc": 1667788081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It looks like Arrimus3D after deciding to take a break for a while has delisted/removed hundreds of videos from the last fifteen years of being on YouTube. There's some concern for his wellbeing in the 3d community here on Reddit as his last couple of videos have had some statements in them that have a few people speculating.\n\nFor those who aren't in the 3D world, his tutorials were some of the best, most specific and most useful tutorials out there for beginners and professionals alike converting everything from basics to very in depth modelling across various pieces if software .\n\nHas anyone got an archive? Was anyone backing up?\nIf would be a huge blow to the 3D community to lose his considerable depth of knowledge and fantastic tuition. He was essentially a reference guide for generalists and 3D modellers and basically anyone as a hobbyist or a professional will either know the username or have unknowingly followed one of his tutorials. It's a big BIG loss if this content is gone forever.", "author_fullname": "t2_eq196", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Arrimus3D - one of the best 3D software tutorial YouTube channel removes content", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo9eob", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1667786557.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667785842.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It looks like Arrimus3D after deciding to take a break for a while has delisted/removed hundreds of videos from the last fifteen years of being on YouTube. There&amp;#39;s some concern for his wellbeing in the 3d community here on Reddit as his last couple of videos have had some statements in them that have a few people speculating.&lt;/p&gt;\n\n&lt;p&gt;For those who aren&amp;#39;t in the 3D world, his tutorials were some of the best, most specific and most useful tutorials out there for beginners and professionals alike converting everything from basics to very in depth modelling across various pieces if software .&lt;/p&gt;\n\n&lt;p&gt;Has anyone got an archive? Was anyone backing up?\nIf would be a huge blow to the 3D community to lose his considerable depth of knowledge and fantastic tuition. He was essentially a reference guide for generalists and 3D modellers and basically anyone as a hobbyist or a professional will either know the username or have unknowingly followed one of his tutorials. It&amp;#39;s a big BIG loss if this content is gone forever.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yo9eob", "is_robot_indexable": true, "report_reasons": null, "author": "jpjapers", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo9eob/arrimus3d_one_of_the_best_3d_software_tutorial/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo9eob/arrimus3d_one_of_the_best_3d_software_tutorial/", "subreddit_subscribers": 652076, "created_utc": 1667785842.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i've been getting paranoid about twitter due to how terribly elon is running it so i wanna scrape some users i follow, i've tried gallery-dl but that only saves media and doesn't scrape text tweets or replies or the amount of likes/retweets as far as i can tell, is there a program that scrapes everything from a twitter profile or is there a way to configure gallery-dl to do this? any help would be appreciated.", "author_fullname": "t2_15ts6z21", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "scrape entire twitter profile?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo89g2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667782641.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;ve been getting paranoid about twitter due to how terribly elon is running it so i wanna scrape some users i follow, i&amp;#39;ve tried gallery-dl but that only saves media and doesn&amp;#39;t scrape text tweets or replies or the amount of likes/retweets as far as i can tell, is there a program that scrapes everything from a twitter profile or is there a way to configure gallery-dl to do this? any help would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo89g2", "is_robot_indexable": true, "report_reasons": null, "author": "neonvolta", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo89g2/scrape_entire_twitter_profile/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo89g2/scrape_entire_twitter_profile/", "subreddit_subscribers": 652076, "created_utc": 1667782641.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've downloaded about 2 Terabytes from google drive with a Terabyte still left to go but, google drive decided that I can only download at 1/10 of my usual download speed, it only happend with google drive. Any ideas why?", "author_fullname": "t2_8ndk4sbe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "1/10 download speed on GoogleDrive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo66pl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667777026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve downloaded about 2 Terabytes from google drive with a Terabyte still left to go but, google drive decided that I can only download at 1/10 of my usual download speed, it only happend with google drive. Any ideas why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo66pl", "is_robot_indexable": true, "report_reasons": null, "author": "QQTabuGuy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo66pl/110_download_speed_on_googledrive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo66pl/110_download_speed_on_googledrive/", "subreddit_subscribers": 652076, "created_utc": 1667777026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Heyah o/ !\n\nThese days, I've been looking for some solutions with my data, only cloud oriented after a bad experience with a NAS. My idea now is to select a Cloud provider in order to have a +1 secure backup between all my PCs, but the main interest for me would be to finally get a nice file Synchronisation solution for all my files that I use between my PCs.\n\nI've ended up doing a Google Sheet based on many, many, many providers. I used all the feedback from here to make it, so thanks again &lt;3\n\n[Yeah... I didn't have anything else to do, and went for a basic setup, it's ugly](https://preview.redd.it/3zntrwmyrey91.png?width=1050&amp;format=png&amp;auto=webp&amp;s=c6016b0675bd93a6568b26a6a855a5b8cc8b5342)\n\nIn the end, only Syncthing isn't Cloud based. But no matter what, it's an useful tool that i'll keep as a bonus tool anyway haha.\n\nAs you can see, Filen is really getting the lead for me. But as in the bad points, my biggest fear is the Life Time Scam.\n\nSo I wanted to know if anybody used Filen and have a good experience with the Life Time thing ? What about OneDrive and Sync which are still perfect tools for my need ?\n\nThanks a lot !", "author_fullname": "t2_w6xai3n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hesitation between 3 Cloud Providers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 53, "top_awarded_type": null, "hide_score": false, "media_metadata": {"3zntrwmyrey91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 41, "x": 108, "u": "https://preview.redd.it/3zntrwmyrey91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a09ed66f10772b8a6f296e695f683ed0e7f7109a"}, {"y": 82, "x": 216, "u": "https://preview.redd.it/3zntrwmyrey91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=08a272713d30fa9b267f06b9d7460051f48b2b59"}, {"y": 122, "x": 320, "u": "https://preview.redd.it/3zntrwmyrey91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e8e65f2a17a8086b33c143c3eff36bb3ab0e53c4"}, {"y": 244, "x": 640, "u": "https://preview.redd.it/3zntrwmyrey91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d07b75e512823b576a954d6c763d73ff5697daf0"}, {"y": 366, "x": 960, "u": "https://preview.redd.it/3zntrwmyrey91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d214eb43feb73062dd5bf5b70754d535ac42030d"}], "s": {"y": 401, "x": 1050, "u": "https://preview.redd.it/3zntrwmyrey91.png?width=1050&amp;format=png&amp;auto=webp&amp;s=c6016b0675bd93a6568b26a6a855a5b8cc8b5342"}, "id": "3zntrwmyrey91"}}, "name": "t3_yo5899", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/z84R3hUbBnWllbPEnfR2NhAKnlEJnuWbK9IZ1IyRg50.jpg", "edited": 1667777163.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667774532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heyah o/ !&lt;/p&gt;\n\n&lt;p&gt;These days, I&amp;#39;ve been looking for some solutions with my data, only cloud oriented after a bad experience with a NAS. My idea now is to select a Cloud provider in order to have a +1 secure backup between all my PCs, but the main interest for me would be to finally get a nice file Synchronisation solution for all my files that I use between my PCs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve ended up doing a Google Sheet based on many, many, many providers. I used all the feedback from here to make it, so thanks again &amp;lt;3&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3zntrwmyrey91.png?width=1050&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6016b0675bd93a6568b26a6a855a5b8cc8b5342\"&gt;Yeah... I didn&amp;#39;t have anything else to do, and went for a basic setup, it&amp;#39;s ugly&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In the end, only Syncthing isn&amp;#39;t Cloud based. But no matter what, it&amp;#39;s an useful tool that i&amp;#39;ll keep as a bonus tool anyway haha.&lt;/p&gt;\n\n&lt;p&gt;As you can see, Filen is really getting the lead for me. But as in the bad points, my biggest fear is the Life Time Scam.&lt;/p&gt;\n\n&lt;p&gt;So I wanted to know if anybody used Filen and have a good experience with the Life Time thing ? What about OneDrive and Sync which are still perfect tools for my need ?&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo5899", "is_robot_indexable": true, "report_reasons": null, "author": "RPAmont", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo5899/hesitation_between_3_cloud_providers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo5899/hesitation_between_3_cloud_providers/", "subreddit_subscribers": 652076, "created_utc": 1667774532.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is a problem with both Resilio Sync or Syncthing where whenever there is syncing, it eats up a good chunk of CPU power for what appears to be all of the validating and scanning. I have a server as a peer, is there a way that most of the work is done on that server and perhaps the only thing that the client has to worry about is sending indexed information? I don't see this level of usage whenever that server is copying via SMB/UNC path so I didn't know if it was theoretically possible to have the copying done THEN any validation on the server. Thanks in advance. Debian server, Windows clients.", "author_fullname": "t2_sc713cll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a syncing solution that puts more load on a server than client?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo0wti", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667764862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a problem with both Resilio Sync or Syncthing where whenever there is syncing, it eats up a good chunk of CPU power for what appears to be all of the validating and scanning. I have a server as a peer, is there a way that most of the work is done on that server and perhaps the only thing that the client has to worry about is sending indexed information? I don&amp;#39;t see this level of usage whenever that server is copying via SMB/UNC path so I didn&amp;#39;t know if it was theoretically possible to have the copying done THEN any validation on the server. Thanks in advance. Debian server, Windows clients.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo0wti", "is_robot_indexable": true, "report_reasons": null, "author": "TAW56234", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo0wti/is_there_a_syncing_solution_that_puts_more_load/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo0wti/is_there_a_syncing_solution_that_puts_more_load/", "subreddit_subscribers": 652076, "created_utc": 1667764862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to play my music from Google Drive. I made every songs available for offline use in The Drive but I can only access it through it and play one song at a time. Is there any way VLC or other Google drive music player can play songs from the offline folder?", "author_fullname": "t2_kcx30n8j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any way to play music from Google Drive \"Offline\" folder through VLC on android?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo0p0b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667764345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to play my music from Google Drive. I made every songs available for offline use in The Drive but I can only access it through it and play one song at a time. Is there any way VLC or other Google drive music player can play songs from the offline folder?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo0p0b", "is_robot_indexable": true, "report_reasons": null, "author": "xxxeggpizzaxxx", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo0p0b/is_there_any_way_to_play_music_from_google_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo0p0b/is_there_any_way_to_play_music_from_google_drive/", "subreddit_subscribers": 652076, "created_utc": 1667764345.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was slowly archiving things in a couple niche areas, about 25tb so far.  The shut down of the library made me realize I need to step it up.  I miss the old demonoid still, used to collect 1000s of old books there, irreplaceable now, and I'll never get the rest I think sadly.   So many other trackers are just gone. \nI'm building a large offline archive of useful stuff for the niche I exist in.  I have access to decent bandwidth now but I'll be fully off grid within a year or so.  Collecting From Wikipedia to full copies of a couple trackers to some site copies via wget. \nThat take down made me realize I need too get at it.   I'm thinking either 4 bay nas or a small 8 bay server.  I liked the serverparts price on 18tb, but the pricing at best buy is not great.  I'll likely use the drives as main drives for 2-3 years then upgrade and turn them to offline backups and scrap/sell my 1/2tbs.", "author_fullname": "t2_f3idepni", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best $/tb in Canada? Need at least 40 more to archive z-lib etc don't mind mfg refurb.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynvzaa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667754391.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was slowly archiving things in a couple niche areas, about 25tb so far.  The shut down of the library made me realize I need to step it up.  I miss the old demonoid still, used to collect 1000s of old books there, irreplaceable now, and I&amp;#39;ll never get the rest I think sadly.   So many other trackers are just gone. \nI&amp;#39;m building a large offline archive of useful stuff for the niche I exist in.  I have access to decent bandwidth now but I&amp;#39;ll be fully off grid within a year or so.  Collecting From Wikipedia to full copies of a couple trackers to some site copies via wget. \nThat take down made me realize I need too get at it.   I&amp;#39;m thinking either 4 bay nas or a small 8 bay server.  I liked the serverparts price on 18tb, but the pricing at best buy is not great.  I&amp;#39;ll likely use the drives as main drives for 2-3 years then upgrade and turn them to offline backups and scrap/sell my 1/2tbs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ynvzaa", "is_robot_indexable": true, "report_reasons": null, "author": "startrackerJ", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ynvzaa/best_tb_in_canada_need_at_least_40_more_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ynvzaa/best_tb_in_canada_need_at_least_40_more_to/", "subreddit_subscribers": 652076, "created_utc": 1667754391.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a huge encrypted external drive and I have some files aand folders which have disappeared from that drive. This was after transferring data from another encrypted external drive. But this is not the first time encountering the problem.\n\nI had the same issue around 8 months ago. The first time it happened was when I attempted to recover data from an another encrypted drive (with uncorectable errors and reallocation events) did this happen. Of course, I gave up on that since I knew then the data was corrupted.\n\nI found the missing files from 8 months ago. The files and folders which have disappeared I don't care about. I'm just worried about the existing data on the drive.\n\n&amp;#x200B;\n\nAs you can see, everything appears to be healthy on drive E.\n\n&amp;#x200B;\n\n[Showing SMART Values of An Encrypted WD Elements Fully Encrypted External Hard Drive](https://preview.redd.it/58a9w2fzmcy91.png?width=674&amp;format=png&amp;auto=webp&amp;s=3b0d49ccb92d5726a85ee84f503a78c69898a7e0)\n\n&amp;#x200B;\n\nNormally, chkdisk could, could solve the issue. But not in my case, right? Since the file system is encrypted. What do I do? Do I delete the files? What about the missing folder? How do I delete the missing folder?\n\nI have backups. Do I restore from that?", "author_fullname": "t2_j0j6wkn0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Disappearing Files And Folders On An Encrypted External Hard Drive After Moving Data From An Corrupted Hard Drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"58a9w2fzmcy91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 115, "x": 108, "u": "https://preview.redd.it/58a9w2fzmcy91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=814ed342b2df633119f4721ac8814912f6715cf4"}, {"y": 231, "x": 216, "u": "https://preview.redd.it/58a9w2fzmcy91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c6a47a96c239494e03a248e6495950e71fdc92be"}, {"y": 342, "x": 320, "u": "https://preview.redd.it/58a9w2fzmcy91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=20650d131f5d3bae3e7397564ac67a35b015795e"}, {"y": 685, "x": 640, "u": "https://preview.redd.it/58a9w2fzmcy91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8bf4939eace3fefe820ff6f3eb974a935b11ce9d"}], "s": {"y": 722, "x": 674, "u": "https://preview.redd.it/58a9w2fzmcy91.png?width=674&amp;format=png&amp;auto=webp&amp;s=3b0d49ccb92d5726a85ee84f503a78c69898a7e0"}, "id": "58a9w2fzmcy91"}}, "name": "t3_yntikb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-MQKY1nORMoAOuz07BAlDH3-yqUn3-eWmNcwDpyKotM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667749519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a huge encrypted external drive and I have some files aand folders which have disappeared from that drive. This was after transferring data from another encrypted external drive. But this is not the first time encountering the problem.&lt;/p&gt;\n\n&lt;p&gt;I had the same issue around 8 months ago. The first time it happened was when I attempted to recover data from an another encrypted drive (with uncorectable errors and reallocation events) did this happen. Of course, I gave up on that since I knew then the data was corrupted.&lt;/p&gt;\n\n&lt;p&gt;I found the missing files from 8 months ago. The files and folders which have disappeared I don&amp;#39;t care about. I&amp;#39;m just worried about the existing data on the drive.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As you can see, everything appears to be healthy on drive E.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/58a9w2fzmcy91.png?width=674&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b0d49ccb92d5726a85ee84f503a78c69898a7e0\"&gt;Showing SMART Values of An Encrypted WD Elements Fully Encrypted External Hard Drive&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Normally, chkdisk could, could solve the issue. But not in my case, right? Since the file system is encrypted. What do I do? Do I delete the files? What about the missing folder? How do I delete the missing folder?&lt;/p&gt;\n\n&lt;p&gt;I have backups. Do I restore from that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yntikb", "is_robot_indexable": true, "report_reasons": null, "author": "Encryptionfactor", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yntikb/disappearing_files_and_folders_on_an_encrypted/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yntikb/disappearing_files_and_folders_on_an_encrypted/", "subreddit_subscribers": 652076, "created_utc": 1667749519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Please tell me I have a chance at recovering the data that was in there.  I tried to look it up but I can't find anything useful and I'm slightly panicking at the realization that all that stuff's gone.", "author_fullname": "t2_coq33uq8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I lost an SSD partition due to some stupid formatting decisions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynst2s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667748101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please tell me I have a chance at recovering the data that was in there.  I tried to look it up but I can&amp;#39;t find anything useful and I&amp;#39;m slightly panicking at the realization that all that stuff&amp;#39;s gone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ynst2s", "is_robot_indexable": true, "report_reasons": null, "author": "cockswain314", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ynst2s/i_lost_an_ssd_partition_due_to_some_stupid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ynst2s/i_lost_an_ssd_partition_due_to_some_stupid/", "subreddit_subscribers": 652076, "created_utc": 1667748101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello\n\nI would like a sanity check for my first NAS plan please. It will be used for a mix of personal video streaming and storing business data.\n\n## NAS\n\n6 or 8 bay (likely 1621+ or 1821+). Haven't yet decided.\n\n## Drives\n\nInitially two 14tb drives and use SHR giving 14tb storage and 1 drive redundancy.\n\nIn future two more 14tb drives and use SHR2, giving 28tb storage and 2 drive redundancy\n\nBeyond that, adding each additional 14tb drive will give another 14tb of storage and maintain 2 drive redundancy.\n\nI've ordered two different models of 14tb drive:\n\n* 1x WD Gold\tWD141KRYZ \t14tb\n* 1x WD Red Pro\tWD141KFGX \t14tb\n\n## Questions\n\n1. I believe the models of Synology are the same apart from number of bays. Is that right?\n2. Is my understanding of SHR2 correct?\n3. I'm hoping mixing models of drive gives some further resilience. Is that right?\n4. will those specific two models play nicely together?\n5. Can I upgrade from SHR to SHR2?\n\n&amp;#x200B;\n\nThank you", "author_fullname": "t2_hi9ke6vf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS Novice - Sanity Check", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynqdud", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667743701.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667742545.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I would like a sanity check for my first NAS plan please. It will be used for a mix of personal video streaming and storing business data.&lt;/p&gt;\n\n&lt;h2&gt;NAS&lt;/h2&gt;\n\n&lt;p&gt;6 or 8 bay (likely 1621+ or 1821+). Haven&amp;#39;t yet decided.&lt;/p&gt;\n\n&lt;h2&gt;Drives&lt;/h2&gt;\n\n&lt;p&gt;Initially two 14tb drives and use SHR giving 14tb storage and 1 drive redundancy.&lt;/p&gt;\n\n&lt;p&gt;In future two more 14tb drives and use SHR2, giving 28tb storage and 2 drive redundancy&lt;/p&gt;\n\n&lt;p&gt;Beyond that, adding each additional 14tb drive will give another 14tb of storage and maintain 2 drive redundancy.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve ordered two different models of 14tb drive:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;1x WD Gold    WD141KRYZ   14tb&lt;/li&gt;\n&lt;li&gt;1x WD Red Pro WD141KFGX   14tb&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Questions&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I believe the models of Synology are the same apart from number of bays. Is that right?&lt;/li&gt;\n&lt;li&gt;Is my understanding of SHR2 correct?&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m hoping mixing models of drive gives some further resilience. Is that right?&lt;/li&gt;\n&lt;li&gt;will those specific two models play nicely together?&lt;/li&gt;\n&lt;li&gt;Can I upgrade from SHR to SHR2?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ynqdud", "is_robot_indexable": true, "report_reasons": null, "author": "omlond", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ynqdud/nas_novice_sanity_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ynqdud/nas_novice_sanity_check/", "subreddit_subscribers": 652076, "created_utc": 1667742545.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My Samsung SSD is in a pool in Drivebender with an older drive that doesn't have an over provisioning mode available. For the Samsung drive OP is currently enabled.  Should I leave it on or does drivebender use it's own provisioning methods that this might just interfere with or make useless?", "author_fullname": "t2_95cjrz7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use SSD over provisioning with Drivebender or no?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo3ius", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667770349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My Samsung SSD is in a pool in Drivebender with an older drive that doesn&amp;#39;t have an over provisioning mode available. For the Samsung drive OP is currently enabled.  Should I leave it on or does drivebender use it&amp;#39;s own provisioning methods that this might just interfere with or make useless?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo3ius", "is_robot_indexable": true, "report_reasons": null, "author": "theveryrealreal", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo3ius/use_ssd_over_provisioning_with_drivebender_or_no/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo3ius/use_ssd_over_provisioning_with_drivebender_or_no/", "subreddit_subscribers": 652076, "created_utc": 1667770349.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}