{"kind": "Listing", "data": {"after": "t3_yo5899", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_582xz2q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reminder: Libgen is also hosted on the IPFS network here, which is decentralized and therefore much harder to take down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yoirof", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 411, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 411, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1667817039.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "libgen-crypto.ipns.dweb.link", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://libgen-crypto.ipns.dweb.link/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yoirof", "is_robot_indexable": true, "report_reasons": null, "author": "Spirited-Pause", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yoirof/reminder_libgen_is_also_hosted_on_the_ipfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://libgen-crypto.ipns.dweb.link/", "subreddit_subscribers": 652221, "created_utc": 1667817039.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need to store about 500GB online, for backup (so never likely to be accessed).\n\nI am aware of:\n\n* iDrive e2: around $4 per month for 1TB (excluding offer for the first year, at $0.33 per month)\n* Backblaze B2: around $2.5 per month (for 500GB)\n* Hetzner Storage Box: around $3.8 for 1TB\n* Hetzner Storage Share (NextCloud): around $5 for 1TB (but with NextCloud web functionality, and 28 backups not taken from storage allowance)\n* Wasabi: around $5 per month per 1TB\n* OVH Cloud Archive: around $3 (but $14 for the initial load)\n\nAnything else cheaper to consider, that I can use with rclone?\n\n=== Edited ===\n\nOther people have mentioned:\n\n* pCloud, at around \u00a34 a month for 500GB\n* Scaleaway Glacier. About 0.85 EUR a month for 500GB. Seems to be possible to store directly to Glacier (without a normal storage step first)\n* AWS Glacier Deep Archive. About $0.5 a month for 500GB. However, seems quite complicated to use. Overall cost is not clear, and minimum 180 day storage\n\nFor the time being, I will use iDrive e2 (since only paying $4 for a year), but if/when they increase their price (I am sure they will), it looks like Backblaze B2 would be the easiest and cheapest option, or Scaleaway Glacier (to be tested). Of course, all may be different in 12 month's time.\n\n&amp;#x200B;", "author_fullname": "t2_lwumyho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the cheapest Cloud storage providers currently? S3 compatible or usable with rclone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo3bgh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 148, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 148, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667806205.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667769926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to store about 500GB online, for backup (so never likely to be accessed).&lt;/p&gt;\n\n&lt;p&gt;I am aware of:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;iDrive e2: around $4 per month for 1TB (excluding offer for the first year, at $0.33 per month)&lt;/li&gt;\n&lt;li&gt;Backblaze B2: around $2.5 per month (for 500GB)&lt;/li&gt;\n&lt;li&gt;Hetzner Storage Box: around $3.8 for 1TB&lt;/li&gt;\n&lt;li&gt;Hetzner Storage Share (NextCloud): around $5 for 1TB (but with NextCloud web functionality, and 28 backups not taken from storage allowance)&lt;/li&gt;\n&lt;li&gt;Wasabi: around $5 per month per 1TB&lt;/li&gt;\n&lt;li&gt;OVH Cloud Archive: around $3 (but $14 for the initial load)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anything else cheaper to consider, that I can use with rclone?&lt;/p&gt;\n\n&lt;p&gt;=== Edited ===&lt;/p&gt;\n\n&lt;p&gt;Other people have mentioned:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;pCloud, at around \u00a34 a month for 500GB&lt;/li&gt;\n&lt;li&gt;Scaleaway Glacier. About 0.85 EUR a month for 500GB. Seems to be possible to store directly to Glacier (without a normal storage step first)&lt;/li&gt;\n&lt;li&gt;AWS Glacier Deep Archive. About $0.5 a month for 500GB. However, seems quite complicated to use. Overall cost is not clear, and minimum 180 day storage&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For the time being, I will use iDrive e2 (since only paying $4 for a year), but if/when they increase their price (I am sure they will), it looks like Backblaze B2 would be the easiest and cheapest option, or Scaleaway Glacier (to be tested). Of course, all may be different in 12 month&amp;#39;s time.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo3bgh", "is_robot_indexable": true, "report_reasons": null, "author": "TedBob99", "discussion_type": null, "num_comments": 68, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo3bgh/what_are_the_cheapest_cloud_storage_providers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo3bgh/what_are_the_cheapest_cloud_storage_providers/", "subreddit_subscribers": 652221, "created_utc": 1667769926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just showing off but I thought this might be useful to people who own several hundred GB of comics. I use it to repack high quality manga on my Kobo, it more than doubled the amount I can store at once.\n\nI know such repacking software already exists because people post on manga sites all the time, but I couldn't find any scripts online, much less ones that were easy to use. This tries to fill that niche, letting you do bulk operations on entire folders.\n\nObvious disclaimer that lossy compression is always, well, lossy, but based on my testing the tradeoff is very favorable (though lossless WebPs are basically a free improvement over PNG, as long as you're okay with using WebP). While this \\*can\\* be used to make images look like a potato, the defaults produce files which are indistinguishable from the source, unless you zoom into individual pixels.\n\nThe thing with CBZ files (which are essentially just a ZIP image folder by a different name), is that many publishers use the JPEG format with parts of the compression algorithm disabled, which while good for distribution, provides little benefit to end users, and by re-enabling those parts you can get free space savings without degrading image quality at all.\n\nThis applies to JPEG images broadly, and this utility can be used for that too (bulk conversion of images), although there are probably better options available.\n\n[https://github.com/avalonv/reCBZ](https://github.com/avalonv/reCBZ)", "author_fullname": "t2_3e6048wu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created a utility that repacks manga and comic books to save disk space", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo3urd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667791450.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667771100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just showing off but I thought this might be useful to people who own several hundred GB of comics. I use it to repack high quality manga on my Kobo, it more than doubled the amount I can store at once.&lt;/p&gt;\n\n&lt;p&gt;I know such repacking software already exists because people post on manga sites all the time, but I couldn&amp;#39;t find any scripts online, much less ones that were easy to use. This tries to fill that niche, letting you do bulk operations on entire folders.&lt;/p&gt;\n\n&lt;p&gt;Obvious disclaimer that lossy compression is always, well, lossy, but based on my testing the tradeoff is very favorable (though lossless WebPs are basically a free improvement over PNG, as long as you&amp;#39;re okay with using WebP). While this *can* be used to make images look like a potato, the defaults produce files which are indistinguishable from the source, unless you zoom into individual pixels.&lt;/p&gt;\n\n&lt;p&gt;The thing with CBZ files (which are essentially just a ZIP image folder by a different name), is that many publishers use the JPEG format with parts of the compression algorithm disabled, which while good for distribution, provides little benefit to end users, and by re-enabling those parts you can get free space savings without degrading image quality at all.&lt;/p&gt;\n\n&lt;p&gt;This applies to JPEG images broadly, and this utility can be used for that too (bulk conversion of images), although there are probably better options available.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/avalonv/reCBZ\"&gt;https://github.com/avalonv/reCBZ&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YqoArXeLJ1Qi373lei88vrP_rFj3Z6MuyEasqZfUaNs.jpg?auto=webp&amp;s=84ff58ace6f502718c4a73d11bd5e34ced6f0bfb", "width": 1028, "height": 505}, "resolutions": [{"url": "https://external-preview.redd.it/YqoArXeLJ1Qi373lei88vrP_rFj3Z6MuyEasqZfUaNs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f454e5cd52ce6c6b4e1409addd7a6c13a092f430", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/YqoArXeLJ1Qi373lei88vrP_rFj3Z6MuyEasqZfUaNs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f98b22a0b3642b8d978f1e4cfb1e693582fa3b9", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/YqoArXeLJ1Qi373lei88vrP_rFj3Z6MuyEasqZfUaNs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e8c20ff787eab1de990309a1014ebdf63321995", "width": 320, "height": 157}, {"url": "https://external-preview.redd.it/YqoArXeLJ1Qi373lei88vrP_rFj3Z6MuyEasqZfUaNs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2bc3ab3f52d88b9052f6540564d6f6e6731f6721", "width": 640, "height": 314}, {"url": "https://external-preview.redd.it/YqoArXeLJ1Qi373lei88vrP_rFj3Z6MuyEasqZfUaNs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b962b767143761b2ef2d6a95dea4d0710fd81a65", "width": 960, "height": 471}], "variants": {}, "id": "PyYxW-cFjbnslNZDJI9-oeGnfzWDwVTI_S8I_ihNSzE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo3urd", "is_robot_indexable": true, "report_reasons": null, "author": "reallyfuckingay", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo3urd/i_created_a_utility_that_repacks_manga_and_comic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo3urd/i_created_a_utility_that_repacks_manga_and_comic/", "subreddit_subscribers": 652221, "created_utc": 1667771100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, I do apologize if this is not the right subreddit, but since this is an issue I came across while organizing my NAS contents, I thought someone here may help, and I do believe after all, this is effectively about archival of files.\n\nI'm currently running a script in a directory structure containing various video files and encodes everything in an x265 MKV file. I thought that the only file present would be mov or mp4, but it turns out there were two jpg files too.  \nThe funny thing, is that HandBrake CLI actually crated an MKV file for those without complaining, and reduced the file size from 3MB to 400KB. Opening it with VLC just flashes the image for a second, while other media players keep the frame, I didn't do any quality analysis, but at first glance the quality seem similar (consider that the picture quality wasn't great to begin with, and I'm with the size being reduced even if lossy)...\n\nNow, aside the fact that for sure there is a way to make a much better JPG and shrink the file size while still being a JPG, has anybody ever actually used x265 for image compression? Seems pretty ridiculous, but I'm kind of curious about it.\n\nClearly thought, an image cannot just stay inside an MKV container, it's too much of a pain.\n\nSorry for the weird question and thanks in advance!", "author_fullname": "t2_7vil65ud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HEVC for image compression?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo6wh7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667778903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I do apologize if this is not the right subreddit, but since this is an issue I came across while organizing my NAS contents, I thought someone here may help, and I do believe after all, this is effectively about archival of files.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently running a script in a directory structure containing various video files and encodes everything in an x265 MKV file. I thought that the only file present would be mov or mp4, but it turns out there were two jpg files too.&lt;br/&gt;\nThe funny thing, is that HandBrake CLI actually crated an MKV file for those without complaining, and reduced the file size from 3MB to 400KB. Opening it with VLC just flashes the image for a second, while other media players keep the frame, I didn&amp;#39;t do any quality analysis, but at first glance the quality seem similar (consider that the picture quality wasn&amp;#39;t great to begin with, and I&amp;#39;m with the size being reduced even if lossy)...&lt;/p&gt;\n\n&lt;p&gt;Now, aside the fact that for sure there is a way to make a much better JPG and shrink the file size while still being a JPG, has anybody ever actually used x265 for image compression? Seems pretty ridiculous, but I&amp;#39;m kind of curious about it.&lt;/p&gt;\n\n&lt;p&gt;Clearly thought, an image cannot just stay inside an MKV container, it&amp;#39;s too much of a pain.&lt;/p&gt;\n\n&lt;p&gt;Sorry for the weird question and thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo6wh7", "is_robot_indexable": true, "report_reasons": null, "author": "LynxesExe", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo6wh7/hevc_for_image_compression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo6wh7/hevc_for_image_compression/", "subreddit_subscribers": 652221, "created_utc": 1667778903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_z1dy9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Samsung Electronics Begins Mass Production of 8th-Gen Vertical NAND With Industry\u2019s Highest Bit Density", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_yoq0g0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/FX8QjYrnNw4FSG5NebkuEtaFpOXQRhoV0OG0Rja9iC8.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667834294.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "news.samsung.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://news.samsung.com/global/samsung-electronics-begins-mass-production-of-8th-gen-vertical-nand-with-industrys-highest-bit-density", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_vA_8Eo0fP4BWtTyoz3ZA1Gq9yVPEkCxe-SFjgtS1XM.jpg?auto=webp&amp;s=32fd6b0b831240d7df83728cb913107f9adad8ec", "width": 728, "height": 410}, "resolutions": [{"url": "https://external-preview.redd.it/_vA_8Eo0fP4BWtTyoz3ZA1Gq9yVPEkCxe-SFjgtS1XM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a3776e4dd14a1f0346357c9b66c3ffb533ef67fa", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/_vA_8Eo0fP4BWtTyoz3ZA1Gq9yVPEkCxe-SFjgtS1XM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=792d20ff8504f931080bde92b27ea690aed4755f", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/_vA_8Eo0fP4BWtTyoz3ZA1Gq9yVPEkCxe-SFjgtS1XM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=84f7c1dbe3e20d047c89868a30e2be027c450f37", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/_vA_8Eo0fP4BWtTyoz3ZA1Gq9yVPEkCxe-SFjgtS1XM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=98dda8cd2f98230ad95b774a5368372aca9a6be8", "width": 640, "height": 360}], "variants": {}, "id": "1QEvcy14OIjkgrX5fEpTTnqPTroYnGMuWIGY4WyK5xU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "84TB QLC + 48TB CMR", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yoq0g0", "is_robot_indexable": true, "report_reasons": null, "author": "etherealshatter", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yoq0g0/samsung_electronics_begins_mass_production_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://news.samsung.com/global/samsung-electronics-begins-mass-production-of-8th-gen-vertical-nand-with-industrys-highest-bit-density", "subreddit_subscribers": 652221, "created_utc": 1667834294.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to download a copy of this site rpgtinker.com for offline use. I'm in the navy and the nature of carrier life prevents me from being able access it on a regular basis.\n\n I run a couple d&amp;d campaigns on the ship, and the ability to generate these statblocks for npcs on the fly is incredibly useful.\n\nI've tried things like Httrack and the like, and the critical function of actually generating the statblocks doesn't seem to work, and I'm way out of my depth here. I'm straight up at the point where I'd be willing to throw money at the issue to get this done. I've dug into any and all alternatives, and there are none that really do what this site does. Sites, apps, I've looked all over. The only alternative is foundry vtt, and that would cost me over 50 bucks and would pale in comparison to the options this website provides for free", "author_fullname": "t2_7lreihth", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need a hand here.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo3w3o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667771198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to download a copy of this site rpgtinker.com for offline use. I&amp;#39;m in the navy and the nature of carrier life prevents me from being able access it on a regular basis.&lt;/p&gt;\n\n&lt;p&gt;I run a couple d&amp;amp;d campaigns on the ship, and the ability to generate these statblocks for npcs on the fly is incredibly useful.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried things like Httrack and the like, and the critical function of actually generating the statblocks doesn&amp;#39;t seem to work, and I&amp;#39;m way out of my depth here. I&amp;#39;m straight up at the point where I&amp;#39;d be willing to throw money at the issue to get this done. I&amp;#39;ve dug into any and all alternatives, and there are none that really do what this site does. Sites, apps, I&amp;#39;ve looked all over. The only alternative is foundry vtt, and that would cost me over 50 bucks and would pale in comparison to the options this website provides for free&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo3w3o", "is_robot_indexable": true, "report_reasons": null, "author": "Detrand", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo3w3o/i_need_a_hand_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo3w3o/i_need_a_hand_here/", "subreddit_subscribers": 652221, "created_utc": 1667771198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There are hundreds of search results to click on out there, and I've clicked on dozens, that all repost the same performance comparison charts. Some even give some helpful history of how CAT7 is not an official standard. But none say what the actual difference in the cable itself is. Does anyone here know or have found quality information? \n\nEdit. Just to save reading time to anyone coming across this in the future search results: there is no clarity as to the physical design.", "author_fullname": "t2_gedtx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the physical difference between CAT7 vs CAT8 Ethernet cabling?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yon4pj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667842746.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667828233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are hundreds of search results to click on out there, and I&amp;#39;ve clicked on dozens, that all repost the same performance comparison charts. Some even give some helpful history of how CAT7 is not an official standard. But none say what the actual difference in the cable itself is. Does anyone here know or have found quality information? &lt;/p&gt;\n\n&lt;p&gt;Edit. Just to save reading time to anyone coming across this in the future search results: there is no clarity as to the physical design.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yon4pj", "is_robot_indexable": true, "report_reasons": null, "author": "Discoveryellow", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yon4pj/whats_the_physical_difference_between_cat7_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yon4pj/whats_the_physical_difference_between_cat7_vs/", "subreddit_subscribers": 652221, "created_utc": 1667828233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It looks like Arrimus3D after deciding to take a break for a while has delisted/removed hundreds of videos from the last fifteen years of being on YouTube. There's some concern for his wellbeing in the 3d community here on Reddit as his last couple of videos have had some statements in them that have a few people speculating.\n\nFor those who aren't in the 3D world, his tutorials were some of the best, most specific and most useful tutorials out there for beginners and professionals alike converting everything from basics to very in depth modelling across various pieces if software .\n\nHas anyone got an archive? Was anyone backing up?\nIf would be a huge blow to the 3D community to lose his considerable depth of knowledge and fantastic tuition. He was essentially a reference guide for generalists and 3D modellers and basically anyone as a hobbyist or a professional will either know the username or have unknowingly followed one of his tutorials. It's a big BIG loss if this content is gone forever.\n\nedit: downvote, geee thanks!", "author_fullname": "t2_eq196", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Arrimus3D - one of the best 3D software tutorial YouTube channel removes content", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo9eob", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1667823183.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667785842.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It looks like Arrimus3D after deciding to take a break for a while has delisted/removed hundreds of videos from the last fifteen years of being on YouTube. There&amp;#39;s some concern for his wellbeing in the 3d community here on Reddit as his last couple of videos have had some statements in them that have a few people speculating.&lt;/p&gt;\n\n&lt;p&gt;For those who aren&amp;#39;t in the 3D world, his tutorials were some of the best, most specific and most useful tutorials out there for beginners and professionals alike converting everything from basics to very in depth modelling across various pieces if software .&lt;/p&gt;\n\n&lt;p&gt;Has anyone got an archive? Was anyone backing up?\nIf would be a huge blow to the 3D community to lose his considerable depth of knowledge and fantastic tuition. He was essentially a reference guide for generalists and 3D modellers and basically anyone as a hobbyist or a professional will either know the username or have unknowingly followed one of his tutorials. It&amp;#39;s a big BIG loss if this content is gone forever.&lt;/p&gt;\n\n&lt;p&gt;edit: downvote, geee thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yo9eob", "is_robot_indexable": true, "report_reasons": null, "author": "jpjapers", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo9eob/arrimus3d_one_of_the_best_3d_software_tutorial/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo9eob/arrimus3d_one_of_the_best_3d_software_tutorial/", "subreddit_subscribers": 652221, "created_utc": 1667785842.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So my grandfather was a computer engineer, he passed away and now i have a bunch of old legit CD's most with keys of windows, microsoft programs and others. (pre 2010)\n\nany suggestions what i should do with them, i know there's a case use out there for them, but (due to need of downgrading storage after a divorce) I'm to the point of just chucking them.  what would be the best way of giving people a chance to grab them.  or is there a digital repository i can upload an ISO?", "author_fullname": "t2_bg78e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "looking to unload physical cd's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yo1fl8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667765974.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So my grandfather was a computer engineer, he passed away and now i have a bunch of old legit CD&amp;#39;s most with keys of windows, microsoft programs and others. (pre 2010)&lt;/p&gt;\n\n&lt;p&gt;any suggestions what i should do with them, i know there&amp;#39;s a case use out there for them, but (due to need of downgrading storage after a divorce) I&amp;#39;m to the point of just chucking them.  what would be the best way of giving people a chance to grab them.  or is there a digital repository i can upload an ISO?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo1fl8", "is_robot_indexable": true, "report_reasons": null, "author": "cyten23", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo1fl8/looking_to_unload_physical_cds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo1fl8/looking_to_unload_physical_cds/", "subreddit_subscribers": 652221, "created_utc": 1667765974.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Say I have a 4 tb NAS drive that I want to make an offline backup of. I have four 1TB drives and only one dock so only one drive can be attached to the system at a time. Is there an elegant way to start copying files to the 4tb drive to the first 1tb drive until it runs out of space, then wait till I swap it with an empty drive, and then keep copying, keeping track of all the drives needed to do this in order? I don't want the drives in RAID, because that would require plugging all of them in at once, and I also don't want one drive failing taking the data stored on all the other drives with it.\n\nIsn't this how tape drives work for storing huge drive arrays? It should be pretty standard archival practice right?\n\nIdeally, I don't want to *have* to fully reassemble the data to find something. I want to be able to plug in a drive, search around for what it contains out of the larger drive, and copy individual files back without doing a full restore. Though if an individual file is larger than any one drive, I'd like it to be split between the drives and then able to be reassembled. Also, if this can be done at the individual file level and written into any filesystem I want, that'd be even better since then I can take advantage of having the archival drives LUKS encrypted and also using filesystem level compression. \n\nIs there a solution that can do all this? I use Linux if that makes a difference on works and what doesn't.", "author_fullname": "t2_1h1hfeve", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there an elegant way of splitting a huge file or folder to be archived over many drives that each alone would not be large enough to hold everything, where I can swap the drives sequentially as they fill up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yowobx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667847622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say I have a 4 tb NAS drive that I want to make an offline backup of. I have four 1TB drives and only one dock so only one drive can be attached to the system at a time. Is there an elegant way to start copying files to the 4tb drive to the first 1tb drive until it runs out of space, then wait till I swap it with an empty drive, and then keep copying, keeping track of all the drives needed to do this in order? I don&amp;#39;t want the drives in RAID, because that would require plugging all of them in at once, and I also don&amp;#39;t want one drive failing taking the data stored on all the other drives with it.&lt;/p&gt;\n\n&lt;p&gt;Isn&amp;#39;t this how tape drives work for storing huge drive arrays? It should be pretty standard archival practice right?&lt;/p&gt;\n\n&lt;p&gt;Ideally, I don&amp;#39;t want to &lt;em&gt;have&lt;/em&gt; to fully reassemble the data to find something. I want to be able to plug in a drive, search around for what it contains out of the larger drive, and copy individual files back without doing a full restore. Though if an individual file is larger than any one drive, I&amp;#39;d like it to be split between the drives and then able to be reassembled. Also, if this can be done at the individual file level and written into any filesystem I want, that&amp;#39;d be even better since then I can take advantage of having the archival drives LUKS encrypted and also using filesystem level compression. &lt;/p&gt;\n\n&lt;p&gt;Is there a solution that can do all this? I use Linux if that makes a difference on works and what doesn&amp;#39;t.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yowobx", "is_robot_indexable": true, "report_reasons": null, "author": "AgreeableLandscape3", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yowobx/is_there_an_elegant_way_of_splitting_a_huge_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yowobx/is_there_an_elegant_way_of_splitting_a_huge_file/", "subreddit_subscribers": 652221, "created_utc": 1667847622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey All,\n\nHas anyone used this case for a home build?\n\nhttps://www.alibaba.com/product-detail/Manufacture-IPFS-6-hot-swap-bays_1600079165228.html\n\nI currently have a 4 bay case that works ok, but I want to be able to expand to 6 disks and this one looks great for the price.", "author_fullname": "t2_33q0p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "6 Bay Server Case from Alibaba", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yospkl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667839670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All,&lt;/p&gt;\n\n&lt;p&gt;Has anyone used this case for a home build?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.alibaba.com/product-detail/Manufacture-IPFS-6-hot-swap-bays_1600079165228.html\"&gt;https://www.alibaba.com/product-detail/Manufacture-IPFS-6-hot-swap-bays_1600079165228.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I currently have a 4 bay case that works ok, but I want to be able to expand to 6 disks and this one looks great for the price.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yospkl", "is_robot_indexable": true, "report_reasons": null, "author": "y2kdread", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yospkl/6_bay_server_case_from_alibaba/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yospkl/6_bay_server_case_from_alibaba/", "subreddit_subscribers": 652221, "created_utc": 1667839670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for advice regarding HDD cache size. I'm planning on adding 2 drives to my linux server in a LVM Raid 0 configuration, it should match my existing raid5 in capacity (22tb usable). old raid 5 will become the backup volume, so no concern for data loss. LVM raid will allow me a on-the-fly conversion from raid 0 to 5 once I add a third disk. The server is used for many things, but 4k streaming via plex is the only real resource consuming thing (although we have only 1 stream at a time).\n\nWD Red Plus 12TB (256MB Cache) for \u20ac298,90 or \u20ac24,908/TB\n\nWD Red Plus 14TB (512MB Cache) for \u20ac373,98 or \u20ac26,713/TB\n\nOriginally I had decided to get the WD Red Plus 12TB, but then I noticed that the 14TB has double the cache size. I'm not sure how strongly that would impact the performance and if it will be worth the extra 150\u20ac? Does anybody have any insights or opinions on that?", "author_fullname": "t2_bewks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Importance of HDD cache within Raid, for Plex", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yop4ch", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667832466.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for advice regarding HDD cache size. I&amp;#39;m planning on adding 2 drives to my linux server in a LVM Raid 0 configuration, it should match my existing raid5 in capacity (22tb usable). old raid 5 will become the backup volume, so no concern for data loss. LVM raid will allow me a on-the-fly conversion from raid 0 to 5 once I add a third disk. The server is used for many things, but 4k streaming via plex is the only real resource consuming thing (although we have only 1 stream at a time).&lt;/p&gt;\n\n&lt;p&gt;WD Red Plus 12TB (256MB Cache) for \u20ac298,90 or \u20ac24,908/TB&lt;/p&gt;\n\n&lt;p&gt;WD Red Plus 14TB (512MB Cache) for \u20ac373,98 or \u20ac26,713/TB&lt;/p&gt;\n\n&lt;p&gt;Originally I had decided to get the WD Red Plus 12TB, but then I noticed that the 14TB has double the cache size. I&amp;#39;m not sure how strongly that would impact the performance and if it will be worth the extra 150\u20ac? Does anybody have any insights or opinions on that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yop4ch", "is_robot_indexable": true, "report_reasons": null, "author": "Rakyr51", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yop4ch/importance_of_hdd_cache_within_raid_for_plex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yop4ch/importance_of_hdd_cache_within_raid_for_plex/", "subreddit_subscribers": 652221, "created_utc": 1667832466.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I own a dedicated server on Hetzner.\n\nJust purchased one of their Storage Boxes.\n\nNow I want to clone the server's disk onto the storage box (can be accessed via a lot of ways, mainly SSH, but also borg, etc...)\n\nWould be amazing to have a similar operation to what VMs can do (hit backup &amp; restore and you are good to go)\n\nthis is the result of running `df -h`\n\n    Filesystem            Size  Used Avail Use% Mounted on\n    udev                   63G     0   63G   0% /dev\n    tmpfs                  13G  1.9M   13G   1% /run\n    /dev/mapper/vg0-root  7.0T  1.2T  5.4T  18% /\n    tmpfs                  63G     0   63G   0% /dev/shm\n    tmpfs                 5.0M     0  5.0M   0% /run/lock\n    tmpfs                  63G     0   63G   0% /sys/fs/cgroup\n    /dev/mapper/vg0-tmp    20G   60M   20G   1% /tmp\n    /dev/md0              485M  228M  232M  50% /boot\n    tmpfs                  13G   20K   13G   1% /run/user/124\n    tmpfs                  13G     0   13G   0% /run/user/0\n    tmpfs                  13G   12K   13G   1% /run/user/1000\n    overlay               7.0T  1.2T  5.4T  18% /var/lib/docker/overlay2/1ba8b1b49fca8915168f1566f1be685050129715e77b080b69b88124b6d90d6c/merged\n\nThis is a **RAID0** setup\n\nWhat I want is an image of the disk I can restore in case something bad happens. **But I can't unmount the partition** while backing it up since there are critical applications running on it.\n\nAny help would be appreciated!", "author_fullname": "t2_30uknr73", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to clone a server disk while it's running to external storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yowe2j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667847066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I own a dedicated server on Hetzner.&lt;/p&gt;\n\n&lt;p&gt;Just purchased one of their Storage Boxes.&lt;/p&gt;\n\n&lt;p&gt;Now I want to clone the server&amp;#39;s disk onto the storage box (can be accessed via a lot of ways, mainly SSH, but also borg, etc...)&lt;/p&gt;\n\n&lt;p&gt;Would be amazing to have a similar operation to what VMs can do (hit backup &amp;amp; restore and you are good to go)&lt;/p&gt;\n\n&lt;p&gt;this is the result of running &lt;code&gt;df -h&lt;/code&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Filesystem            Size  Used Avail Use% Mounted on\nudev                   63G     0   63G   0% /dev\ntmpfs                  13G  1.9M   13G   1% /run\n/dev/mapper/vg0-root  7.0T  1.2T  5.4T  18% /\ntmpfs                  63G     0   63G   0% /dev/shm\ntmpfs                 5.0M     0  5.0M   0% /run/lock\ntmpfs                  63G     0   63G   0% /sys/fs/cgroup\n/dev/mapper/vg0-tmp    20G   60M   20G   1% /tmp\n/dev/md0              485M  228M  232M  50% /boot\ntmpfs                  13G   20K   13G   1% /run/user/124\ntmpfs                  13G     0   13G   0% /run/user/0\ntmpfs                  13G   12K   13G   1% /run/user/1000\noverlay               7.0T  1.2T  5.4T  18% /var/lib/docker/overlay2/1ba8b1b49fca8915168f1566f1be685050129715e77b080b69b88124b6d90d6c/merged\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This is a &lt;strong&gt;RAID0&lt;/strong&gt; setup&lt;/p&gt;\n\n&lt;p&gt;What I want is an image of the disk I can restore in case something bad happens. &lt;strong&gt;But I can&amp;#39;t unmount the partition&lt;/strong&gt; while backing it up since there are critical applications running on it.&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yowe2j", "is_robot_indexable": true, "report_reasons": null, "author": "leonardofiori", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yowe2j/best_way_to_clone_a_server_disk_while_its_running/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yowe2j/best_way_to_clone_a_server_disk_while_its_running/", "subreddit_subscribers": 652221, "created_utc": 1667847066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Working on archiving some YouTube channels I enjoy. What do you typically archive at? 1080p? Full 4K if it\u2019s available? Does it depend on the channel?", "author_fullname": "t2_1h4qk0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What quality do you guys archive YouTube videos at?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yovlvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667845515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on archiving some YouTube channels I enjoy. What do you typically archive at? 1080p? Full 4K if it\u2019s available? Does it depend on the channel?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yovlvo", "is_robot_indexable": true, "report_reasons": null, "author": "awkw4rdkid", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yovlvo/what_quality_do_you_guys_archive_youtube_videos_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yovlvo/what_quality_do_you_guys_archive_youtube_videos_at/", "subreddit_subscribers": 652221, "created_utc": 1667845515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have sufficient \"restore things to the way they were before the disaster\" back-ups, but I'm I've started my own business and I'm looking to getting back to my \"Sure, I have the version of that file from five years ago.\" CYA archiving. Through the years I've migrated through the \"stack of floppies\", \"stack of 1/4\" tapes\", \"stack of CDs\" and \"stack of DVDs\". Is there anything that should stay current for the next decade? What (w/ proper storage) should last until it's time to migrate to the next \"Latest &amp; Greatest\"? \n\nIt kinda seems that there is no longevity/cost-to-start/cost-to-maintain sweet-spot. M-Discs seem good longevity-wise, but they're expensive/TB and their individual capacity is a bit on the low side. Duplicate HDs are good on the cost/TB side, but how long would they last sitting in a firesafe once filled? LTO is great on the capacity and cost/TB, but start-up costs are very high and living through the floppy &amp; 1/4\" tape era, I'm leery about the longevity of tape in storage.  \n\nHow reliable are refurbished LTO drives? They make the start-up costs more manageable, but how likely is a drive to clobber a tape and make it unreadable?", "author_fullname": "t2_mv7lo41", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Future Proofing\" Long Term Archives (Old School Datahorder needing to get back into it)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_you704", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667842638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have sufficient &amp;quot;restore things to the way they were before the disaster&amp;quot; back-ups, but I&amp;#39;m I&amp;#39;ve started my own business and I&amp;#39;m looking to getting back to my &amp;quot;Sure, I have the version of that file from five years ago.&amp;quot; CYA archiving. Through the years I&amp;#39;ve migrated through the &amp;quot;stack of floppies&amp;quot;, &amp;quot;stack of 1/4&amp;quot; tapes&amp;quot;, &amp;quot;stack of CDs&amp;quot; and &amp;quot;stack of DVDs&amp;quot;. Is there anything that should stay current for the next decade? What (w/ proper storage) should last until it&amp;#39;s time to migrate to the next &amp;quot;Latest &amp;amp; Greatest&amp;quot;? &lt;/p&gt;\n\n&lt;p&gt;It kinda seems that there is no longevity/cost-to-start/cost-to-maintain sweet-spot. M-Discs seem good longevity-wise, but they&amp;#39;re expensive/TB and their individual capacity is a bit on the low side. Duplicate HDs are good on the cost/TB side, but how long would they last sitting in a firesafe once filled? LTO is great on the capacity and cost/TB, but start-up costs are very high and living through the floppy &amp;amp; 1/4&amp;quot; tape era, I&amp;#39;m leery about the longevity of tape in storage.  &lt;/p&gt;\n\n&lt;p&gt;How reliable are refurbished LTO drives? They make the start-up costs more manageable, but how likely is a drive to clobber a tape and make it unreadable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "you704", "is_robot_indexable": true, "report_reasons": null, "author": "MeButNotMeToo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/you704/future_proofing_long_term_archives_old_school/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/you704/future_proofing_long_term_archives_old_school/", "subreddit_subscribers": 652221, "created_utc": 1667842638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is a \"problem\" I've had for many years, on multiple systems, distros and so on that I can't really figure out and maybe there's just no solution to it.\n\nThe problem is that when I write to a disk (a 8TB WD Red in this case) it completely tanks the read speeds of it. Trying to read a file from the disk during the write operation is almost impossible, for example trying to play a video file over samba will take 10 seconds to even start. The disk is happily writing at 150-170MB/s while at most I can get around 1MB/s trying to read something, almost as if the write operation gets complete priority. As soon as the write is done I'm back to full read speeds which is about 170MB/s.\n\nOne consequences of this is that if someone is watching Plex and the particular file they are streaming resides on \"disk1\" and I need to copy some files to that disk at the same time, 'cp /mnt/disk2/file /mnt/disk1/file', the Plex stream is very likely to start buffering.\n\nTo me it seems like a I/O scheduler issue but I've tried all of them with no success. Currently I'm using BFQ and not even ionice -c 3 is helping. It's also possible that this is just the nature of hard disks and there's no way to get around that, I don't know.\n\nSome info about my system:\n\n* OS: Arch Linux\n* Kernel: 6.0.7-arch1-1\n* Asus Prime H510M-K\n* Intel i3-10100\n* 16GB of RAM (usually my usage is around 3-4GB)\n* Disks directly connected to the motherboard\n\nI experienced the same issue back when I was running WD Red 3TB disks on a completely different system and different distro (Debian) so I highly doubt it's an hardware issue.\n\nI'm running LUKS on the disks with BTRFS on top and then pooled with MergerFS. During my troubleshooting I'm copying files directly from one disk to another, bypassing MergerFS so that should have nothing to do with it.\n\nI have also played around with 'vm.dirty\\_ratio' and 'vm.dirty\\_background\\_ratio' but it makes no difference.", "author_fullname": "t2_s484g8uy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Writing to disk slows read speeds down to almost zero", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_you62o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667842578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a &amp;quot;problem&amp;quot; I&amp;#39;ve had for many years, on multiple systems, distros and so on that I can&amp;#39;t really figure out and maybe there&amp;#39;s just no solution to it.&lt;/p&gt;\n\n&lt;p&gt;The problem is that when I write to a disk (a 8TB WD Red in this case) it completely tanks the read speeds of it. Trying to read a file from the disk during the write operation is almost impossible, for example trying to play a video file over samba will take 10 seconds to even start. The disk is happily writing at 150-170MB/s while at most I can get around 1MB/s trying to read something, almost as if the write operation gets complete priority. As soon as the write is done I&amp;#39;m back to full read speeds which is about 170MB/s.&lt;/p&gt;\n\n&lt;p&gt;One consequences of this is that if someone is watching Plex and the particular file they are streaming resides on &amp;quot;disk1&amp;quot; and I need to copy some files to that disk at the same time, &amp;#39;cp /mnt/disk2/file /mnt/disk1/file&amp;#39;, the Plex stream is very likely to start buffering.&lt;/p&gt;\n\n&lt;p&gt;To me it seems like a I/O scheduler issue but I&amp;#39;ve tried all of them with no success. Currently I&amp;#39;m using BFQ and not even ionice -c 3 is helping. It&amp;#39;s also possible that this is just the nature of hard disks and there&amp;#39;s no way to get around that, I don&amp;#39;t know.&lt;/p&gt;\n\n&lt;p&gt;Some info about my system:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OS: Arch Linux&lt;/li&gt;\n&lt;li&gt;Kernel: 6.0.7-arch1-1&lt;/li&gt;\n&lt;li&gt;Asus Prime H510M-K&lt;/li&gt;\n&lt;li&gt;Intel i3-10100&lt;/li&gt;\n&lt;li&gt;16GB of RAM (usually my usage is around 3-4GB)&lt;/li&gt;\n&lt;li&gt;Disks directly connected to the motherboard&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I experienced the same issue back when I was running WD Red 3TB disks on a completely different system and different distro (Debian) so I highly doubt it&amp;#39;s an hardware issue.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running LUKS on the disks with BTRFS on top and then pooled with MergerFS. During my troubleshooting I&amp;#39;m copying files directly from one disk to another, bypassing MergerFS so that should have nothing to do with it.&lt;/p&gt;\n\n&lt;p&gt;I have also played around with &amp;#39;vm.dirty_ratio&amp;#39; and &amp;#39;vm.dirty_background_ratio&amp;#39; but it makes no difference.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "you62o", "is_robot_indexable": true, "report_reasons": null, "author": "bepsifif", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/you62o/writing_to_disk_slows_read_speeds_down_to_almost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/you62o/writing_to_disk_slows_read_speeds_down_to_almost/", "subreddit_subscribers": 652221, "created_utc": 1667842578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello r/DataHoarder\n\nQuite often have seen in comments that people suggest to move to HBA expansion cards for users who run bunch of HDDs, as this is more safe and provides more slots for those who run 8 or even more drives. By HBA, mean SAS HBA devices.\n\nMy question would be - **Why such devices are preferred in comparison to plugging many HDDs to SATA ports?** \n\nAt the moment I have 6 drives connected, these already populate entire motherboard's available SATA ports. Also have an PCI Express SATA Controller Expansion Card (1 x PCIe X1 to 4 SATA), but currently there is no need in more slots. While on other hand probably I would like to expand drive pool later. Heard such cards are not recommended, because often cheaply built and can affect speeds as all drives will share single interface.", "author_fullname": "t2_cg19jzl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HBA or not HBA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yotlik", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667841415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/DataHoarder\"&gt;r/DataHoarder&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Quite often have seen in comments that people suggest to move to HBA expansion cards for users who run bunch of HDDs, as this is more safe and provides more slots for those who run 8 or even more drives. By HBA, mean SAS HBA devices.&lt;/p&gt;\n\n&lt;p&gt;My question would be - &lt;strong&gt;Why such devices are preferred in comparison to plugging many HDDs to SATA ports?&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;At the moment I have 6 drives connected, these already populate entire motherboard&amp;#39;s available SATA ports. Also have an PCI Express SATA Controller Expansion Card (1 x PCIe X1 to 4 SATA), but currently there is no need in more slots. While on other hand probably I would like to expand drive pool later. Heard such cards are not recommended, because often cheaply built and can affect speeds as all drives will share single interface.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "4-4-4-12-12-12TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yotlik", "is_robot_indexable": true, "report_reasons": null, "author": "q1525882", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yotlik/hba_or_not_hba/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yotlik/hba_or_not_hba/", "subreddit_subscribers": 652221, "created_utc": 1667841415.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've the necessity to organize my comics in folders. I need some tool that automatically creates a folder from the filename. The files have names as \"name of the series\" \"number of the issue\". I am able using an utility named \"robobasket\" to create something like that but works only with the first word of the name. So if I have some files that start with \"the\" for istance I have the folder \"the\" but the files are not related to each other. Do you have any suggestion? thanks in advance", "author_fullname": "t2_yf9aa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizing comics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yosu62", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667839918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve the necessity to organize my comics in folders. I need some tool that automatically creates a folder from the filename. The files have names as &amp;quot;name of the series&amp;quot; &amp;quot;number of the issue&amp;quot;. I am able using an utility named &amp;quot;robobasket&amp;quot; to create something like that but works only with the first word of the name. So if I have some files that start with &amp;quot;the&amp;quot; for istance I have the folder &amp;quot;the&amp;quot; but the files are not related to each other. Do you have any suggestion? thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yosu62", "is_robot_indexable": true, "report_reasons": null, "author": "alexross80", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yosu62/organizing_comics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yosu62/organizing_comics/", "subreddit_subscribers": 652221, "created_utc": 1667839918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't want to download the files one by one", "author_fullname": "t2_nnj9ml6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is there any way to download folders from mediafire for free?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_youyzn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667844283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t want to download the files one by one&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "youyzn", "is_robot_indexable": true, "report_reasons": null, "author": "SaiaExitt", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/youyzn/is_there_any_way_to_download_folders_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/youyzn/is_there_any_way_to_download_folders_from/", "subreddit_subscribers": 652221, "created_utc": 1667844283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Easystore 14 TB (Example) - 5400 RPM, 2 year warranty $199.99 on sale...\n\nWestern Digital 14TB WD Red Pro NAS - 7200 RPM, 5 year warranty $260.00 on sale...\n\nYou guys really think the better speeds and 3 years extra warranty isn't worth it?", "author_fullname": "t2_iver8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "easystore drives compared to PRO red drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yodojy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667799532.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667798613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Easystore 14 TB (Example) - 5400 RPM, 2 year warranty $199.99 on sale...&lt;/p&gt;\n\n&lt;p&gt;Western Digital 14TB WD Red Pro NAS - 7200 RPM, 5 year warranty $260.00 on sale...&lt;/p&gt;\n\n&lt;p&gt;You guys really think the better speeds and 3 years extra warranty isn&amp;#39;t worth it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yodojy", "is_robot_indexable": true, "report_reasons": null, "author": "SickestGuy", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yodojy/easystore_drives_compared_to_pro_red_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yodojy/easystore_drives_compared_to_pro_red_drives/", "subreddit_subscribers": 652221, "created_utc": 1667798613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_j0olzbht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Facebook is proceeding to employ the Web3 platform to archive its creators' digital collectibles.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_yosuqs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/d8FkzU9neEUw8tzAR2Ic5wR4f09aa_p49XnYxqIEObQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667839949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "coindesk.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "http://www.coindesk.com", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lSu9jSN3_LuAKIs74or5z0N6oYPDP4QBEKGRLBviyA4.jpg?auto=webp&amp;s=10d0c73d9de9320da8c8045d1ba679dd7a109171", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/lSu9jSN3_LuAKIs74or5z0N6oYPDP4QBEKGRLBviyA4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e2575eeefa7ded330e986697df829811ab27491e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/lSu9jSN3_LuAKIs74or5z0N6oYPDP4QBEKGRLBviyA4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8ed45377e88d4473bd499072762dfdb9abfc81bf", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/lSu9jSN3_LuAKIs74or5z0N6oYPDP4QBEKGRLBviyA4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d372db64a122d5b6c429a2813f4c88d6faf9b9f", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/lSu9jSN3_LuAKIs74or5z0N6oYPDP4QBEKGRLBviyA4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a798e0f24ce7c0a0eb8b0d7bbfa0a8aac3cdf828", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/lSu9jSN3_LuAKIs74or5z0N6oYPDP4QBEKGRLBviyA4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b4113f47eab24a841bb45d2054ae1fc759f4c12", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/lSu9jSN3_LuAKIs74or5z0N6oYPDP4QBEKGRLBviyA4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=490976a8e071d026bfbbf23d61fac3742d052610", "width": 1080, "height": 565}], "variants": {}, "id": "imHFrJOqhRqE7iEXCtHyjqQTEIOgw1FEkvgeIQSA_Fg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yosuqs", "is_robot_indexable": true, "report_reasons": null, "author": "paragonofhype", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yosuqs/facebook_is_proceeding_to_employ_the_web3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "http://www.coindesk.com", "subreddit_subscribers": 652221, "created_utc": 1667839949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a bit fed up with Google's storage issues (my photos go to my Gmail account that shares 15gb with email) and I recently realized that Amazon claims to offer unlimited photo storage.\n\nHas anybody taken advantage of this? Are there any scripts or libraries i can use to upload to it from my workstation? Not all my pictures are from a phone but i do sync everything to my home server so that's the ideal place to sync from.\n\nAlso, do the \"family\" accounts get the same? I would like to add wife and mom as well if possible.\n\nI'm only considering this as an a quick and easy way for us to have a Google photos like experience with minimal effort on my part (vs self hosting something)", "author_fullname": "t2_1ijfzrf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon photos anyone?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yoddzl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667797670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a bit fed up with Google&amp;#39;s storage issues (my photos go to my Gmail account that shares 15gb with email) and I recently realized that Amazon claims to offer unlimited photo storage.&lt;/p&gt;\n\n&lt;p&gt;Has anybody taken advantage of this? Are there any scripts or libraries i can use to upload to it from my workstation? Not all my pictures are from a phone but i do sync everything to my home server so that&amp;#39;s the ideal place to sync from.&lt;/p&gt;\n\n&lt;p&gt;Also, do the &amp;quot;family&amp;quot; accounts get the same? I would like to add wife and mom as well if possible.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m only considering this as an a quick and easy way for us to have a Google photos like experience with minimal effort on my part (vs self hosting something)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yoddzl", "is_robot_indexable": true, "report_reasons": null, "author": "mdeanda", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yoddzl/amazon_photos_anyone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yoddzl/amazon_photos_anyone/", "subreddit_subscribers": 652221, "created_utc": 1667797670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not so recently google took down a link to an extremely detailed beautifully formatted spreadsheet of all the AM4 motherboards and for a while I thought it was gone forever. I recently however found a mirror link to a copy of it but it's still on Google Sheets and also does not have any of the export options enabled.\n\nI was wondering if there's still any way to download this file given copying, downloading, and printing have all been disabled (I tried copying it to a new sheet and then downloading that but they've fixed that trick). I don't trust google to keep the mirror up forever and even then the owner could still delete it.\n\nHere is a link to the spreadsheet for those that want to look at it.  \n[https://docs.google.com/spreadsheets/d/1-cw7A2MDHPvA-oB3OKXivdUo9BbTcsss1Rzy3J4hRyA/edit#gid=2112472504](https://docs.google.com/spreadsheets/d/1-cw7A2MDHPvA-oB3OKXivdUo9BbTcsss1Rzy3J4hRyA/edit#gid=2112472504)", "author_fullname": "t2_3d6imjg5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating Copy From Google Sheets When Export Is Disabled", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yoa7tz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667788081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not so recently google took down a link to an extremely detailed beautifully formatted spreadsheet of all the AM4 motherboards and for a while I thought it was gone forever. I recently however found a mirror link to a copy of it but it&amp;#39;s still on Google Sheets and also does not have any of the export options enabled.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if there&amp;#39;s still any way to download this file given copying, downloading, and printing have all been disabled (I tried copying it to a new sheet and then downloading that but they&amp;#39;ve fixed that trick). I don&amp;#39;t trust google to keep the mirror up forever and even then the owner could still delete it.&lt;/p&gt;\n\n&lt;p&gt;Here is a link to the spreadsheet for those that want to look at it.&lt;br/&gt;\n&lt;a href=\"https://docs.google.com/spreadsheets/d/1-cw7A2MDHPvA-oB3OKXivdUo9BbTcsss1Rzy3J4hRyA/edit#gid=2112472504\"&gt;https://docs.google.com/spreadsheets/d/1-cw7A2MDHPvA-oB3OKXivdUo9BbTcsss1Rzy3J4hRyA/edit#gid=2112472504&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?auto=webp&amp;s=74ee7b7f273686d3a6d63132dead8fd1764cf746", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c19110c068d76ab200f1245f86280073ea0f58a6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44c18606b62aece853ea487bd43c828325a0bdce", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=209ec2a10a56fe45fe6cb9d261c1ab221b6e9ac6", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d4eeeb63a94a849ea3cf810b90f2da22c354be9", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=be1258b942082c46071af527419ef9e0a648cf73", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0a5b3638d48be6c610b283e28d05c05ed93c981", "width": 1080, "height": 567}], "variants": {}, "id": "sJTk-8u5yRf8kptjtc2UKTd4Hk25h5aqolxUj_u1Y2k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yoa7tz", "is_robot_indexable": true, "report_reasons": null, "author": "Awkward_Elf", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yoa7tz/creating_copy_from_google_sheets_when_export_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yoa7tz/creating_copy_from_google_sheets_when_export_is/", "subreddit_subscribers": 652221, "created_utc": 1667788081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there,\n\nTrying to export a lot of data from google drive and it's split into 80 50GB zip files. Is there a way to mass download all of them at once using a download manager since I tried using motirx but it seems to not be able to download them as google seems to auth each one. There is no option to download all the files at once and due to having to click download 80 times and wait it's going to take a while so not sure if there is a way to speed it up a bit.\n\nThanks!", "author_fullname": "t2_ksns1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Takeout mass download", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yoh4yi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667810955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;Trying to export a lot of data from google drive and it&amp;#39;s split into 80 50GB zip files. Is there a way to mass download all of them at once using a download manager since I tried using motirx but it seems to not be able to download them as google seems to auth each one. There is no option to download all the files at once and due to having to click download 80 times and wait it&amp;#39;s going to take a while so not sure if there is a way to speed it up a bit.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yoh4yi", "is_robot_indexable": true, "report_reasons": null, "author": "BV1717", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yoh4yi/google_takeout_mass_download/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yoh4yi/google_takeout_mass_download/", "subreddit_subscribers": 652221, "created_utc": 1667810955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Heyah o/ !\n\nThese days, I've been looking for some solutions with my data, only cloud oriented after a bad experience with a NAS. My idea now is to select a Cloud provider in order to have a +1 secure backup between all my PCs, but the main interest for me would be to finally get a nice file Synchronisation solution for all my files that I use between my PCs.\n\nI've ended up doing a Google Sheet based on many, many, many providers. I used all the feedback from here to make it, so thanks again &lt;3\n\n[Yeah... I didn't have anything else to do, and went for a basic setup, it's ugly](https://preview.redd.it/3zntrwmyrey91.png?width=1050&amp;format=png&amp;auto=webp&amp;s=c6016b0675bd93a6568b26a6a855a5b8cc8b5342)\n\nIn the end, only Syncthing isn't Cloud based. But no matter what, it's an useful tool that i'll keep as a bonus tool anyway haha.\n\nAs you can see, Filen is really getting the lead for me. But as in the bad points, my biggest fear is the Life Time Scam.\n\nSo I wanted to know if anybody used Filen and have a good experience with the Life Time thing ? What about OneDrive and Sync which are still perfect tools for my need ?\n\nThanks a lot !", "author_fullname": "t2_w6xai3n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hesitation between 3 Cloud Providers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 53, "top_awarded_type": null, "hide_score": false, "media_metadata": {"3zntrwmyrey91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 41, "x": 108, "u": "https://preview.redd.it/3zntrwmyrey91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a09ed66f10772b8a6f296e695f683ed0e7f7109a"}, {"y": 82, "x": 216, "u": "https://preview.redd.it/3zntrwmyrey91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=08a272713d30fa9b267f06b9d7460051f48b2b59"}, {"y": 122, "x": 320, "u": "https://preview.redd.it/3zntrwmyrey91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e8e65f2a17a8086b33c143c3eff36bb3ab0e53c4"}, {"y": 244, "x": 640, "u": "https://preview.redd.it/3zntrwmyrey91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d07b75e512823b576a954d6c763d73ff5697daf0"}, {"y": 366, "x": 960, "u": "https://preview.redd.it/3zntrwmyrey91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d214eb43feb73062dd5bf5b70754d535ac42030d"}], "s": {"y": 401, "x": 1050, "u": "https://preview.redd.it/3zntrwmyrey91.png?width=1050&amp;format=png&amp;auto=webp&amp;s=c6016b0675bd93a6568b26a6a855a5b8cc8b5342"}, "id": "3zntrwmyrey91"}}, "name": "t3_yo5899", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.47, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/z84R3hUbBnWllbPEnfR2NhAKnlEJnuWbK9IZ1IyRg50.jpg", "edited": 1667777163.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667774532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heyah o/ !&lt;/p&gt;\n\n&lt;p&gt;These days, I&amp;#39;ve been looking for some solutions with my data, only cloud oriented after a bad experience with a NAS. My idea now is to select a Cloud provider in order to have a +1 secure backup between all my PCs, but the main interest for me would be to finally get a nice file Synchronisation solution for all my files that I use between my PCs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve ended up doing a Google Sheet based on many, many, many providers. I used all the feedback from here to make it, so thanks again &amp;lt;3&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3zntrwmyrey91.png?width=1050&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6016b0675bd93a6568b26a6a855a5b8cc8b5342\"&gt;Yeah... I didn&amp;#39;t have anything else to do, and went for a basic setup, it&amp;#39;s ugly&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In the end, only Syncthing isn&amp;#39;t Cloud based. But no matter what, it&amp;#39;s an useful tool that i&amp;#39;ll keep as a bonus tool anyway haha.&lt;/p&gt;\n\n&lt;p&gt;As you can see, Filen is really getting the lead for me. But as in the bad points, my biggest fear is the Life Time Scam.&lt;/p&gt;\n\n&lt;p&gt;So I wanted to know if anybody used Filen and have a good experience with the Life Time thing ? What about OneDrive and Sync which are still perfect tools for my need ?&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yo5899", "is_robot_indexable": true, "report_reasons": null, "author": "RPAmont", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yo5899/hesitation_between_3_cloud_providers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yo5899/hesitation_between_3_cloud_providers/", "subreddit_subscribers": 652221, "created_utc": 1667774532.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}