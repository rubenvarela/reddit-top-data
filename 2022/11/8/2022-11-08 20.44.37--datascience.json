{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_crcnkrwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seems a bit crazy, 400 applications within 3 days! Does this put anyone else off applying?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_yp082p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 599, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 599, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/PXsWFT3cp5FJONJiIfnjsBt2dX9c4oNFavHYwg_aHX8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667854847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/bavvk6e8fly91.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/bavvk6e8fly91.jpg?auto=webp&amp;s=ec9d28b417dc9826e6c388eec4417738919e23da", "width": 1170, "height": 2532}, "resolutions": [{"url": "https://preview.redd.it/bavvk6e8fly91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=381e300142c46a2c2f9d913bf597cb926b9befb6", "width": 108, "height": 216}, {"url": "https://preview.redd.it/bavvk6e8fly91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=66b8d15b6d4c4ed2e3f39dfdab4fc719af4e1ea2", "width": 216, "height": 432}, {"url": "https://preview.redd.it/bavvk6e8fly91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=94687a6f542a8c2024e9daeb8e6303e1d4099f2f", "width": 320, "height": 640}, {"url": "https://preview.redd.it/bavvk6e8fly91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7c1c9a304c2411a076e142af79cd005601850bee", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/bavvk6e8fly91.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d3a4e4ce12b1723885164b04f6a178450cdc8276", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/bavvk6e8fly91.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9d26df46130b5194ccf4b6ba8b1b59a9409f2168", "width": 1080, "height": 2160}], "variants": {}, "id": "8mC0GvA709QFreE85T4V4vtGUoCmMqDjNjBvDlmcy6c"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "yp082p", "is_robot_indexable": true, "report_reasons": null, "author": "layinad126", "discussion_type": null, "num_comments": 187, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yp082p/seems_a_bit_crazy_400_applications_within_3_days/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/bavvk6e8fly91.jpg", "subreddit_subscribers": 818238, "created_utc": 1667854847.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "People are obsessed with pursuing data science roles for some reason. I guess it's interesting work with a high skill ceiling. Thats why I'm pursuing it. But nobody talks about the data analyst. The folks who write SQL for reporting, create dashboards, and provide insights. Data science does do all this in a more sophisticated way, but the reality is most tech companies or start ups do not even have an appetite for that kind of work since they are so focused on growth. If you're struggling to get into data science, consider analytics. The pay is still good (100k plus if you're doing product analytics) and a natural growth path from there can totally be data science. Don't rule it out, you have options. End \ud83d\ude0a", "author_fullname": "t2_8o0eldke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "hot take: forget data science, we need more analysts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypr93q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 184, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 184, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667924978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People are obsessed with pursuing data science roles for some reason. I guess it&amp;#39;s interesting work with a high skill ceiling. Thats why I&amp;#39;m pursuing it. But nobody talks about the data analyst. The folks who write SQL for reporting, create dashboards, and provide insights. Data science does do all this in a more sophisticated way, but the reality is most tech companies or start ups do not even have an appetite for that kind of work since they are so focused on growth. If you&amp;#39;re struggling to get into data science, consider analytics. The pay is still good (100k plus if you&amp;#39;re doing product analytics) and a natural growth path from there can totally be data science. Don&amp;#39;t rule it out, you have options. End \ud83d\ude0a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 200, "id": "award_1703f934-cf44-40cc-a96d-3729d0b48262", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=16&amp;height=16&amp;auto=webp&amp;s=e3adc32e42cf534e27afea719ff932b1ce797cfd", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=32&amp;height=32&amp;auto=webp&amp;s=08542909c94777e870c41a35413bce688ca2fd6c", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=48&amp;height=48&amp;auto=webp&amp;s=4d85746d584b5494087da3561944d6d241f57674", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=64&amp;height=64&amp;auto=webp&amp;s=fd7683c8de2839998a432e7e53e1e06d66c35ad3", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=128&amp;height=128&amp;auto=webp&amp;s=a750da7a573bb231bd863be9725abece0332b828", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "My kindergarten teacher, my cat, my mom, and you.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "I'd Like to Thank...", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=16&amp;height=16&amp;auto=webp&amp;s=e3adc32e42cf534e27afea719ff932b1ce797cfd", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=32&amp;height=32&amp;auto=webp&amp;s=08542909c94777e870c41a35413bce688ca2fd6c", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=48&amp;height=48&amp;auto=webp&amp;s=4d85746d584b5494087da3561944d6d241f57674", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=64&amp;height=64&amp;auto=webp&amp;s=fd7683c8de2839998a432e7e53e1e06d66c35ad3", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=128&amp;height=128&amp;auto=webp&amp;s=a750da7a573bb231bd863be9725abece0332b828", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypr93q", "is_robot_indexable": true, "report_reasons": null, "author": "djaycat", "discussion_type": null, "num_comments": 61, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypr93q/hot_take_forget_data_science_we_need_more_analysts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypr93q/hot_take_forget_data_science_we_need_more_analysts/", "subreddit_subscribers": 818238, "created_utc": 1667924978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi. Every time i follow engineers or watch YouTube videos, they always have some controversial opinion on DA role as if the role wasn\u2019t necessary or it\u2019s less than other roles. Why is that?\n\n- After reading everyone\u2019s responses I am now more  clear on both sides. Im trying to learn more about data in general &amp; I have a interest for DS. If you work in DS and wanna to connect please message me.", "author_fullname": "t2_9avvyyhm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does other people In tech \u201chate\u201d data scientists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp9jzi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 69, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 69, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667921970.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667877182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. Every time i follow engineers or watch YouTube videos, they always have some controversial opinion on DA role as if the role wasn\u2019t necessary or it\u2019s less than other roles. Why is that?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;After reading everyone\u2019s responses I am now more  clear on both sides. Im trying to learn more about data in general &amp;amp; I have a interest for DS. If you work in DS and wanna to connect please message me.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yp9jzi", "is_robot_indexable": true, "report_reasons": null, "author": "Wonderful_Cry_8401", "discussion_type": null, "num_comments": 118, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yp9jzi/why_does_other_people_in_tech_hate_data_scientists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yp9jzi/why_does_other_people_in_tech_hate_data_scientists/", "subreddit_subscribers": 818238, "created_utc": 1667877182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I came across a recent post highlighting that one of the main points that bring others to hate DS people is their crappy code that they hand over to production, and it got me wondering:\n\nWhat is a \"clean\" or \"good\" code that you expect a good DS professional to hand you for production so that you don't hate him/her for it? It'd be good to get some pointers to avoid these mistakes in the future and be hated for them.\n\nAnd please don't reply \"depends on the situation\"; please give good generalisable advice.", "author_fullname": "t2_71lk6sl4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pointers to write \"clean\" code for production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yphayx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 59, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667901095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came across a recent post highlighting that one of the main points that bring others to hate DS people is their crappy code that they hand over to production, and it got me wondering:&lt;/p&gt;\n\n&lt;p&gt;What is a &amp;quot;clean&amp;quot; or &amp;quot;good&amp;quot; code that you expect a good DS professional to hand you for production so that you don&amp;#39;t hate him/her for it? It&amp;#39;d be good to get some pointers to avoid these mistakes in the future and be hated for them.&lt;/p&gt;\n\n&lt;p&gt;And please don&amp;#39;t reply &amp;quot;depends on the situation&amp;quot;; please give good generalisable advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yphayx", "is_robot_indexable": true, "report_reasons": null, "author": "William_Rosebud", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yphayx/pointers_to_write_clean_code_for_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yphayx/pointers_to_write_clean_code_for_production/", "subreddit_subscribers": 818238, "created_utc": 1667901095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " In my work as a datascientist I use a lot of blackbox algoritms such as gradient boosting, random forrest and neural networks. One question I ALWAYS get from the business I'm making the models for is, what features are important? How does the model make descicions. So to answer that I do the usual feature analysis, correlation matrices, partial dependence plots, mdi, model extraction. But I still fill like I'm not entirely able to answer what variables are the most important for example.\n\nNow I was thinking of a new method to determine feature importance. First we need the trained model, and the feature distributions. If we take a feature, we look at the sorted values and take 11 values corresponding to 0% - 10% - .. - 100% of the feature distribution. Next we take for example 1000 random states of the other features and test per random state the 11 options for the selected feature. For this 11 values of the feature, we check the number of times the y-value (label) changes. After doing this for all features, we should have an order of feature importance, as a higer rate of changes indicates more influence on the labels outcome. Would als be applicable for discrete variables and continuous labels with some minor adjustments.\n\nI love to hear your experiences in this regard and what you think of the proposed method?", "author_fullname": "t2_k8jayuq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Demystify the blackbox, what do you think of my idea", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypn2rg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667916038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my work as a datascientist I use a lot of blackbox algoritms such as gradient boosting, random forrest and neural networks. One question I ALWAYS get from the business I&amp;#39;m making the models for is, what features are important? How does the model make descicions. So to answer that I do the usual feature analysis, correlation matrices, partial dependence plots, mdi, model extraction. But I still fill like I&amp;#39;m not entirely able to answer what variables are the most important for example.&lt;/p&gt;\n\n&lt;p&gt;Now I was thinking of a new method to determine feature importance. First we need the trained model, and the feature distributions. If we take a feature, we look at the sorted values and take 11 values corresponding to 0% - 10% - .. - 100% of the feature distribution. Next we take for example 1000 random states of the other features and test per random state the 11 options for the selected feature. For this 11 values of the feature, we check the number of times the y-value (label) changes. After doing this for all features, we should have an order of feature importance, as a higer rate of changes indicates more influence on the labels outcome. Would als be applicable for discrete variables and continuous labels with some minor adjustments.&lt;/p&gt;\n\n&lt;p&gt;I love to hear your experiences in this regard and what you think of the proposed method?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypn2rg", "is_robot_indexable": true, "report_reasons": null, "author": "localhoststream", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypn2rg/demystify_the_blackbox_what_do_you_think_of_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypn2rg/demystify_the_blackbox_what_do_you_think_of_my/", "subreddit_subscribers": 818238, "created_utc": 1667916038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "**Engineers for Ukraine** is an international team of volunteers working on a machine learning tool to identify Russian equipment in real time (with minimal human involvement) to increase the speed at which accurate information about Russian soldiers/equipment in the area passes from local civilians on the ground to the Ukrainian warfighter. \n\nEngineers for Ukraine has four teams: \n\n1. The Data Team, which builds the training datasets for the machine learning models. This is the easiest team to join since the tasks are straightforward and the training is short/easy thanks to the help of our beloved data scientists. \n2. The Machine Learning Team, which builds and trains machine learning models. This team needs a bit more experience and/or time to get through readings to get up to speed. If you are familiar with AWS, AWS SageMaker, AWS S3, AWS Rekognition, AWS Comprehend, Lambda, machine learning attacks, machine learning security, dedicated red team work, and/or data science, please join the machine learning team.\n3. The Development Team, which handles much of the project infrastructure and builds the relevant web pages, services, and user interfaces.  If you are familiar with AWS, Lambda, JavaScript, React.js, Node.js, API's, plug-ins, and/or devops/SRE/cloud engineering, please join the dev team.\n4. The Cybersecurity Team, which works heavily with the Development Team searching for and fixing vulnerabilities, but also creates threat models, does red team vs blue team work, penetration testing, and occasionally OSINT work. If you are interested in learning cool cybersecurity skills from a team of professionals who are just super busy and need more hands on deck to knock out tasks OR if you are also skilled in cybersecurity, please join the cybersecurity team.\n\nI\u2019ve talked about Engineers for Ukraine before in this Reddit post: [https://www.reddit.com/r/ukraine/comments/vlsaka/volunteers\\_needed\\_for\\_proukraine\\_project/](https://www.reddit.com/r/ukraine/comments/vlsaka/volunteers_needed_for_proukraine_project/)\n\nIf you are interested in either of these groups, please reach out to breaker25789@gmail.com with the project (AidSupply or Engineers for Ukraine) and team you are interested in.\n\nWe will reach out and schedule a video call in which you can verify that we aren\u2019t Russian bots and we can verify that you are not a Russian bot by both showing a government-issued photo ID and two social media accounts. As part of the recruitment process, each volunteer may be asked to complete an introductory assignment specific to the project/team they are applying to. This isn\u2019t meant as a barrier, just as a way to get people onboarded faster while giving the project leadership a sense of each volunteer\u2019s skill level.", "author_fullname": "t2_1323h2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Volunteers Needed for Pro-Ukraine Machine Learning Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yptkvg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667930260.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Engineers for Ukraine&lt;/strong&gt; is an international team of volunteers working on a machine learning tool to identify Russian equipment in real time (with minimal human involvement) to increase the speed at which accurate information about Russian soldiers/equipment in the area passes from local civilians on the ground to the Ukrainian warfighter. &lt;/p&gt;\n\n&lt;p&gt;Engineers for Ukraine has four teams: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The Data Team, which builds the training datasets for the machine learning models. This is the easiest team to join since the tasks are straightforward and the training is short/easy thanks to the help of our beloved data scientists. &lt;/li&gt;\n&lt;li&gt;The Machine Learning Team, which builds and trains machine learning models. This team needs a bit more experience and/or time to get through readings to get up to speed. If you are familiar with AWS, AWS SageMaker, AWS S3, AWS Rekognition, AWS Comprehend, Lambda, machine learning attacks, machine learning security, dedicated red team work, and/or data science, please join the machine learning team.&lt;/li&gt;\n&lt;li&gt;The Development Team, which handles much of the project infrastructure and builds the relevant web pages, services, and user interfaces.  If you are familiar with AWS, Lambda, JavaScript, React.js, Node.js, API&amp;#39;s, plug-ins, and/or devops/SRE/cloud engineering, please join the dev team.&lt;/li&gt;\n&lt;li&gt;The Cybersecurity Team, which works heavily with the Development Team searching for and fixing vulnerabilities, but also creates threat models, does red team vs blue team work, penetration testing, and occasionally OSINT work. If you are interested in learning cool cybersecurity skills from a team of professionals who are just super busy and need more hands on deck to knock out tasks OR if you are also skilled in cybersecurity, please join the cybersecurity team.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I\u2019ve talked about Engineers for Ukraine before in this Reddit post: &lt;a href=\"https://www.reddit.com/r/ukraine/comments/vlsaka/volunteers_needed_for_proukraine_project/\"&gt;https://www.reddit.com/r/ukraine/comments/vlsaka/volunteers_needed_for_proukraine_project/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you are interested in either of these groups, please reach out to &lt;a href=\"mailto:breaker25789@gmail.com\"&gt;breaker25789@gmail.com&lt;/a&gt; with the project (AidSupply or Engineers for Ukraine) and team you are interested in.&lt;/p&gt;\n\n&lt;p&gt;We will reach out and schedule a video call in which you can verify that we aren\u2019t Russian bots and we can verify that you are not a Russian bot by both showing a government-issued photo ID and two social media accounts. As part of the recruitment process, each volunteer may be asked to complete an introductory assignment specific to the project/team they are applying to. This isn\u2019t meant as a barrier, just as a way to get people onboarded faster while giving the project leadership a sense of each volunteer\u2019s skill level.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yptkvg", "is_robot_indexable": true, "report_reasons": null, "author": "OttersAreDevilSpawn", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yptkvg/volunteers_needed_for_proukraine_machine_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yptkvg/volunteers_needed_for_proukraine_machine_learning/", "subreddit_subscribers": 818238, "created_utc": 1667930260.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, I wanted to pick your brains on any project management or tracking tools you use in your day to day data science roles. \n\n\nA little background, I work on a small 3 person team, with each of us working on our own individual projects. We do a lot of smaller ad-hoc analytical projects for the organization at large, and we're quite siloed in our work. There's almost zero overlap between our analytical/DS projects. We haven't been able to find a satisfactory project management software or methodology that fits our needs - currently we're using a huge/messy kanban board that our boss manages through Azure DevOps - it is far from ideal since it seems to be geared towards larger agile development teams working on a single project. Our projects are small but numerous, with each having multiple parts. We're using sprints to represent projects, and stories to represent tasks within each project - as you can probably guess, it gets quite messy. I think we'd benefit from something geared towards the solo developer/analyst (since that's basically what we are from a work perspective). Something a little more guided/managed than github docs, but not as as managed Azure DevOps. \n\n\nDo you have anything that you'd recommend that you use for work, or even your own solo projects? Thanks in advance!\n\nEdit: Added details.", "author_fullname": "t2_xwrjw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Project Management tools for DS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp0ija", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667857047.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667855434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I wanted to pick your brains on any project management or tracking tools you use in your day to day data science roles. &lt;/p&gt;\n\n&lt;p&gt;A little background, I work on a small 3 person team, with each of us working on our own individual projects. We do a lot of smaller ad-hoc analytical projects for the organization at large, and we&amp;#39;re quite siloed in our work. There&amp;#39;s almost zero overlap between our analytical/DS projects. We haven&amp;#39;t been able to find a satisfactory project management software or methodology that fits our needs - currently we&amp;#39;re using a huge/messy kanban board that our boss manages through Azure DevOps - it is far from ideal since it seems to be geared towards larger agile development teams working on a single project. Our projects are small but numerous, with each having multiple parts. We&amp;#39;re using sprints to represent projects, and stories to represent tasks within each project - as you can probably guess, it gets quite messy. I think we&amp;#39;d benefit from something geared towards the solo developer/analyst (since that&amp;#39;s basically what we are from a work perspective). Something a little more guided/managed than github docs, but not as as managed Azure DevOps. &lt;/p&gt;\n\n&lt;p&gt;Do you have anything that you&amp;#39;d recommend that you use for work, or even your own solo projects? Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;Edit: Added details.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yp0ija", "is_robot_indexable": true, "report_reasons": null, "author": "LumchBox", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yp0ija/project_management_tools_for_ds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yp0ija/project_management_tools_for_ds/", "subreddit_subscribers": 818238, "created_utc": 1667855434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "how common are hierarchical bayesian models in retail forecasting or supply chain? in the past  I worked in a retail startup which used this method, but I do not know how common it is.", "author_fullname": "t2_cpzb2rdw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how common are hierarchical bayesian models in retail forecasting?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp9gc8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667876889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;how common are hierarchical bayesian models in retail forecasting or supply chain? in the past  I worked in a retail startup which used this method, but I do not know how common it is.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yp9gc8", "is_robot_indexable": true, "report_reasons": null, "author": "awesomedatascientist", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yp9gc8/how_common_are_hierarchical_bayesian_models_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yp9gc8/how_common_are_hierarchical_bayesian_models_in/", "subreddit_subscribers": 818238, "created_utc": 1667876889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a question regarding Textmining, I want to get information out of a semi-structured text like the highlighted price or the size of the property. The data is currently in a txt.file but I could also safe it somewhere else. What is the best way to get this information into a structured format? Thank you for your help! \n\nhttps://preview.redd.it/0pyrw00iupy91.png?width=747&amp;format=png&amp;auto=webp&amp;s=c413dee0771dd7071907d4ff3b519e4f8cbf9116", "author_fullname": "t2_tyjml9b0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question Textscraping", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"0pyrw00iupy91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 116, "x": 108, "u": "https://preview.redd.it/0pyrw00iupy91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=68153e297febf102f4ce35060ddfcc3cbf07f145"}, {"y": 233, "x": 216, "u": "https://preview.redd.it/0pyrw00iupy91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b031ddda885c56674026b02333ad7b06226ae3e"}, {"y": 345, "x": 320, "u": "https://preview.redd.it/0pyrw00iupy91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7355378c733c11254324605b7130ac17c264dc59"}, {"y": 691, "x": 640, "u": "https://preview.redd.it/0pyrw00iupy91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ffe6b08d69dabcd4d3cfd0e52ab6d0ad23dd1f84"}], "s": {"y": 807, "x": 747, "u": "https://preview.redd.it/0pyrw00iupy91.png?width=747&amp;format=png&amp;auto=webp&amp;s=c413dee0771dd7071907d4ff3b519e4f8cbf9116"}, "id": "0pyrw00iupy91"}}, "name": "t3_ypjwln", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KhYlSufCJURrtUn8CDQUesWnhSSXOr4U-NzYoziDysY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667908454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a question regarding Textmining, I want to get information out of a semi-structured text like the highlighted price or the size of the property. The data is currently in a txt.file but I could also safe it somewhere else. What is the best way to get this information into a structured format? Thank you for your help! &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0pyrw00iupy91.png?width=747&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c413dee0771dd7071907d4ff3b519e4f8cbf9116\"&gt;https://preview.redd.it/0pyrw00iupy91.png?width=747&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c413dee0771dd7071907d4ff3b519e4f8cbf9116&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypjwln", "is_robot_indexable": true, "report_reasons": null, "author": "Ferry_Carondelet", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypjwln/question_textscraping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypjwln/question_textscraping/", "subreddit_subscribers": 818238, "created_utc": 1667908454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "The EDA and data cleaning part of the project is always a messy phase for me. I was wondering what practices people are doing to standardize it and make things more automated and streamlined?\n\nI've also been trying to make my code more extensible to avoid having to code refactor every time new things comes into play. Since its easy to go in one direction, but harder to go in the other. Keeping things more modular and abstract. I know data science is particularly not fit for some parts of these, since data cleaning is very order based, and EDA is very unplanned and prone to rabbit-holes, but i try to put up with it anyways.\n\nFor example, I no longer do df.groupby(X), but put the key into a list like df.groupby(\\[X\\]), and maybe take out that and keep it as a list variable for data abstraction. Or do things like df\\[df\\[\"col\"\\].isin(\\[X\\])\\] instead of df\\[df\\[\"col\"\\]==X\\].\n\nSure, these create awkward one-element lists at the start, but the point is that they might not end up being one-element anymore in the future. The main con is slower readability with these oxbow lake code. Sometimes my team mate tries to revert and \"de-abstract\" the code back and I have to explain to them why extensibility is good.", "author_fullname": "t2_b7eh4ujn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organization in EDA and data cleaning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypg2yb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667897500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The EDA and data cleaning part of the project is always a messy phase for me. I was wondering what practices people are doing to standardize it and make things more automated and streamlined?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also been trying to make my code more extensible to avoid having to code refactor every time new things comes into play. Since its easy to go in one direction, but harder to go in the other. Keeping things more modular and abstract. I know data science is particularly not fit for some parts of these, since data cleaning is very order based, and EDA is very unplanned and prone to rabbit-holes, but i try to put up with it anyways.&lt;/p&gt;\n\n&lt;p&gt;For example, I no longer do df.groupby(X), but put the key into a list like df.groupby([X]), and maybe take out that and keep it as a list variable for data abstraction. Or do things like df[df[&amp;quot;col&amp;quot;].isin([X])] instead of df[df[&amp;quot;col&amp;quot;]==X].&lt;/p&gt;\n\n&lt;p&gt;Sure, these create awkward one-element lists at the start, but the point is that they might not end up being one-element anymore in the future. The main con is slower readability with these oxbow lake code. Sometimes my team mate tries to revert and &amp;quot;de-abstract&amp;quot; the code back and I have to explain to them why extensibility is good.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypg2yb", "is_robot_indexable": true, "report_reasons": null, "author": "countlinard", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypg2yb/organization_in_eda_and_data_cleaning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypg2yb/organization_in_eda_and_data_cleaning/", "subreddit_subscribers": 818238, "created_utc": 1667897500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been looking for some advice or general best practices when it comes to wrangling survey response data that consists mostly of \"click all that apply\" responses and then free text responses (for reference the survey was distributed through Survey Monkey). The data is structured in a wide format, wherein the possible responses for questions are distributed in the columns of the data. For example a question on race would have each of possible values for race as a column variable: \n\n    white &lt;- c(white, NA, NA, white) \n    black &lt;- c(NA, black, NA, black) \n    asian &lt;- c(asian, NA, asian, NA)\n    unique_id &lt;- x(1,2,3)\n    df &lt;- tibble(white, black, asian, unique_id) \n\nEach respondent's responses are captured in a single row and every respondent has a unique ID, so respondent 1 would have checked the boxes for White and Asian. My question is in regards to how best to go about formatting the data for analysis. Given that there are multiple of these \"check all that apply\" or dummy variable columns, is it best to leave this data in a wide format? Or is it better to gather all of these columns into a single variable and pivot the data longer? My only concern is that there are about 20 questions like this, so pivoting each of these variables would create a really long data frame. Additionally, I need to have the data formatted in a way that would require little to no preprocessing in Tableau. This is primarily due to work policies on formatting research publications and data visuals (I know it's not ideal but I can't change it). \n\nI have it currently set up in two way: the first is with each question separated out in different excel sheets that can be joined by their unique ID. The second is with all the questions combined into a single data frame. Both are formatted wide. Thanks in advance for any help or advice, I can also provide additional anonymous data if that would help you help me better :)", "author_fullname": "t2_9337qo7s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cleaning Survey Response Data for Analysis in R | Best Practices or Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ypveb6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667934361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been looking for some advice or general best practices when it comes to wrangling survey response data that consists mostly of &amp;quot;click all that apply&amp;quot; responses and then free text responses (for reference the survey was distributed through Survey Monkey). The data is structured in a wide format, wherein the possible responses for questions are distributed in the columns of the data. For example a question on race would have each of possible values for race as a column variable: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;white &amp;lt;- c(white, NA, NA, white) \nblack &amp;lt;- c(NA, black, NA, black) \nasian &amp;lt;- c(asian, NA, asian, NA)\nunique_id &amp;lt;- x(1,2,3)\ndf &amp;lt;- tibble(white, black, asian, unique_id) \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Each respondent&amp;#39;s responses are captured in a single row and every respondent has a unique ID, so respondent 1 would have checked the boxes for White and Asian. My question is in regards to how best to go about formatting the data for analysis. Given that there are multiple of these &amp;quot;check all that apply&amp;quot; or dummy variable columns, is it best to leave this data in a wide format? Or is it better to gather all of these columns into a single variable and pivot the data longer? My only concern is that there are about 20 questions like this, so pivoting each of these variables would create a really long data frame. Additionally, I need to have the data formatted in a way that would require little to no preprocessing in Tableau. This is primarily due to work policies on formatting research publications and data visuals (I know it&amp;#39;s not ideal but I can&amp;#39;t change it). &lt;/p&gt;\n\n&lt;p&gt;I have it currently set up in two way: the first is with each question separated out in different excel sheets that can be joined by their unique ID. The second is with all the questions combined into a single data frame. Both are formatted wide. Thanks in advance for any help or advice, I can also provide additional anonymous data if that would help you help me better :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypveb6", "is_robot_indexable": true, "report_reasons": null, "author": "Legal_Television_944", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypveb6/cleaning_survey_response_data_for_analysis_in_r/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypveb6/cleaning_survey_response_data_for_analysis_in_r/", "subreddit_subscribers": 818238, "created_utc": 1667934361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hello,\n\nI am working on curve's peaks and valleys detections.\n\n**Peak detection function**  \nIn Python there is no shortage of nice functions to do it like with scipy.signal.find\\_peaks.  \nAnd most functions have many parameters (distance, height, prominence, etc.) which have for objective to detect more or less peaks, which mean ignoring the minor ones, considered as noise.\n\n**Curve smoothing**  \nThen comes smoothing the curve before peak detecting.  \nI often see it done in blogs and examples.  \nThe savgol\\_filter is quite often used.  \nBasically it smooths the curves, so noise is removed and then you detect only major peaks.\n\n**Here are my questions**:\n\n**1/ Is curve smoothing before peak detection of any use ?**  \nI have the intuition that it's overlapping, and basically doing twice the same thing via different means.  \nAnd that if you fine tune your peak detection function parameters (distance, height, prominence, etc.) enough, smoothing before will never provide any upside.\n\n**2/ If curve smoothing is useful how to optimize it with peak detection?**  \nSay you want to detect only visually major peaks (I know it's subjective, but let's say you want only a limited number of peaks, while maximizing the gap between peaks and valleys).   \nIs there a way to find the correct mix of smoothing and parameters?  \nOr do you bruteforce millions of combinaisons until you have minimized the number of peaks while maximized the average gaps between peaks and valleys? \n\nWhat are your thoughts?  \nthx", "author_fullname": "t2_5559lwqj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Curve peak finding: isn't curve smoothing overlapping with peak detection functions parameters?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypqxuf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667924311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am working on curve&amp;#39;s peaks and valleys detections.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Peak detection function&lt;/strong&gt;&lt;br/&gt;\nIn Python there is no shortage of nice functions to do it like with scipy.signal.find_peaks.&lt;br/&gt;\nAnd most functions have many parameters (distance, height, prominence, etc.) which have for objective to detect more or less peaks, which mean ignoring the minor ones, considered as noise.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Curve smoothing&lt;/strong&gt;&lt;br/&gt;\nThen comes smoothing the curve before peak detecting.&lt;br/&gt;\nI often see it done in blogs and examples.&lt;br/&gt;\nThe savgol_filter is quite often used.&lt;br/&gt;\nBasically it smooths the curves, so noise is removed and then you detect only major peaks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here are my questions&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1/ Is curve smoothing before peak detection of any use ?&lt;/strong&gt;&lt;br/&gt;\nI have the intuition that it&amp;#39;s overlapping, and basically doing twice the same thing via different means.&lt;br/&gt;\nAnd that if you fine tune your peak detection function parameters (distance, height, prominence, etc.) enough, smoothing before will never provide any upside.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2/ If curve smoothing is useful how to optimize it with peak detection?&lt;/strong&gt;&lt;br/&gt;\nSay you want to detect only visually major peaks (I know it&amp;#39;s subjective, but let&amp;#39;s say you want only a limited number of peaks, while maximizing the gap between peaks and valleys).&lt;br/&gt;\nIs there a way to find the correct mix of smoothing and parameters?&lt;br/&gt;\nOr do you bruteforce millions of combinaisons until you have minimized the number of peaks while maximized the average gaps between peaks and valleys? &lt;/p&gt;\n\n&lt;p&gt;What are your thoughts?&lt;br/&gt;\nthx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypqxuf", "is_robot_indexable": true, "report_reasons": null, "author": "Vince_peak", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypqxuf/curve_peak_finding_isnt_curve_smoothing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypqxuf/curve_peak_finding_isnt_curve_smoothing/", "subreddit_subscribers": 818238, "created_utc": 1667924311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a list of several dairy products sold at a grocery store, and data on how many were sold per day over the course of two years.  I need to create a data-driven ordering system using this data to model how many of these products the store should order at any given time.\n\nI'm not used to working with time series, and am rather unsure of how to approach this problem.  One model I was thinking could include lag variables for amount sold of this product for the past 7 days.  I'm not sure where I could take it from there.\n\nAppreciate any thoughts from people more expert than me on this on relatively simple methods to solve this problem.  This is for a take home assignment for a job interview, and I could use ideas.  Links to resources and methods would be very helpful.", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modeling Product Orders Based on Quantities Sold", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypa1yt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667878626.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a list of several dairy products sold at a grocery store, and data on how many were sold per day over the course of two years.  I need to create a data-driven ordering system using this data to model how many of these products the store should order at any given time.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not used to working with time series, and am rather unsure of how to approach this problem.  One model I was thinking could include lag variables for amount sold of this product for the past 7 days.  I&amp;#39;m not sure where I could take it from there.&lt;/p&gt;\n\n&lt;p&gt;Appreciate any thoughts from people more expert than me on this on relatively simple methods to solve this problem.  This is for a take home assignment for a job interview, and I could use ideas.  Links to resources and methods would be very helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypa1yt", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypa1yt/modeling_product_orders_based_on_quantities_sold/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypa1yt/modeling_product_orders_based_on_quantities_sold/", "subreddit_subscribers": 818238, "created_utc": 1667878626.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Well wrt deep learning, I'm not sure how they differ, but my impression is that hyperparameter tuning deals with parameters of a specific layer, whereas ablation study deals with different parts of a model (meaning different layers themselves). Am I correct? Is there anything else to it?\u00a0And lastly, is one subset of the other?", "author_fullname": "t2_ayk7yaw4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ablation study vs Hyperparameter tuning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp2zxd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667860467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Well wrt deep learning, I&amp;#39;m not sure how they differ, but my impression is that hyperparameter tuning deals with parameters of a specific layer, whereas ablation study deals with different parts of a model (meaning different layers themselves). Am I correct? Is there anything else to it?\u00a0And lastly, is one subset of the other?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yp2zxd", "is_robot_indexable": true, "report_reasons": null, "author": "Ancient_Barber_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yp2zxd/ablation_study_vs_hyperparameter_tuning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yp2zxd/ablation_study_vs_hyperparameter_tuning/", "subreddit_subscribers": 818238, "created_utc": 1667860467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I recently started a new job as a data scientist in a small startup of 10 people approx. My team is me and other 4 interns/juniors w/ a few months experience max, all first job (Im also a junior). Since its a small startup our manager is the CEO.  The problem is since he is the CEO he doesnt really have any time for us. He is mostly in meetings with investors and were just thrown in the water with the code and the problems.  Non of us had a training of more than 1-2 days besides just a recap of the code so far. We try to help each other but theres a limit to how much an intern can help another intern. We spend hours and days on problems that can be solved within minutes and when the CEO does have time for us he just gives us a hint and not actually sitting and helping or directing us, or we discuss on whatsapp and its unbearable to explain a problem like that or he is just confused with his sh\\*t and starts a nonsense discussion on something which is not the problem. Eventually we get all the heat on why we are not on schedule or why its taking so long.\n\nIm doing my best to give results but its very hard when Im all by myself as an intern with no one to ask. Beside it I like the work itself (100% pure Data Science), the location, the other interns. But this work culture is very problematic for me and Im afraid I will end up with experience on paper but no actual knowledge or experience.\n\nAny advice from you guys will be appreciated", "author_fullname": "t2_rxcyu5rh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Workplace culture is chaotic and a complete mess, dont know what to do.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp1qjj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667857969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I recently started a new job as a data scientist in a small startup of 10 people approx. My team is me and other 4 interns/juniors w/ a few months experience max, all first job (Im also a junior). Since its a small startup our manager is the CEO.  The problem is since he is the CEO he doesnt really have any time for us. He is mostly in meetings with investors and were just thrown in the water with the code and the problems.  Non of us had a training of more than 1-2 days besides just a recap of the code so far. We try to help each other but theres a limit to how much an intern can help another intern. We spend hours and days on problems that can be solved within minutes and when the CEO does have time for us he just gives us a hint and not actually sitting and helping or directing us, or we discuss on whatsapp and its unbearable to explain a problem like that or he is just confused with his sh*t and starts a nonsense discussion on something which is not the problem. Eventually we get all the heat on why we are not on schedule or why its taking so long.&lt;/p&gt;\n\n&lt;p&gt;Im doing my best to give results but its very hard when Im all by myself as an intern with no one to ask. Beside it I like the work itself (100% pure Data Science), the location, the other interns. But this work culture is very problematic for me and Im afraid I will end up with experience on paper but no actual knowledge or experience.&lt;/p&gt;\n\n&lt;p&gt;Any advice from you guys will be appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yp1qjj", "is_robot_indexable": true, "report_reasons": null, "author": "EmotionalLiving9112", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yp1qjj/workplace_culture_is_chaotic_and_a_complete_mess/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yp1qjj/workplace_culture_is_chaotic_and_a_complete_mess/", "subreddit_subscribers": 818238, "created_utc": 1667857969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am working with a data set that needs to remain strictly confidential (though not due to HIPAA or any such legal requirements, just out of respect to the source). I also need to protype a web dashboard for visualizing and manipulating the data to demonstrate possible ways of communicating the data and the \"story\" within if they made the data publicly accessible.\n\nI have two questions (which may have different answers):\n\n1. How would you minimally secure (i.e. password protect, but perhaps not with a full-on authentication system) a basic (probably static, HTML/CSS/Javascript) web app in 2022?\n2. How do you deal with confidential/proprietary data when developing prototypes?\n3. (Bonus) Happen to have an example of how you tackled a similar scenario?\n\nI'll edit this post as needed if clarifying questions are asked below. Thanks in advance!", "author_fullname": "t2_f260834", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Securing a Data Science Project in 2022", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ypvho0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667934580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with a data set that needs to remain strictly confidential (though not due to HIPAA or any such legal requirements, just out of respect to the source). I also need to protype a web dashboard for visualizing and manipulating the data to demonstrate possible ways of communicating the data and the &amp;quot;story&amp;quot; within if they made the data publicly accessible.&lt;/p&gt;\n\n&lt;p&gt;I have two questions (which may have different answers):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How would you minimally secure (i.e. password protect, but perhaps not with a full-on authentication system) a basic (probably static, HTML/CSS/Javascript) web app in 2022?&lt;/li&gt;\n&lt;li&gt;How do you deal with confidential/proprietary data when developing prototypes?&lt;/li&gt;\n&lt;li&gt;(Bonus) Happen to have an example of how you tackled a similar scenario?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ll edit this post as needed if clarifying questions are asked below. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypvho0", "is_robot_indexable": true, "report_reasons": null, "author": "BoxBeatMan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypvho0/securing_a_data_science_project_in_2022/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypvho0/securing_a_data_science_project_in_2022/", "subreddit_subscribers": 818238, "created_utc": 1667934580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Does a multiple regression model contain combination of predictor variables (eg. x1x2, x2x4, etc. where x1, x2, x3, x4, etc. are the predictor variables) or only separate multiple predictor variables?", "author_fullname": "t2_pwif3is8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does multiple regression model contain combination of predictor variables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypd1uv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667887879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does a multiple regression model contain combination of predictor variables (eg. x1x2, x2x4, etc. where x1, x2, x3, x4, etc. are the predictor variables) or only separate multiple predictor variables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypd1uv", "is_robot_indexable": true, "report_reasons": null, "author": "random5842786439", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypd1uv/does_multiple_regression_model_contain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypd1uv/does_multiple_regression_model_contain/", "subreddit_subscribers": 818238, "created_utc": 1667887879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_11jnpsdw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the delineation between scope of responsibilities for a Jr Data Scientist vs a Senior Data Scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypaots", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667880490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypaots", "is_robot_indexable": true, "report_reasons": null, "author": "Blackcatsloveme", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypaots/what_is_the_delineation_between_scope_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypaots/what_is_the_delineation_between_scope_of/", "subreddit_subscribers": 818238, "created_utc": 1667880490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For the next year, I am funded to work on \"anything i want\". This is kind of a tall order. Basically we have this bayesian hierarchical model that we use to estimate latent processes that drive spatio-temporal observations. Typically this is used when there are multiple ongoing physical processes and we want to do source separation to see what process contributes where. This model has been used in the past on global estimations of sea-level rise, and now I am supposed to come up with new problems (in any domain!) to apply it to. And the trouble is that I am a bit stuck on such an open question. Anyone have any ideas? So far we come up with energy poverty problems, understanding subglacial drainage systems, estimating snow discharge for hydropower, and estimating serpentinization rates for co2 sequestration applications. But it would be nice if there is some kind of problem, with really good data (e.g., google earth engine, census, etc.), that i could attack without super deep domain knowledge myself and instead working with domain experts.", "author_fullname": "t2_3hhb8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "having trouble being in a privileged position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yoz6sk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667852703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the next year, I am funded to work on &amp;quot;anything i want&amp;quot;. This is kind of a tall order. Basically we have this bayesian hierarchical model that we use to estimate latent processes that drive spatio-temporal observations. Typically this is used when there are multiple ongoing physical processes and we want to do source separation to see what process contributes where. This model has been used in the past on global estimations of sea-level rise, and now I am supposed to come up with new problems (in any domain!) to apply it to. And the trouble is that I am a bit stuck on such an open question. Anyone have any ideas? So far we come up with energy poverty problems, understanding subglacial drainage systems, estimating snow discharge for hydropower, and estimating serpentinization rates for co2 sequestration applications. But it would be nice if there is some kind of problem, with really good data (e.g., google earth engine, census, etc.), that i could attack without super deep domain knowledge myself and instead working with domain experts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yoz6sk", "is_robot_indexable": true, "report_reasons": null, "author": "mnky9800n", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yoz6sk/having_trouble_being_in_a_privileged_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yoz6sk/having_trouble_being_in_a_privileged_position/", "subreddit_subscribers": 818238, "created_utc": 1667852703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m looking for courses/books/certifications to learn DS in a way that can be applied to marketing (like ads, social, etc)", "author_fullname": "t2_58mtq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations for learning DS for marketing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yoypg8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667851703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking for courses/books/certifications to learn DS in a way that can be applied to marketing (like ads, social, etc)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yoypg8", "is_robot_indexable": true, "report_reasons": null, "author": "hot_as_a_toaster", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yoypg8/recommendations_for_learning_ds_for_marketing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yoypg8/recommendations_for_learning_ds_for_marketing/", "subreddit_subscribers": 818238, "created_utc": 1667851703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was thinking about the actual updating of the weights and biases for the past couple of days, and i realized something that i havent been able to get off my mind.. if your learning rate is less than one (that is &lt;1 and &gt;0 or &gt;-1 and &lt;0 ) (which i dont see how it couldnt be. Anything greater than one converges to infinite error as far as i can tell, and using a number less than one is the only way to actually use the information about the sensitivity of the cost function), anywhere you find a derivative less than one is going to make your change huge and probably change your weight too much\u2026 what\u2026 what am i missing here?\n\n(Btw sorry if this question seems inappropriate but i dont really know where else to ask r/askdatascience is dead)", "author_fullname": "t2_3hk48w5g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Back Propagation Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ypv4jy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667933756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was thinking about the actual updating of the weights and biases for the past couple of days, and i realized something that i havent been able to get off my mind.. if your learning rate is less than one (that is &amp;lt;1 and &amp;gt;0 or &amp;gt;-1 and &amp;lt;0 ) (which i dont see how it couldnt be. Anything greater than one converges to infinite error as far as i can tell, and using a number less than one is the only way to actually use the information about the sensitivity of the cost function), anywhere you find a derivative less than one is going to make your change huge and probably change your weight too much\u2026 what\u2026 what am i missing here?&lt;/p&gt;\n\n&lt;p&gt;(Btw sorry if this question seems inappropriate but i dont really know where else to ask &lt;a href=\"/r/askdatascience\"&gt;r/askdatascience&lt;/a&gt; is dead)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypv4jy", "is_robot_indexable": true, "report_reasons": null, "author": "Sharpeye1994", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypv4jy/back_propagation_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypv4jy/back_propagation_question/", "subreddit_subscribers": 818238, "created_utc": 1667933756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I started working for a new company which actually has big data. I dont have a senior to ask this and My search made me left even more confused.\n\nWe have a BigQuery table. Includes 8m rows. It is basically a monthly demand, price, and taxonomy data for products. . \n I am expected to make outlier analysis and try to find seasonal trends on this data. (also train some prediction models) \n\nWhat is the correct platform to process the data? Sure I can send queries using bq python api but i wont be able to get all the rows since it takes so much RAM. And getting partial data wont do since trend analysis needs all data rows. \n\nI tried vertex-ai, Opened up 32 gb n1 notebook instance.  loading the data to pandas df failed. It crushed runtime. I made couple of tries. \n\nI tried using data studio for insight hunt and ploting things. Sure it works but i want to make very different plots to discover the data and data studio is not flexible. i need to write absurdly long sql queries to accomplish something simple. \n\nWhat am I missing here?  I know dealing with big data is basic for data engineer but I never get to learn it and now i am struggling.  I checked tutorials and none of them are actually using real big data. GCP tutorials always get data using \"LIMIT 10000\" type of codes. \n\nMaybe what I need is spark? i am not sure.. \n\nIs there anyone who can tell me the mistake I am doing?", "author_fullname": "t2_qzy7otr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I process big BiqQuery data with python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp1cpx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667857175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started working for a new company which actually has big data. I dont have a senior to ask this and My search made me left even more confused.&lt;/p&gt;\n\n&lt;p&gt;We have a BigQuery table. Includes 8m rows. It is basically a monthly demand, price, and taxonomy data for products. . \n I am expected to make outlier analysis and try to find seasonal trends on this data. (also train some prediction models) &lt;/p&gt;\n\n&lt;p&gt;What is the correct platform to process the data? Sure I can send queries using bq python api but i wont be able to get all the rows since it takes so much RAM. And getting partial data wont do since trend analysis needs all data rows. &lt;/p&gt;\n\n&lt;p&gt;I tried vertex-ai, Opened up 32 gb n1 notebook instance.  loading the data to pandas df failed. It crushed runtime. I made couple of tries. &lt;/p&gt;\n\n&lt;p&gt;I tried using data studio for insight hunt and ploting things. Sure it works but i want to make very different plots to discover the data and data studio is not flexible. i need to write absurdly long sql queries to accomplish something simple. &lt;/p&gt;\n\n&lt;p&gt;What am I missing here?  I know dealing with big data is basic for data engineer but I never get to learn it and now i am struggling.  I checked tutorials and none of them are actually using real big data. GCP tutorials always get data using &amp;quot;LIMIT 10000&amp;quot; type of codes. &lt;/p&gt;\n\n&lt;p&gt;Maybe what I need is spark? i am not sure.. &lt;/p&gt;\n\n&lt;p&gt;Is there anyone who can tell me the mistake I am doing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yp1cpx", "is_robot_indexable": true, "report_reasons": null, "author": "karaposu", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yp1cpx/how_can_i_process_big_biqquery_data_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yp1cpx/how_can_i_process_big_biqquery_data_with_python/", "subreddit_subscribers": 818238, "created_utc": 1667857175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hearing from today's real-life data leaders on the Customer Insight Leader podcast (episode 58, Olivia Gambelin, founder of Ethical Associates)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_ypr2ph", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 456, "scrolling": false, "height": 152}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_x3vk1", "secure_media": {"type": "open.spotify.com", "oembed": {"provider_url": "https://spotify.com", "description": "Listen to this episode from Customer Insight Leader podcast on Spotify. For episode 58, I am in conversation with Olivia Gambelin, who joins us from Brussels. Olivia has the exciting job title of being an AI Ethicist and is the founder &amp; CEO of Ethical Intelligence.", "title": "Episode 58 - Olivia Gambelin (Ethical Associates)", "type": "rich", "thumbnail_width": 300, "height": 152, "width": 456, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Spotify", "thumbnail_url": "https://i.scdn.co/image/ab67656300005f1f4ff0d1ef2c05ba7d265e5f4f", "thumbnail_height": 300}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 456, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/ypr2ph", "height": 152}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RWH8pICvv7NDhLx5sbV0Aa1cq0f2YsitYApFJi9s39w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "Leadership", "selftext": "", "author_fullname": "t2_x3vk1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hearing from today's real-life data leaders on the Customer Insight Leader podcast (episode 58, Olivia Gambelin, founder of Ethical Associates)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/Leadership", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_ypr26s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 456, "scrolling": false, "height": 152}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "open.spotify.com", "oembed": {"provider_url": "https://spotify.com", "description": "Listen to this episode from Customer Insight Leader podcast on Spotify. For episode 58, I am in conversation with Olivia Gambelin, who joins us from Brussels. Olivia has the exciting job title of being an AI Ethicist and is the founder &amp; CEO of Ethical Intelligence.", "title": "Episode 58 - Olivia Gambelin (Ethical Associates)", "type": "rich", "thumbnail_width": 300, "height": 152, "width": 456, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Spotify", "thumbnail_url": "https://i.scdn.co/image/ab67656300005f1f4ff0d1ef2c05ba7d265e5f4f", "thumbnail_height": 300}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 456, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/ypr26s", "height": 152}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1667924568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.spotify.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.spotify.com/episode/3DohlbDgAzi70uAndfoR4u", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ifdTymPAzu-kpVxBdeeXnnD3rcbWKbGr777sRQk47oM.jpg?auto=webp&amp;s=9d4047ccd9f4e29da026c82ffa2f45d459dd8d1b", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/ifdTymPAzu-kpVxBdeeXnnD3rcbWKbGr777sRQk47oM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c0cee7cd1ff0ca2bf92105d6865e46fa8236b54", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ifdTymPAzu-kpVxBdeeXnnD3rcbWKbGr777sRQk47oM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58fdad66b2a4d68337f5cd3caa2284ca5a5865ea", "width": 216, "height": 216}], "variants": {}, "id": "fen4KphRVEK7hAjmH__71xK1oXG8c5eLoJwiUxVTHXQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r7ks", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypr26s", "is_robot_indexable": true, "report_reasons": null, "author": "PaulLaughlin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/Leadership/comments/ypr26s/hearing_from_todays_reallife_data_leaders_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.spotify.com/episode/3DohlbDgAzi70uAndfoR4u", "subreddit_subscribers": 29838, "created_utc": 1667924568.0, "num_crossposts": 1, "media": {"type": "open.spotify.com", "oembed": {"provider_url": "https://spotify.com", "description": "Listen to this episode from Customer Insight Leader podcast on Spotify. For episode 58, I am in conversation with Olivia Gambelin, who joins us from Brussels. Olivia has the exciting job title of being an AI Ethicist and is the founder &amp; CEO of Ethical Intelligence.", "title": "Episode 58 - Olivia Gambelin (Ethical Associates)", "type": "rich", "thumbnail_width": 300, "height": 152, "width": 456, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Spotify", "thumbnail_url": "https://i.scdn.co/image/ab67656300005f1f4ff0d1ef2c05ba7d265e5f4f", "thumbnail_height": 300}}, "is_video": false}], "created": 1667924599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.spotify.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://open.spotify.com/episode/3DohlbDgAzi70uAndfoR4u", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ifdTymPAzu-kpVxBdeeXnnD3rcbWKbGr777sRQk47oM.jpg?auto=webp&amp;s=9d4047ccd9f4e29da026c82ffa2f45d459dd8d1b", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/ifdTymPAzu-kpVxBdeeXnnD3rcbWKbGr777sRQk47oM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c0cee7cd1ff0ca2bf92105d6865e46fa8236b54", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ifdTymPAzu-kpVxBdeeXnnD3rcbWKbGr777sRQk47oM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58fdad66b2a4d68337f5cd3caa2284ca5a5865ea", "width": 216, "height": 216}], "variants": {}, "id": "fen4KphRVEK7hAjmH__71xK1oXG8c5eLoJwiUxVTHXQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypr2ph", "is_robot_indexable": true, "report_reasons": null, "author": "PaulLaughlin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_ypr26s", "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypr2ph/hearing_from_todays_reallife_data_leaders_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.spotify.com/episode/3DohlbDgAzi70uAndfoR4u", "subreddit_subscribers": 818238, "created_utc": 1667924599.0, "num_crossposts": 0, "media": {"type": "open.spotify.com", "oembed": {"provider_url": "https://spotify.com", "description": "Listen to this episode from Customer Insight Leader podcast on Spotify. For episode 58, I am in conversation with Olivia Gambelin, who joins us from Brussels. Olivia has the exciting job title of being an AI Ethicist and is the founder &amp; CEO of Ethical Intelligence.", "title": "Episode 58 - Olivia Gambelin (Ethical Associates)", "type": "rich", "thumbnail_width": 300, "height": 152, "width": 456, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Spotify", "thumbnail_url": "https://i.scdn.co/image/ab67656300005f1f4ff0d1ef2c05ba7d265e5f4f", "thumbnail_height": 300}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There's a boss I know that wants to get a Data Scientist, and the budget is $100,000. I told them that won't be enough for a decent data scientist, I also said what do you need it for they answered \"I don't know... They will do machine learning I think\".\nWhat do you guys think?", "author_fullname": "t2_7i9shveb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is $100,000 a year bad for a DS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypebfp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.22, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667892321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a boss I know that wants to get a Data Scientist, and the budget is $100,000. I told them that won&amp;#39;t be enough for a decent data scientist, I also said what do you need it for they answered &amp;quot;I don&amp;#39;t know... They will do machine learning I think&amp;quot;.\nWhat do you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypebfp", "is_robot_indexable": true, "report_reasons": null, "author": "byggmesterPRO", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypebfp/is_100000_a_year_bad_for_a_ds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypebfp/is_100000_a_year_bad_for_a_ds/", "subreddit_subscribers": 818238, "created_utc": 1667892321.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}