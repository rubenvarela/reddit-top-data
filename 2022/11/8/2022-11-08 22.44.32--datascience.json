{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "People are obsessed with pursuing data science roles for some reason. I guess it's interesting work with a high skill ceiling. Thats why I'm pursuing it. But nobody talks about the data analyst. The folks who write SQL for reporting, create dashboards, and provide insights. Data science does do all this in a more sophisticated way, but the reality is most tech companies or start ups do not even have an appetite for that kind of work since they are so focused on growth. If you're struggling to get into data science, consider analytics. The pay is still good (100k plus if you're doing product analytics) and a natural growth path from there can totally be data science. Don't rule it out, you have options. End \ud83d\ude0a", "author_fullname": "t2_8o0eldke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "hot take: forget data science, we need more analysts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypr93q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 305, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 305, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667924978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People are obsessed with pursuing data science roles for some reason. I guess it&amp;#39;s interesting work with a high skill ceiling. Thats why I&amp;#39;m pursuing it. But nobody talks about the data analyst. The folks who write SQL for reporting, create dashboards, and provide insights. Data science does do all this in a more sophisticated way, but the reality is most tech companies or start ups do not even have an appetite for that kind of work since they are so focused on growth. If you&amp;#39;re struggling to get into data science, consider analytics. The pay is still good (100k plus if you&amp;#39;re doing product analytics) and a natural growth path from there can totally be data science. Don&amp;#39;t rule it out, you have options. End \ud83d\ude0a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 200, "id": "award_1703f934-cf44-40cc-a96d-3729d0b48262", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=16&amp;height=16&amp;auto=webp&amp;s=e3adc32e42cf534e27afea719ff932b1ce797cfd", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=32&amp;height=32&amp;auto=webp&amp;s=08542909c94777e870c41a35413bce688ca2fd6c", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=48&amp;height=48&amp;auto=webp&amp;s=4d85746d584b5494087da3561944d6d241f57674", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=64&amp;height=64&amp;auto=webp&amp;s=fd7683c8de2839998a432e7e53e1e06d66c35ad3", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=128&amp;height=128&amp;auto=webp&amp;s=a750da7a573bb231bd863be9725abece0332b828", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "My kindergarten teacher, my cat, my mom, and you.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "I'd Like to Thank...", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=16&amp;height=16&amp;auto=webp&amp;s=e3adc32e42cf534e27afea719ff932b1ce797cfd", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=32&amp;height=32&amp;auto=webp&amp;s=08542909c94777e870c41a35413bce688ca2fd6c", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=48&amp;height=48&amp;auto=webp&amp;s=4d85746d584b5494087da3561944d6d241f57674", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=64&amp;height=64&amp;auto=webp&amp;s=fd7683c8de2839998a432e7e53e1e06d66c35ad3", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png?width=128&amp;height=128&amp;auto=webp&amp;s=a750da7a573bb231bd863be9725abece0332b828", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/8ad2jffnclf41_Thanks.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypr93q", "is_robot_indexable": true, "report_reasons": null, "author": "djaycat", "discussion_type": null, "num_comments": 92, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypr93q/hot_take_forget_data_science_we_need_more_analysts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypr93q/hot_take_forget_data_science_we_need_more_analysts/", "subreddit_subscribers": 818259, "created_utc": 1667924978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi. Every time i follow engineers or watch YouTube videos, they always have some controversial opinion on DA role as if the role wasn\u2019t necessary or it\u2019s less than other roles. Why is that?\n\n- After reading everyone\u2019s responses I am now more  clear on both sides. Im trying to learn more about data in general &amp; I have a interest for DS. If you work in DS and wanna to connect please message me.", "author_fullname": "t2_9avvyyhm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does other people In tech \u201chate\u201d data scientists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp9jzi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 74, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 74, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667921970.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667877182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. Every time i follow engineers or watch YouTube videos, they always have some controversial opinion on DA role as if the role wasn\u2019t necessary or it\u2019s less than other roles. Why is that?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;After reading everyone\u2019s responses I am now more  clear on both sides. Im trying to learn more about data in general &amp;amp; I have a interest for DS. If you work in DS and wanna to connect please message me.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yp9jzi", "is_robot_indexable": true, "report_reasons": null, "author": "Wonderful_Cry_8401", "discussion_type": null, "num_comments": 124, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yp9jzi/why_does_other_people_in_tech_hate_data_scientists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yp9jzi/why_does_other_people_in_tech_hate_data_scientists/", "subreddit_subscribers": 818259, "created_utc": 1667877182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I came across a recent post highlighting that one of the main points that bring others to hate DS people is their crappy code that they hand over to production, and it got me wondering:\n\nWhat is a \"clean\" or \"good\" code that you expect a good DS professional to hand you for production so that you don't hate him/her for it? It'd be good to get some pointers to avoid these mistakes in the future and be hated for them.\n\nAnd please don't reply \"depends on the situation\"; please give good generalisable advice.", "author_fullname": "t2_71lk6sl4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pointers to write \"clean\" code for production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yphayx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 60, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 60, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667901095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came across a recent post highlighting that one of the main points that bring others to hate DS people is their crappy code that they hand over to production, and it got me wondering:&lt;/p&gt;\n\n&lt;p&gt;What is a &amp;quot;clean&amp;quot; or &amp;quot;good&amp;quot; code that you expect a good DS professional to hand you for production so that you don&amp;#39;t hate him/her for it? It&amp;#39;d be good to get some pointers to avoid these mistakes in the future and be hated for them.&lt;/p&gt;\n\n&lt;p&gt;And please don&amp;#39;t reply &amp;quot;depends on the situation&amp;quot;; please give good generalisable advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yphayx", "is_robot_indexable": true, "report_reasons": null, "author": "William_Rosebud", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yphayx/pointers_to_write_clean_code_for_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yphayx/pointers_to_write_clean_code_for_production/", "subreddit_subscribers": 818259, "created_utc": 1667901095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "**Engineers for Ukraine** is an international team of volunteers working on a machine learning tool to identify Russian equipment in real time (with minimal human involvement) to increase the speed at which accurate information about Russian soldiers/equipment in the area passes from local civilians on the ground to the Ukrainian warfighter. \n\nEngineers for Ukraine has four teams: \n\n1. The Data Team, which builds the training datasets for the machine learning models. This is the easiest team to join since the tasks are straightforward and the training is short/easy thanks to the help of our beloved data scientists. \n2. The Machine Learning Team, which builds and trains machine learning models. This team needs a bit more experience and/or time to get through readings to get up to speed. If you are familiar with AWS, AWS SageMaker, AWS S3, AWS Rekognition, AWS Comprehend, Lambda, machine learning attacks, machine learning security, dedicated red team work, and/or data science, please join the machine learning team.\n3. The Development Team, which handles much of the project infrastructure and builds the relevant web pages, services, and user interfaces.  If you are familiar with AWS, Lambda, JavaScript, React.js, Node.js, API's, plug-ins, and/or devops/SRE/cloud engineering, please join the dev team.\n4. The Cybersecurity Team, which works heavily with the Development Team searching for and fixing vulnerabilities, but also creates threat models, does red team vs blue team work, penetration testing, and occasionally OSINT work. If you are interested in learning cool cybersecurity skills from a team of professionals who are just super busy and need more hands on deck to knock out tasks OR if you are also skilled in cybersecurity, please join the cybersecurity team.\n\nI\u2019ve talked about Engineers for Ukraine before in this Reddit post: [https://www.reddit.com/r/ukraine/comments/vlsaka/volunteers\\_needed\\_for\\_proukraine\\_project/](https://www.reddit.com/r/ukraine/comments/vlsaka/volunteers_needed_for_proukraine_project/)\n\nIf you are interested in either of these groups, please reach out to breaker25789@gmail.com with the project (AidSupply or Engineers for Ukraine) and team you are interested in.\n\nWe will reach out and schedule a video call in which you can verify that we aren\u2019t Russian bots and we can verify that you are not a Russian bot by both showing a government-issued photo ID and two social media accounts. As part of the recruitment process, each volunteer may be asked to complete an introductory assignment specific to the project/team they are applying to. This isn\u2019t meant as a barrier, just as a way to get people onboarded faster while giving the project leadership a sense of each volunteer\u2019s skill level.", "author_fullname": "t2_1323h2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Volunteers Needed for Pro-Ukraine Machine Learning Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yptkvg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667930260.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Engineers for Ukraine&lt;/strong&gt; is an international team of volunteers working on a machine learning tool to identify Russian equipment in real time (with minimal human involvement) to increase the speed at which accurate information about Russian soldiers/equipment in the area passes from local civilians on the ground to the Ukrainian warfighter. &lt;/p&gt;\n\n&lt;p&gt;Engineers for Ukraine has four teams: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The Data Team, which builds the training datasets for the machine learning models. This is the easiest team to join since the tasks are straightforward and the training is short/easy thanks to the help of our beloved data scientists. &lt;/li&gt;\n&lt;li&gt;The Machine Learning Team, which builds and trains machine learning models. This team needs a bit more experience and/or time to get through readings to get up to speed. If you are familiar with AWS, AWS SageMaker, AWS S3, AWS Rekognition, AWS Comprehend, Lambda, machine learning attacks, machine learning security, dedicated red team work, and/or data science, please join the machine learning team.&lt;/li&gt;\n&lt;li&gt;The Development Team, which handles much of the project infrastructure and builds the relevant web pages, services, and user interfaces.  If you are familiar with AWS, Lambda, JavaScript, React.js, Node.js, API&amp;#39;s, plug-ins, and/or devops/SRE/cloud engineering, please join the dev team.&lt;/li&gt;\n&lt;li&gt;The Cybersecurity Team, which works heavily with the Development Team searching for and fixing vulnerabilities, but also creates threat models, does red team vs blue team work, penetration testing, and occasionally OSINT work. If you are interested in learning cool cybersecurity skills from a team of professionals who are just super busy and need more hands on deck to knock out tasks OR if you are also skilled in cybersecurity, please join the cybersecurity team.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I\u2019ve talked about Engineers for Ukraine before in this Reddit post: &lt;a href=\"https://www.reddit.com/r/ukraine/comments/vlsaka/volunteers_needed_for_proukraine_project/\"&gt;https://www.reddit.com/r/ukraine/comments/vlsaka/volunteers_needed_for_proukraine_project/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you are interested in either of these groups, please reach out to &lt;a href=\"mailto:breaker25789@gmail.com\"&gt;breaker25789@gmail.com&lt;/a&gt; with the project (AidSupply or Engineers for Ukraine) and team you are interested in.&lt;/p&gt;\n\n&lt;p&gt;We will reach out and schedule a video call in which you can verify that we aren\u2019t Russian bots and we can verify that you are not a Russian bot by both showing a government-issued photo ID and two social media accounts. As part of the recruitment process, each volunteer may be asked to complete an introductory assignment specific to the project/team they are applying to. This isn\u2019t meant as a barrier, just as a way to get people onboarded faster while giving the project leadership a sense of each volunteer\u2019s skill level.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yptkvg", "is_robot_indexable": true, "report_reasons": null, "author": "OttersAreDevilSpawn", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yptkvg/volunteers_needed_for_proukraine_machine_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yptkvg/volunteers_needed_for_proukraine_machine_learning/", "subreddit_subscribers": 818259, "created_utc": 1667930260.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " In my work as a datascientist I use a lot of blackbox algoritms such as gradient boosting, random forrest and neural networks. One question I ALWAYS get from the business I'm making the models for is, what features are important? How does the model make descicions. So to answer that I do the usual feature analysis, correlation matrices, partial dependence plots, mdi, model extraction. But I still fill like I'm not entirely able to answer what variables are the most important for example.\n\nNow I was thinking of a new method to determine feature importance. First we need the trained model, and the feature distributions. If we take a feature, we look at the sorted values and take 11 values corresponding to 0% - 10% - .. - 100% of the feature distribution. Next we take for example 1000 random states of the other features and test per random state the 11 options for the selected feature. For this 11 values of the feature, we check the number of times the y-value (label) changes. After doing this for all features, we should have an order of feature importance, as a higer rate of changes indicates more influence on the labels outcome. Would als be applicable for discrete variables and continuous labels with some minor adjustments.\n\nI love to hear your experiences in this regard and what you think of the proposed method?", "author_fullname": "t2_k8jayuq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Demystify the blackbox, what do you think of my idea", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypn2rg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667916038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my work as a datascientist I use a lot of blackbox algoritms such as gradient boosting, random forrest and neural networks. One question I ALWAYS get from the business I&amp;#39;m making the models for is, what features are important? How does the model make descicions. So to answer that I do the usual feature analysis, correlation matrices, partial dependence plots, mdi, model extraction. But I still fill like I&amp;#39;m not entirely able to answer what variables are the most important for example.&lt;/p&gt;\n\n&lt;p&gt;Now I was thinking of a new method to determine feature importance. First we need the trained model, and the feature distributions. If we take a feature, we look at the sorted values and take 11 values corresponding to 0% - 10% - .. - 100% of the feature distribution. Next we take for example 1000 random states of the other features and test per random state the 11 options for the selected feature. For this 11 values of the feature, we check the number of times the y-value (label) changes. After doing this for all features, we should have an order of feature importance, as a higer rate of changes indicates more influence on the labels outcome. Would als be applicable for discrete variables and continuous labels with some minor adjustments.&lt;/p&gt;\n\n&lt;p&gt;I love to hear your experiences in this regard and what you think of the proposed method?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypn2rg", "is_robot_indexable": true, "report_reasons": null, "author": "localhoststream", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypn2rg/demystify_the_blackbox_what_do_you_think_of_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypn2rg/demystify_the_blackbox_what_do_you_think_of_my/", "subreddit_subscribers": 818259, "created_utc": 1667916038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "how common are hierarchical bayesian models in retail forecasting or supply chain? in the past  I worked in a retail startup which used this method, but I do not know how common it is.", "author_fullname": "t2_cpzb2rdw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how common are hierarchical bayesian models in retail forecasting?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp9gc8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667876889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;how common are hierarchical bayesian models in retail forecasting or supply chain? in the past  I worked in a retail startup which used this method, but I do not know how common it is.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yp9gc8", "is_robot_indexable": true, "report_reasons": null, "author": "awesomedatascientist", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yp9gc8/how_common_are_hierarchical_bayesian_models_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yp9gc8/how_common_are_hierarchical_bayesian_models_in/", "subreddit_subscribers": 818259, "created_utc": 1667876889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "The EDA and data cleaning part of the project is always a messy phase for me. I was wondering what practices people are doing to standardize it and make things more automated and streamlined?\n\nI've also been trying to make my code more extensible to avoid having to code refactor every time new things comes into play. Since its easy to go in one direction, but harder to go in the other. Keeping things more modular and abstract. I know data science is particularly not fit for some parts of these, since data cleaning is very order based, and EDA is very unplanned and prone to rabbit-holes, but i try to put up with it anyways.\n\nFor example, I no longer do df.groupby(X), but put the key into a list like df.groupby(\\[X\\]), and maybe take out that and keep it as a list variable for data abstraction. Or do things like df\\[df\\[\"col\"\\].isin(\\[X\\])\\] instead of df\\[df\\[\"col\"\\]==X\\].\n\nSure, these create awkward one-element lists at the start, but the point is that they might not end up being one-element anymore in the future. The main con is slower readability with these oxbow lake code. Sometimes my team mate tries to revert and \"de-abstract\" the code back and I have to explain to them why extensibility is good.", "author_fullname": "t2_b7eh4ujn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organization in EDA and data cleaning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypg2yb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667897500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The EDA and data cleaning part of the project is always a messy phase for me. I was wondering what practices people are doing to standardize it and make things more automated and streamlined?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also been trying to make my code more extensible to avoid having to code refactor every time new things comes into play. Since its easy to go in one direction, but harder to go in the other. Keeping things more modular and abstract. I know data science is particularly not fit for some parts of these, since data cleaning is very order based, and EDA is very unplanned and prone to rabbit-holes, but i try to put up with it anyways.&lt;/p&gt;\n\n&lt;p&gt;For example, I no longer do df.groupby(X), but put the key into a list like df.groupby([X]), and maybe take out that and keep it as a list variable for data abstraction. Or do things like df[df[&amp;quot;col&amp;quot;].isin([X])] instead of df[df[&amp;quot;col&amp;quot;]==X].&lt;/p&gt;\n\n&lt;p&gt;Sure, these create awkward one-element lists at the start, but the point is that they might not end up being one-element anymore in the future. The main con is slower readability with these oxbow lake code. Sometimes my team mate tries to revert and &amp;quot;de-abstract&amp;quot; the code back and I have to explain to them why extensibility is good.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypg2yb", "is_robot_indexable": true, "report_reasons": null, "author": "countlinard", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypg2yb/organization_in_eda_and_data_cleaning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypg2yb/organization_in_eda_and_data_cleaning/", "subreddit_subscribers": 818259, "created_utc": 1667897500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a question regarding Textmining, I want to get information out of a semi-structured text like the highlighted price or the size of the property. The data is currently in a txt.file but I could also safe it somewhere else. What is the best way to get this information into a structured format? Thank you for your help! \n\nhttps://preview.redd.it/0pyrw00iupy91.png?width=747&amp;format=png&amp;auto=webp&amp;s=c413dee0771dd7071907d4ff3b519e4f8cbf9116", "author_fullname": "t2_tyjml9b0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question Textscraping", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"0pyrw00iupy91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 116, "x": 108, "u": "https://preview.redd.it/0pyrw00iupy91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=68153e297febf102f4ce35060ddfcc3cbf07f145"}, {"y": 233, "x": 216, "u": "https://preview.redd.it/0pyrw00iupy91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b031ddda885c56674026b02333ad7b06226ae3e"}, {"y": 345, "x": 320, "u": "https://preview.redd.it/0pyrw00iupy91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7355378c733c11254324605b7130ac17c264dc59"}, {"y": 691, "x": 640, "u": "https://preview.redd.it/0pyrw00iupy91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ffe6b08d69dabcd4d3cfd0e52ab6d0ad23dd1f84"}], "s": {"y": 807, "x": 747, "u": "https://preview.redd.it/0pyrw00iupy91.png?width=747&amp;format=png&amp;auto=webp&amp;s=c413dee0771dd7071907d4ff3b519e4f8cbf9116"}, "id": "0pyrw00iupy91"}}, "name": "t3_ypjwln", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KhYlSufCJURrtUn8CDQUesWnhSSXOr4U-NzYoziDysY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667908454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a question regarding Textmining, I want to get information out of a semi-structured text like the highlighted price or the size of the property. The data is currently in a txt.file but I could also safe it somewhere else. What is the best way to get this information into a structured format? Thank you for your help! &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0pyrw00iupy91.png?width=747&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c413dee0771dd7071907d4ff3b519e4f8cbf9116\"&gt;https://preview.redd.it/0pyrw00iupy91.png?width=747&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c413dee0771dd7071907d4ff3b519e4f8cbf9116&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypjwln", "is_robot_indexable": true, "report_reasons": null, "author": "Ferry_Carondelet", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypjwln/question_textscraping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypjwln/question_textscraping/", "subreddit_subscribers": 818259, "created_utc": 1667908454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_dkjfx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cyber criminals: Data \u201ctoo dirty\u201d for dark web.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_ypzgf2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/e3LfSkgo3-W6ANFXPGuKhTP5wwO6JQS0FIr6OWyOGpk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667943732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/5fu6mrcjrsy91.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/5fu6mrcjrsy91.jpg?auto=webp&amp;s=05128d7c4a0ef3d3e9663d3a0f40b7178bf612ea", "width": 1124, "height": 1541}, "resolutions": [{"url": "https://preview.redd.it/5fu6mrcjrsy91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a046400361b0ac0fa11b1552273639be0e77939", "width": 108, "height": 148}, {"url": "https://preview.redd.it/5fu6mrcjrsy91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a5a6be9e8af41a511ac0e8f3985c1e4318541e8", "width": 216, "height": 296}, {"url": "https://preview.redd.it/5fu6mrcjrsy91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c1f65ecfd5e4b74e2d33e4dcf74c0037a454eaa3", "width": 320, "height": 438}, {"url": "https://preview.redd.it/5fu6mrcjrsy91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=14767a3820718b756061f8016eed6301cc23463a", "width": 640, "height": 877}, {"url": "https://preview.redd.it/5fu6mrcjrsy91.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d3fadbfbaa4dd61472e403f18bbd8ae0fea0d68", "width": 960, "height": 1316}, {"url": "https://preview.redd.it/5fu6mrcjrsy91.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=187e19effd58d509e6fbc6e53c8c62456e9ad30b", "width": 1080, "height": 1480}], "variants": {}, "id": "XsBoj2TNTQ3mlna6p7Ubws8jtHmRG-ni-fxhsepQLIQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypzgf2", "is_robot_indexable": true, "report_reasons": null, "author": "bpalmerau", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypzgf2/cyber_criminals_data_too_dirty_for_dark_web/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/5fu6mrcjrsy91.jpg", "subreddit_subscribers": 818259, "created_utc": 1667943732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In my two previous roles we were just using Python scripts orchestrated with Airflow. At my current job, we use databricks. I think mostly because there\u2019s a lot of junior DS and they like cramming stuff into notebooks. I can\u2019t find a good workflow. The fact that the databricks environment is so bad compared to a nice IDE like PyCharm is really limiting for me. Ideally I want to do most of the development locally and use databricks to run the code when I need a lot or resources but I can\u2019t find a way that seems reasonably user friendly and end up going back and forth between local and cloud/db env using git. \n\nDoes anyone have a good flow to recommend?", "author_fullname": "t2_fqwkw26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s your workflow when using databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ypz7zw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667943190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my two previous roles we were just using Python scripts orchestrated with Airflow. At my current job, we use databricks. I think mostly because there\u2019s a lot of junior DS and they like cramming stuff into notebooks. I can\u2019t find a good workflow. The fact that the databricks environment is so bad compared to a nice IDE like PyCharm is really limiting for me. Ideally I want to do most of the development locally and use databricks to run the code when I need a lot or resources but I can\u2019t find a way that seems reasonably user friendly and end up going back and forth between local and cloud/db env using git. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a good flow to recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypz7zw", "is_robot_indexable": true, "report_reasons": null, "author": "jobeta", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypz7zw/whats_your_workflow_when_using_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypz7zw/whats_your_workflow_when_using_databricks/", "subreddit_subscribers": 818259, "created_utc": 1667943190.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been looking for some advice or general best practices when it comes to wrangling survey response data that consists mostly of \"click all that apply\" responses and then free text responses (for reference the survey was distributed through Survey Monkey). The data is structured in a wide format, wherein the possible responses for questions are distributed in the columns of the data. For example a question on race would have each of possible values for race as a column variable: \n\n    white &lt;- c(white, NA, NA, white) \n    black &lt;- c(NA, black, NA, black) \n    asian &lt;- c(asian, NA, asian, NA)\n    unique_id &lt;- x(1,2,3)\n    df &lt;- tibble(white, black, asian, unique_id) \n\nEach respondent's responses are captured in a single row and every respondent has a unique ID, so respondent 1 would have checked the boxes for White and Asian. My question is in regards to how best to go about formatting the data for analysis. Given that there are multiple of these \"check all that apply\" or dummy variable columns, is it best to leave this data in a wide format? Or is it better to gather all of these columns into a single variable and pivot the data longer? My only concern is that there are about 20 questions like this, so pivoting each of these variables would create a really long data frame. Additionally, I need to have the data formatted in a way that would require little to no preprocessing in Tableau. This is primarily due to work policies on formatting research publications and data visuals (I know it's not ideal but I can't change it). \n\nI have it currently set up in two way: the first is with each question separated out in different excel sheets that can be joined by their unique ID. The second is with all the questions combined into a single data frame. Both are formatted wide. Thanks in advance for any help or advice, I can also provide additional anonymous data if that would help you help me better :)", "author_fullname": "t2_9337qo7s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cleaning Survey Response Data for Analysis in R | Best Practices or Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypveb6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667934361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been looking for some advice or general best practices when it comes to wrangling survey response data that consists mostly of &amp;quot;click all that apply&amp;quot; responses and then free text responses (for reference the survey was distributed through Survey Monkey). The data is structured in a wide format, wherein the possible responses for questions are distributed in the columns of the data. For example a question on race would have each of possible values for race as a column variable: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;white &amp;lt;- c(white, NA, NA, white) \nblack &amp;lt;- c(NA, black, NA, black) \nasian &amp;lt;- c(asian, NA, asian, NA)\nunique_id &amp;lt;- x(1,2,3)\ndf &amp;lt;- tibble(white, black, asian, unique_id) \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Each respondent&amp;#39;s responses are captured in a single row and every respondent has a unique ID, so respondent 1 would have checked the boxes for White and Asian. My question is in regards to how best to go about formatting the data for analysis. Given that there are multiple of these &amp;quot;check all that apply&amp;quot; or dummy variable columns, is it best to leave this data in a wide format? Or is it better to gather all of these columns into a single variable and pivot the data longer? My only concern is that there are about 20 questions like this, so pivoting each of these variables would create a really long data frame. Additionally, I need to have the data formatted in a way that would require little to no preprocessing in Tableau. This is primarily due to work policies on formatting research publications and data visuals (I know it&amp;#39;s not ideal but I can&amp;#39;t change it). &lt;/p&gt;\n\n&lt;p&gt;I have it currently set up in two way: the first is with each question separated out in different excel sheets that can be joined by their unique ID. The second is with all the questions combined into a single data frame. Both are formatted wide. Thanks in advance for any help or advice, I can also provide additional anonymous data if that would help you help me better :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypveb6", "is_robot_indexable": true, "report_reasons": null, "author": "Legal_Television_944", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypveb6/cleaning_survey_response_data_for_analysis_in_r/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypveb6/cleaning_survey_response_data_for_analysis_in_r/", "subreddit_subscribers": 818259, "created_utc": 1667934361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hello,\n\nI am working on curve's peaks and valleys detections.\n\n**Peak detection function**  \nIn Python there is no shortage of nice functions to do it like with scipy.signal.find\\_peaks.  \nAnd most functions have many parameters (distance, height, prominence, etc.) which have for objective to detect more or less peaks, which mean ignoring the minor ones, considered as noise.\n\n**Curve smoothing**  \nThen comes smoothing the curve before peak detecting.  \nI often see it done in blogs and examples.  \nThe savgol\\_filter is quite often used.  \nBasically it smooths the curves, so noise is removed and then you detect only major peaks.\n\n**Here are my questions**:\n\n**1/ Is curve smoothing before peak detection of any use ?**  \nI have the intuition that it's overlapping, and basically doing twice the same thing via different means.  \nAnd that if you fine tune your peak detection function parameters (distance, height, prominence, etc.) enough, smoothing before will never provide any upside.\n\n**2/ If curve smoothing is useful how to optimize it with peak detection?**  \nSay you want to detect only visually major peaks (I know it's subjective, but let's say you want only a limited number of peaks, while maximizing the gap between peaks and valleys).   \nIs there a way to find the correct mix of smoothing and parameters?  \nOr do you bruteforce millions of combinaisons until you have minimized the number of peaks while maximized the average gaps between peaks and valleys? \n\nWhat are your thoughts?  \nthx", "author_fullname": "t2_5559lwqj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Curve peak finding: isn't curve smoothing overlapping with peak detection functions parameters?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypqxuf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667924311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am working on curve&amp;#39;s peaks and valleys detections.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Peak detection function&lt;/strong&gt;&lt;br/&gt;\nIn Python there is no shortage of nice functions to do it like with scipy.signal.find_peaks.&lt;br/&gt;\nAnd most functions have many parameters (distance, height, prominence, etc.) which have for objective to detect more or less peaks, which mean ignoring the minor ones, considered as noise.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Curve smoothing&lt;/strong&gt;&lt;br/&gt;\nThen comes smoothing the curve before peak detecting.&lt;br/&gt;\nI often see it done in blogs and examples.&lt;br/&gt;\nThe savgol_filter is quite often used.&lt;br/&gt;\nBasically it smooths the curves, so noise is removed and then you detect only major peaks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here are my questions&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1/ Is curve smoothing before peak detection of any use ?&lt;/strong&gt;&lt;br/&gt;\nI have the intuition that it&amp;#39;s overlapping, and basically doing twice the same thing via different means.&lt;br/&gt;\nAnd that if you fine tune your peak detection function parameters (distance, height, prominence, etc.) enough, smoothing before will never provide any upside.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2/ If curve smoothing is useful how to optimize it with peak detection?&lt;/strong&gt;&lt;br/&gt;\nSay you want to detect only visually major peaks (I know it&amp;#39;s subjective, but let&amp;#39;s say you want only a limited number of peaks, while maximizing the gap between peaks and valleys).&lt;br/&gt;\nIs there a way to find the correct mix of smoothing and parameters?&lt;br/&gt;\nOr do you bruteforce millions of combinaisons until you have minimized the number of peaks while maximized the average gaps between peaks and valleys? &lt;/p&gt;\n\n&lt;p&gt;What are your thoughts?&lt;br/&gt;\nthx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypqxuf", "is_robot_indexable": true, "report_reasons": null, "author": "Vince_peak", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypqxuf/curve_peak_finding_isnt_curve_smoothing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypqxuf/curve_peak_finding_isnt_curve_smoothing/", "subreddit_subscribers": 818259, "created_utc": 1667924311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a list of several dairy products sold at a grocery store, and data on how many were sold per day over the course of two years.  I need to create a data-driven ordering system using this data to model how many of these products the store should order at any given time.\n\nI'm not used to working with time series, and am rather unsure of how to approach this problem.  One model I was thinking could include lag variables for amount sold of this product for the past 7 days.  I'm not sure where I could take it from there.\n\nAppreciate any thoughts from people more expert than me on this on relatively simple methods to solve this problem.  This is for a take home assignment for a job interview, and I could use ideas.  Links to resources and methods would be very helpful.", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modeling Product Orders Based on Quantities Sold", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypa1yt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667878626.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a list of several dairy products sold at a grocery store, and data on how many were sold per day over the course of two years.  I need to create a data-driven ordering system using this data to model how many of these products the store should order at any given time.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not used to working with time series, and am rather unsure of how to approach this problem.  One model I was thinking could include lag variables for amount sold of this product for the past 7 days.  I&amp;#39;m not sure where I could take it from there.&lt;/p&gt;\n\n&lt;p&gt;Appreciate any thoughts from people more expert than me on this on relatively simple methods to solve this problem.  This is for a take home assignment for a job interview, and I could use ideas.  Links to resources and methods would be very helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypa1yt", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypa1yt/modeling_product_orders_based_on_quantities_sold/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypa1yt/modeling_product_orders_based_on_quantities_sold/", "subreddit_subscribers": 818259, "created_utc": 1667878626.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Well wrt deep learning, I'm not sure how they differ, but my impression is that hyperparameter tuning deals with parameters of a specific layer, whereas ablation study deals with different parts of a model (meaning different layers themselves). Am I correct? Is there anything else to it?\u00a0And lastly, is one subset of the other?", "author_fullname": "t2_ayk7yaw4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ablation study vs Hyperparameter tuning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp2zxd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667860467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Well wrt deep learning, I&amp;#39;m not sure how they differ, but my impression is that hyperparameter tuning deals with parameters of a specific layer, whereas ablation study deals with different parts of a model (meaning different layers themselves). Am I correct? Is there anything else to it?\u00a0And lastly, is one subset of the other?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yp2zxd", "is_robot_indexable": true, "report_reasons": null, "author": "Ancient_Barber_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yp2zxd/ablation_study_vs_hyperparameter_tuning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yp2zxd/ablation_study_vs_hyperparameter_tuning/", "subreddit_subscribers": 818259, "created_utc": 1667860467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Does a multiple regression model contain combination of predictor variables (eg. x1x2, x2x4, etc. where x1, x2, x3, x4, etc. are the predictor variables) or only separate multiple predictor variables?", "author_fullname": "t2_pwif3is8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does multiple regression model contain combination of predictor variables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypd1uv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667887879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does a multiple regression model contain combination of predictor variables (eg. x1x2, x2x4, etc. where x1, x2, x3, x4, etc. are the predictor variables) or only separate multiple predictor variables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypd1uv", "is_robot_indexable": true, "report_reasons": null, "author": "random5842786439", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypd1uv/does_multiple_regression_model_contain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypd1uv/does_multiple_regression_model_contain/", "subreddit_subscribers": 818259, "created_utc": 1667887879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_11jnpsdw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the delineation between scope of responsibilities for a Jr Data Scientist vs a Senior Data Scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypaots", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667880490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypaots", "is_robot_indexable": true, "report_reasons": null, "author": "Blackcatsloveme", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypaots/what_is_the_delineation_between_scope_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypaots/what_is_the_delineation_between_scope_of/", "subreddit_subscribers": 818259, "created_utc": 1667880490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am working with a data set that needs to remain strictly confidential (though not due to HIPAA or any such legal requirements, just out of respect to the source). I also need to protype a web dashboard for visualizing and manipulating the data to demonstrate possible ways of communicating the data and the \"story\" within if they made the data publicly accessible.\n\nI have two questions (which may have different answers):\n\n1. How would you minimally secure (i.e. password protect, but perhaps not with a full-on authentication system) a basic (probably static, HTML/CSS/Javascript) web app in 2022?\n2. How do you deal with confidential/proprietary data when developing prototypes?\n3. (Bonus) Happen to have an example of how you tackled a similar scenario?\n\nI'll edit this post as needed if clarifying questions are asked below. Thanks in advance!", "author_fullname": "t2_f260834", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Securing a Data Science Project in 2022", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypvho0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667934580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with a data set that needs to remain strictly confidential (though not due to HIPAA or any such legal requirements, just out of respect to the source). I also need to protype a web dashboard for visualizing and manipulating the data to demonstrate possible ways of communicating the data and the &amp;quot;story&amp;quot; within if they made the data publicly accessible.&lt;/p&gt;\n\n&lt;p&gt;I have two questions (which may have different answers):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How would you minimally secure (i.e. password protect, but perhaps not with a full-on authentication system) a basic (probably static, HTML/CSS/Javascript) web app in 2022?&lt;/li&gt;\n&lt;li&gt;How do you deal with confidential/proprietary data when developing prototypes?&lt;/li&gt;\n&lt;li&gt;(Bonus) Happen to have an example of how you tackled a similar scenario?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ll edit this post as needed if clarifying questions are asked below. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypvho0", "is_robot_indexable": true, "report_reasons": null, "author": "BoxBeatMan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypvho0/securing_a_data_science_project_in_2022/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypvho0/securing_a_data_science_project_in_2022/", "subreddit_subscribers": 818259, "created_utc": 1667934580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was thinking about the actual updating of the weights and biases for the past couple of days, and i realized something that i havent been able to get off my mind.. if your learning rate is less than one (which i dont see how it couldnt be. Anything greater than one converges to infinite error as far as i can tell, and using a number less than one is the only way to actually use the information about the sensitivity of the cost function), anywhere you find a derivative less than one is going to make your change huge and probably change your weight too much\u2026 what\u2026 what am i missing here?\n\n(Btw sorry if this question seems inappropriate but i dont really know where else to ask r/askdatascience is dead)", "author_fullname": "t2_3hk48w5g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Back Propagation Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypv4jy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667945065.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667933756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was thinking about the actual updating of the weights and biases for the past couple of days, and i realized something that i havent been able to get off my mind.. if your learning rate is less than one (which i dont see how it couldnt be. Anything greater than one converges to infinite error as far as i can tell, and using a number less than one is the only way to actually use the information about the sensitivity of the cost function), anywhere you find a derivative less than one is going to make your change huge and probably change your weight too much\u2026 what\u2026 what am i missing here?&lt;/p&gt;\n\n&lt;p&gt;(Btw sorry if this question seems inappropriate but i dont really know where else to ask &lt;a href=\"/r/askdatascience\"&gt;r/askdatascience&lt;/a&gt; is dead)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypv4jy", "is_robot_indexable": true, "report_reasons": null, "author": "Sharpeye1994", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypv4jy/back_propagation_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypv4jy/back_propagation_question/", "subreddit_subscribers": 818259, "created_utc": 1667933756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Companies right now are flush with wannabe data scientists and, worse yet, underpaid, overlooked \"data\" analysts that put \"Microsoft Office Suite\" on their resumes still. Everyday I see people who want to break into the field of data science by posting shitty juPyter notebooks chock-full of violets or lillies or whatever that stupid floral data set is. What would really get them hired? A well-formatted PowerBI dashboard showing how many failed attempts to switch from marketing to data science involved predicting the survival rate of a hypothetical person on the USS Titanic. Bonus points if the person who made the dashboard could spell Git.\n\nListen, I know you doubt me, but please understand the following: companies don't know anything about anything ever and never will. Stakeholders currently make decisions by flipping a coin and whether or not there's a tingling in their elbows. They don't want data scientist, data analysts, data engineers, data product managers, data project managers, machine-learning engineers, machine-learning analysts, machine-learning scientists, research scientists, or machine-learning research scientists analysts. If you take the set of \\[\"data\", \"machine-learning\", \"research\"\\] and the set of \\['scientist', 'engineer', 'manager', 'analyst', 'intern'\\] and took ever combination of them\\*, then you'd arrive at a complete list of dogshit jobs that no one is hiring for, and will never hire for. Why? Because data is stupid. The whole thing is stupid. No one is doing anything. How do I know? Because I looked at the data the same way a stakeholder would: I read the headlines on this reddit and went with my gut-feeling to make decisions.\n\nSpeaking of gut feelings, you know what companies need more of? Anal.\n\n&amp;#x200B;\n\n\\*Only those combinations with \"engineer\" at the end will know how to import itertools, fools.\n\nEdit: now that we're done jerking off talking about how data science is a top-tier C-suite level job that only people with 15,000 years of experience and a doctorate from Oxford or Yale can hold, can we discuss the actual content, news, methodologies, and developments within the field of data science? ", "author_fullname": "t2_lwmkqytr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "hot take: the \"yst\" is not needed.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ypyhfj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.36, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": 1667941801.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667941527.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Companies right now are flush with wannabe data scientists and, worse yet, underpaid, overlooked &amp;quot;data&amp;quot; analysts that put &amp;quot;Microsoft Office Suite&amp;quot; on their resumes still. Everyday I see people who want to break into the field of data science by posting shitty juPyter notebooks chock-full of violets or lillies or whatever that stupid floral data set is. What would really get them hired? A well-formatted PowerBI dashboard showing how many failed attempts to switch from marketing to data science involved predicting the survival rate of a hypothetical person on the USS Titanic. Bonus points if the person who made the dashboard could spell Git.&lt;/p&gt;\n\n&lt;p&gt;Listen, I know you doubt me, but please understand the following: companies don&amp;#39;t know anything about anything ever and never will. Stakeholders currently make decisions by flipping a coin and whether or not there&amp;#39;s a tingling in their elbows. They don&amp;#39;t want data scientist, data analysts, data engineers, data product managers, data project managers, machine-learning engineers, machine-learning analysts, machine-learning scientists, research scientists, or machine-learning research scientists analysts. If you take the set of [&amp;quot;data&amp;quot;, &amp;quot;machine-learning&amp;quot;, &amp;quot;research&amp;quot;] and the set of [&amp;#39;scientist&amp;#39;, &amp;#39;engineer&amp;#39;, &amp;#39;manager&amp;#39;, &amp;#39;analyst&amp;#39;, &amp;#39;intern&amp;#39;] and took ever combination of them*, then you&amp;#39;d arrive at a complete list of dogshit jobs that no one is hiring for, and will never hire for. Why? Because data is stupid. The whole thing is stupid. No one is doing anything. How do I know? Because I looked at the data the same way a stakeholder would: I read the headlines on this reddit and went with my gut-feeling to make decisions.&lt;/p&gt;\n\n&lt;p&gt;Speaking of gut feelings, you know what companies need more of? Anal.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;*Only those combinations with &amp;quot;engineer&amp;quot; at the end will know how to import itertools, fools.&lt;/p&gt;\n\n&lt;p&gt;Edit: now that we&amp;#39;re done jerking off talking about how data science is a top-tier C-suite level job that only people with 15,000 years of experience and a doctorate from Oxford or Yale can hold, can we discuss the actual content, news, methodologies, and developments within the field of data science? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypyhfj", "is_robot_indexable": true, "report_reasons": null, "author": "c0ntrap0sitive", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypyhfj/hot_take_the_yst_is_not_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypyhfj/hot_take_the_yst_is_not_needed/", "subreddit_subscribers": 818259, "created_utc": 1667941527.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hearing from today's real-life data leaders on the Customer Insight Leader podcast (episode 58, Olivia Gambelin, founder of Ethical Associates)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_ypr2ph", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 456, "scrolling": false, "height": 152}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_x3vk1", "secure_media": {"type": "open.spotify.com", "oembed": {"provider_url": "https://spotify.com", "description": "Listen to this episode from Customer Insight Leader podcast on Spotify. For episode 58, I am in conversation with Olivia Gambelin, who joins us from Brussels. Olivia has the exciting job title of being an AI Ethicist and is the founder &amp; CEO of Ethical Intelligence.", "title": "Episode 58 - Olivia Gambelin (Ethical Associates)", "type": "rich", "thumbnail_width": 300, "height": 152, "width": 456, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Spotify", "thumbnail_url": "https://i.scdn.co/image/ab67656300005f1f4ff0d1ef2c05ba7d265e5f4f", "thumbnail_height": 300}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 456, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/ypr2ph", "height": 152}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RWH8pICvv7NDhLx5sbV0Aa1cq0f2YsitYApFJi9s39w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "Leadership", "selftext": "", "author_fullname": "t2_x3vk1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hearing from today's real-life data leaders on the Customer Insight Leader podcast (episode 58, Olivia Gambelin, founder of Ethical Associates)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/Leadership", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_ypr26s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 456, "scrolling": false, "height": 152}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "open.spotify.com", "oembed": {"provider_url": "https://spotify.com", "description": "Listen to this episode from Customer Insight Leader podcast on Spotify. For episode 58, I am in conversation with Olivia Gambelin, who joins us from Brussels. Olivia has the exciting job title of being an AI Ethicist and is the founder &amp; CEO of Ethical Intelligence.", "title": "Episode 58 - Olivia Gambelin (Ethical Associates)", "type": "rich", "thumbnail_width": 300, "height": 152, "width": 456, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Spotify", "thumbnail_url": "https://i.scdn.co/image/ab67656300005f1f4ff0d1ef2c05ba7d265e5f4f", "thumbnail_height": 300}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "width": 456, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/ypr26s", "height": 152}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1667924568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.spotify.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.spotify.com/episode/3DohlbDgAzi70uAndfoR4u", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ifdTymPAzu-kpVxBdeeXnnD3rcbWKbGr777sRQk47oM.jpg?auto=webp&amp;s=9d4047ccd9f4e29da026c82ffa2f45d459dd8d1b", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/ifdTymPAzu-kpVxBdeeXnnD3rcbWKbGr777sRQk47oM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c0cee7cd1ff0ca2bf92105d6865e46fa8236b54", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ifdTymPAzu-kpVxBdeeXnnD3rcbWKbGr777sRQk47oM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58fdad66b2a4d68337f5cd3caa2284ca5a5865ea", "width": 216, "height": 216}], "variants": {}, "id": "fen4KphRVEK7hAjmH__71xK1oXG8c5eLoJwiUxVTHXQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r7ks", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypr26s", "is_robot_indexable": true, "report_reasons": null, "author": "PaulLaughlin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/Leadership/comments/ypr26s/hearing_from_todays_reallife_data_leaders_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.spotify.com/episode/3DohlbDgAzi70uAndfoR4u", "subreddit_subscribers": 29838, "created_utc": 1667924568.0, "num_crossposts": 1, "media": {"type": "open.spotify.com", "oembed": {"provider_url": "https://spotify.com", "description": "Listen to this episode from Customer Insight Leader podcast on Spotify. For episode 58, I am in conversation with Olivia Gambelin, who joins us from Brussels. Olivia has the exciting job title of being an AI Ethicist and is the founder &amp; CEO of Ethical Intelligence.", "title": "Episode 58 - Olivia Gambelin (Ethical Associates)", "type": "rich", "thumbnail_width": 300, "height": 152, "width": 456, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Spotify", "thumbnail_url": "https://i.scdn.co/image/ab67656300005f1f4ff0d1ef2c05ba7d265e5f4f", "thumbnail_height": 300}}, "is_video": false}], "created": 1667924599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.spotify.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://open.spotify.com/episode/3DohlbDgAzi70uAndfoR4u", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ifdTymPAzu-kpVxBdeeXnnD3rcbWKbGr777sRQk47oM.jpg?auto=webp&amp;s=9d4047ccd9f4e29da026c82ffa2f45d459dd8d1b", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/ifdTymPAzu-kpVxBdeeXnnD3rcbWKbGr777sRQk47oM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c0cee7cd1ff0ca2bf92105d6865e46fa8236b54", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ifdTymPAzu-kpVxBdeeXnnD3rcbWKbGr777sRQk47oM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58fdad66b2a4d68337f5cd3caa2284ca5a5865ea", "width": 216, "height": 216}], "variants": {}, "id": "fen4KphRVEK7hAjmH__71xK1oXG8c5eLoJwiUxVTHXQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypr2ph", "is_robot_indexable": true, "report_reasons": null, "author": "PaulLaughlin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_ypr26s", "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypr2ph/hearing_from_todays_reallife_data_leaders_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.spotify.com/episode/3DohlbDgAzi70uAndfoR4u", "subreddit_subscribers": 818259, "created_utc": 1667924599.0, "num_crossposts": 0, "media": {"type": "open.spotify.com", "oembed": {"provider_url": "https://spotify.com", "description": "Listen to this episode from Customer Insight Leader podcast on Spotify. For episode 58, I am in conversation with Olivia Gambelin, who joins us from Brussels. Olivia has the exciting job title of being an AI Ethicist and is the founder &amp; CEO of Ethical Intelligence.", "title": "Episode 58 - Olivia Gambelin (Ethical Associates)", "type": "rich", "thumbnail_width": 300, "height": 152, "width": 456, "html": "&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fopen.spotify.com%2Fembed%2Fepisode%2F3DohlbDgAzi70uAndfoR4u%3Futm_source%3Doembed&amp;display_name=Spotify&amp;url=https%3A%2F%2Fopen.spotify.com%2Fepisode%2F3DohlbDgAzi70uAndfoR4u&amp;image=https%3A%2F%2Fi.scdn.co%2Fimage%2Fab67656300005f1f4ff0d1ef2c05ba7d265e5f4f&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=spotify\" width=\"456\" height=\"152\" scrolling=\"no\" title=\"Spotify embed\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"true\"&gt;&lt;/iframe&gt;", "version": "1.0", "provider_name": "Spotify", "thumbnail_url": "https://i.scdn.co/image/ab67656300005f1f4ff0d1ef2c05ba7d265e5f4f", "thumbnail_height": 300}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There's a boss I know that wants to get a Data Scientist, and the budget is $100,000. I told them that won't be enough for a decent data scientist, I also said what do you need it for they answered \"I don't know... They will do machine learning I think\".\nWhat do you guys think?", "author_fullname": "t2_7i9shveb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is $100,000 a year bad for a DS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ypebfp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.3, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667892321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a boss I know that wants to get a Data Scientist, and the budget is $100,000. I told them that won&amp;#39;t be enough for a decent data scientist, I also said what do you need it for they answered &amp;quot;I don&amp;#39;t know... They will do machine learning I think&amp;quot;.\nWhat do you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ypebfp", "is_robot_indexable": true, "report_reasons": null, "author": "byggmesterPRO", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ypebfp/is_100000_a_year_bad_for_a_ds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ypebfp/is_100000_a_year_bad_for_a_ds/", "subreddit_subscribers": 818259, "created_utc": 1667892321.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}