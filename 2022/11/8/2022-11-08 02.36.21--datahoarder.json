{"kind": "Listing", "data": {"after": "t3_yocn5s", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_582xz2q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reminder: Libgen is also hosted on the IPFS network here, which is decentralized and therefore much harder to take down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yoirof", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 672, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 672, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1667817039.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "libgen-crypto.ipns.dweb.link", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://libgen-crypto.ipns.dweb.link/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yoirof", "is_robot_indexable": true, "report_reasons": null, "author": "Spirited-Pause", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yoirof/reminder_libgen_is_also_hosted_on_the_ipfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://libgen-crypto.ipns.dweb.link/", "subreddit_subscribers": 652306, "created_utc": 1667817039.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_z1dy9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Samsung Electronics Begins Mass Production of 8th-Gen Vertical NAND With Industry\u2019s Highest Bit Density", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_yoq0g0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/FX8QjYrnNw4FSG5NebkuEtaFpOXQRhoV0OG0Rja9iC8.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667834294.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "news.samsung.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://news.samsung.com/global/samsung-electronics-begins-mass-production-of-8th-gen-vertical-nand-with-industrys-highest-bit-density", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_vA_8Eo0fP4BWtTyoz3ZA1Gq9yVPEkCxe-SFjgtS1XM.jpg?auto=webp&amp;s=32fd6b0b831240d7df83728cb913107f9adad8ec", "width": 728, "height": 410}, "resolutions": [{"url": "https://external-preview.redd.it/_vA_8Eo0fP4BWtTyoz3ZA1Gq9yVPEkCxe-SFjgtS1XM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a3776e4dd14a1f0346357c9b66c3ffb533ef67fa", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/_vA_8Eo0fP4BWtTyoz3ZA1Gq9yVPEkCxe-SFjgtS1XM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=792d20ff8504f931080bde92b27ea690aed4755f", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/_vA_8Eo0fP4BWtTyoz3ZA1Gq9yVPEkCxe-SFjgtS1XM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=84f7c1dbe3e20d047c89868a30e2be027c450f37", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/_vA_8Eo0fP4BWtTyoz3ZA1Gq9yVPEkCxe-SFjgtS1XM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=98dda8cd2f98230ad95b774a5368372aca9a6be8", "width": 640, "height": 360}], "variants": {}, "id": "1QEvcy14OIjkgrX5fEpTTnqPTroYnGMuWIGY4WyK5xU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "84TB QLC + 48TB CMR", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yoq0g0", "is_robot_indexable": true, "report_reasons": null, "author": "etherealshatter", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yoq0g0/samsung_electronics_begins_mass_production_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://news.samsung.com/global/samsung-electronics-begins-mass-production-of-8th-gen-vertical-nand-with-industrys-highest-bit-density", "subreddit_subscribers": 652306, "created_utc": 1667834294.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know of a good 3.5\" to 5.25\" adapter for the new WD screw hole standard that the 14TB drives use? \n\nI just recently bought 4, 14TB drives and ran out of 3.5 bays to use in my Enthoo Pro 1 and would like to put the 3, 5.25 bays to work.\n\nThanks", "author_fullname": "t2_5hsmi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3.5\" to 5.25\" Adapter for Shucked Easystore 14TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp50x7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667864840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of a good 3.5&amp;quot; to 5.25&amp;quot; adapter for the new WD screw hole standard that the 14TB drives use? &lt;/p&gt;\n\n&lt;p&gt;I just recently bought 4, 14TB drives and ran out of 3.5 bays to use in my Enthoo Pro 1 and would like to put the 3, 5.25 bays to work.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yp50x7", "is_robot_indexable": true, "report_reasons": null, "author": "SuperBio", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yp50x7/35_to_525_adapter_for_shucked_easystore_14tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yp50x7/35_to_525_adapter_for_shucked_easystore_14tb/", "subreddit_subscribers": 652306, "created_utc": 1667864840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For example, it seems like a video is 10 minutes long, but its actually 2 minutes long because its kind of corrupt (this is a recovered file). I can play the video using video regular players. After 2 minutes, it just ends. I get no error from the player.\n\nThere are a few hundred unhealthy videos among a few thousand videos\n\nHow can I bulk-check them?\n\nGoal: Identify unhealthy movies. Write their filename and folder info into a text file. Then remove the files", "author_fullname": "t2_teoa5nr0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bulk video health check? (macOS)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp32wi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667867016.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667860638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For example, it seems like a video is 10 minutes long, but its actually 2 minutes long because its kind of corrupt (this is a recovered file). I can play the video using video regular players. After 2 minutes, it just ends. I get no error from the player.&lt;/p&gt;\n\n&lt;p&gt;There are a few hundred unhealthy videos among a few thousand videos&lt;/p&gt;\n\n&lt;p&gt;How can I bulk-check them?&lt;/p&gt;\n\n&lt;p&gt;Goal: Identify unhealthy movies. Write their filename and folder info into a text file. Then remove the files&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yp32wi", "is_robot_indexable": true, "report_reasons": null, "author": "Whole-Chemistry7585", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yp32wi/bulk_video_health_check_macos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yp32wi/bulk_video_health_check_macos/", "subreddit_subscribers": 652306, "created_utc": 1667860638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've tried looking at this on the internet, but have found several conflicting answers. I'd like to use a 4tb or 8tb External HDD for some old video project files that I just don't need quick immediate access to. I'd just like to put it on there, and then forget about it until I maybe need it for something. Each video project with all assets is about 10gb.\n\n&amp;#x200B;\n\nNot necessarily looking in to like a NAS or second pc or something like that.\n\n&amp;#x200B;\n\nIf I go through with this, would leaving it plugged in or unplugged be okay?", "author_fullname": "t2_jc7km0ri", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using an External HDD for mass storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp2c12", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667859151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried looking at this on the internet, but have found several conflicting answers. I&amp;#39;d like to use a 4tb or 8tb External HDD for some old video project files that I just don&amp;#39;t need quick immediate access to. I&amp;#39;d just like to put it on there, and then forget about it until I maybe need it for something. Each video project with all assets is about 10gb.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Not necessarily looking in to like a NAS or second pc or something like that.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If I go through with this, would leaving it plugged in or unplugged be okay?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yp2c12", "is_robot_indexable": true, "report_reasons": null, "author": "Initial-Complex7477", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yp2c12/using_an_external_hdd_for_mass_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yp2c12/using_an_external_hdd_for_mass_storage/", "subreddit_subscribers": 652306, "created_utc": 1667859151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "as the title says, I have some films on my drive that are missing metadata (title, author, that kinda stuff) and have been wondering if any of you know a tool that can add that to them automatically, kinda like Uniconverter does with its metadata fixer tool but for free. I don't know if this is the right sub for this but any help would be appreciated!", "author_fullname": "t2_1ovofknf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "add metadata to movie files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yozc8b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667853011.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as the title says, I have some films on my drive that are missing metadata (title, author, that kinda stuff) and have been wondering if any of you know a tool that can add that to them automatically, kinda like Uniconverter does with its metadata fixer tool but for free. I don&amp;#39;t know if this is the right sub for this but any help would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yozc8b", "is_robot_indexable": true, "report_reasons": null, "author": "gagaboom", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yozc8b/add_metadata_to_movie_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yozc8b/add_metadata_to_movie_files/", "subreddit_subscribers": 652306, "created_utc": 1667853011.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for advice regarding HDD cache size. I'm planning on adding 2 drives to my linux server in a LVM Raid 0 configuration, it should match my existing raid5 in capacity (22tb usable). old raid 5 will become the backup volume, so no concern for data loss. LVM raid will allow me a on-the-fly conversion from raid 0 to 5 once I add a third disk. The server is used for many things, but 4k streaming via plex is the only real resource consuming thing (although we have only 1 stream at a time).\n\nWD Red Plus 12TB (256MB Cache) for \u20ac298,90 or \u20ac24,908/TB\n\nWD Red Plus 14TB (512MB Cache) for \u20ac373,98 or \u20ac26,713/TB\n\nOriginally I had decided to get the WD Red Plus 12TB, but then I noticed that the 14TB has double the cache size. I'm not sure how strongly that would impact the performance and if it will be worth the extra 150\u20ac? Does anybody have any insights or opinions on that?", "author_fullname": "t2_bewks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Importance of HDD cache within Raid, for Plex", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yop4ch", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667832466.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for advice regarding HDD cache size. I&amp;#39;m planning on adding 2 drives to my linux server in a LVM Raid 0 configuration, it should match my existing raid5 in capacity (22tb usable). old raid 5 will become the backup volume, so no concern for data loss. LVM raid will allow me a on-the-fly conversion from raid 0 to 5 once I add a third disk. The server is used for many things, but 4k streaming via plex is the only real resource consuming thing (although we have only 1 stream at a time).&lt;/p&gt;\n\n&lt;p&gt;WD Red Plus 12TB (256MB Cache) for \u20ac298,90 or \u20ac24,908/TB&lt;/p&gt;\n\n&lt;p&gt;WD Red Plus 14TB (512MB Cache) for \u20ac373,98 or \u20ac26,713/TB&lt;/p&gt;\n\n&lt;p&gt;Originally I had decided to get the WD Red Plus 12TB, but then I noticed that the 14TB has double the cache size. I&amp;#39;m not sure how strongly that would impact the performance and if it will be worth the extra 150\u20ac? Does anybody have any insights or opinions on that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yop4ch", "is_robot_indexable": true, "report_reasons": null, "author": "Rakyr51", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yop4ch/importance_of_hdd_cache_within_raid_for_plex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yop4ch/importance_of_hdd_cache_within_raid_for_plex/", "subreddit_subscribers": 652306, "created_utc": 1667832466.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I didn't know about backup and parity (still know lil to none) but i lost almost 2 tb data due to wd passport external hdd malfunctioning.\nSo I'm looking for an enclosure and a 2tb ssd(patriot had one for sale only 90 usd) which I bought.\nThe problem is I'm looking for a good enclosure I don't want crazy speed or stuff, but i need it to be rigged as i don't want something that will break so please suggest me some.\nI can also go for nvme but i just need to store data that i move around n don't need to boot from it so seems like unnecessary.", "author_fullname": "t2_5aa9juz9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best enclosure for 2.5inch sata ssd that is rugged and drop prone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp4sd8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667864229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I didn&amp;#39;t know about backup and parity (still know lil to none) but i lost almost 2 tb data due to wd passport external hdd malfunctioning.\nSo I&amp;#39;m looking for an enclosure and a 2tb ssd(patriot had one for sale only 90 usd) which I bought.\nThe problem is I&amp;#39;m looking for a good enclosure I don&amp;#39;t want crazy speed or stuff, but i need it to be rigged as i don&amp;#39;t want something that will break so please suggest me some.\nI can also go for nvme but i just need to store data that i move around n don&amp;#39;t need to boot from it so seems like unnecessary.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yp4sd8", "is_robot_indexable": true, "report_reasons": null, "author": "Astavicious", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yp4sd8/best_enclosure_for_25inch_sata_ssd_that_is_rugged/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yp4sd8/best_enclosure_for_25inch_sata_ssd_that_is_rugged/", "subreddit_subscribers": 652306, "created_utc": 1667864229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Working on archiving some YouTube channels I enjoy. What do you typically archive at? 1080p? Full 4K if it\u2019s available? Does it depend on the channel?", "author_fullname": "t2_1h4qk0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What quality do you guys archive YouTube videos at?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yovlvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667845515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on archiving some YouTube channels I enjoy. What do you typically archive at? 1080p? Full 4K if it\u2019s available? Does it depend on the channel?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yovlvo", "is_robot_indexable": true, "report_reasons": null, "author": "awkw4rdkid", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yovlvo/what_quality_do_you_guys_archive_youtube_videos_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yovlvo/what_quality_do_you_guys_archive_youtube_videos_at/", "subreddit_subscribers": 652306, "created_utc": 1667845515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have sufficient \"restore things to the way they were before the disaster\" back-ups, but I'm I've started my own business and I'm looking to getting back to my \"Sure, I have the version of that file from five years ago.\" CYA archiving. Through the years I've migrated through the \"stack of floppies\", \"stack of 1/4\" tapes\", \"stack of CDs\" and \"stack of DVDs\". Is there anything that should stay current for the next decade? What (w/ proper storage) should last until it's time to migrate to the next \"Latest &amp; Greatest\"? \n\nIt kinda seems that there is no longevity/cost-to-start/cost-to-maintain sweet-spot. M-Discs seem good longevity-wise, but they're expensive/TB and their individual capacity is a bit on the low side. Duplicate HDs are good on the cost/TB side, but how long would they last sitting in a firesafe once filled? LTO is great on the capacity and cost/TB, but start-up costs are very high and living through the floppy &amp; 1/4\" tape era, I'm leery about the longevity of tape in storage.  \n\nHow reliable are refurbished LTO drives? They make the start-up costs more manageable, but how likely is a drive to clobber a tape and make it unreadable?", "author_fullname": "t2_mv7lo41", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Future Proofing\" Long Term Archives (Old School Datahorder needing to get back into it)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_you704", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667842638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have sufficient &amp;quot;restore things to the way they were before the disaster&amp;quot; back-ups, but I&amp;#39;m I&amp;#39;ve started my own business and I&amp;#39;m looking to getting back to my &amp;quot;Sure, I have the version of that file from five years ago.&amp;quot; CYA archiving. Through the years I&amp;#39;ve migrated through the &amp;quot;stack of floppies&amp;quot;, &amp;quot;stack of 1/4&amp;quot; tapes&amp;quot;, &amp;quot;stack of CDs&amp;quot; and &amp;quot;stack of DVDs&amp;quot;. Is there anything that should stay current for the next decade? What (w/ proper storage) should last until it&amp;#39;s time to migrate to the next &amp;quot;Latest &amp;amp; Greatest&amp;quot;? &lt;/p&gt;\n\n&lt;p&gt;It kinda seems that there is no longevity/cost-to-start/cost-to-maintain sweet-spot. M-Discs seem good longevity-wise, but they&amp;#39;re expensive/TB and their individual capacity is a bit on the low side. Duplicate HDs are good on the cost/TB side, but how long would they last sitting in a firesafe once filled? LTO is great on the capacity and cost/TB, but start-up costs are very high and living through the floppy &amp;amp; 1/4&amp;quot; tape era, I&amp;#39;m leery about the longevity of tape in storage.  &lt;/p&gt;\n\n&lt;p&gt;How reliable are refurbished LTO drives? They make the start-up costs more manageable, but how likely is a drive to clobber a tape and make it unreadable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "you704", "is_robot_indexable": true, "report_reasons": null, "author": "MeButNotMeToo", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/you704/future_proofing_long_term_archives_old_school/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/you704/future_proofing_long_term_archives_old_school/", "subreddit_subscribers": 652306, "created_utc": 1667842638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is a \"problem\" I've had for many years, on multiple systems, distros and so on that I can't really figure out and maybe there's just no solution to it.\n\nThe problem is that when I write to a disk (a 8TB WD Red in this case) it completely tanks the read speeds of it. Trying to read a file from the disk during the write operation is almost impossible, for example trying to play a video file over samba will take 10 seconds to even start. The disk is happily writing at 150-170MB/s while at most I can get around 1MB/s trying to read something, almost as if the write operation gets complete priority. As soon as the write is done I'm back to full read speeds which is about 170MB/s.\n\nOne consequences of this is that if someone is watching Plex and the particular file they are streaming resides on \"disk1\" and I need to copy some files to that disk at the same time, 'cp /mnt/disk2/file /mnt/disk1/file', the Plex stream is very likely to start buffering.\n\nTo me it seems like a I/O scheduler issue but I've tried all of them with no success. Currently I'm using BFQ and not even ionice -c 3 is helping. It's also possible that this is just the nature of hard disks and there's no way to get around that, I don't know.\n\nSome info about my system:\n\n* OS: Arch Linux\n* Kernel: 6.0.7-arch1-1\n* Asus Prime H510M-K\n* Intel i3-10100\n* 16GB of RAM (usually my usage is around 3-4GB)\n* Disks directly connected to the motherboard\n\nI experienced the same issue back when I was running WD Red 3TB disks on a completely different system and different distro (Debian) so I highly doubt it's an hardware issue.\n\nI'm running LUKS on the disks with BTRFS on top and then pooled with MergerFS. During my troubleshooting I'm copying files directly from one disk to another, bypassing MergerFS so that should have nothing to do with it.\n\nI have also played around with 'vm.dirty\\_ratio' and 'vm.dirty\\_background\\_ratio' but it makes no difference.", "author_fullname": "t2_s484g8uy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Writing to disk slows read speeds down to almost zero", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_you62o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667842578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a &amp;quot;problem&amp;quot; I&amp;#39;ve had for many years, on multiple systems, distros and so on that I can&amp;#39;t really figure out and maybe there&amp;#39;s just no solution to it.&lt;/p&gt;\n\n&lt;p&gt;The problem is that when I write to a disk (a 8TB WD Red in this case) it completely tanks the read speeds of it. Trying to read a file from the disk during the write operation is almost impossible, for example trying to play a video file over samba will take 10 seconds to even start. The disk is happily writing at 150-170MB/s while at most I can get around 1MB/s trying to read something, almost as if the write operation gets complete priority. As soon as the write is done I&amp;#39;m back to full read speeds which is about 170MB/s.&lt;/p&gt;\n\n&lt;p&gt;One consequences of this is that if someone is watching Plex and the particular file they are streaming resides on &amp;quot;disk1&amp;quot; and I need to copy some files to that disk at the same time, &amp;#39;cp /mnt/disk2/file /mnt/disk1/file&amp;#39;, the Plex stream is very likely to start buffering.&lt;/p&gt;\n\n&lt;p&gt;To me it seems like a I/O scheduler issue but I&amp;#39;ve tried all of them with no success. Currently I&amp;#39;m using BFQ and not even ionice -c 3 is helping. It&amp;#39;s also possible that this is just the nature of hard disks and there&amp;#39;s no way to get around that, I don&amp;#39;t know.&lt;/p&gt;\n\n&lt;p&gt;Some info about my system:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OS: Arch Linux&lt;/li&gt;\n&lt;li&gt;Kernel: 6.0.7-arch1-1&lt;/li&gt;\n&lt;li&gt;Asus Prime H510M-K&lt;/li&gt;\n&lt;li&gt;Intel i3-10100&lt;/li&gt;\n&lt;li&gt;16GB of RAM (usually my usage is around 3-4GB)&lt;/li&gt;\n&lt;li&gt;Disks directly connected to the motherboard&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I experienced the same issue back when I was running WD Red 3TB disks on a completely different system and different distro (Debian) so I highly doubt it&amp;#39;s an hardware issue.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running LUKS on the disks with BTRFS on top and then pooled with MergerFS. During my troubleshooting I&amp;#39;m copying files directly from one disk to another, bypassing MergerFS so that should have nothing to do with it.&lt;/p&gt;\n\n&lt;p&gt;I have also played around with &amp;#39;vm.dirty_ratio&amp;#39; and &amp;#39;vm.dirty_background_ratio&amp;#39; but it makes no difference.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "you62o", "is_robot_indexable": true, "report_reasons": null, "author": "bepsifif", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/you62o/writing_to_disk_slows_read_speeds_down_to_almost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/you62o/writing_to_disk_slows_read_speeds_down_to_almost/", "subreddit_subscribers": 652306, "created_utc": 1667842578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello r/DataHoarder\n\nQuite often have seen in comments that people suggest to move to HBA expansion cards for users who run bunch of HDDs, as this is more safe and provides more slots for those who run 8 or even more drives. By HBA, mean SAS HBA devices.\n\nMy question would be - **Why such devices are preferred in comparison to plugging many HDDs to SATA ports?** \n\nAt the moment I have 6 drives connected, these already populate entire motherboard's available SATA ports. Also have an PCI Express SATA Controller Expansion Card (1 x PCIe X1 to 4 SATA), but currently there is no need in more slots. While on other hand probably I would like to expand drive pool later. Heard such cards are not recommended, because often cheaply built and can affect speeds as all drives will share single interface.", "author_fullname": "t2_cg19jzl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HBA or not HBA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yotlik", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667841415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/DataHoarder\"&gt;r/DataHoarder&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Quite often have seen in comments that people suggest to move to HBA expansion cards for users who run bunch of HDDs, as this is more safe and provides more slots for those who run 8 or even more drives. By HBA, mean SAS HBA devices.&lt;/p&gt;\n\n&lt;p&gt;My question would be - &lt;strong&gt;Why such devices are preferred in comparison to plugging many HDDs to SATA ports?&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;At the moment I have 6 drives connected, these already populate entire motherboard&amp;#39;s available SATA ports. Also have an PCI Express SATA Controller Expansion Card (1 x PCIe X1 to 4 SATA), but currently there is no need in more slots. While on other hand probably I would like to expand drive pool later. Heard such cards are not recommended, because often cheaply built and can affect speeds as all drives will share single interface.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "4-4-4-12-12-12TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yotlik", "is_robot_indexable": true, "report_reasons": null, "author": "q1525882", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yotlik/hba_or_not_hba/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yotlik/hba_or_not_hba/", "subreddit_subscribers": 652306, "created_utc": 1667841415.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've the necessity to organize my comics in folders. I need some tool that automatically creates a folder from the filename. The files have names as \"name of the series\" \"number of the issue\". I am able using an utility named \"robobasket\" to create something like that but works only with the first word of the name. So if I have some files that start with \"the\" for istance I have the folder \"the\" but the files are not related to each other. Do you have any suggestion? thanks in advance", "author_fullname": "t2_yf9aa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizing comics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yosu62", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667839918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve the necessity to organize my comics in folders. I need some tool that automatically creates a folder from the filename. The files have names as &amp;quot;name of the series&amp;quot; &amp;quot;number of the issue&amp;quot;. I am able using an utility named &amp;quot;robobasket&amp;quot; to create something like that but works only with the first word of the name. So if I have some files that start with &amp;quot;the&amp;quot; for istance I have the folder &amp;quot;the&amp;quot; but the files are not related to each other. Do you have any suggestion? thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yosu62", "is_robot_indexable": true, "report_reasons": null, "author": "alexross80", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yosu62/organizing_comics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yosu62/organizing_comics/", "subreddit_subscribers": 652306, "created_utc": 1667839918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey All,\n\nHas anyone used this case for a home build?\n\nhttps://www.alibaba.com/product-detail/Manufacture-IPFS-6-hot-swap-bays_1600079165228.html\n\nI currently have a 4 bay case that works ok, but I want to be able to expand to 6 disks and this one looks great for the price.", "author_fullname": "t2_33q0p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "6 Bay Server Case from Alibaba", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yospkl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667839670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All,&lt;/p&gt;\n\n&lt;p&gt;Has anyone used this case for a home build?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.alibaba.com/product-detail/Manufacture-IPFS-6-hot-swap-bays_1600079165228.html\"&gt;https://www.alibaba.com/product-detail/Manufacture-IPFS-6-hot-swap-bays_1600079165228.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I currently have a 4 bay case that works ok, but I want to be able to expand to 6 disks and this one looks great for the price.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yospkl", "is_robot_indexable": true, "report_reasons": null, "author": "y2kdread", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yospkl/6_bay_server_case_from_alibaba/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yospkl/6_bay_server_case_from_alibaba/", "subreddit_subscribers": 652306, "created_utc": 1667839670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Easystore 14 TB (Example) - 5400 RPM, 2 year warranty $199.99 on sale...\n\nWestern Digital 14TB WD Red Pro NAS - 7200 RPM, 5 year warranty $260.00 on sale...\n\nYou guys really think the better speeds and 3 years extra warranty isn't worth it?", "author_fullname": "t2_iver8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "easystore drives compared to PRO red drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yodojy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667799532.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667798613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Easystore 14 TB (Example) - 5400 RPM, 2 year warranty $199.99 on sale...&lt;/p&gt;\n\n&lt;p&gt;Western Digital 14TB WD Red Pro NAS - 7200 RPM, 5 year warranty $260.00 on sale...&lt;/p&gt;\n\n&lt;p&gt;You guys really think the better speeds and 3 years extra warranty isn&amp;#39;t worth it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yodojy", "is_robot_indexable": true, "report_reasons": null, "author": "SickestGuy", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yodojy/easystore_drives_compared_to_pro_red_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yodojy/easystore_drives_compared_to_pro_red_drives/", "subreddit_subscribers": 652306, "created_utc": 1667798613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not so recently google took down a link to an extremely detailed beautifully formatted spreadsheet of all the AM4 motherboards and for a while I thought it was gone forever. I recently however found a mirror link to a copy of it but it's still on Google Sheets and also does not have any of the export options enabled.\n\nI was wondering if there's still any way to download this file given copying, downloading, and printing have all been disabled (I tried copying it to a new sheet and then downloading that but they've fixed that trick). I don't trust google to keep the mirror up forever and even then the owner could still delete it.\n\nHere is a link to the spreadsheet for those that want to look at it.  \n[https://docs.google.com/spreadsheets/d/1-cw7A2MDHPvA-oB3OKXivdUo9BbTcsss1Rzy3J4hRyA/edit#gid=2112472504](https://docs.google.com/spreadsheets/d/1-cw7A2MDHPvA-oB3OKXivdUo9BbTcsss1Rzy3J4hRyA/edit#gid=2112472504)", "author_fullname": "t2_3d6imjg5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating Copy From Google Sheets When Export Is Disabled", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yoa7tz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667788081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not so recently google took down a link to an extremely detailed beautifully formatted spreadsheet of all the AM4 motherboards and for a while I thought it was gone forever. I recently however found a mirror link to a copy of it but it&amp;#39;s still on Google Sheets and also does not have any of the export options enabled.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if there&amp;#39;s still any way to download this file given copying, downloading, and printing have all been disabled (I tried copying it to a new sheet and then downloading that but they&amp;#39;ve fixed that trick). I don&amp;#39;t trust google to keep the mirror up forever and even then the owner could still delete it.&lt;/p&gt;\n\n&lt;p&gt;Here is a link to the spreadsheet for those that want to look at it.&lt;br/&gt;\n&lt;a href=\"https://docs.google.com/spreadsheets/d/1-cw7A2MDHPvA-oB3OKXivdUo9BbTcsss1Rzy3J4hRyA/edit#gid=2112472504\"&gt;https://docs.google.com/spreadsheets/d/1-cw7A2MDHPvA-oB3OKXivdUo9BbTcsss1Rzy3J4hRyA/edit#gid=2112472504&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?auto=webp&amp;s=74ee7b7f273686d3a6d63132dead8fd1764cf746", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c19110c068d76ab200f1245f86280073ea0f58a6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44c18606b62aece853ea487bd43c828325a0bdce", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=209ec2a10a56fe45fe6cb9d261c1ab221b6e9ac6", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d4eeeb63a94a849ea3cf810b90f2da22c354be9", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=be1258b942082c46071af527419ef9e0a648cf73", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/qHyy1too4GTFMAhK63JcEIIrCppE_DmhJu66r2toXOQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0a5b3638d48be6c610b283e28d05c05ed93c981", "width": 1080, "height": 567}], "variants": {}, "id": "sJTk-8u5yRf8kptjtc2UKTd4Hk25h5aqolxUj_u1Y2k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yoa7tz", "is_robot_indexable": true, "report_reasons": null, "author": "Awkward_Elf", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yoa7tz/creating_copy_from_google_sheets_when_export_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yoa7tz/creating_copy_from_google_sheets_when_export_is/", "subreddit_subscribers": 652306, "created_utc": 1667788081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I posted a thread over on r/zfs, but I didn't really get any feedback, I've got a TreuNAS Mini XL+ arriving later this week and I'm debating plans to populate it.\n\nI've currently got 4x 8TB WD Reds in an ubuntu box with a raidz1 array, My original plan was to lift and shift the array into the new NAS, but I'm leaning more towards getting new drives and copying my data over, then I can migrate the drives and see if moving the pool would have worked.\n\nI can get 16TB IronWolf pros from my local MicroCenter for $280, and it looks like RaidZ expansion has yet to land, so I'm thinking I'd be best getting 4 drives up front and doing a raidz1. I guess I'm looking for a sanity check, if that's a good way forward for storing all my linux isos :D", "author_fullname": "t2_3i04t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expanding into a new NAS, what are the best drives to consider?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yoyhwi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.46, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667851282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I posted a thread over on &lt;a href=\"/r/zfs\"&gt;r/zfs&lt;/a&gt;, but I didn&amp;#39;t really get any feedback, I&amp;#39;ve got a TreuNAS Mini XL+ arriving later this week and I&amp;#39;m debating plans to populate it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve currently got 4x 8TB WD Reds in an ubuntu box with a raidz1 array, My original plan was to lift and shift the array into the new NAS, but I&amp;#39;m leaning more towards getting new drives and copying my data over, then I can migrate the drives and see if moving the pool would have worked.&lt;/p&gt;\n\n&lt;p&gt;I can get 16TB IronWolf pros from my local MicroCenter for $280, and it looks like RaidZ expansion has yet to land, so I&amp;#39;m thinking I&amp;#39;d be best getting 4 drives up front and doing a raidz1. I guess I&amp;#39;m looking for a sanity check, if that&amp;#39;s a good way forward for storing all my linux isos :D&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "21TB (32TB Raw)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yoyhwi", "is_robot_indexable": true, "report_reasons": null, "author": "tesfox", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yoyhwi/expanding_into_a_new_nas_what_are_the_best_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yoyhwi/expanding_into_a_new_nas_what_are_the_best_drives/", "subreddit_subscribers": 652306, "created_utc": 1667851282.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to buy a 2TB WD Blue but there are 2 versions\n\nWD20EZAZ (SMR 7200RPM)\n\nWD20EZRZ (CMR 5400RPM)\n\n&amp;#x200B;\n\nis SMR fine if i want to install repack games from fitgirl, dodi etc and install games from steam or should i get the slower rpm but CMR drive?", "author_fullname": "t2_ocggg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "are SMR drives fine for installing games through steam and repacking?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yox4jy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667848534.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to buy a 2TB WD Blue but there are 2 versions&lt;/p&gt;\n\n&lt;p&gt;WD20EZAZ (SMR 7200RPM)&lt;/p&gt;\n\n&lt;p&gt;WD20EZRZ (CMR 5400RPM)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;is SMR fine if i want to install repack games from fitgirl, dodi etc and install games from steam or should i get the slower rpm but CMR drive?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yox4jy", "is_robot_indexable": true, "report_reasons": null, "author": "CraftPlayerI2", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yox4jy/are_smr_drives_fine_for_installing_games_through/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yox4jy/are_smr_drives_fine_for_installing_games_through/", "subreddit_subscribers": 652306, "created_utc": 1667848534.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Say I have a 4 tb NAS drive that I want to make an offline backup of. I have four 1TB drives and only one dock so only one drive can be attached to the system at a time. Is there an elegant way to start copying files to the 4tb drive to the first 1tb drive until it runs out of space, then wait till I swap it with an empty drive, and then keep copying, keeping track of all the drives needed to do this in order? I don't want the drives in RAID, because that would require plugging all of them in at once, and I also don't want one drive failing taking the data stored on all the other drives with it.\n\nIsn't this how tape drives work for storing huge drive arrays? It should be pretty standard archival practice right?\n\nIdeally, I don't want to *have* to fully reassemble the data to find something. I want to be able to plug in a drive, search around for what it contains out of the larger drive, and copy individual files back without doing a full restore. Though if an individual file is larger than any one drive, I'd like it to be split between the drives and then able to be reassembled. Also, if this can be done at the individual file level and written into any filesystem I want, that'd be even better since then I can take advantage of having the archival drives LUKS encrypted and also using filesystem level compression. \n\nIs there a solution that can do all this? I use Linux if that makes a difference on works and what doesn't.", "author_fullname": "t2_1h1hfeve", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there an elegant way of splitting a huge file or folder to be archived over many drives that each alone would not be large enough to hold everything, where I can swap the drives sequentially as they fill up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yowobx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.46, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667847622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say I have a 4 tb NAS drive that I want to make an offline backup of. I have four 1TB drives and only one dock so only one drive can be attached to the system at a time. Is there an elegant way to start copying files to the 4tb drive to the first 1tb drive until it runs out of space, then wait till I swap it with an empty drive, and then keep copying, keeping track of all the drives needed to do this in order? I don&amp;#39;t want the drives in RAID, because that would require plugging all of them in at once, and I also don&amp;#39;t want one drive failing taking the data stored on all the other drives with it.&lt;/p&gt;\n\n&lt;p&gt;Isn&amp;#39;t this how tape drives work for storing huge drive arrays? It should be pretty standard archival practice right?&lt;/p&gt;\n\n&lt;p&gt;Ideally, I don&amp;#39;t want to &lt;em&gt;have&lt;/em&gt; to fully reassemble the data to find something. I want to be able to plug in a drive, search around for what it contains out of the larger drive, and copy individual files back without doing a full restore. Though if an individual file is larger than any one drive, I&amp;#39;d like it to be split between the drives and then able to be reassembled. Also, if this can be done at the individual file level and written into any filesystem I want, that&amp;#39;d be even better since then I can take advantage of having the archival drives LUKS encrypted and also using filesystem level compression. &lt;/p&gt;\n\n&lt;p&gt;Is there a solution that can do all this? I use Linux if that makes a difference on works and what doesn&amp;#39;t.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yowobx", "is_robot_indexable": true, "report_reasons": null, "author": "AgreeableLandscape3", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yowobx/is_there_an_elegant_way_of_splitting_a_huge_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yowobx/is_there_an_elegant_way_of_splitting_a_huge_file/", "subreddit_subscribers": 652306, "created_utc": 1667847622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I own a dedicated server on Hetzner.\n\nJust purchased one of their Storage Boxes.\n\nNow I want to clone the server's disk onto the storage box (can be accessed via a lot of ways, mainly SSH, but also borg, etc...)\n\nWould be amazing to have a similar operation to what VMs can do (hit backup &amp; restore and you are good to go)\n\nthis is the result of running `df -h`\n\n    Filesystem            Size  Used Avail Use% Mounted on\n    udev                   63G     0   63G   0% /dev\n    tmpfs                  13G  1.9M   13G   1% /run\n    /dev/mapper/vg0-root  7.0T  1.2T  5.4T  18% /\n    tmpfs                  63G     0   63G   0% /dev/shm\n    tmpfs                 5.0M     0  5.0M   0% /run/lock\n    tmpfs                  63G     0   63G   0% /sys/fs/cgroup\n    /dev/mapper/vg0-tmp    20G   60M   20G   1% /tmp\n    /dev/md0              485M  228M  232M  50% /boot\n    tmpfs                  13G   20K   13G   1% /run/user/124\n    tmpfs                  13G     0   13G   0% /run/user/0\n    tmpfs                  13G   12K   13G   1% /run/user/1000\n    overlay               7.0T  1.2T  5.4T  18% /var/lib/docker/overlay2/1ba8b1b49fca8915168f1566f1be685050129715e77b080b69b88124b6d90d6c/merged\n\nThis is a **RAID0** setup\n\nWhat I want is an image of the disk I can restore in case something bad happens. **But I can't unmount the partition** while backing it up since there are critical applications running on it.\n\nAny help would be appreciated!", "author_fullname": "t2_30uknr73", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to clone a server disk while it's running to external storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yowe2j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667847066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I own a dedicated server on Hetzner.&lt;/p&gt;\n\n&lt;p&gt;Just purchased one of their Storage Boxes.&lt;/p&gt;\n\n&lt;p&gt;Now I want to clone the server&amp;#39;s disk onto the storage box (can be accessed via a lot of ways, mainly SSH, but also borg, etc...)&lt;/p&gt;\n\n&lt;p&gt;Would be amazing to have a similar operation to what VMs can do (hit backup &amp;amp; restore and you are good to go)&lt;/p&gt;\n\n&lt;p&gt;this is the result of running &lt;code&gt;df -h&lt;/code&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Filesystem            Size  Used Avail Use% Mounted on\nudev                   63G     0   63G   0% /dev\ntmpfs                  13G  1.9M   13G   1% /run\n/dev/mapper/vg0-root  7.0T  1.2T  5.4T  18% /\ntmpfs                  63G     0   63G   0% /dev/shm\ntmpfs                 5.0M     0  5.0M   0% /run/lock\ntmpfs                  63G     0   63G   0% /sys/fs/cgroup\n/dev/mapper/vg0-tmp    20G   60M   20G   1% /tmp\n/dev/md0              485M  228M  232M  50% /boot\ntmpfs                  13G   20K   13G   1% /run/user/124\ntmpfs                  13G     0   13G   0% /run/user/0\ntmpfs                  13G   12K   13G   1% /run/user/1000\noverlay               7.0T  1.2T  5.4T  18% /var/lib/docker/overlay2/1ba8b1b49fca8915168f1566f1be685050129715e77b080b69b88124b6d90d6c/merged\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This is a &lt;strong&gt;RAID0&lt;/strong&gt; setup&lt;/p&gt;\n\n&lt;p&gt;What I want is an image of the disk I can restore in case something bad happens. &lt;strong&gt;But I can&amp;#39;t unmount the partition&lt;/strong&gt; while backing it up since there are critical applications running on it.&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yowe2j", "is_robot_indexable": true, "report_reasons": null, "author": "leonardofiori", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yowe2j/best_way_to_clone_a_server_disk_while_its_running/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yowe2j/best_way_to_clone_a_server_disk_while_its_running/", "subreddit_subscribers": 652306, "created_utc": 1667847066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, is it worth to use a WD Elements Portable 4TB always connected to a server 24/7 in order to move data often, for example for a seedbox/torrent box? I'm not sure if these portable disk are good enough to resist such a use, I don't want to trash it after 1 year.", "author_fullname": "t2_41b4i981", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is it safe to use a WD Elements Portable as 24/7 server disk ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yp03ax", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667854579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, is it worth to use a WD Elements Portable 4TB always connected to a server 24/7 in order to move data often, for example for a seedbox/torrent box? I&amp;#39;m not sure if these portable disk are good enough to resist such a use, I don&amp;#39;t want to trash it after 1 year.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yp03ax", "is_robot_indexable": true, "report_reasons": null, "author": "P0lpett0n3", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yp03ax/is_it_safe_to_use_a_wd_elements_portable_as_247/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yp03ax/is_it_safe_to_use_a_wd_elements_portable_as_247/", "subreddit_subscribers": 652306, "created_utc": 1667854579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't want to download the files one by one", "author_fullname": "t2_nnj9ml6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is there any way to download folders from mediafire for free?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_youyzn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667844283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t want to download the files one by one&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "youyzn", "is_robot_indexable": true, "report_reasons": null, "author": "SaiaExitt", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/youyzn/is_there_any_way_to_download_folders_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/youyzn/is_there_any_way_to_download_folders_from/", "subreddit_subscribers": 652306, "created_utc": 1667844283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there,\n\nTrying to export a lot of data from google drive and it's split into 80 50GB zip files. Is there a way to mass download all of them at once using a download manager since I tried using motirx but it seems to not be able to download them as google seems to auth each one. There is no option to download all the files at once and due to having to click download 80 times and wait it's going to take a while so not sure if there is a way to speed it up a bit.\n\nThanks!", "author_fullname": "t2_ksns1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Takeout mass download", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yoh4yi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667810955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;Trying to export a lot of data from google drive and it&amp;#39;s split into 80 50GB zip files. Is there a way to mass download all of them at once using a download manager since I tried using motirx but it seems to not be able to download them as google seems to auth each one. There is no option to download all the files at once and due to having to click download 80 times and wait it&amp;#39;s going to take a while so not sure if there is a way to speed it up a bit.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yoh4yi", "is_robot_indexable": true, "report_reasons": null, "author": "BV1717", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yoh4yi/google_takeout_mass_download/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yoh4yi/google_takeout_mass_download/", "subreddit_subscribers": 652306, "created_utc": 1667810955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a bit fed up with Google's storage issues (my photos go to my Gmail account that shares 15gb with email) and I recently realized that Amazon claims to offer unlimited photo storage.\n\nHas anybody taken advantage of this? Are there any scripts or libraries i can use to upload to it from my workstation? Not all my pictures are from a phone but i do sync everything to my home server so that's the ideal place to sync from.\n\nAlso, do the \"family\" accounts get the same? I would like to add wife and mom as well if possible.\n\nI'm only considering this as an a quick and easy way for us to have a Google photos like experience with minimal effort on my part (vs self hosting something)", "author_fullname": "t2_1ijfzrf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon photos anyone?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yoddzl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667797670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a bit fed up with Google&amp;#39;s storage issues (my photos go to my Gmail account that shares 15gb with email) and I recently realized that Amazon claims to offer unlimited photo storage.&lt;/p&gt;\n\n&lt;p&gt;Has anybody taken advantage of this? Are there any scripts or libraries i can use to upload to it from my workstation? Not all my pictures are from a phone but i do sync everything to my home server so that&amp;#39;s the ideal place to sync from.&lt;/p&gt;\n\n&lt;p&gt;Also, do the &amp;quot;family&amp;quot; accounts get the same? I would like to add wife and mom as well if possible.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m only considering this as an a quick and easy way for us to have a Google photos like experience with minimal effort on my part (vs self hosting something)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yoddzl", "is_robot_indexable": true, "report_reasons": null, "author": "mdeanda", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yoddzl/amazon_photos_anyone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yoddzl/amazon_photos_anyone/", "subreddit_subscribers": 652306, "created_utc": 1667797670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**Context:**  \nHi, I do YouTube video editing and I need a proper storage solution. To make a video, I usually need footage from many different people in order to edit it on my computer, so in the past that has meant using my sole 14tb drive to do so, and having people who participate in the videos upload their footage through mediafire and whatnot. But to be honest I don't really have a backup, and my drive is currently full. Also mediafire is really slow, and can be really hard for larger files. So right now, I'm thinking of setting up a 3-Drive Raid 5 setup, (with the same 14tb drive I have right now) so I can have more footage. But I also want people to be able to directly upload their files onto my own home PC and cut the slow file uploading middleman.\n\n**What I need help with:**  \nI want to be able to get anyone to upload a video file directly onto a server that I host at home so I can edit it easier and faster. But at the same time, I don't want to worry about my IP Getting leaked (I can't entirely trust the people that upload the footage. Of course I won't open any files they upload that aren't explicitly media files) so I need something that is actually secure. Does anyone know how I can do this?p.s I'm not tech savvy. If you can, try to link me into a good resource that I can follow.\n\nThanks in advance.", "author_fullname": "t2_6he1vnsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would I set up a NAS Drive on a server that I can share to other people can upload without risking my ip?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yocn5s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.36, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667795308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt;&lt;br/&gt;\nHi, I do YouTube video editing and I need a proper storage solution. To make a video, I usually need footage from many different people in order to edit it on my computer, so in the past that has meant using my sole 14tb drive to do so, and having people who participate in the videos upload their footage through mediafire and whatnot. But to be honest I don&amp;#39;t really have a backup, and my drive is currently full. Also mediafire is really slow, and can be really hard for larger files. So right now, I&amp;#39;m thinking of setting up a 3-Drive Raid 5 setup, (with the same 14tb drive I have right now) so I can have more footage. But I also want people to be able to directly upload their files onto my own home PC and cut the slow file uploading middleman.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I need help with:&lt;/strong&gt;&lt;br/&gt;\nI want to be able to get anyone to upload a video file directly onto a server that I host at home so I can edit it easier and faster. But at the same time, I don&amp;#39;t want to worry about my IP Getting leaked (I can&amp;#39;t entirely trust the people that upload the footage. Of course I won&amp;#39;t open any files they upload that aren&amp;#39;t explicitly media files) so I need something that is actually secure. Does anyone know how I can do this?p.s I&amp;#39;m not tech savvy. If you can, try to link me into a good resource that I can follow.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yocn5s", "is_robot_indexable": true, "report_reasons": null, "author": "Hey_2b2t", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yocn5s/how_would_i_set_up_a_nas_drive_on_a_server_that_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yocn5s/how_would_i_set_up_a_nas_drive_on_a_server_that_i/", "subreddit_subscribers": 652306, "created_utc": 1667795308.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}