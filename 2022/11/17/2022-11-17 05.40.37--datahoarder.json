{"kind": "Listing", "data": {"after": "t3_yx5zl3", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_97qw4856", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\u20ac95/5TB Does it get any better for Europe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "name": "t3_ywqnac", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 415, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 415, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/SRy9YHX9uTCWVgALND56o7pcLuOEy-7tP0nCO2KED0Q.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668597069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1opx7t45qa0a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1opx7t45qa0a1.png?auto=webp&amp;s=36dc02229276e2912fe152709994fde3884127cd", "width": 1402, "height": 778}, "resolutions": [{"url": "https://preview.redd.it/1opx7t45qa0a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0742b145429d0c6e5fcedd4ad2d532050c61fbe9", "width": 108, "height": 59}, {"url": "https://preview.redd.it/1opx7t45qa0a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc5b6a6ad674993dad96ecd4f1f777a3245c6e32", "width": 216, "height": 119}, {"url": "https://preview.redd.it/1opx7t45qa0a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0862b4d16b3eb44017635cfcd704f43877f483e7", "width": 320, "height": 177}, {"url": "https://preview.redd.it/1opx7t45qa0a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=577446c10bd687e3bd9c3450884f81ce1c5ad51f", "width": 640, "height": 355}, {"url": "https://preview.redd.it/1opx7t45qa0a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dee6246a3894865346f1130c3c7afbd02027ea36", "width": 960, "height": 532}, {"url": "https://preview.redd.it/1opx7t45qa0a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7588f541211787846a55e425ac178d24cddfaafe", "width": 1080, "height": 599}], "variants": {}, "id": "jD438II8W_HxHdVc24kwpw5TKmru-JM8RvgSPQZovfo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ywqnac", "is_robot_indexable": true, "report_reasons": null, "author": "Sypermarket3", "discussion_type": null, "num_comments": 104, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywqnac/955tb_does_it_get_any_better_for_europe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1opx7t45qa0a1.png", "subreddit_subscribers": 654314, "created_utc": 1668597069.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all!\n\nSo I'm currently digitising all my grandma's photos from pictures from the 1950s and 1960s right through to the 2000s and 2010s. I was just wondering how you'd organise them all? (1000+). At the moment I've got a folder for each person, then within that I've got \"Group\" folder \"Solo\" folder and \"Documents\" folder. But when multiple people are in the group photos, having 10/15 copies of one photo seems overkill. There must be a better way to approach this? \n\nWould love to see any examples whether it's screenshots or links to a folder set up. Just any help would be much appreciated! Also naming conventions too. Initially I had \"Surname, First Name - Event\" however, my grandma has 11 siblings so I can't list everyone within that. \n\nI'll be using Google Drive to share the photos.", "author_fullname": "t2_redre", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you organise 1000+ family photos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywyf3m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668616528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all!&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m currently digitising all my grandma&amp;#39;s photos from pictures from the 1950s and 1960s right through to the 2000s and 2010s. I was just wondering how you&amp;#39;d organise them all? (1000+). At the moment I&amp;#39;ve got a folder for each person, then within that I&amp;#39;ve got &amp;quot;Group&amp;quot; folder &amp;quot;Solo&amp;quot; folder and &amp;quot;Documents&amp;quot; folder. But when multiple people are in the group photos, having 10/15 copies of one photo seems overkill. There must be a better way to approach this? &lt;/p&gt;\n\n&lt;p&gt;Would love to see any examples whether it&amp;#39;s screenshots or links to a folder set up. Just any help would be much appreciated! Also naming conventions too. Initially I had &amp;quot;Surname, First Name - Event&amp;quot; however, my grandma has 11 siblings so I can&amp;#39;t list everyone within that. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll be using Google Drive to share the photos.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywyf3m", "is_robot_indexable": true, "report_reasons": null, "author": "theferrolgamer", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywyf3m/how_would_you_organise_1000_family_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywyf3m/how_would_you_organise_1000_family_photos/", "subreddit_subscribers": 654314, "created_utc": 1668616528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My wife and I just picked up shiny new Pixel 7/Pro's and plan to take a whole bunch of videos in 4k of our family, kids, etc.  However after seeing what 5 minutes of a 4k video eats up, I'm starting to worry about the limits of my 2tb plan.  I saw there were some options, including buying a pixel 1... or paying $30/month for google enterprise that has either a 5tb or potentially unlimited space.  \n\nTwo questions - is the latter still a thing, or is it limited to 5tb?  If it's limited, wouldn't it just make sense for me to pay for 5tb plan on google one for $20/year (annual)?  \n\nAnd second, is there a better option out there?  Or am I worrying for nothing and probably won't hit even my 2tb limit anytime soon?", "author_fullname": "t2_o08jv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best option to future proof data hoarding on Google Photos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywu7vw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668607348.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My wife and I just picked up shiny new Pixel 7/Pro&amp;#39;s and plan to take a whole bunch of videos in 4k of our family, kids, etc.  However after seeing what 5 minutes of a 4k video eats up, I&amp;#39;m starting to worry about the limits of my 2tb plan.  I saw there were some options, including buying a pixel 1... or paying $30/month for google enterprise that has either a 5tb or potentially unlimited space.  &lt;/p&gt;\n\n&lt;p&gt;Two questions - is the latter still a thing, or is it limited to 5tb?  If it&amp;#39;s limited, wouldn&amp;#39;t it just make sense for me to pay for 5tb plan on google one for $20/year (annual)?  &lt;/p&gt;\n\n&lt;p&gt;And second, is there a better option out there?  Or am I worrying for nothing and probably won&amp;#39;t hit even my 2tb limit anytime soon?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywu7vw", "is_robot_indexable": true, "report_reasons": null, "author": "reezick", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywu7vw/best_option_to_future_proof_data_hoarding_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywu7vw/best_option_to_future_proof_data_hoarding_on/", "subreddit_subscribers": 654314, "created_utc": 1668607348.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to download video from a streaming website.\n\nHere's an [example](https://www.americastestkitchen.com/cookscountry/episode/829-low-country-party) of the link.\n\nI have tried using Inspect Element &gt; Network &gt; Media, but I don't see any link.\n\nI have also tried various softwares, but it doesn't work. Anyone can help?", "author_fullname": "t2_uciunzpl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to Download video from a streaming website.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywwzkq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668613653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to download video from a streaming website.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an &lt;a href=\"https://www.americastestkitchen.com/cookscountry/episode/829-low-country-party\"&gt;example&lt;/a&gt; of the link.&lt;/p&gt;\n\n&lt;p&gt;I have tried using Inspect Element &amp;gt; Network &amp;gt; Media, but I don&amp;#39;t see any link.&lt;/p&gt;\n\n&lt;p&gt;I have also tried various softwares, but it doesn&amp;#39;t work. Anyone can help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywwzkq", "is_robot_indexable": true, "report_reasons": null, "author": "oneminutetimemachine", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywwzkq/trying_to_download_video_from_a_streaming_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywwzkq/trying_to_download_video_from_a_streaming_website/", "subreddit_subscribers": 654314, "created_utc": 1668613653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Chinese companies will literally manufacture anything. Amazon is full of new cassette players recently manufactured but no one makes an affordable solutions to play those old video files. \n\nJust curious why that is? I recently digitized a lot of old Hi8, mini DV, and vhs tapes and I had to buy a 25 year old Hi8 player because my parents old one didn't power on when I started the project. It was like 100 dollars for a piece of 25 year old tech, lol. It just seems like SOME company out there has to make one, and I'm not looking in the right places online.", "author_fullname": "t2_528j4y3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why doesn't any company make affordable Hi8/Mini DV video players?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywujrj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668608097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Chinese companies will literally manufacture anything. Amazon is full of new cassette players recently manufactured but no one makes an affordable solutions to play those old video files. &lt;/p&gt;\n\n&lt;p&gt;Just curious why that is? I recently digitized a lot of old Hi8, mini DV, and vhs tapes and I had to buy a 25 year old Hi8 player because my parents old one didn&amp;#39;t power on when I started the project. It was like 100 dollars for a piece of 25 year old tech, lol. It just seems like SOME company out there has to make one, and I&amp;#39;m not looking in the right places online.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "ywujrj", "is_robot_indexable": true, "report_reasons": null, "author": "vaper1122", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywujrj/why_doesnt_any_company_make_affordable_hi8mini_dv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywujrj/why_doesnt_any_company_make_affordable_hi8mini_dv/", "subreddit_subscribers": 654314, "created_utc": 1668608097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_95l1i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Picked this up for next to nothing. Could I make a PLEX server out of it?? (details in comments)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_yx8iqx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Z6TkknwAqlhNp4dHIVTm8yhOVgHcvpDMye63Pz1c7oc.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668638068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgur.com/a/Nv9n17H", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/F9eQ6FjPfeGQ4pcNDoLKkDMq7T7WVyBHz9lEgBjMwLM.jpg?auto=webp&amp;s=072ff5139529f25b4c79236e9a7c5493d24ac00c", "width": 4656, "height": 3496}, "resolutions": [{"url": "https://external-preview.redd.it/F9eQ6FjPfeGQ4pcNDoLKkDMq7T7WVyBHz9lEgBjMwLM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa848b301f6f14cfb9c4bcfa02324ea8a0e71eec", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/F9eQ6FjPfeGQ4pcNDoLKkDMq7T7WVyBHz9lEgBjMwLM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=02751e28fe4fe7bad49edf4c84d17ced838607bc", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/F9eQ6FjPfeGQ4pcNDoLKkDMq7T7WVyBHz9lEgBjMwLM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e637aecd629dead59300065b106c5857f42bc10c", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/F9eQ6FjPfeGQ4pcNDoLKkDMq7T7WVyBHz9lEgBjMwLM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=11cb0b6eac62ac959d33b99051026f9f095a6e31", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/F9eQ6FjPfeGQ4pcNDoLKkDMq7T7WVyBHz9lEgBjMwLM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=27513035f3b744de917cedbe9e7154b162896b4d", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/F9eQ6FjPfeGQ4pcNDoLKkDMq7T7WVyBHz9lEgBjMwLM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=64d9c64a8f93616b8e3b56fcbe91817d5ac7c213", "width": 1080, "height": 810}], "variants": {}, "id": "0jewuXscY0Cd64ldyeodbdkCqeLkmqMkaE5slNVuR2s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "6.5TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx8iqx", "is_robot_indexable": true, "report_reasons": null, "author": "noelgoo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yx8iqx/picked_this_up_for_next_to_nothing_could_i_make_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgur.com/a/Nv9n17H", "subreddit_subscribers": 654314, "created_utc": 1668638068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was using Tiny Media, but discovered that it is rentalware.  I don't mind buying a tool, but abhor subscriptions.  Anyone have a good suggestion?\n\nThe hoard is on a Synology serving Nvidia Shields and Kodi.  I was using Tiny to scrape the metadata and had Kodi pulling it locally instead of searching the net (faster and organized/renamed the files).  \n\nI'm open to other ways of doing this that do not involve Plex.", "author_fullname": "t2_2ztygbcz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a Video Metadata Scraper", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx5hm0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668631131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was using Tiny Media, but discovered that it is rentalware.  I don&amp;#39;t mind buying a tool, but abhor subscriptions.  Anyone have a good suggestion?&lt;/p&gt;\n\n&lt;p&gt;The hoard is on a Synology serving Nvidia Shields and Kodi.  I was using Tiny to scrape the metadata and had Kodi pulling it locally instead of searching the net (faster and organized/renamed the files).  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m open to other ways of doing this that do not involve Plex.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx5hm0", "is_robot_indexable": true, "report_reasons": null, "author": "Bushpylot", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx5hm0/looking_for_a_video_metadata_scraper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx5hm0/looking_for_a_video_metadata_scraper/", "subreddit_subscribers": 654314, "created_utc": 1668631131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Any recommendations for flash drives or thumb drives that should last forever? \n\nPrinting photo books for my kids, and I'd like to include a drive that holds all of the photos (beyond the ones printed).\n\nEx : Year One (drive holds all photos for the year)", "author_fullname": "t2_buinl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photo Archive / Flash Drive for Photo Books", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywyi8j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668616715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any recommendations for flash drives or thumb drives that should last forever? &lt;/p&gt;\n\n&lt;p&gt;Printing photo books for my kids, and I&amp;#39;d like to include a drive that holds all of the photos (beyond the ones printed).&lt;/p&gt;\n\n&lt;p&gt;Ex : Year One (drive holds all photos for the year)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywyi8j", "is_robot_indexable": true, "report_reasons": null, "author": "k0rtnie", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywyi8j/photo_archive_flash_drive_for_photo_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywyi8j/photo_archive_flash_drive_for_photo_books/", "subreddit_subscribers": 654314, "created_utc": 1668616715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As title, I am able to get either at $279.99 (with a coupon from WD). Which one would be better? From my research that the easystore may be HGST ultrastar HC550 inside, which maybe (?) is better than WD red? But it will be taking a chance there as there is no guarantee that's the exact drive. And warranty would be less, but I am not sure if the warranty is needed if the drive won't fail. Thoughts?\n\nEDIT: thanks everyone who provided suggestions. I will go for the bare drives", "author_fullname": "t2_yf3rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shucked Easystore 18TB vs WD Red Pro 18TB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywygfs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668646411.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668616607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As title, I am able to get either at $279.99 (with a coupon from WD). Which one would be better? From my research that the easystore may be HGST ultrastar HC550 inside, which maybe (?) is better than WD red? But it will be taking a chance there as there is no guarantee that&amp;#39;s the exact drive. And warranty would be less, but I am not sure if the warranty is needed if the drive won&amp;#39;t fail. Thoughts?&lt;/p&gt;\n\n&lt;p&gt;EDIT: thanks everyone who provided suggestions. I will go for the bare drives&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywygfs", "is_robot_indexable": true, "report_reasons": null, "author": "arthurroos", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywygfs/shucked_easystore_18tb_vs_wd_red_pro_18tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywygfs/shucked_easystore_18tb_vs_wd_red_pro_18tb/", "subreddit_subscribers": 654314, "created_utc": 1668616607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to figure out how I want to backup the data of my daily driver. It's about 1.5 TB worth of documents and family pictures/videos. I don't need to access the data daily. I'd only ever need it if I lost all my data and needed to restore/recover.\n\nI don't need or want to backup everything on my disk. I have specific folders I'd select and then would, ideally, want to exclude certain sub-folders like `.git` or `node_modules`.\n\nI've been researching this for days and ... I just need some outside perspective. Here are my thoughts:\n\n* BackBlaze Personal Backup is $7 a month for unlimited backups. But the backup client backups everything and you have to specify all the excludes. The UI is cumbersome as hell and I'll have to edit an XML file to get it to do what I want (like exclude any `.git` folder). I'd only have to edit the file once, probably, but it's still a PITA. \n* I could use something like Duplicati or Duplicacy with B2 or S3 Glacier. I don't have experience with either so I'm not sure.\n* S3 Glacier seems cheaper but I'm not sure if that's necessarily a good thing over B2.\n\nI'm pretty tech savvy but I want something as simple as possible here. Ideally I'd select the root level folders I want to back up, set some kind of encryption, and then have them auto back up, with versions.\n\nBeing able to remotely access files when I'm not at home is an added bonus but not  a must. If I really needed that I'd just install Nextcloud or something.", "author_fullname": "t2_5wpob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need help with my analysis paralysis: BackBlaze unlimited vs Duplicati or Duplicacy with B2 or S3 Glacier", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yxfwx3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668657651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to figure out how I want to backup the data of my daily driver. It&amp;#39;s about 1.5 TB worth of documents and family pictures/videos. I don&amp;#39;t need to access the data daily. I&amp;#39;d only ever need it if I lost all my data and needed to restore/recover.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t need or want to backup everything on my disk. I have specific folders I&amp;#39;d select and then would, ideally, want to exclude certain sub-folders like &lt;code&gt;.git&lt;/code&gt; or &lt;code&gt;node_modules&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been researching this for days and ... I just need some outside perspective. Here are my thoughts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;BackBlaze Personal Backup is $7 a month for unlimited backups. But the backup client backups everything and you have to specify all the excludes. The UI is cumbersome as hell and I&amp;#39;ll have to edit an XML file to get it to do what I want (like exclude any &lt;code&gt;.git&lt;/code&gt; folder). I&amp;#39;d only have to edit the file once, probably, but it&amp;#39;s still a PITA. &lt;/li&gt;\n&lt;li&gt;I could use something like Duplicati or Duplicacy with B2 or S3 Glacier. I don&amp;#39;t have experience with either so I&amp;#39;m not sure.&lt;/li&gt;\n&lt;li&gt;S3 Glacier seems cheaper but I&amp;#39;m not sure if that&amp;#39;s necessarily a good thing over B2.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m pretty tech savvy but I want something as simple as possible here. Ideally I&amp;#39;d select the root level folders I want to back up, set some kind of encryption, and then have them auto back up, with versions.&lt;/p&gt;\n\n&lt;p&gt;Being able to remotely access files when I&amp;#39;m not at home is an added bonus but not  a must. If I really needed that I&amp;#39;d just install Nextcloud or something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxfwx3", "is_robot_indexable": true, "report_reasons": null, "author": "imthenachoman", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxfwx3/i_need_help_with_my_analysis_paralysis_backblaze/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxfwx3/i_need_help_with_my_analysis_paralysis_backblaze/", "subreddit_subscribers": 654314, "created_utc": 1668657651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "# utzoo-mongo-db-converter\n\nI wrote some scripts for converting the UTZOO Usenet archive to a Mongo Database.\n\n## Background\n\nI recently found out about the [UTZOO Usenet Archive](https://archive.org/details/utzoo-wiseman-usenet-archive), a collection of something like two million [Usenet](https://en.wikipedia.org/wiki/Usenet) posts archived by [Henry Spencer](https://en.wikipedia.org/wiki/Henry_Spencer) from 1981 to 1991. I wanted to play around with the data, so I wrote some Node script to convert the files to a Mongo database. It's a little bit hacky, but it works.\n\n## Getting the data:\n\nThe data was available on the Internet Archive. You could download the data from the [UTZOO Usenet Archive](https://archive.org/details/utzoo-wiseman-usenet-archive) page. After legal demands were placed requesting the internet archive to remove the data, the data was removed and replaced with its checksum. Fortunately, some [friendly folks on Reddit](https://www.reddit.com/r/DataHoarder/comments/i2btuu/utzoo_archives_have_been_removed_from_archiveorg/) have preserved the data. You can download the UTZOO archive from a torrent linked in that post.\n\n## The files\n\nThe UTZOO archive has individual Usenet post saved individual files. This makes searching its contents pretty slow, especially running a search using in Windows. [GREP](https://man7.org/linux/man-pages/man1/grep.1.html) was faster, but still doesn't take advantage of search indexing. Seeking through the contents of files this way is always dead slow. I thought about a few ways of being able to quickly search the data. [Maybe a RAM disk?](https://en.wikipedia.org/wiki/RAM_drive). Faster storage? The drive I'm working from is already pretty fast (Gen 3 NVMe). Maybe I could use a database? I decided try using a database.\n\n## Why Mongo?\n\nI've been playing around with Mongo for a bit. Other implementations of database of the UTZOO archives are [SQL based](https://www.reddit.com/r/java/comments/bkm5h5/how_i_converted_utzoo_usenet_archive_from/) and I thought it might be interesting to try something different, NoSQL. What I would like to test is the ability to quickly search the data. I'm not sure if Mongo is the best choice for this, but I thought UTZOO could be a good test case for [Mongo's search indexing](https://www.mongodb.com/basics/search-index) features.\n\n## Decompressing the data:\n\nRather than decompress the data in Node, I decompressed it using 7zip, so once you retrieve the archive, you'll also need to do so.\n\n## How to run:\n\nYou must have the contents of the UTZOO Usenet archive extracted in a directory named UTZOO, in the same directory as the server and\n\n(If you are using [MongoDB.com](http://mongodb.com/) and) open the serverfolder and:\n\n* Create a file called credentials.js. In it, include:\n\n&amp;#x200B;\n\n    const clusterURL = `your_cluster_url`; \n    const credentials = {username : \"your_username\", password : \"your_password\" }  module.exports = {credentials: credentials, clusterURL: clusterURL }\n\n* Modify server.js:\n\n&amp;#x200B;\n\n    var { credentials, clusterURL } = require('./credentials'); \n    var dbURI = `mongodb+srv://${credentials.username}:${credentials.password}@${clusterURL}`;\n\n* Run \"node server\" to start the server.\n\nOr, if you are using a local database:\n\n* Install [MongoDB](https://www.mongodb.com/) and start a server\n* Open the serverfolder and modify server.js:\n\n&amp;#x200B;\n\n    var {clusterURL } = require('./credentials'); var dbURI = `${clusterURL}`;\n\n* Or, change clusterURLin credentials.jsto the URL of your local database, or replace the string with your database url, ex:\n\n&amp;#x200B;\n\n    var dbURI = \"mongodb://localhost:27017\";\n\nRun \"node server\" to start the server.\n\nImport:\n\nTo start adding data to the database, leave the server running and open the folder named \"importer\" and run \"node listworker.js\"\n\nThis will start inserting the data into the database. It will take a while.\n\n## Problems:\n\nI am still working on parsing the data, so the importer is not complete. I will update this when it is. There is a wide variety of header formats in the UTZOO archive. This includes a variety of different date codes. Later headers include a key to the header's value, early headers lack this. I still need to fix some of the date parsing. There's also a few headers that I haven't accounted for yet. I'm working on it.\n\n## Using the data:\n\nYou can use either [MongoDB.com](http://mongodb.com/)'s dashboard (if you host a remote database) or Mongo Compass to run queries on the data or you can modify the express middleware with your own queries. I'm still working on the API, so it's not very robust yet. I will update this when it is.\n\n## Performance\n\nThis script recursively crawls through the UTZOO directory and adds the files to the Mongo database using fetch calls to the expressbased API. Saving the data to the database took about 12 hours on my machine to a local MongoDB server.\n\nThere's definitely faster ways of doing this  \u00af\\_(\u30c4)\\_/\u00af. Ditching the middleware library (Express) would probably help.\n\n## Code:\n\nThe code is available on [Github](https://github.com/LiamOsler/utzoo-mongo-db-converter).\n\n## Going Forward:\n\nOnce I have the Database built up, I'm going to try and build a small front-end that will allow you to search the contents of the Usenet posts, something along the lines of [www.usenetarchives.com/](https://file+.vscode-resource.vscode-cdn.net/c:/Users/Magenta/Desktop/www.usenetarchives.com) but built with a MERN stack. Feel free to copy and modify the code.", "author_fullname": "t2_74ub1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I wrote some scripts for converting the UTZOO Usenet archive to a Mongo Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx44mx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668630289.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668628305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;utzoo-mongo-db-converter&lt;/h1&gt;\n\n&lt;p&gt;I wrote some scripts for converting the UTZOO Usenet archive to a Mongo Database.&lt;/p&gt;\n\n&lt;h2&gt;Background&lt;/h2&gt;\n\n&lt;p&gt;I recently found out about the &lt;a href=\"https://archive.org/details/utzoo-wiseman-usenet-archive\"&gt;UTZOO Usenet Archive&lt;/a&gt;, a collection of something like two million &lt;a href=\"https://en.wikipedia.org/wiki/Usenet\"&gt;Usenet&lt;/a&gt; posts archived by &lt;a href=\"https://en.wikipedia.org/wiki/Henry_Spencer\"&gt;Henry Spencer&lt;/a&gt; from 1981 to 1991. I wanted to play around with the data, so I wrote some Node script to convert the files to a Mongo database. It&amp;#39;s a little bit hacky, but it works.&lt;/p&gt;\n\n&lt;h2&gt;Getting the data:&lt;/h2&gt;\n\n&lt;p&gt;The data was available on the Internet Archive. You could download the data from the &lt;a href=\"https://archive.org/details/utzoo-wiseman-usenet-archive\"&gt;UTZOO Usenet Archive&lt;/a&gt; page. After legal demands were placed requesting the internet archive to remove the data, the data was removed and replaced with its checksum. Fortunately, some &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/i2btuu/utzoo_archives_have_been_removed_from_archiveorg/\"&gt;friendly folks on Reddit&lt;/a&gt; have preserved the data. You can download the UTZOO archive from a torrent linked in that post.&lt;/p&gt;\n\n&lt;h2&gt;The files&lt;/h2&gt;\n\n&lt;p&gt;The UTZOO archive has individual Usenet post saved individual files. This makes searching its contents pretty slow, especially running a search using in Windows. &lt;a href=\"https://man7.org/linux/man-pages/man1/grep.1.html\"&gt;GREP&lt;/a&gt; was faster, but still doesn&amp;#39;t take advantage of search indexing. Seeking through the contents of files this way is always dead slow. I thought about a few ways of being able to quickly search the data. &lt;a href=\"https://en.wikipedia.org/wiki/RAM_drive\"&gt;Maybe a RAM disk?&lt;/a&gt;. Faster storage? The drive I&amp;#39;m working from is already pretty fast (Gen 3 NVMe). Maybe I could use a database? I decided try using a database.&lt;/p&gt;\n\n&lt;h2&gt;Why Mongo?&lt;/h2&gt;\n\n&lt;p&gt;I&amp;#39;ve been playing around with Mongo for a bit. Other implementations of database of the UTZOO archives are &lt;a href=\"https://www.reddit.com/r/java/comments/bkm5h5/how_i_converted_utzoo_usenet_archive_from/\"&gt;SQL based&lt;/a&gt; and I thought it might be interesting to try something different, NoSQL. What I would like to test is the ability to quickly search the data. I&amp;#39;m not sure if Mongo is the best choice for this, but I thought UTZOO could be a good test case for &lt;a href=\"https://www.mongodb.com/basics/search-index\"&gt;Mongo&amp;#39;s search indexing&lt;/a&gt; features.&lt;/p&gt;\n\n&lt;h2&gt;Decompressing the data:&lt;/h2&gt;\n\n&lt;p&gt;Rather than decompress the data in Node, I decompressed it using 7zip, so once you retrieve the archive, you&amp;#39;ll also need to do so.&lt;/p&gt;\n\n&lt;h2&gt;How to run:&lt;/h2&gt;\n\n&lt;p&gt;You must have the contents of the UTZOO Usenet archive extracted in a directory named UTZOO, in the same directory as the server and&lt;/p&gt;\n\n&lt;p&gt;(If you are using &lt;a href=\"http://mongodb.com/\"&gt;MongoDB.com&lt;/a&gt; and) open the serverfolder and:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Create a file called credentials.js. In it, include:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;const clusterURL = `your_cluster_url`; \nconst credentials = {username : &amp;quot;your_username&amp;quot;, password : &amp;quot;your_password&amp;quot; }  module.exports = {credentials: credentials, clusterURL: clusterURL }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Modify server.js:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;var { credentials, clusterURL } = require(&amp;#39;./credentials&amp;#39;); \nvar dbURI = `mongodb+srv://${credentials.username}:${credentials.password}@${clusterURL}`;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Run &amp;quot;node server&amp;quot; to start the server.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Or, if you are using a local database:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Install &lt;a href=\"https://www.mongodb.com/\"&gt;MongoDB&lt;/a&gt; and start a server&lt;/li&gt;\n&lt;li&gt;Open the serverfolder and modify server.js:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;var {clusterURL } = require(&amp;#39;./credentials&amp;#39;); var dbURI = `${clusterURL}`;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Or, change clusterURLin credentials.jsto the URL of your local database, or replace the string with your database url, ex:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;var dbURI = &amp;quot;mongodb://localhost:27017&amp;quot;;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Run &amp;quot;node server&amp;quot; to start the server.&lt;/p&gt;\n\n&lt;p&gt;Import:&lt;/p&gt;\n\n&lt;p&gt;To start adding data to the database, leave the server running and open the folder named &amp;quot;importer&amp;quot; and run &amp;quot;node listworker.js&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;This will start inserting the data into the database. It will take a while.&lt;/p&gt;\n\n&lt;h2&gt;Problems:&lt;/h2&gt;\n\n&lt;p&gt;I am still working on parsing the data, so the importer is not complete. I will update this when it is. There is a wide variety of header formats in the UTZOO archive. This includes a variety of different date codes. Later headers include a key to the header&amp;#39;s value, early headers lack this. I still need to fix some of the date parsing. There&amp;#39;s also a few headers that I haven&amp;#39;t accounted for yet. I&amp;#39;m working on it.&lt;/p&gt;\n\n&lt;h2&gt;Using the data:&lt;/h2&gt;\n\n&lt;p&gt;You can use either &lt;a href=\"http://mongodb.com/\"&gt;MongoDB.com&lt;/a&gt;&amp;#39;s dashboard (if you host a remote database) or Mongo Compass to run queries on the data or you can modify the express middleware with your own queries. I&amp;#39;m still working on the API, so it&amp;#39;s not very robust yet. I will update this when it is.&lt;/p&gt;\n\n&lt;h2&gt;Performance&lt;/h2&gt;\n\n&lt;p&gt;This script recursively crawls through the UTZOO directory and adds the files to the Mongo database using fetch calls to the expressbased API. Saving the data to the database took about 12 hours on my machine to a local MongoDB server.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s definitely faster ways of doing this  \u00af_(\u30c4)_/\u00af. Ditching the middleware library (Express) would probably help.&lt;/p&gt;\n\n&lt;h2&gt;Code:&lt;/h2&gt;\n\n&lt;p&gt;The code is available on &lt;a href=\"https://github.com/LiamOsler/utzoo-mongo-db-converter\"&gt;Github&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h2&gt;Going Forward:&lt;/h2&gt;\n\n&lt;p&gt;Once I have the Database built up, I&amp;#39;m going to try and build a small front-end that will allow you to search the contents of the Usenet posts, something along the lines of &lt;a href=\"https://file+.vscode-resource.vscode-cdn.net/c:/Users/Magenta/Desktop/www.usenetarchives.com\"&gt;www.usenetarchives.com/&lt;/a&gt; but built with a MERN stack. Feel free to copy and modify the code.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EjEbcSRhInDbL38pGa6AqM8eZoVgo3z49ki35Fj7_zg.jpg?auto=webp&amp;s=e98b6b090958ed73451630b5d6218dab2dd87f96", "width": 180, "height": 144}, "resolutions": [{"url": "https://external-preview.redd.it/EjEbcSRhInDbL38pGa6AqM8eZoVgo3z49ki35Fj7_zg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=91ff2761db12355841a21002593f06f933362fe7", "width": 108, "height": 86}], "variants": {}, "id": "L618DVNfFyrYa22IPEcRYVxjn8GGPvNgDb1EWYQsaNo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx44mx", "is_robot_indexable": true, "report_reasons": null, "author": "Democedes", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx44mx/i_wrote_some_scripts_for_converting_the_utzoo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx44mx/i_wrote_some_scripts_for_converting_the_utzoo/", "subreddit_subscribers": 654314, "created_utc": 1668628305.0, "num_crossposts": 4, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know Amazon tends to just throw them in a envelope so that's out.\n\nI'm not sure what the current state of Newegg is but it doesn't seem great.\n\nWhat are online sellers properly package and ship hard drives these days?", "author_fullname": "t2_176242", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to buy hard drives online", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yxg3np", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668658193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know Amazon tends to just throw them in a envelope so that&amp;#39;s out.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure what the current state of Newegg is but it doesn&amp;#39;t seem great.&lt;/p&gt;\n\n&lt;p&gt;What are online sellers properly package and ship hard drives these days?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxg3np", "is_robot_indexable": true, "report_reasons": null, "author": "Cobra__Commander", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yxg3np/where_to_buy_hard_drives_online/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxg3np/where_to_buy_hard_drives_online/", "subreddit_subscribers": 654314, "created_utc": 1668658193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking into an LTO drive and seeing if setting one up is even slightly reasonable... hardware wise I currently have an UNRAID server setup using SAS cards:\n\n\\- [LSI 6Gbps \u200bSAS HBA LS\u200bI 9211-8i](https://www.ebay.com/itm/163846248833)\n\n\\- IBM ServeRAID 16-Port 6Gbps SAS-2 SATA Expansion Adapter 46M0997 Firmware 634A\n\nIf I got an \"External SAS Tape Drive\" would I be able to just connect it to the existing cards or would I need some additional hardware?", "author_fullname": "t2_xo6fu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTO Drive Hardware Requirements", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ywvk8p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668610434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking into an LTO drive and seeing if setting one up is even slightly reasonable... hardware wise I currently have an UNRAID server setup using SAS cards:&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://www.ebay.com/itm/163846248833\"&gt;LSI 6Gbps \u200bSAS HBA LS\u200bI 9211-8i&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;- IBM ServeRAID 16-Port 6Gbps SAS-2 SATA Expansion Adapter 46M0997 Firmware 634A&lt;/p&gt;\n\n&lt;p&gt;If I got an &amp;quot;External SAS Tape Drive&amp;quot; would I be able to just connect it to the existing cards or would I need some additional hardware?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aIxVIf65gdQa4Q52Am42_u1QDI2zWZrM5zd_7W3-RMg.jpg?auto=webp&amp;s=7332324fe116ca72801047496a80e8303ce6002d", "width": 400, "height": 306}, "resolutions": [{"url": "https://external-preview.redd.it/aIxVIf65gdQa4Q52Am42_u1QDI2zWZrM5zd_7W3-RMg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=58cb98cc3ebd95ddc2acbf09d1e13dcb2701c081", "width": 108, "height": 82}, {"url": "https://external-preview.redd.it/aIxVIf65gdQa4Q52Am42_u1QDI2zWZrM5zd_7W3-RMg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=69f736b1ec9c931ce9f71d4f934ec4d298899e04", "width": 216, "height": 165}, {"url": "https://external-preview.redd.it/aIxVIf65gdQa4Q52Am42_u1QDI2zWZrM5zd_7W3-RMg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=73d224d395cc4693fbf1f2c05bc0d0ab0b196a35", "width": 320, "height": 244}], "variants": {}, "id": "Gex7KEbK2g7DN-YtBgcB0bTCnsjcN0X6QcaLI0SldGo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ywvk8p", "is_robot_indexable": true, "report_reasons": null, "author": "emotion_chip", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ywvk8p/lto_drive_hardware_requirements/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ywvk8p/lto_drive_hardware_requirements/", "subreddit_subscribers": 654314, "created_utc": 1668610434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys need to buy an NAS setup. As it is for someone who isn't technically sound too much i am going for pre bult NAS. His requirements are for accessing official documents and files anywhere . I think 16 TB would be fine. My query is which NAS to buy and should he go with SSD or HDD.", "author_fullname": "t2_zwztc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for NAS setup for accessing official documents and miscellaneous stuff.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yxgmlc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668659748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys need to buy an NAS setup. As it is for someone who isn&amp;#39;t technically sound too much i am going for pre bult NAS. His requirements are for accessing official documents and files anywhere . I think 16 TB would be fine. My query is which NAS to buy and should he go with SSD or HDD.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxgmlc", "is_robot_indexable": true, "report_reasons": null, "author": "lordofabyss", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxgmlc/advice_for_nas_setup_for_accessing_official/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxgmlc/advice_for_nas_setup_for_accessing_official/", "subreddit_subscribers": 654314, "created_utc": 1668659748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there,  \nSomehow the only tool I found for downloading the posts of a public substack was this project: [https://github.com/nosajio/substack-dl/](https://github.com/nosajio/substack-dl/)  \nAfter an hour of installing stuff and troubleshooting, I couldn't get it to work. Does anyone know of another way?  \n\n\n\\----Error (I'm on Win10)  \nRunning \\`target\\\\debug\\\\substack-dl.exe substack-dl [nosaj.substack.com](https://nosaj.substack.com) \\~/save\\_posts\\_location --overwrite --fmt-all\\`\n\nthread 'main' panicked at 'Could not fetch &amp; parse posts: reqwest::Error { kind: Request, url: Url { scheme: \"https\", cannot\\_be\\_a\\_base: false, username: \"\", password: None, host: Some(Domain(\"substack-dl\")), port: None, path: \"/feed\", query: None, fragment: None }, source: hyper::Error(Connect, ConnectError(\"dns error\", Os { code: 11001, kind: Uncategorized, message: \"Este host n\u00e3o \u00e9 conhecido.\" })) }', src\\\\main.rs:16:10\n\nnote: run with \\`RUST\\_BACKTRACE=1\\` environment variable to display a backtrace", "author_fullname": "t2_dsmf5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download all posts of a substack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yxg3qd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668658201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;br/&gt;\nSomehow the only tool I found for downloading the posts of a public substack was this project: &lt;a href=\"https://github.com/nosajio/substack-dl/\"&gt;https://github.com/nosajio/substack-dl/&lt;/a&gt;&lt;br/&gt;\nAfter an hour of installing stuff and troubleshooting, I couldn&amp;#39;t get it to work. Does anyone know of another way?  &lt;/p&gt;\n\n&lt;p&gt;----Error (I&amp;#39;m on Win10)&lt;br/&gt;\nRunning `target\\debug\\substack-dl.exe substack-dl &lt;a href=\"https://nosaj.substack.com\"&gt;nosaj.substack.com&lt;/a&gt; ~/save_posts_location --overwrite --fmt-all`&lt;/p&gt;\n\n&lt;p&gt;thread &amp;#39;main&amp;#39; panicked at &amp;#39;Could not fetch &amp;amp; parse posts: reqwest::Error { kind: Request, url: Url { scheme: &amp;quot;https&amp;quot;, cannot_be_a_base: false, username: &amp;quot;&amp;quot;, password: None, host: Some(Domain(&amp;quot;substack-dl&amp;quot;)), port: None, path: &amp;quot;/feed&amp;quot;, query: None, fragment: None }, source: hyper::Error(Connect, ConnectError(&amp;quot;dns error&amp;quot;, Os { code: 11001, kind: Uncategorized, message: &amp;quot;Este host n\u00e3o \u00e9 conhecido.&amp;quot; })) }&amp;#39;, src\\main.rs:16:10&lt;/p&gt;\n\n&lt;p&gt;note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XPAa4ROPb-ZX3OrdbjNpEUOvHXMj6U1zKgDVavLeopg.jpg?auto=webp&amp;s=24a86f151acb57ca217696d4cdefaaa055635920", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/XPAa4ROPb-ZX3OrdbjNpEUOvHXMj6U1zKgDVavLeopg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=472525ba03396279061d707abb5be99f6cd52a26", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/XPAa4ROPb-ZX3OrdbjNpEUOvHXMj6U1zKgDVavLeopg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e5964d5f9c5b4b5bc416d67146e33ea08b1b8e0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/XPAa4ROPb-ZX3OrdbjNpEUOvHXMj6U1zKgDVavLeopg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9126c453578416c95068dcfb1f476bb5922c640f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/XPAa4ROPb-ZX3OrdbjNpEUOvHXMj6U1zKgDVavLeopg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d9e8aecd07d908c5d773512dae5d520c05c634fa", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/XPAa4ROPb-ZX3OrdbjNpEUOvHXMj6U1zKgDVavLeopg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1afd002f45f1a4e57ff818441c9506a8a71b8f9a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/XPAa4ROPb-ZX3OrdbjNpEUOvHXMj6U1zKgDVavLeopg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9d8e33aec6b66b9eedf486a7ba69e09921555fe8", "width": 1080, "height": 540}], "variants": {}, "id": "ZrlYvTzd6yXl6M2SYT-OQpvIh35zIjSmP_xmSzCYGBc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxg3qd", "is_robot_indexable": true, "report_reasons": null, "author": "mvhamm", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxg3qd/how_to_download_all_posts_of_a_substack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxg3qd/how_to_download_all_posts_of_a_substack/", "subreddit_subscribers": 654314, "created_utc": 1668658201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Forgive me if this is a stupid question, I just do this casually. In your opinion, is Toshiba a trustworthy brand for external hard drives? I got a 2 TB drive earlier this year to store things, the majority videos and VODs that I think were at risk of being deleted, that I wanted to save just in case. ([This](https://a.co/d/9UY9XI4) is the specific product). I just want to make sure that it\u2019ll last me at least a good few years, I\u2019ll look at more long term solutions with time, but I just want to be sure it\u2019ll be okay for a little while. Thank you so much for your help if you can let me know or point me in the right direction &lt;3", "author_fullname": "t2_8dwcyjkh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Toshiba Brand", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxd68p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668650108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Forgive me if this is a stupid question, I just do this casually. In your opinion, is Toshiba a trustworthy brand for external hard drives? I got a 2 TB drive earlier this year to store things, the majority videos and VODs that I think were at risk of being deleted, that I wanted to save just in case. (&lt;a href=\"https://a.co/d/9UY9XI4\"&gt;This&lt;/a&gt; is the specific product). I just want to make sure that it\u2019ll last me at least a good few years, I\u2019ll look at more long term solutions with time, but I just want to be sure it\u2019ll be okay for a little while. Thank you so much for your help if you can let me know or point me in the right direction &amp;lt;3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxd68p", "is_robot_indexable": true, "report_reasons": null, "author": "StrawberryBubbleTea7", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxd68p/toshiba_brand/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxd68p/toshiba_brand/", "subreddit_subscribers": 654314, "created_utc": 1668650108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Greetings to the datahoarder community,\n\n  \n\n\nThank you for being one out your has knowledge or experience regarding the failure rates of the aforementioned hard drives. Sadly I don't see any third-party failure statistics from cool people like backblaze, but I really wanna buy a few of these and increase the resolution of my library :-)\n\nThese ones:\n\n[https://www.westerndigital.com/products/outlet/internal-drives/data-center-drives/ultrastar-dc-hc560-hdd#0F38755](https://www.westerndigital.com/products/outlet/internal-drives/data-center-drives/ultrastar-dc-hc560-hdd#0F38755)\n\n  \n\n\n[https://www.westerndigital.com/products/internal-drives/data-center-drives/ultrastar-dc-hc570-hdd#0F48155](https://www.westerndigital.com/products/internal-drives/data-center-drives/ultrastar-dc-hc570-hdd#0F48155)\n\n  \n\n\n  \n\n\nI really appreciate any input. These discs will be used very heavily. For me the most important factor is reliability. Premier failed disc has resulted in loss of happiness.\n\n  \n\n\nThe price is also dropped and for the first time, these seem like they can fit into the budget.", "author_fullname": "t2_1mgual0e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any thoughts on reliability of Ultrastar DC HC560 / HC570", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxa06s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668641742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings to the datahoarder community,&lt;/p&gt;\n\n&lt;p&gt;Thank you for being one out your has knowledge or experience regarding the failure rates of the aforementioned hard drives. Sadly I don&amp;#39;t see any third-party failure statistics from cool people like backblaze, but I really wanna buy a few of these and increase the resolution of my library :-)&lt;/p&gt;\n\n&lt;p&gt;These ones:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.westerndigital.com/products/outlet/internal-drives/data-center-drives/ultrastar-dc-hc560-hdd#0F38755\"&gt;https://www.westerndigital.com/products/outlet/internal-drives/data-center-drives/ultrastar-dc-hc560-hdd#0F38755&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.westerndigital.com/products/internal-drives/data-center-drives/ultrastar-dc-hc570-hdd#0F48155\"&gt;https://www.westerndigital.com/products/internal-drives/data-center-drives/ultrastar-dc-hc570-hdd#0F48155&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I really appreciate any input. These discs will be used very heavily. For me the most important factor is reliability. Premier failed disc has resulted in loss of happiness.&lt;/p&gt;\n\n&lt;p&gt;The price is also dropped and for the first time, these seem like they can fit into the budget.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LP3csfUmVh2Scl8w8IA5GRCG36fK4aAhMni75IxwgdI.jpg?auto=webp&amp;s=8d2e158c0c72f5397b842fe547bacac2d450cfc2", "width": 1680, "height": 1680}, "resolutions": [{"url": "https://external-preview.redd.it/LP3csfUmVh2Scl8w8IA5GRCG36fK4aAhMni75IxwgdI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=81cf745324646f70b3ec0a5f3ffc19d1330834fe", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/LP3csfUmVh2Scl8w8IA5GRCG36fK4aAhMni75IxwgdI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ca8e27280e1b67121880fda68e1772301ae8d99a", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/LP3csfUmVh2Scl8w8IA5GRCG36fK4aAhMni75IxwgdI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d0e82b4c84f6e9c1dd749fc7f183a65802c386b", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/LP3csfUmVh2Scl8w8IA5GRCG36fK4aAhMni75IxwgdI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1431f4d533515ff82fd0455bcdea2a72e18c5976", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/LP3csfUmVh2Scl8w8IA5GRCG36fK4aAhMni75IxwgdI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d4973466e690d44e3fab8b43159da5314b8e1b0", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/LP3csfUmVh2Scl8w8IA5GRCG36fK4aAhMni75IxwgdI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=73f21b349418238bdb1bc0219aa01a2c1d6ce07b", "width": 1080, "height": 1080}], "variants": {}, "id": "LDh5lfWRphhbTLF86U9gEL3TXcmdlJmv8cLbkOrwZoI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxa06s", "is_robot_indexable": true, "report_reasons": null, "author": "skunkytuna", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxa06s/any_thoughts_on_reliability_of_ultrastar_dc_hc560/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxa06s/any_thoughts_on_reliability_of_ultrastar_dc_hc560/", "subreddit_subscribers": 654314, "created_utc": 1668641742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all! Hard for me to summarize this, so apologies for block of text. I'll try to bold the more important bits. Thanks in advance.\n\nI have a **12U Gator rack** I won't be using now that I'm selling my DJ equipment, and I want to build a *reasonably* quiet gaming desktop that's rackmounted. It'll fit nicely with the Furman power conditioner and shelf.\n\nIn case it helps, here's my current [PC Part Picker list](https://pcpartpicker.com/list/4CBjC6).\n\nMy problems begin with the **rack's depth** only being **16.5\"**.\n\nThat makes finding a rackmount chassis that fits difficult enough. So far the best one I've found (15\" deep, reasonable price) looks to be here:\n\n[Sliger CX4150a](https://www.sliger.com/products/rackmount/4u/cx4150a/)\n\nI'd love to find some 6U or 8U unit so I could get loads of fans running at low speed to keep it quiet *and* a bunch of 5.25\" bays, but north of 4U appears to be either unicorns, or priced as unicorns. The two other variants of that linked one above (\"i\" and \"e\" suffixes) have one or two 5.25\" bays, but can't fit a deep power supply like that \"a\" variant.\n\nI don't intend to have dozens of drives hooked up daily or anything. **I just want three to four 5.25\" bays available for 1) a Blu-Ray drive, 2) at least one 3.5\" hot-swap bay, and 3) at least one 2.5\" hot-swap bay.**\n\nSo I'm wondering if it's possible to have a separate case mounted directly above or below the linked one, and run the SATA and power cables to it (plus extra fans, I assume) for these drives?\n\nThis subreddit just introduced me to \"JBOD\" (funniest acronym ever?), but even with that hint, most of what I'm finding is still into four figures. I'm literally just looking for a 1-2U piece with a handful of 5.25\" bays that wouldn't be too difficult to link into from the other piece.\n\nAnyone have ideas?", "author_fullname": "t2_tdogbn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rackmount 5.25\" bays? buildapc x datahoarder crossover", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx9vf4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668641408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all! Hard for me to summarize this, so apologies for block of text. I&amp;#39;ll try to bold the more important bits. Thanks in advance.&lt;/p&gt;\n\n&lt;p&gt;I have a &lt;strong&gt;12U Gator rack&lt;/strong&gt; I won&amp;#39;t be using now that I&amp;#39;m selling my DJ equipment, and I want to build a &lt;em&gt;reasonably&lt;/em&gt; quiet gaming desktop that&amp;#39;s rackmounted. It&amp;#39;ll fit nicely with the Furman power conditioner and shelf.&lt;/p&gt;\n\n&lt;p&gt;In case it helps, here&amp;#39;s my current &lt;a href=\"https://pcpartpicker.com/list/4CBjC6\"&gt;PC Part Picker list&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;My problems begin with the &lt;strong&gt;rack&amp;#39;s depth&lt;/strong&gt; only being &lt;strong&gt;16.5&amp;quot;&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;That makes finding a rackmount chassis that fits difficult enough. So far the best one I&amp;#39;ve found (15&amp;quot; deep, reasonable price) looks to be here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.sliger.com/products/rackmount/4u/cx4150a/\"&gt;Sliger CX4150a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to find some 6U or 8U unit so I could get loads of fans running at low speed to keep it quiet &lt;em&gt;and&lt;/em&gt; a bunch of 5.25&amp;quot; bays, but north of 4U appears to be either unicorns, or priced as unicorns. The two other variants of that linked one above (&amp;quot;i&amp;quot; and &amp;quot;e&amp;quot; suffixes) have one or two 5.25&amp;quot; bays, but can&amp;#39;t fit a deep power supply like that &amp;quot;a&amp;quot; variant.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t intend to have dozens of drives hooked up daily or anything. &lt;strong&gt;I just want three to four 5.25&amp;quot; bays available for 1) a Blu-Ray drive, 2) at least one 3.5&amp;quot; hot-swap bay, and 3) at least one 2.5&amp;quot; hot-swap bay.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m wondering if it&amp;#39;s possible to have a separate case mounted directly above or below the linked one, and run the SATA and power cables to it (plus extra fans, I assume) for these drives?&lt;/p&gt;\n\n&lt;p&gt;This subreddit just introduced me to &amp;quot;JBOD&amp;quot; (funniest acronym ever?), but even with that hint, most of what I&amp;#39;m finding is still into four figures. I&amp;#39;m literally just looking for a 1-2U piece with a handful of 5.25&amp;quot; bays that wouldn&amp;#39;t be too difficult to link into from the other piece.&lt;/p&gt;\n\n&lt;p&gt;Anyone have ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx9vf4", "is_robot_indexable": true, "report_reasons": null, "author": "Weirderal1337", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx9vf4/rackmount_525_bays_buildapc_x_datahoarder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx9vf4/rackmount_525_bays_buildapc_x_datahoarder/", "subreddit_subscribers": 654314, "created_utc": 1668641408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_16vs44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connected WD Elements 5tb for the first time to my Windows 7 64bit. Didn\u2019t understand what is the WD SES Device USB Server? It seems I can store on the driver nevertheless", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "name": "t3_yx8m8m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.48, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/4bQxiAoL9G6KbKZSNQ4ZkZuOU9x57WLx0_LiuH7Qs58.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668638319.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/dhzjpwow4e0a1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/dhzjpwow4e0a1.jpg?auto=webp&amp;s=5bb5b3f9bbca62a71fe18d77510ec92125e11b87", "width": 534, "height": 227}, "resolutions": [{"url": "https://preview.redd.it/dhzjpwow4e0a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e360a80bfb1413dcbea018f6c24e53b1e756a8b", "width": 108, "height": 45}, {"url": "https://preview.redd.it/dhzjpwow4e0a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=22ca476e787c05adcb35a9061c30f85fc79e32cb", "width": 216, "height": 91}, {"url": "https://preview.redd.it/dhzjpwow4e0a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7da249cb818d1271bb94801f7a3d49dff8f9f2dc", "width": 320, "height": 136}], "variants": {}, "id": "eKKn4WUZNLb1Wqz7ciA27wsJPDdgDGIGCXHJQReziQA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx8m8m", "is_robot_indexable": true, "report_reasons": null, "author": "toktok159", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx8m8m/connected_wd_elements_5tb_for_the_first_time_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/dhzjpwow4e0a1.jpg", "subreddit_subscribers": 654314, "created_utc": 1668638319.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "title explains most of it. i have ~1k+ songs liked on youtube music and even more videos liked on youtube. i want to download them just in case anything gets taken down or region banned (happened several times)\n\nis there a way to automate this? and i'm worried that if i do this i get flagged for suspicious activity and risk my account getting banned. so how can i minimize the risk of account issues? (e.g. batching them?)\n\nmainly interested in archiving my youtube music likes", "author_fullname": "t2_b0ac2pk3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to download my liked music and videos from youtube is there a way to do automate this and not risk getting my account banned?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx3t5k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668627616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title explains most of it. i have ~1k+ songs liked on youtube music and even more videos liked on youtube. i want to download them just in case anything gets taken down or region banned (happened several times)&lt;/p&gt;\n\n&lt;p&gt;is there a way to automate this? and i&amp;#39;m worried that if i do this i get flagged for suspicious activity and risk my account getting banned. so how can i minimize the risk of account issues? (e.g. batching them?)&lt;/p&gt;\n\n&lt;p&gt;mainly interested in archiving my youtube music likes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx3t5k", "is_robot_indexable": true, "report_reasons": null, "author": "ChaosFairyMagic", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx3t5k/i_want_to_download_my_liked_music_and_videos_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx3t5k/i_want_to_download_my_liked_music_and_videos_from/", "subreddit_subscribers": 654314, "created_utc": 1668627616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "hi\n\nis it possible to use a common SATA drive with in a hot swappable server that uses a SAS raid controller? I am using the Huawei RH2288H V3 and the Huawei SR320 (LSI SAS2208) raid controller card.", "author_fullname": "t2_oghke8s2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SAS/SATA use common SATA drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx3kie", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668627093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi&lt;/p&gt;\n\n&lt;p&gt;is it possible to use a common SATA drive with in a hot swappable server that uses a SAS raid controller? I am using the Huawei RH2288H V3 and the Huawei SR320 (LSI SAS2208) raid controller card.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx3kie", "is_robot_indexable": true, "report_reasons": null, "author": "Aggressive_Kiwi2984", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx3kie/sassata_use_common_sata_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx3kie/sassata_use_common_sata_drive/", "subreddit_subscribers": 654314, "created_utc": 1668627093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Asking because I'm going to replace my faulty hard drive with an old seagate expansion that has zero issues.", "author_fullname": "t2_p97cwuq7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are older seagate hard drives faulty too?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx3j8o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668627024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Asking because I&amp;#39;m going to replace my faulty hard drive with an old seagate expansion that has zero issues.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx3j8o", "is_robot_indexable": true, "report_reasons": null, "author": "name_here_201", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx3j8o/are_older_seagate_hard_drives_faulty_too/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx3j8o/are_older_seagate_hard_drives_faulty_too/", "subreddit_subscribers": 654314, "created_utc": 1668627024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi team!\n\nIt's a long shot, but there is a chance someone had a similar issue. \n\nThe trouble is, at the very end on the system load, the NetApp \"blinks\", all the drives disconnect for a moment and then reconnect. With different device names. /dev/sdc it becomes the /dev/sdaa or something similar. It messes up the ZFS for some reason. Even thou it looks like the pools is created by disk ID's not by block device names:\n\n            NAME                                      STATE     READ WRITE CKSUM\n            netapp                                    ONLINE       0     0     0\n              raidz2-0                                ONLINE       0     0     0\n                86ffd8c5-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n                86e21de7-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n                86e436d1-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n                86c8c77b-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n                871e53bd-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n                8794c8f7-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n                877f64c4-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n                87696980-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n                8701fee6-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n                8751e62f-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n                873a3410-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n                86b433c3-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n                86979dde-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n                86e657fe-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            spares\n              sdq                                     AVAIL\n              sdr                                     AVAIL\n\nI think, something is faulty in my box. Either an LSI or the NetApp. This exact same thing happened in Ubuntu and now happens in the TrueNAS Scale.\n\nI have two cables, so far, I tried to connect a single cable to NetApp IOM1. I didn't try it with only IOM2. That would be a next thing to try, but I am not eager to mess with my data without absolute need.\n\nSo far, I managed to load, by unplugging the NetApp, booting the NAS and hot-plugging it back. zpool import picks up the pool with no issues.\n\nHave anyone experienced anything similar?\n\nThank you.", "author_fullname": "t2_pnh0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NetApp DS4246 and LSI SAS2308 on TrueNAS issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxd17g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668649723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team!&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a long shot, but there is a chance someone had a similar issue. &lt;/p&gt;\n\n&lt;p&gt;The trouble is, at the very end on the system load, the NetApp &amp;quot;blinks&amp;quot;, all the drives disconnect for a moment and then reconnect. With different device names. /dev/sdc it becomes the /dev/sdaa or something similar. It messes up the ZFS for some reason. Even thou it looks like the pools is created by disk ID&amp;#39;s not by block device names:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;        NAME                                      STATE     READ WRITE CKSUM\n        netapp                                    ONLINE       0     0     0\n          raidz2-0                                ONLINE       0     0     0\n            86ffd8c5-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            86e21de7-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            86e436d1-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            86c8c77b-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            871e53bd-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            8794c8f7-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            877f64c4-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            87696980-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            8701fee6-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            8751e62f-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            873a3410-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            86b433c3-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            86979dde-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n            86e657fe-0210-11ed-8990-44a84209fe44  ONLINE       0     0     0\n        spares\n          sdq                                     AVAIL\n          sdr                                     AVAIL\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I think, something is faulty in my box. Either an LSI or the NetApp. This exact same thing happened in Ubuntu and now happens in the TrueNAS Scale.&lt;/p&gt;\n\n&lt;p&gt;I have two cables, so far, I tried to connect a single cable to NetApp IOM1. I didn&amp;#39;t try it with only IOM2. That would be a next thing to try, but I am not eager to mess with my data without absolute need.&lt;/p&gt;\n\n&lt;p&gt;So far, I managed to load, by unplugging the NetApp, booting the NAS and hot-plugging it back. zpool import picks up the pool with no issues.&lt;/p&gt;\n\n&lt;p&gt;Have anyone experienced anything similar?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxd17g", "is_robot_indexable": true, "report_reasons": null, "author": "sergedubovsky", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxd17g/netapp_ds4246_and_lsi_sas2308_on_truenas_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxd17g/netapp_ds4246_and_lsi_sas2308_on_truenas_issue/", "subreddit_subscribers": 654314, "created_utc": 1668649723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need to get a new storage solution for what's now about 25 TB of data (mostly video files) with room to grow at several TB per year. We previously had some DROBO units but due to downsizing we got rid of the server they were attached to and the units basically bricked when we tried to update the firmware with the intent of moving and repopulating them. So now that data is just sitting on some external drives until we can get a new system. \n\nI was initially looking at something along the lines of a QNAP tr-004 DAS (or similar device) to populate with 16 TB drives (I was looking at Ironwolf Pro) and run in RAID 1 for 32 TB usable space, but I've seen some bad things about them so I'm doubting that plan now, and it would only last a couple more years. I have absolutely no desire to build a NAS from scratch, and am wary of anything that might be locked to a specific device without recovery options if it fails. I'm wondering if I should just get the drives and any old enclosure as opposed to running a RAID at all, then make my own redundant copy? \n\nTo be clear, this isn't data that we're actively working with. It's archives that we just need to be able to access on occasion. I'm open to suggestions on how to approach this, as my research consistently leaves me with as many questions as answers.", "author_fullname": "t2_bshy2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need a Video Storage Solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx9g18", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668640337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to get a new storage solution for what&amp;#39;s now about 25 TB of data (mostly video files) with room to grow at several TB per year. We previously had some DROBO units but due to downsizing we got rid of the server they were attached to and the units basically bricked when we tried to update the firmware with the intent of moving and repopulating them. So now that data is just sitting on some external drives until we can get a new system. &lt;/p&gt;\n\n&lt;p&gt;I was initially looking at something along the lines of a QNAP tr-004 DAS (or similar device) to populate with 16 TB drives (I was looking at Ironwolf Pro) and run in RAID 1 for 32 TB usable space, but I&amp;#39;ve seen some bad things about them so I&amp;#39;m doubting that plan now, and it would only last a couple more years. I have absolutely no desire to build a NAS from scratch, and am wary of anything that might be locked to a specific device without recovery options if it fails. I&amp;#39;m wondering if I should just get the drives and any old enclosure as opposed to running a RAID at all, then make my own redundant copy? &lt;/p&gt;\n\n&lt;p&gt;To be clear, this isn&amp;#39;t data that we&amp;#39;re actively working with. It&amp;#39;s archives that we just need to be able to access on occasion. I&amp;#39;m open to suggestions on how to approach this, as my research consistently leaves me with as many questions as answers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx9g18", "is_robot_indexable": true, "report_reasons": null, "author": "Hydroc777", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx9g18/need_a_video_storage_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx9g18/need_a_video_storage_solution/", "subreddit_subscribers": 654314, "created_utc": 1668640337.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I downloaded dozens of .html files which represent my wesbite.  (Actually .mhtml)\n\nI want to browse this website offline.   Is there an efficient way of changing the links in those .mhtml files to look locally in the current directory for the next page. \n\nCurrently I am looking at Notepad++ find/replace in Files.  The links can be sometime a little complex, e.g.\n\n    http://www.mysitename.org/index.html?page=section2\n    (should translate into:   index.html?page=section2\n    or\n    http://www.mysitename.org/index.html#bookmark", "author_fullname": "t2_1cx04ck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I downloaded my website (a lot of .html files), is there a efficient way to change the links in those files to look locally ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx5zl3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668632236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded dozens of .html files which represent my wesbite.  (Actually .mhtml)&lt;/p&gt;\n\n&lt;p&gt;I want to browse this website offline.   Is there an efficient way of changing the links in those .mhtml files to look locally in the current directory for the next page. &lt;/p&gt;\n\n&lt;p&gt;Currently I am looking at Notepad++ find/replace in Files.  The links can be sometime a little complex, e.g.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;http://www.mysitename.org/index.html?page=section2\n(should translate into:   index.html?page=section2\nor\nhttp://www.mysitename.org/index.html#bookmark\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yx5zl3", "is_robot_indexable": true, "report_reasons": null, "author": "Ian_SAfc", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yx5zl3/i_downloaded_my_website_a_lot_of_html_files_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yx5zl3/i_downloaded_my_website_a_lot_of_html_files_is/", "subreddit_subscribers": 654314, "created_utc": 1668632236.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}