{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_anyz9dbz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you monitoring your data pipelines and what are you using to debug production issues?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 138, "top_awarded_type": null, "hide_score": false, "name": "t3_yx2qsb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 280, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 280, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ulVxDHwoPgs7grvN4OIvTtcDZNo9k1144eMQGmlFs2c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668625393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/f9dmcrn92d0a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/f9dmcrn92d0a1.jpg?auto=webp&amp;s=2daf4589a1ae35f6708494339d746c84cc96b175", "width": 503, "height": 496}, "resolutions": [{"url": "https://preview.redd.it/f9dmcrn92d0a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95f2e5138517caaabf9f1ca4abd0980785be139b", "width": 108, "height": 106}, {"url": "https://preview.redd.it/f9dmcrn92d0a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83783f49cdf096944a13f92067e7508d9156029e", "width": 216, "height": 212}, {"url": "https://preview.redd.it/f9dmcrn92d0a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6525e84673dfe733708686931d685be35a2dd6b", "width": 320, "height": 315}], "variants": {}, "id": "ZpHlfqAkDfnHSNk-gcsYdmv5sqzzy9iMPkIgicZJAGc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "yx2qsb", "is_robot_indexable": true, "report_reasons": null, "author": "tchungry", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yx2qsb/how_are_you_monitoring_your_data_pipelines_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/f9dmcrn92d0a1.jpg", "subreddit_subscribers": 80215, "created_utc": 1668625393.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Zach wilson on linkedin said that rust is going to have a great future in data engineering, what do you think about that?", "author_fullname": "t2_th46eirx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rust in data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxj5fz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668667723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Zach wilson on linkedin said that rust is going to have a great future in data engineering, what do you think about that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxj5fz", "is_robot_indexable": true, "report_reasons": null, "author": "PopTheTenYasha", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxj5fz/rust_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxj5fz/rust_in_data_engineering/", "subreddit_subscribers": 80215, "created_utc": 1668667723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_31ilk7t8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake announces Alerts in private preview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": false, "name": "t3_yxbgcd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 17, "total_awards_received": 0, "media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Now in private preview on &lt;a href=\"https://twitter.com/SnowflakeDB?ref_src=twsrc%5Etfw\"&gt;@SnowflakeDB&lt;/a&gt;, you can set up an alert that periodically performs an action under specific conditions, based on data within Snowflake. &lt;a href=\"https://t.co/R2wBkvZK7N\"&gt;pic.twitter.com/R2wBkvZK7N&lt;/a&gt;&lt;/p&gt;&amp;mdash; SELECT (@select_dev) &lt;a href=\"https://twitter.com/select_dev/status/1593029267700723712?ref_src=twsrc%5Etfw\"&gt;November 16, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/select_dev/status/1593029267700723712", "author_name": "SELECT", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Now in private preview on &lt;a href=\"https://twitter.com/SnowflakeDB?ref_src=twsrc%5Etfw\"&gt;@SnowflakeDB&lt;/a&gt;, you can set up an alert that periodically performs an action under specific conditions, based on data within Snowflake. &lt;a href=\"https://t.co/R2wBkvZK7N\"&gt;pic.twitter.com/R2wBkvZK7N&lt;/a&gt;&lt;/p&gt;&amp;mdash; SELECT (@select_dev) &lt;a href=\"https://twitter.com/select_dev/status/1593029267700723712?ref_src=twsrc%5Etfw\"&gt;November 16, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/select_dev", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Now in private preview on &lt;a href=\"https://twitter.com/SnowflakeDB?ref_src=twsrc%5Etfw\"&gt;@SnowflakeDB&lt;/a&gt;, you can set up an alert that periodically performs an action under specific conditions, based on data within Snowflake. &lt;a href=\"https://t.co/R2wBkvZK7N\"&gt;pic.twitter.com/R2wBkvZK7N&lt;/a&gt;&lt;/p&gt;&amp;mdash; SELECT (@select_dev) &lt;a href=\"https://twitter.com/select_dev/status/1593029267700723712?ref_src=twsrc%5Etfw\"&gt;November 16, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/yxbgcd", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/h-EKhX84KuPV-JEav9G5IkZ2LGidrionX03Q2i1uyfQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668645509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "twitter.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://twitter.com/select_dev/status/1593029267700723712", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p7R-c37C3OglF6slv6i60wsm8BxwO4fIiQaGjDTkwTU.jpg?auto=webp&amp;s=9c66b26fb8147f42fb5fe588afd4724a70861c56", "width": 140, "height": 88}, "resolutions": [{"url": "https://external-preview.redd.it/p7R-c37C3OglF6slv6i60wsm8BxwO4fIiQaGjDTkwTU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9d6c48a84e4d3ae0bf7852508419ab98a7e09214", "width": 108, "height": 67}], "variants": {}, "id": "RDb0ZIEZiY5QRmP6koPKGj5NByF3KV1F3PLEhIxT-GQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxbgcd", "is_robot_indexable": true, "report_reasons": null, "author": "ian-whitestone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxbgcd/snowflake_announces_alerts_in_private_preview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://twitter.com/select_dev/status/1593029267700723712", "subreddit_subscribers": 80215, "created_utc": 1668645509.0, "num_crossposts": 1, "media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/select_dev/status/1593029267700723712", "author_name": "SELECT", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Now in private preview on &lt;a href=\"https://twitter.com/SnowflakeDB?ref_src=twsrc%5Etfw\"&gt;@SnowflakeDB&lt;/a&gt;, you can set up an alert that periodically performs an action under specific conditions, based on data within Snowflake. &lt;a href=\"https://t.co/R2wBkvZK7N\"&gt;pic.twitter.com/R2wBkvZK7N&lt;/a&gt;&lt;/p&gt;&amp;mdash; SELECT (@select_dev) &lt;a href=\"https://twitter.com/select_dev/status/1593029267700723712?ref_src=twsrc%5Etfw\"&gt;November 16, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/select_dev", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently use python operator and have a bunch of processes that would run at same time. We don't want the processes to contend for resources and one high memory process bringing down the entire airflow server. How can this be handled in Airflow? \n\nIs kubernates operator right solution for managing resources effectively? How are things handled in Airflow in your organization? Thanks.", "author_fullname": "t2_jfqnf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is your Airflow architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxe8au", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668652943.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently use python operator and have a bunch of processes that would run at same time. We don&amp;#39;t want the processes to contend for resources and one high memory process bringing down the entire airflow server. How can this be handled in Airflow? &lt;/p&gt;\n\n&lt;p&gt;Is kubernates operator right solution for managing resources effectively? How are things handled in Airflow in your organization? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxe8au", "is_robot_indexable": true, "report_reasons": null, "author": "curidpostn", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxe8au/what_is_your_airflow_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxe8au/what_is_your_airflow_architecture/", "subreddit_subscribers": 80215, "created_utc": 1668652943.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am transferring GA data from big query to snowflake. I am using DBT to flatten each of the JSON columns. I am stuck on custom dimensions. I can parse through each element but how can I loop through them using DBT? Given that they may have n array elements.", "author_fullname": "t2_5d6daxht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone worked on data transfer from GC Bq to snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx7m4s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668635989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am transferring GA data from big query to snowflake. I am using DBT to flatten each of the JSON columns. I am stuck on custom dimensions. I can parse through each element but how can I loop through them using DBT? Given that they may have n array elements.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yx7m4s", "is_robot_indexable": true, "report_reasons": null, "author": "sankalpthakur2610", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yx7m4s/has_anyone_worked_on_data_transfer_from_gc_bq_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yx7m4s/has_anyone_worked_on_data_transfer_from_gc_bq_to/", "subreddit_subscribers": 80215, "created_utc": 1668635989.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ez8d4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MotherDuck scores $47.5m to prove scale-up databases are not quackers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_yxo09b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xDt98sOzCHeuymb3U-ihUEOVA7co6_K0D0fa9Ztn8eI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668685031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "theregister.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.theregister.com/2022/11/17/475_million_says_scaleup_databases/?utm_medium=share&amp;utm_content=article&amp;utm_source=reddit", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KMt0zuiV34OZe0m-dQtNN6jf029X6W3nWej8bLwCUsc.jpg?auto=webp&amp;s=0dfa7d8b0152756a1773dc38b2ce42b98316c558", "width": 648, "height": 429}, "resolutions": [{"url": "https://external-preview.redd.it/KMt0zuiV34OZe0m-dQtNN6jf029X6W3nWej8bLwCUsc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8bafdd5d56d30718219f5dd49d8334eb00a881d0", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/KMt0zuiV34OZe0m-dQtNN6jf029X6W3nWej8bLwCUsc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=04d45f0757d64120d60590dc6279ac82a04cfc9c", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/KMt0zuiV34OZe0m-dQtNN6jf029X6W3nWej8bLwCUsc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=35f359e75f60d3e14dda780dbee0f3473056d3a5", "width": 320, "height": 211}, {"url": "https://external-preview.redd.it/KMt0zuiV34OZe0m-dQtNN6jf029X6W3nWej8bLwCUsc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce6a0f7efba4a56836e229bd30394b60e4b8fb32", "width": 640, "height": 423}], "variants": {}, "id": "C4Zahor1I_MVU7Ugl_ADOspvaGrRqoIqcYaEHgflLTM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "yxo09b", "is_robot_indexable": true, "report_reasons": null, "author": "Bazencourt", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxo09b/motherduck_scores_475m_to_prove_scaleup_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.theregister.com/2022/11/17/475_million_says_scaleup_databases/?utm_medium=share&amp;utm_content=article&amp;utm_source=reddit", "subreddit_subscribers": 80215, "created_utc": 1668685031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you guys take care of timestamp columns during daylight savings off for streaming data pipelines ?, We had an issue where because of daylight savings off , timestamp difference between source and target is different as time will fall back. How do you solve this in your team/company ? TIA", "author_fullname": "t2_3ep37bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Daylight savings off - Data Issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxq8rl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668691663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you guys take care of timestamp columns during daylight savings off for streaming data pipelines ?, We had an issue where because of daylight savings off , timestamp difference between source and target is different as time will fall back. How do you solve this in your team/company ? TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxq8rl", "is_robot_indexable": true, "report_reasons": null, "author": "chanu4dincha", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxq8rl/daylight_savings_off_data_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxq8rl/daylight_savings_off_data_issue/", "subreddit_subscribers": 80215, "created_utc": 1668691663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is working with vibrations analytics. We are manually measure some component and got WAV file. In order to analysis it, we convert the content to numpy array. In the future we are going to receive this data from a sensor and need to convert the data to a unified schema.\n\nThis array might be really large ( &gt; 500K items) and we also need to record some metadata on each measurement. This additional data can be measured\\_time, component, site of the record...\n\nThe question is what will be the best way to store it to answer on queries like: give me all the measurements from 2020-01-01 for the component \"motor\"?\n\nWhat we tried so far:\n\n1. Store in relational DB with star schema where the facts table contains a file name. Later we download the relevant content. This can't work since we might get data in JSON array format without a compound file.\n2. Store the measurements in a large Parquet files in a Hive format in GCS. Create a BigQuery table with id and measurement content as binary data in Hive structure. Later, move the metadata to BigQuery as well and join it with the previous table.This is better since we don't use files anymore and can handle any format of data. However, we reach the BigQuery limit on the size of the data ( the data can be really large)\n\nError:\n\ngoogle.api\\_core.exceptions.BadRequest: 400 Resources exceeded during query execution: Out of memory. Failed to import values for column 'content' This might happen if the file contains a row that is too large, or if the total size of the pages loaded for the queried columns is too large.; Failed to read Parquet file /bigstore/vibrations-samples/date=2022-08-30/sample\\_2022-08-30.parquet. This might happen if the file contains a row that is too large, or if the total size of the pages loaded for the queried columns is too large.\n\nIs this modeling issue or I can just increase some BQ config?\n\nThanks", "author_fullname": "t2_ll7atfr1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Store WAV files data into BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxrjhf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668695285.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668694773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is working with vibrations analytics. We are manually measure some component and got WAV file. In order to analysis it, we convert the content to numpy array. In the future we are going to receive this data from a sensor and need to convert the data to a unified schema.&lt;/p&gt;\n\n&lt;p&gt;This array might be really large ( &amp;gt; 500K items) and we also need to record some metadata on each measurement. This additional data can be measured_time, component, site of the record...&lt;/p&gt;\n\n&lt;p&gt;The question is what will be the best way to store it to answer on queries like: give me all the measurements from 2020-01-01 for the component &amp;quot;motor&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;What we tried so far:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Store in relational DB with star schema where the facts table contains a file name. Later we download the relevant content. This can&amp;#39;t work since we might get data in JSON array format without a compound file.&lt;/li&gt;\n&lt;li&gt;Store the measurements in a large Parquet files in a Hive format in GCS. Create a BigQuery table with id and measurement content as binary data in Hive structure. Later, move the metadata to BigQuery as well and join it with the previous table.This is better since we don&amp;#39;t use files anymore and can handle any format of data. However, we reach the BigQuery limit on the size of the data ( the data can be really large)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Error:&lt;/p&gt;\n\n&lt;p&gt;google.api_core.exceptions.BadRequest: 400 Resources exceeded during query execution: Out of memory. Failed to import values for column &amp;#39;content&amp;#39; This might happen if the file contains a row that is too large, or if the total size of the pages loaded for the queried columns is too large.; Failed to read Parquet file /bigstore/vibrations-samples/date=2022-08-30/sample_2022-08-30.parquet. This might happen if the file contains a row that is too large, or if the total size of the pages loaded for the queried columns is too large.&lt;/p&gt;\n\n&lt;p&gt;Is this modeling issue or I can just increase some BQ config?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxrjhf", "is_robot_indexable": true, "report_reasons": null, "author": "Leon_Bam", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxrjhf/store_wav_files_data_into_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxrjhf/store_wav_files_data_into_bigquery/", "subreddit_subscribers": 80215, "created_utc": 1668694773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I would like to propose an open-source metadata management and data governance tool in my company.\n\n   \nWe use Kubernetes as our deployment platform.   \nAny feedback on one of these open source data catalogs ?  \n  \\- [https://atlas.apache.org/#/](https://atlas.apache.org/#/)  \n  \\- [https://opendatadiscovery.org/](https://opendatadiscovery.org/)  \n  \\- [https://open-metadata.org/](https://open-metadata.org/)  \n  \\- [https://marquezproject.github.io/marquez/](https://marquezproject.github.io/marquez/)  \n  \\- [https://datahubproject.io/](https://datahubproject.io/)  \n  \\- [https://www.amundsen.io/](https://www.amundsen.io/)  \n  \\- [https://ckan.org/](https://ckan.org/)  \n  \\- [https://magda.io/](https://magda.io/)", "author_fullname": "t2_10vw8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Metadata Store - Which one to Choose ? OpenMetadata vs Datahub ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxrh9y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668694611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I would like to propose an open-source metadata management and data governance tool in my company.&lt;/p&gt;\n\n&lt;p&gt;We use Kubernetes as our deployment platform.&lt;br/&gt;\nAny feedback on one of these open source data catalogs ?&lt;br/&gt;\n  - &lt;a href=\"https://atlas.apache.org/#/\"&gt;https://atlas.apache.org/#/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://opendatadiscovery.org/\"&gt;https://opendatadiscovery.org/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://open-metadata.org/\"&gt;https://open-metadata.org/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://marquezproject.github.io/marquez/\"&gt;https://marquezproject.github.io/marquez/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://datahubproject.io/\"&gt;https://datahubproject.io/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://www.amundsen.io/\"&gt;https://www.amundsen.io/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://ckan.org/\"&gt;https://ckan.org/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://magda.io/\"&gt;https://magda.io/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxrh9y", "is_robot_indexable": true, "report_reasons": null, "author": "MoiSanh", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxrh9y/metadata_store_which_one_to_choose_openmetadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxrh9y/metadata_store_which_one_to_choose_openmetadata/", "subreddit_subscribers": 80215, "created_utc": 1668694611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If anyone interested in azure Synapse analytics,this tutorial will help you, thanks in advance\n\n https://link.medium.com/sEunVJMy1ub", "author_fullname": "t2_7ssutue8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synapse analytics Tutorial", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxkntn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668673218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If anyone interested in azure Synapse analytics,this tutorial will help you, thanks in advance&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://link.medium.com/sEunVJMy1ub\"&gt;https://link.medium.com/sEunVJMy1ub&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mwId_ZxwFXvY_WJrYEPl1b9DC_WbXdk_7zImch2WV6M.jpg?auto=webp&amp;s=f11dbc436c657ece1988f8a0696d30532e132605", "width": 1046, "height": 299}, "resolutions": [{"url": "https://external-preview.redd.it/mwId_ZxwFXvY_WJrYEPl1b9DC_WbXdk_7zImch2WV6M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7959609354bba23f902617c582da5965d3488ec5", "width": 108, "height": 30}, {"url": "https://external-preview.redd.it/mwId_ZxwFXvY_WJrYEPl1b9DC_WbXdk_7zImch2WV6M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a489b5381f4513bafa288a0d123687509d4c31b", "width": 216, "height": 61}, {"url": "https://external-preview.redd.it/mwId_ZxwFXvY_WJrYEPl1b9DC_WbXdk_7zImch2WV6M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=138c723f9a18485b0327662c489e61db4c372451", "width": 320, "height": 91}, {"url": "https://external-preview.redd.it/mwId_ZxwFXvY_WJrYEPl1b9DC_WbXdk_7zImch2WV6M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=29ff8d270da4084f76dd28acf9741b6071f2704a", "width": 640, "height": 182}, {"url": "https://external-preview.redd.it/mwId_ZxwFXvY_WJrYEPl1b9DC_WbXdk_7zImch2WV6M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=97732932532733856da7f96d63f541e14cc71e20", "width": 960, "height": 274}], "variants": {}, "id": "dvGCojG4WDv9ttbAGYv6q4tpdoIRh-oYoL_pelyJocU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yxkntn", "is_robot_indexable": true, "report_reasons": null, "author": "Ansam93", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxkntn/synapse_analytics_tutorial/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxkntn/synapse_analytics_tutorial/", "subreddit_subscribers": 80215, "created_utc": 1668673218.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6qpanfe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing YAML Data Modeling in Cube, the headless BI platform\u2014goodbye, JavaScript", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_yxtm3z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dcZl0R8JbX0fLO3neSWrO3Co6MlStxvLSTnQZaDOR6o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668700100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cube.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://cube.dev/blog/introducing-cube-support-for-yaml-data-modeling", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?auto=webp&amp;s=70f5d72a00d9591117dad065c302e8000bfae673", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=574bae08b02252c74b3eadf29a8e065c0b6da172", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6bef6a5f7c7a29634da9704d4078870a1add4b73", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d0c1e1b94073d4b1016ef19d6e685fb442133bb", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a2d8f39ec54712b2476b09190de32484f2bba2ba", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e87f295629c5589636e83c4ae41cb2341e6a1b3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e0f9cbe459cc97c6d9e0e75162490739ffef3996", "width": 1080, "height": 567}], "variants": {}, "id": "VtTqNdgZSXnTbk2mzaCyMvkS473GvRDuf-RnYWRn3JM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yxtm3z", "is_robot_indexable": true, "report_reasons": null, "author": "igorlukanin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxtm3z/introducing_yaml_data_modeling_in_cube_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://cube.dev/blog/introducing-cube-support-for-yaml-data-modeling", "subreddit_subscribers": 80215, "created_utc": 1668700100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_73sd7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone using StarRocks DB instead of ClickHouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_yxo6hd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/NcOQ8WSDZdQ_DEzTyqaMj2WlVRiwS881k2PBILrhIr8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668685584.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/StarRocks/StarRocks", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?auto=webp&amp;s=154e9fa3405ff9149ccffc08b770363fd2302b70", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=56bc3a6e306285fab823bc833ece979a93645d69", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d397ea2b0dd2e2229b1178f4693a615c414b7075", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0159b401ff7e1333a3ad97dd115fe8abc5c3a29e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=543cc6f54952604b6aaa1f5061bbe26397758647", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ffa8542f33ba8cc0bcfd97e537ecbd0ea83a1fc4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d5bfd99f2987fbe60113e4099b6ad26eefb9e1e8", "width": 1080, "height": 540}], "variants": {}, "id": "BB_cFx1_beS1zbZZ7YrZr7olAiCnnZknb9WMiVbKkyk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "yxo6hd", "is_robot_indexable": true, "report_reasons": null, "author": "intellidumb", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxo6hd/anyone_using_starrocks_db_instead_of_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/StarRocks/StarRocks", "subreddit_subscribers": 80215, "created_utc": 1668685584.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nWe have an Airflow MWAA cluster and huge volume of Data in our Redshift data warehouse. We currently process the data directly in Redshift (w/ SQL) but given the amount of data, this puts a lot of pressure in the data warehouse and it is less and less resilient.\n\nA potential solution we found would be to decouple the data storage (Redshift) from the data processing (Spark), first of all, what do you think about this solution?\n\nTo do this, we would like to use Airflow MWAA and SparkSQL to:\n\n\\- Transfer data from Redshift to Spark\n\n\\- Process the SQL scripts that were previously done in Redshift\n\n\\- Transfer the newly created table from Spark to Redshift  \n\n\nIs it a use case that someone here has already put in production?\n\n  \nWhat would in your opinion be the best way to interact with the Spark Cluster ? EmrAddStepsOperator vs PythonOperator + PySpark?\n\nThank you!", "author_fullname": "t2_f1ixi4vt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to process Redshift data on Spark (EMR) via Airflow MWAA ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxo2e1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668685220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;We have an Airflow MWAA cluster and huge volume of Data in our Redshift data warehouse. We currently process the data directly in Redshift (w/ SQL) but given the amount of data, this puts a lot of pressure in the data warehouse and it is less and less resilient.&lt;/p&gt;\n\n&lt;p&gt;A potential solution we found would be to decouple the data storage (Redshift) from the data processing (Spark), first of all, what do you think about this solution?&lt;/p&gt;\n\n&lt;p&gt;To do this, we would like to use Airflow MWAA and SparkSQL to:&lt;/p&gt;\n\n&lt;p&gt;- Transfer data from Redshift to Spark&lt;/p&gt;\n\n&lt;p&gt;- Process the SQL scripts that were previously done in Redshift&lt;/p&gt;\n\n&lt;p&gt;- Transfer the newly created table from Spark to Redshift  &lt;/p&gt;\n\n&lt;p&gt;Is it a use case that someone here has already put in production?&lt;/p&gt;\n\n&lt;p&gt;What would in your opinion be the best way to interact with the Spark Cluster ? EmrAddStepsOperator vs PythonOperator + PySpark?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yxo2e1", "is_robot_indexable": true, "report_reasons": null, "author": "No_Fudge1060", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxo2e1/best_way_to_process_redshift_data_on_spark_emr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxo2e1/best_way_to_process_redshift_data_on_spark_emr/", "subreddit_subscribers": 80215, "created_utc": 1668685220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have ELT job that has to loop through an API to extract all Invoice line items per invoice. I have made an API call for each invoice at a time  currently, the job can take up 3 hours to run. just needed suggestions and advice in improving the performance. I am thinking of using Spark to parallel processes against the API. If you have recommendations, please let me know.", "author_fullname": "t2_6mggwqyo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "API pagination performance issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxh4hk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668661245.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have ELT job that has to loop through an API to extract all Invoice line items per invoice. I have made an API call for each invoice at a time  currently, the job can take up 3 hours to run. just needed suggestions and advice in improving the performance. I am thinking of using Spark to parallel processes against the API. If you have recommendations, please let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yxh4hk", "is_robot_indexable": true, "report_reasons": null, "author": "Professional-Goal892", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxh4hk/api_pagination_performance_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxh4hk/api_pagination_performance_issues/", "subreddit_subscribers": 80215, "created_utc": 1668661245.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, I would like to gather important nuances in copying hdfs files to bigquery Or GCS. \n\nThere are 100 hive tables and it's data from Hdfs locations are needed to be pushed to GCP either BQ or GCS on a daily basis. What are the all points to be noted or challenges while copying via gsutil commands? Please advise.\n\nFlow would be from on prem to GCP only. \n\nNote : I need to perform gpg encryption before copy.", "author_fullname": "t2_4cullil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copy Hdfs files to Big query/GCS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx5d9o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668630883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, I would like to gather important nuances in copying hdfs files to bigquery Or GCS. &lt;/p&gt;\n\n&lt;p&gt;There are 100 hive tables and it&amp;#39;s data from Hdfs locations are needed to be pushed to GCP either BQ or GCS on a daily basis. What are the all points to be noted or challenges while copying via gsutil commands? Please advise.&lt;/p&gt;\n\n&lt;p&gt;Flow would be from on prem to GCP only. &lt;/p&gt;\n\n&lt;p&gt;Note : I need to perform gpg encryption before copy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yx5d9o", "is_robot_indexable": true, "report_reasons": null, "author": "tmanipra", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yx5d9o/copy_hdfs_files_to_big_querygcs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yx5d9o/copy_hdfs_files_to_big_querygcs/", "subreddit_subscribers": 80215, "created_utc": 1668630883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When I was applying for DA and DE positions over the summer the ones I prioritized applying for were the ones that clearly defined what responsibilities and task would be owned by the candidate chosen for the role.\n\nFor example at my previous Data Analyst positions I owned the SSRS and PowerBI reporting dashboards, the ETL processes to pull the data from the source systems onto our DW, the handling and prioritizing of report request tickets as they came in and integrating data from outside sources like legacy EHRs or flat CVS files into our DW.\n\nNow that I am working as a Data Engineer my ownership includes everything and anything to do with SSIS or Azure Data Factory, spinning up test environments via either Docker or HyperV Linux VMs, management of our Git Hub Organization (we are currently on the free tier but are thinking of upgrading to the \"teams\" one if anyone has any feedback on this that would be awesome), QA processes for the data being ETL. And everyone's favorite heavily documenting everything.\n\nThings I DONT own in this position are any DBA task aside from the test environments. A lot of our work is done on external clients databases so it's a constant game of tag asking their DBA for new permissions, creating specific DB roles, asking for more space etc. I also have nothing to do with the front end BI tool that our data warehouse connects to, and I don't orchestrate gaining access to outside data systems. Some of them have us use a Ctirix Desktop , others have a VPN set up so we can still work on our native desktop and connect to their DBs via our locally installed SSMS and Visual Studio. \n\nSo what do you own ?", "author_fullname": "t2_dfcot0zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some of the things you \"own\" in your job title.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxral1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668694131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I was applying for DA and DE positions over the summer the ones I prioritized applying for were the ones that clearly defined what responsibilities and task would be owned by the candidate chosen for the role.&lt;/p&gt;\n\n&lt;p&gt;For example at my previous Data Analyst positions I owned the SSRS and PowerBI reporting dashboards, the ETL processes to pull the data from the source systems onto our DW, the handling and prioritizing of report request tickets as they came in and integrating data from outside sources like legacy EHRs or flat CVS files into our DW.&lt;/p&gt;\n\n&lt;p&gt;Now that I am working as a Data Engineer my ownership includes everything and anything to do with SSIS or Azure Data Factory, spinning up test environments via either Docker or HyperV Linux VMs, management of our Git Hub Organization (we are currently on the free tier but are thinking of upgrading to the &amp;quot;teams&amp;quot; one if anyone has any feedback on this that would be awesome), QA processes for the data being ETL. And everyone&amp;#39;s favorite heavily documenting everything.&lt;/p&gt;\n\n&lt;p&gt;Things I DONT own in this position are any DBA task aside from the test environments. A lot of our work is done on external clients databases so it&amp;#39;s a constant game of tag asking their DBA for new permissions, creating specific DB roles, asking for more space etc. I also have nothing to do with the front end BI tool that our data warehouse connects to, and I don&amp;#39;t orchestrate gaining access to outside data systems. Some of them have us use a Ctirix Desktop , others have a VPN set up so we can still work on our native desktop and connect to their DBs via our locally installed SSMS and Visual Studio. &lt;/p&gt;\n\n&lt;p&gt;So what do you own ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxral1", "is_robot_indexable": true, "report_reasons": null, "author": "DrRedmondNYC", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxral1/what_are_some_of_the_things_you_own_in_your_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxral1/what_are_some_of_the_things_you_own_in_your_job/", "subreddit_subscribers": 80215, "created_utc": 1668694131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improve your first Airflow DAG for Beginners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_yxqu3h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/PbxI0Jyvlx0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Airflow DAG: Improving your first DAG for Beginners\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow DAG: Improving your first DAG for Beginners", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/PbxI0Jyvlx0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Airflow DAG: Improving your first DAG for Beginners\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/PbxI0Jyvlx0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/PbxI0Jyvlx0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Airflow DAG: Improving your first DAG for Beginners\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/yxqu3h", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Xn3WbAxnw799tDd9ijigLXGMdkE66MXWIr5Fn_WeJes.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668693043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/PbxI0Jyvlx0", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yLANk1T7rlyJ7SbNQWzMk5-TgMZUNaK55gIIyxg5_9g.jpg?auto=webp&amp;s=80cc9e1b851516520e19278f3f1ca46d85ee3cb5", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/yLANk1T7rlyJ7SbNQWzMk5-TgMZUNaK55gIIyxg5_9g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=37e4add9e698dfa4373452351ec8529b3f749208", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/yLANk1T7rlyJ7SbNQWzMk5-TgMZUNaK55gIIyxg5_9g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2700b9139284bbda6d9a3ae75e59fca2fefdbc7b", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/yLANk1T7rlyJ7SbNQWzMk5-TgMZUNaK55gIIyxg5_9g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e9f99bb280b7704c11ae4c9598f792d354f0ba6", "width": 320, "height": 240}], "variants": {}, "id": "3dfk3R-YGk3OQRrlA7g6DrrWa8GPsDh_-v1ao8Hj8x0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yxqu3h", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxqu3h/improve_your_first_airflow_dag_for_beginners/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/PbxI0Jyvlx0", "subreddit_subscribers": 80215, "created_utc": 1668693043.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow DAG: Improving your first DAG for Beginners", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/PbxI0Jyvlx0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Airflow DAG: Improving your first DAG for Beginners\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/PbxI0Jyvlx0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our team has been put in charge of creating a process to move customers from one product DB to another. \n\nThis is currently done using SSIS packages that are pretty much a black box to anyone wondering what logic they're using to move data.\n\nWe're building out a new workflow, I'd like to take a step back and consider another option.\n\nAs a software engineer I'd really like to be able to write test cases, do version control, all that good stuff while also documenting business logic of DB values and how they relate across products?", "author_fullname": "t2_exqc18fi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tool for creating a process to move customers from one product/DB to another", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxq1tz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668691157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our team has been put in charge of creating a process to move customers from one product DB to another. &lt;/p&gt;\n\n&lt;p&gt;This is currently done using SSIS packages that are pretty much a black box to anyone wondering what logic they&amp;#39;re using to move data.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re building out a new workflow, I&amp;#39;d like to take a step back and consider another option.&lt;/p&gt;\n\n&lt;p&gt;As a software engineer I&amp;#39;d really like to be able to write test cases, do version control, all that good stuff while also documenting business logic of DB values and how they relate across products?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxq1tz", "is_robot_indexable": true, "report_reasons": null, "author": "No-Swimming-3", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxq1tz/best_tool_for_creating_a_process_to_move/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxq1tz/best_tool_for_creating_a_process_to_move/", "subreddit_subscribers": 80215, "created_utc": 1668691157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm not interested in the cert,  just in the Content in the course.\n\n&amp;#x200B;\n\nDoes anyone know which is better?", "author_fullname": "t2_a9icd9le", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IBM Data Engineering Professional Certificate Or Meta Data Engineer Cert", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxnu68", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668684490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not interested in the cert,  just in the Content in the course.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Does anyone know which is better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxnu68", "is_robot_indexable": true, "report_reasons": null, "author": "Then_Landscape6474", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxnu68/ibm_data_engineering_professional_certificate_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxnu68/ibm_data_engineering_professional_certificate_or/", "subreddit_subscribers": 80215, "created_utc": 1668684490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Tomorrow I'm hosting [Ergest Xheblati](https://www.linkedin.com/in/ergestxheblati/) (Data Architect &amp; Author of [*Minimum Viable SQL Patterns*](https://ergestx.gumroad.com/l/sqlpatterns)) for a data modeling workshop - [How to Design a Lasting Business Blueprint](https://www.operationalanalytics.club/events/data-modeling-how-to-design-a-lasting-business-blueprint-w-ergest-xheblati?utm_campaign=Parker%20Rogers%20-%20Organic%20Social%20Campaign&amp;utm_source=reddit&amp;utm_medium=social&amp;utm_content=ergest-workshop-data-modeling)  \n\n\nIt's a workshop. Not a webinar. There's no SaaS tool being sold. It's an honest, educational opportunity hosted by a data architect with 15+ years experience.   \n\n\nIn this workshop, Ergest will teach:  \n\ud83c\udfaf How to conceptually model a fictional SaaS business (Create a conceptual model BEFORE the physical implementation)\n\n\ud83c\udfaf How the modeling approaches above will effect said SaaS business (One Big Table, Start Schema, Activity Schema, Data Vault, etc.)\n\n\ud83c\udfaf Why you might choose one approach or another (and the tradeoffs between them)\n\n\ud83c\udfaf How to move from one approach to the next if circumstances change  \n\n\nIf this interests you, sign up [here](https://www.operationalanalytics.club/events/data-modeling-how-to-design-a-lasting-business-blueprint-w-ergest-xheblati?utm_campaign=Parker%20Rogers%20-%20Organic%20Social%20Campaign&amp;utm_source=reddit&amp;utm_medium=social&amp;utm_content=ergest-workshop-data-modeling).", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling Workshop (A workshop, not a webinar w/ subjective hot takes \ud83e\udd23)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx10ab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668621819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tomorrow I&amp;#39;m hosting &lt;a href=\"https://www.linkedin.com/in/ergestxheblati/\"&gt;Ergest Xheblati&lt;/a&gt; (Data Architect &amp;amp; Author of &lt;a href=\"https://ergestx.gumroad.com/l/sqlpatterns\"&gt;&lt;em&gt;Minimum Viable SQL Patterns&lt;/em&gt;&lt;/a&gt;) for a data modeling workshop - &lt;a href=\"https://www.operationalanalytics.club/events/data-modeling-how-to-design-a-lasting-business-blueprint-w-ergest-xheblati?utm_campaign=Parker%20Rogers%20-%20Organic%20Social%20Campaign&amp;amp;utm_source=reddit&amp;amp;utm_medium=social&amp;amp;utm_content=ergest-workshop-data-modeling\"&gt;How to Design a Lasting Business Blueprint&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a workshop. Not a webinar. There&amp;#39;s no SaaS tool being sold. It&amp;#39;s an honest, educational opportunity hosted by a data architect with 15+ years experience.   &lt;/p&gt;\n\n&lt;p&gt;In this workshop, Ergest will teach:&lt;br/&gt;\n\ud83c\udfaf How to conceptually model a fictional SaaS business (Create a conceptual model BEFORE the physical implementation)&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udfaf How the modeling approaches above will effect said SaaS business (One Big Table, Start Schema, Activity Schema, Data Vault, etc.)&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udfaf Why you might choose one approach or another (and the tradeoffs between them)&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udfaf How to move from one approach to the next if circumstances change  &lt;/p&gt;\n\n&lt;p&gt;If this interests you, sign up &lt;a href=\"https://www.operationalanalytics.club/events/data-modeling-how-to-design-a-lasting-business-blueprint-w-ergest-xheblati?utm_campaign=Parker%20Rogers%20-%20Organic%20Social%20Campaign&amp;amp;utm_source=reddit&amp;amp;utm_medium=social&amp;amp;utm_content=ergest-workshop-data-modeling\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yx10ab", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yx10ab/data_modeling_workshop_a_workshop_not_a_webinar_w/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yx10ab/data_modeling_workshop_a_workshop_not_a_webinar_w/", "subreddit_subscribers": 80215, "created_utc": 1668621819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "HI all,\n\n&amp;#x200B;\n\nI'm getting to the point where I'm starting to apply for data engineer roles. I am currently an application engineer that utilizes SQL in order to perform necessary tasks. In terms of SQL, we utilize basic queries such as select, update, delete, joins, cases, creating SQL scripts, etc. in a Microsoft SQL Server. Unfortunately, this does not give me any experience that translates to data engineering other than utilizing SQL.\n\nI also obtained a Bachelor's in Computer Science degree where Python was our main language. \n\nSo far, I've learned about Power Bi, DBT, ETL, etc. through udemy courses or YouTube videos. Obviously, I don't have real-world experience with this yet. \n\nI just wanted to know if there is anything else I can do to make myself a better candidate. Are there any courses that will steer me in the right direction? How much more studying do I need to do? I've seen people go straight into the field out of college and learned these skillsets on the job. \n\nthank you!", "author_fullname": "t2_7gkhxmgl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I prepare for job interviews especially those that require to take home challenges?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx086z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668620189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;HI all,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m getting to the point where I&amp;#39;m starting to apply for data engineer roles. I am currently an application engineer that utilizes SQL in order to perform necessary tasks. In terms of SQL, we utilize basic queries such as select, update, delete, joins, cases, creating SQL scripts, etc. in a Microsoft SQL Server. Unfortunately, this does not give me any experience that translates to data engineering other than utilizing SQL.&lt;/p&gt;\n\n&lt;p&gt;I also obtained a Bachelor&amp;#39;s in Computer Science degree where Python was our main language. &lt;/p&gt;\n\n&lt;p&gt;So far, I&amp;#39;ve learned about Power Bi, DBT, ETL, etc. through udemy courses or YouTube videos. Obviously, I don&amp;#39;t have real-world experience with this yet. &lt;/p&gt;\n\n&lt;p&gt;I just wanted to know if there is anything else I can do to make myself a better candidate. Are there any courses that will steer me in the right direction? How much more studying do I need to do? I&amp;#39;ve seen people go straight into the field out of college and learned these skillsets on the job. &lt;/p&gt;\n\n&lt;p&gt;thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yx086z", "is_robot_indexable": true, "report_reasons": null, "author": "SnooWalruses7164", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yx086z/how_can_i_prepare_for_job_interviews_especially/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yx086z/how_can_i_prepare_for_job_interviews_especially/", "subreddit_subscribers": 80215, "created_utc": 1668620189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to write an Airflow DAG that exports from BigQuery to a GCS Bucket, and I am using the following resource: https://cloud.google.com/bigquery/docs/exporting-data#python\n\nNow, i have the following code:\n```\ndef test_extract_table(client, to_delete):\n    bucket_name = \"extract_shakespeare_{}\".format(_millis())\n    storage_client = storage.Client()\n    bucket = retry_storage_errors(storage_client.create_bucket)(bucket_name)\n    to_delete.append(bucket)\n```\nHowever. I am unsure about what this line of code means:\n```\nbucket_name = \"extract_shakespeare_{}\".format(_millis())\n```", "author_fullname": "t2_13ussz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does this line of code mean?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yxthn8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668699787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to write an Airflow DAG that exports from BigQuery to a GCS Bucket, and I am using the following resource: &lt;a href=\"https://cloud.google.com/bigquery/docs/exporting-data#python\"&gt;https://cloud.google.com/bigquery/docs/exporting-data#python&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Now, i have the following code:\n&lt;code&gt;\ndef test_extract_table(client, to_delete):\n    bucket_name = &amp;quot;extract_shakespeare_{}&amp;quot;.format(_millis())\n    storage_client = storage.Client()\n    bucket = retry_storage_errors(storage_client.create_bucket)(bucket_name)\n    to_delete.append(bucket)\n&lt;/code&gt;\nHowever. I am unsure about what this line of code means:\n&lt;code&gt;\nbucket_name = &amp;quot;extract_shakespeare_{}&amp;quot;.format(_millis())\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?auto=webp&amp;s=ada93f0d146c09556ac26cc3fa125a0aa7102150", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=05af26b5ec35c95ed95b5c40dbde3c1cc04dce06", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f36f0de65cdcbed3b705b8446710c7c83e0475e4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4067aebaadaec227b271d9c19c7af833c230fd32", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec46bff13e5c5bc616be11b375484d9d2a7fcbe8", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5db472c15d5717384ab8f8f64e9fd89efe70fa59", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/cmUpHS82A2NmQUetG5xkM6py3mx8MKKYEYyam7WGO4M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fab49c1958487e16d598ede6d81140177c5c9a31", "width": 1080, "height": 567}], "variants": {}, "id": "DsiOIzUSicS_9zIKwMDQbNT2LOE1o29sSYs49HAmO_k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yxthn8", "is_robot_indexable": true, "report_reasons": null, "author": "saucyhambon", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxthn8/what_does_this_line_of_code_mean/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxthn8/what_does_this_line_of_code_mean/", "subreddit_subscribers": 80215, "created_utc": 1668699787.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nCurrently I have 2+ yoe, a master\u2019s degree in engineering ( non-CS) and have been working in a F500 modern data stack company for 2 years. I started as a data analyst ( Covid lack of options), and have been a data engineer ( promoted from junior to mid level recently).  I currently make 93k. \n\nI was searching for new jobs and I have been interviewing pretty well with leetcode for sql/ python. I received one offer for 121k which was immediately revoked because the team decided \u201cto go in a different direction\u201d this week. I received a second offer for a tax preparation small firm for 132k, but I found our they are being  acquired by a Private Equity firm. The offer is great and I would learn a lot, but I feel conflicted as the PE firm was fined multiple times for medication theft/fraud by the NHS. I think I will reject the offer and wait for a less ethical fraught \noffer? Thoughts", "author_fullname": "t2_tsrtqcem", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ethical Concerns with Offer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx1r12", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668623344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;Currently I have 2+ yoe, a master\u2019s degree in engineering ( non-CS) and have been working in a F500 modern data stack company for 2 years. I started as a data analyst ( Covid lack of options), and have been a data engineer ( promoted from junior to mid level recently).  I currently make 93k. &lt;/p&gt;\n\n&lt;p&gt;I was searching for new jobs and I have been interviewing pretty well with leetcode for sql/ python. I received one offer for 121k which was immediately revoked because the team decided \u201cto go in a different direction\u201d this week. I received a second offer for a tax preparation small firm for 132k, but I found our they are being  acquired by a Private Equity firm. The offer is great and I would learn a lot, but I feel conflicted as the PE firm was fined multiple times for medication theft/fraud by the NHS. I think I will reject the offer and wait for a less ethical fraught \noffer? Thoughts&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yx1r12", "is_robot_indexable": true, "report_reasons": null, "author": "CookingGoBlue", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yx1r12/ethical_concerns_with_offer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yx1r12/ethical_concerns_with_offer/", "subreddit_subscribers": 80215, "created_utc": 1668623344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. My company after two years of pandemic want to switch back to hybrid work (1-2 days in a week in office). \nFor my all team this change do not make sense. Tomorrow we have a call with boss of my boss. \n\nDo you have any ideas of how to argument that we want to work remotely?\n\nDid any of you have similar situation?", "author_fullname": "t2_50lq8gs1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching from remote to hybrid", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxm64i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668678681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. My company after two years of pandemic want to switch back to hybrid work (1-2 days in a week in office). \nFor my all team this change do not make sense. Tomorrow we have a call with boss of my boss. &lt;/p&gt;\n\n&lt;p&gt;Do you have any ideas of how to argument that we want to work remotely?&lt;/p&gt;\n\n&lt;p&gt;Did any of you have similar situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxm64i", "is_robot_indexable": true, "report_reasons": null, "author": "truverol", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxm64i/switching_from_remote_to_hybrid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxm64i/switching_from_remote_to_hybrid/", "subreddit_subscribers": 80215, "created_utc": 1668678681.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}