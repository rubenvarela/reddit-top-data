{"kind": "Listing", "data": {"after": null, "dist": 20, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Zach wilson on linkedin said that rust is going to have a great future in data engineering, what do you think about that?", "author_fullname": "t2_th46eirx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rust in data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxj5fz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668667723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Zach wilson on linkedin said that rust is going to have a great future in data engineering, what do you think about that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxj5fz", "is_robot_indexable": true, "report_reasons": null, "author": "PopTheTenYasha", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxj5fz/rust_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxj5fz/rust_in_data_engineering/", "subreddit_subscribers": 80227, "created_utc": 1668667723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently use python operator and have a bunch of processes that would run at same time. We don't want the processes to contend for resources and one high memory process bringing down the entire airflow server. How can this be handled in Airflow? \n\nIs kubernates operator right solution for managing resources effectively? How are things handled in Airflow in your organization? Thanks.", "author_fullname": "t2_jfqnf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what is your Airflow architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxe8au", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668652943.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently use python operator and have a bunch of processes that would run at same time. We don&amp;#39;t want the processes to contend for resources and one high memory process bringing down the entire airflow server. How can this be handled in Airflow? &lt;/p&gt;\n\n&lt;p&gt;Is kubernates operator right solution for managing resources effectively? How are things handled in Airflow in your organization? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxe8au", "is_robot_indexable": true, "report_reasons": null, "author": "curidpostn", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxe8au/what_is_your_airflow_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxe8au/what_is_your_airflow_architecture/", "subreddit_subscribers": 80227, "created_utc": 1668652943.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_31ilk7t8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake announces Alerts in private preview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": false, "name": "t3_yxbgcd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 18, "total_awards_received": 0, "media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Now in private preview on &lt;a href=\"https://twitter.com/SnowflakeDB?ref_src=twsrc%5Etfw\"&gt;@SnowflakeDB&lt;/a&gt;, you can set up an alert that periodically performs an action under specific conditions, based on data within Snowflake. &lt;a href=\"https://t.co/R2wBkvZK7N\"&gt;pic.twitter.com/R2wBkvZK7N&lt;/a&gt;&lt;/p&gt;&amp;mdash; SELECT (@select_dev) &lt;a href=\"https://twitter.com/select_dev/status/1593029267700723712?ref_src=twsrc%5Etfw\"&gt;November 16, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/select_dev/status/1593029267700723712", "author_name": "SELECT", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Now in private preview on &lt;a href=\"https://twitter.com/SnowflakeDB?ref_src=twsrc%5Etfw\"&gt;@SnowflakeDB&lt;/a&gt;, you can set up an alert that periodically performs an action under specific conditions, based on data within Snowflake. &lt;a href=\"https://t.co/R2wBkvZK7N\"&gt;pic.twitter.com/R2wBkvZK7N&lt;/a&gt;&lt;/p&gt;&amp;mdash; SELECT (@select_dev) &lt;a href=\"https://twitter.com/select_dev/status/1593029267700723712?ref_src=twsrc%5Etfw\"&gt;November 16, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/select_dev", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Now in private preview on &lt;a href=\"https://twitter.com/SnowflakeDB?ref_src=twsrc%5Etfw\"&gt;@SnowflakeDB&lt;/a&gt;, you can set up an alert that periodically performs an action under specific conditions, based on data within Snowflake. &lt;a href=\"https://t.co/R2wBkvZK7N\"&gt;pic.twitter.com/R2wBkvZK7N&lt;/a&gt;&lt;/p&gt;&amp;mdash; SELECT (@select_dev) &lt;a href=\"https://twitter.com/select_dev/status/1593029267700723712?ref_src=twsrc%5Etfw\"&gt;November 16, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/yxbgcd", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/h-EKhX84KuPV-JEav9G5IkZ2LGidrionX03Q2i1uyfQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668645509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "twitter.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://twitter.com/select_dev/status/1593029267700723712", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p7R-c37C3OglF6slv6i60wsm8BxwO4fIiQaGjDTkwTU.jpg?auto=webp&amp;s=9c66b26fb8147f42fb5fe588afd4724a70861c56", "width": 140, "height": 88}, "resolutions": [{"url": "https://external-preview.redd.it/p7R-c37C3OglF6slv6i60wsm8BxwO4fIiQaGjDTkwTU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9d6c48a84e4d3ae0bf7852508419ab98a7e09214", "width": 108, "height": 67}], "variants": {}, "id": "RDb0ZIEZiY5QRmP6koPKGj5NByF3KV1F3PLEhIxT-GQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxbgcd", "is_robot_indexable": true, "report_reasons": null, "author": "ian-whitestone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxbgcd/snowflake_announces_alerts_in_private_preview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://twitter.com/select_dev/status/1593029267700723712", "subreddit_subscribers": 80227, "created_utc": 1668645509.0, "num_crossposts": 1, "media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/select_dev/status/1593029267700723712", "author_name": "SELECT", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Now in private preview on &lt;a href=\"https://twitter.com/SnowflakeDB?ref_src=twsrc%5Etfw\"&gt;@SnowflakeDB&lt;/a&gt;, you can set up an alert that periodically performs an action under specific conditions, based on data within Snowflake. &lt;a href=\"https://t.co/R2wBkvZK7N\"&gt;pic.twitter.com/R2wBkvZK7N&lt;/a&gt;&lt;/p&gt;&amp;mdash; SELECT (@select_dev) &lt;a href=\"https://twitter.com/select_dev/status/1593029267700723712?ref_src=twsrc%5Etfw\"&gt;November 16, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/select_dev", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am transferring GA data from big query to snowflake. I am using DBT to flatten each of the JSON columns. I am stuck on custom dimensions. I can parse through each element but how can I loop through them using DBT? Given that they may have n array elements.", "author_fullname": "t2_5d6daxht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone worked on data transfer from GC Bq to snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx7m4s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668635989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am transferring GA data from big query to snowflake. I am using DBT to flatten each of the JSON columns. I am stuck on custom dimensions. I can parse through each element but how can I loop through them using DBT? Given that they may have n array elements.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yx7m4s", "is_robot_indexable": true, "report_reasons": null, "author": "sankalpthakur2610", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yx7m4s/has_anyone_worked_on_data_transfer_from_gc_bq_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yx7m4s/has_anyone_worked_on_data_transfer_from_gc_bq_to/", "subreddit_subscribers": 80227, "created_utc": 1668635989.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I would like to propose an open-source metadata management and data governance tool in my company.\n\n   \nWe use Kubernetes as our deployment platform.   \nAny feedback on one of these open source data catalogs ?  \n  \\- [https://atlas.apache.org/#/](https://atlas.apache.org/#/)  \n  \\- [https://opendatadiscovery.org/](https://opendatadiscovery.org/)  \n  \\- [https://open-metadata.org/](https://open-metadata.org/)  \n  \\- [https://marquezproject.github.io/marquez/](https://marquezproject.github.io/marquez/)  \n  \\- [https://datahubproject.io/](https://datahubproject.io/)  \n  \\- [https://www.amundsen.io/](https://www.amundsen.io/)  \n  \\- [https://ckan.org/](https://ckan.org/)  \n  \\- [https://magda.io/](https://magda.io/)", "author_fullname": "t2_10vw8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Metadata Store - Which one to Choose ? OpenMetadata vs Datahub ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxrh9y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668694611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I would like to propose an open-source metadata management and data governance tool in my company.&lt;/p&gt;\n\n&lt;p&gt;We use Kubernetes as our deployment platform.&lt;br/&gt;\nAny feedback on one of these open source data catalogs ?&lt;br/&gt;\n  - &lt;a href=\"https://atlas.apache.org/#/\"&gt;https://atlas.apache.org/#/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://opendatadiscovery.org/\"&gt;https://opendatadiscovery.org/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://open-metadata.org/\"&gt;https://open-metadata.org/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://marquezproject.github.io/marquez/\"&gt;https://marquezproject.github.io/marquez/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://datahubproject.io/\"&gt;https://datahubproject.io/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://www.amundsen.io/\"&gt;https://www.amundsen.io/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://ckan.org/\"&gt;https://ckan.org/&lt;/a&gt;&lt;br/&gt;\n  - &lt;a href=\"https://magda.io/\"&gt;https://magda.io/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxrh9y", "is_robot_indexable": true, "report_reasons": null, "author": "MoiSanh", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxrh9y/metadata_store_which_one_to_choose_openmetadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxrh9y/metadata_store_which_one_to_choose_openmetadata/", "subreddit_subscribers": 80227, "created_utc": 1668694611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you guys take care of timestamp columns during daylight savings off for streaming data pipelines ?, We had an issue where because of daylight savings off , timestamp difference between source and target is different as time will fall back. How do you solve this in your team/company ? TIA", "author_fullname": "t2_3ep37bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Daylight savings off - Data Issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxq8rl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668691663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you guys take care of timestamp columns during daylight savings off for streaming data pipelines ?, We had an issue where because of daylight savings off , timestamp difference between source and target is different as time will fall back. How do you solve this in your team/company ? TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxq8rl", "is_robot_indexable": true, "report_reasons": null, "author": "chanu4dincha", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxq8rl/daylight_savings_off_data_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxq8rl/daylight_savings_off_data_issue/", "subreddit_subscribers": 80227, "created_utc": 1668691663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ez8d4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MotherDuck scores $47.5m to prove scale-up databases are not quackers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_yxo09b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xDt98sOzCHeuymb3U-ihUEOVA7co6_K0D0fa9Ztn8eI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668685031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "theregister.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.theregister.com/2022/11/17/475_million_says_scaleup_databases/?utm_medium=share&amp;utm_content=article&amp;utm_source=reddit", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KMt0zuiV34OZe0m-dQtNN6jf029X6W3nWej8bLwCUsc.jpg?auto=webp&amp;s=0dfa7d8b0152756a1773dc38b2ce42b98316c558", "width": 648, "height": 429}, "resolutions": [{"url": "https://external-preview.redd.it/KMt0zuiV34OZe0m-dQtNN6jf029X6W3nWej8bLwCUsc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8bafdd5d56d30718219f5dd49d8334eb00a881d0", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/KMt0zuiV34OZe0m-dQtNN6jf029X6W3nWej8bLwCUsc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=04d45f0757d64120d60590dc6279ac82a04cfc9c", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/KMt0zuiV34OZe0m-dQtNN6jf029X6W3nWej8bLwCUsc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=35f359e75f60d3e14dda780dbee0f3473056d3a5", "width": 320, "height": 211}, {"url": "https://external-preview.redd.it/KMt0zuiV34OZe0m-dQtNN6jf029X6W3nWej8bLwCUsc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce6a0f7efba4a56836e229bd30394b60e4b8fb32", "width": 640, "height": 423}], "variants": {}, "id": "C4Zahor1I_MVU7Ugl_ADOspvaGrRqoIqcYaEHgflLTM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "yxo09b", "is_robot_indexable": true, "report_reasons": null, "author": "Bazencourt", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxo09b/motherduck_scores_475m_to_prove_scaleup_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.theregister.com/2022/11/17/475_million_says_scaleup_databases/?utm_medium=share&amp;utm_content=article&amp;utm_source=reddit", "subreddit_subscribers": 80227, "created_utc": 1668685031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is working with vibrations analytics. We are manually measure some component and got WAV file. In order to analysis it, we convert the content to numpy array. In the future we are going to receive this data from a sensor and need to convert the data to a unified schema.\n\nThis array might be really large ( &gt; 500K items) and we also need to record some metadata on each measurement. This additional data can be measured\\_time, component, site of the record...\n\nThe question is what will be the best way to store it to answer on queries like: give me all the measurements from 2020-01-01 for the component \"motor\"?\n\nWhat we tried so far:\n\n1. Store in relational DB with star schema where the facts table contains a file name. Later we download the relevant content. This can't work since we might get data in JSON array format without a compound file.\n2. Store the measurements in a large Parquet files in a Hive format in GCS. Create a BigQuery table with id and measurement content as binary data in Hive structure. Later, move the metadata to BigQuery as well and join it with the previous table.This is better since we don't use files anymore and can handle any format of data. However, we reach the BigQuery limit on the size of the data ( the data can be really large)\n\nError:\n\ngoogle.api\\_core.exceptions.BadRequest: 400 Resources exceeded during query execution: Out of memory. Failed to import values for column 'content' This might happen if the file contains a row that is too large, or if the total size of the pages loaded for the queried columns is too large.; Failed to read Parquet file /bigstore/vibrations-samples/date=2022-08-30/sample\\_2022-08-30.parquet. This might happen if the file contains a row that is too large, or if the total size of the pages loaded for the queried columns is too large.\n\nIs this modeling issue or I can just increase some BQ config?\n\nThanks", "author_fullname": "t2_ll7atfr1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Store WAV files data into BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxrjhf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668695285.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668694773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is working with vibrations analytics. We are manually measure some component and got WAV file. In order to analysis it, we convert the content to numpy array. In the future we are going to receive this data from a sensor and need to convert the data to a unified schema.&lt;/p&gt;\n\n&lt;p&gt;This array might be really large ( &amp;gt; 500K items) and we also need to record some metadata on each measurement. This additional data can be measured_time, component, site of the record...&lt;/p&gt;\n\n&lt;p&gt;The question is what will be the best way to store it to answer on queries like: give me all the measurements from 2020-01-01 for the component &amp;quot;motor&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;What we tried so far:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Store in relational DB with star schema where the facts table contains a file name. Later we download the relevant content. This can&amp;#39;t work since we might get data in JSON array format without a compound file.&lt;/li&gt;\n&lt;li&gt;Store the measurements in a large Parquet files in a Hive format in GCS. Create a BigQuery table with id and measurement content as binary data in Hive structure. Later, move the metadata to BigQuery as well and join it with the previous table.This is better since we don&amp;#39;t use files anymore and can handle any format of data. However, we reach the BigQuery limit on the size of the data ( the data can be really large)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Error:&lt;/p&gt;\n\n&lt;p&gt;google.api_core.exceptions.BadRequest: 400 Resources exceeded during query execution: Out of memory. Failed to import values for column &amp;#39;content&amp;#39; This might happen if the file contains a row that is too large, or if the total size of the pages loaded for the queried columns is too large.; Failed to read Parquet file /bigstore/vibrations-samples/date=2022-08-30/sample_2022-08-30.parquet. This might happen if the file contains a row that is too large, or if the total size of the pages loaded for the queried columns is too large.&lt;/p&gt;\n\n&lt;p&gt;Is this modeling issue or I can just increase some BQ config?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxrjhf", "is_robot_indexable": true, "report_reasons": null, "author": "Leon_Bam", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxrjhf/store_wav_files_data_into_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxrjhf/store_wav_files_data_into_bigquery/", "subreddit_subscribers": 80227, "created_utc": 1668694773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_73sd7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone using StarRocks DB instead of ClickHouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_yxo6hd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/NcOQ8WSDZdQ_DEzTyqaMj2WlVRiwS881k2PBILrhIr8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668685584.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/StarRocks/StarRocks", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?auto=webp&amp;s=154e9fa3405ff9149ccffc08b770363fd2302b70", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=56bc3a6e306285fab823bc833ece979a93645d69", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d397ea2b0dd2e2229b1178f4693a615c414b7075", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0159b401ff7e1333a3ad97dd115fe8abc5c3a29e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=543cc6f54952604b6aaa1f5061bbe26397758647", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ffa8542f33ba8cc0bcfd97e537ecbd0ea83a1fc4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/cIND3gDBPY7foVBGQaDVwtHzSBhHgHf781MjGiBpVpQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d5bfd99f2987fbe60113e4099b6ad26eefb9e1e8", "width": 1080, "height": 540}], "variants": {}, "id": "BB_cFx1_beS1zbZZ7YrZr7olAiCnnZknb9WMiVbKkyk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "yxo6hd", "is_robot_indexable": true, "report_reasons": null, "author": "intellidumb", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxo6hd/anyone_using_starrocks_db_instead_of_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/StarRocks/StarRocks", "subreddit_subscribers": 80227, "created_utc": 1668685584.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nWe have an Airflow MWAA cluster and huge volume of Data in our Redshift data warehouse. We currently process the data directly in Redshift (w/ SQL) but given the amount of data, this puts a lot of pressure in the data warehouse and it is less and less resilient.\n\nA potential solution we found would be to decouple the data storage (Redshift) from the data processing (Spark), first of all, what do you think about this solution?\n\nTo do this, we would like to use Airflow MWAA and SparkSQL to:\n\n\\- Transfer data from Redshift to Spark\n\n\\- Process the SQL scripts that were previously done in Redshift\n\n\\- Transfer the newly created table from Spark to Redshift  \n\n\nIs it a use case that someone here has already put in production?\n\n  \nWhat would in your opinion be the best way to interact with the Spark Cluster ? EmrAddStepsOperator vs PythonOperator + PySpark?\n\nThank you!", "author_fullname": "t2_f1ixi4vt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to process Redshift data on Spark (EMR) via Airflow MWAA ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxo2e1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668685220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;We have an Airflow MWAA cluster and huge volume of Data in our Redshift data warehouse. We currently process the data directly in Redshift (w/ SQL) but given the amount of data, this puts a lot of pressure in the data warehouse and it is less and less resilient.&lt;/p&gt;\n\n&lt;p&gt;A potential solution we found would be to decouple the data storage (Redshift) from the data processing (Spark), first of all, what do you think about this solution?&lt;/p&gt;\n\n&lt;p&gt;To do this, we would like to use Airflow MWAA and SparkSQL to:&lt;/p&gt;\n\n&lt;p&gt;- Transfer data from Redshift to Spark&lt;/p&gt;\n\n&lt;p&gt;- Process the SQL scripts that were previously done in Redshift&lt;/p&gt;\n\n&lt;p&gt;- Transfer the newly created table from Spark to Redshift  &lt;/p&gt;\n\n&lt;p&gt;Is it a use case that someone here has already put in production?&lt;/p&gt;\n\n&lt;p&gt;What would in your opinion be the best way to interact with the Spark Cluster ? EmrAddStepsOperator vs PythonOperator + PySpark?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yxo2e1", "is_robot_indexable": true, "report_reasons": null, "author": "No_Fudge1060", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxo2e1/best_way_to_process_redshift_data_on_spark_emr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxo2e1/best_way_to_process_redshift_data_on_spark_emr/", "subreddit_subscribers": 80227, "created_utc": 1668685220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If anyone interested in azure Synapse analytics,this tutorial will help you, thanks in advance\n\n https://link.medium.com/sEunVJMy1ub", "author_fullname": "t2_7ssutue8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synapse analytics Tutorial", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxkntn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668673218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If anyone interested in azure Synapse analytics,this tutorial will help you, thanks in advance&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://link.medium.com/sEunVJMy1ub\"&gt;https://link.medium.com/sEunVJMy1ub&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mwId_ZxwFXvY_WJrYEPl1b9DC_WbXdk_7zImch2WV6M.jpg?auto=webp&amp;s=f11dbc436c657ece1988f8a0696d30532e132605", "width": 1046, "height": 299}, "resolutions": [{"url": "https://external-preview.redd.it/mwId_ZxwFXvY_WJrYEPl1b9DC_WbXdk_7zImch2WV6M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7959609354bba23f902617c582da5965d3488ec5", "width": 108, "height": 30}, {"url": "https://external-preview.redd.it/mwId_ZxwFXvY_WJrYEPl1b9DC_WbXdk_7zImch2WV6M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a489b5381f4513bafa288a0d123687509d4c31b", "width": 216, "height": 61}, {"url": "https://external-preview.redd.it/mwId_ZxwFXvY_WJrYEPl1b9DC_WbXdk_7zImch2WV6M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=138c723f9a18485b0327662c489e61db4c372451", "width": 320, "height": 91}, {"url": "https://external-preview.redd.it/mwId_ZxwFXvY_WJrYEPl1b9DC_WbXdk_7zImch2WV6M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=29ff8d270da4084f76dd28acf9741b6071f2704a", "width": 640, "height": 182}, {"url": "https://external-preview.redd.it/mwId_ZxwFXvY_WJrYEPl1b9DC_WbXdk_7zImch2WV6M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=97732932532733856da7f96d63f541e14cc71e20", "width": 960, "height": 274}], "variants": {}, "id": "dvGCojG4WDv9ttbAGYv6q4tpdoIRh-oYoL_pelyJocU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yxkntn", "is_robot_indexable": true, "report_reasons": null, "author": "Ansam93", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxkntn/synapse_analytics_tutorial/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxkntn/synapse_analytics_tutorial/", "subreddit_subscribers": 80227, "created_utc": 1668673218.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have ELT job that has to loop through an API to extract all Invoice line items per invoice. I have made an API call for each invoice at a time  currently, the job can take up 3 hours to run. just needed suggestions and advice in improving the performance. I am thinking of using Spark to parallel processes against the API. If you have recommendations, please let me know.", "author_fullname": "t2_6mggwqyo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "API pagination performance issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxh4hk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668661245.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have ELT job that has to loop through an API to extract all Invoice line items per invoice. I have made an API call for each invoice at a time  currently, the job can take up 3 hours to run. just needed suggestions and advice in improving the performance. I am thinking of using Spark to parallel processes against the API. If you have recommendations, please let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yxh4hk", "is_robot_indexable": true, "report_reasons": null, "author": "Professional-Goal892", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxh4hk/api_pagination_performance_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxh4hk/api_pagination_performance_issues/", "subreddit_subscribers": 80227, "created_utc": 1668661245.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, I would like to gather important nuances in copying hdfs files to bigquery Or GCS. \n\nThere are 100 hive tables and it's data from Hdfs locations are needed to be pushed to GCP either BQ or GCS on a daily basis. What are the all points to be noted or challenges while copying via gsutil commands? Please advise.\n\nFlow would be from on prem to GCP only. \n\nNote : I need to perform gpg encryption before copy.", "author_fullname": "t2_4cullil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copy Hdfs files to Big query/GCS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yx5d9o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668630883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, I would like to gather important nuances in copying hdfs files to bigquery Or GCS. &lt;/p&gt;\n\n&lt;p&gt;There are 100 hive tables and it&amp;#39;s data from Hdfs locations are needed to be pushed to GCP either BQ or GCS on a daily basis. What are the all points to be noted or challenges while copying via gsutil commands? Please advise.&lt;/p&gt;\n\n&lt;p&gt;Flow would be from on prem to GCP only. &lt;/p&gt;\n\n&lt;p&gt;Note : I need to perform gpg encryption before copy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yx5d9o", "is_robot_indexable": true, "report_reasons": null, "author": "tmanipra", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yx5d9o/copy_hdfs_files_to_big_querygcs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yx5d9o/copy_hdfs_files_to_big_querygcs/", "subreddit_subscribers": 80227, "created_utc": 1668630883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nMy company is looking to extract data from SQL Server and Oracle to Snowflake. What are some pros and cons of these tools?\n\n\\- Matillion\n\n\\- Fivetran\n\n\\- Wherescape\n\n\\- ADF\n\n\\- or any other?\n\nPreferably looking for one tool to fulfill the whole ELT process if possible.", "author_fullname": "t2_r0krk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELT Stack to Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yxxrm4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668712840.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668710152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;My company is looking to extract data from SQL Server and Oracle to Snowflake. What are some pros and cons of these tools?&lt;/p&gt;\n\n&lt;p&gt;- Matillion&lt;/p&gt;\n\n&lt;p&gt;- Fivetran&lt;/p&gt;\n\n&lt;p&gt;- Wherescape&lt;/p&gt;\n\n&lt;p&gt;- ADF&lt;/p&gt;\n\n&lt;p&gt;- or any other?&lt;/p&gt;\n\n&lt;p&gt;Preferably looking for one tool to fulfill the whole ELT process if possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxxrm4", "is_robot_indexable": true, "report_reasons": null, "author": "sizzlepoop", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxxrm4/elt_stack_to_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxxrm4/elt_stack_to_snowflake/", "subreddit_subscribers": 80227, "created_utc": 1668710152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6qpanfe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing YAML Data Modeling in Cube, the headless BI platform\u2014goodbye, JavaScript", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_yxtm3z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dcZl0R8JbX0fLO3neSWrO3Co6MlStxvLSTnQZaDOR6o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668700100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cube.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://cube.dev/blog/introducing-cube-support-for-yaml-data-modeling", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?auto=webp&amp;s=70f5d72a00d9591117dad065c302e8000bfae673", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=574bae08b02252c74b3eadf29a8e065c0b6da172", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6bef6a5f7c7a29634da9704d4078870a1add4b73", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d0c1e1b94073d4b1016ef19d6e685fb442133bb", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a2d8f39ec54712b2476b09190de32484f2bba2ba", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e87f295629c5589636e83c4ae41cb2341e6a1b3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/NB2qyp82AET0ff_tvlpuH5r_9GPEqCHk7NyB-XM_3rg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e0f9cbe459cc97c6d9e0e75162490739ffef3996", "width": 1080, "height": 567}], "variants": {}, "id": "VtTqNdgZSXnTbk2mzaCyMvkS473GvRDuf-RnYWRn3JM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yxtm3z", "is_robot_indexable": true, "report_reasons": null, "author": "igorlukanin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxtm3z/introducing_yaml_data_modeling_in_cube_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://cube.dev/blog/introducing-cube-support-for-yaml-data-modeling", "subreddit_subscribers": 80227, "created_utc": 1668700100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When I was applying for DA and DE positions over the summer the ones I prioritized applying for were the ones that clearly defined what responsibilities and task would be owned by the candidate chosen for the role.\n\nFor example at my previous Data Analyst positions I owned the SSRS and PowerBI reporting dashboards, the ETL processes to pull the data from the source systems onto our DW, the handling and prioritizing of report request tickets as they came in and integrating data from outside sources like legacy EHRs or flat CVS files into our DW.\n\nNow that I am working as a Data Engineer my ownership includes everything and anything to do with SSIS or Azure Data Factory, spinning up test environments via either Docker or HyperV Linux VMs, management of our Git Hub Organization (we are currently on the free tier but are thinking of upgrading to the \"teams\" one if anyone has any feedback on this that would be awesome), QA processes for the data being ETL. And everyone's favorite heavily documenting everything.\n\nThings I DONT own in this position are any DBA task aside from the test environments. A lot of our work is done on external clients databases so it's a constant game of tag asking their DBA for new permissions, creating specific DB roles, asking for more space etc. I also have nothing to do with the front end BI tool that our data warehouse connects to, and I don't orchestrate gaining access to outside data systems. Some of them have us use a Ctirix Desktop , others have a VPN set up so we can still work on our native desktop and connect to their DBs via our locally installed SSMS and Visual Studio. \n\nSo what do you own ?", "author_fullname": "t2_dfcot0zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some of the things you \"own\" in your job title.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxral1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668694131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I was applying for DA and DE positions over the summer the ones I prioritized applying for were the ones that clearly defined what responsibilities and task would be owned by the candidate chosen for the role.&lt;/p&gt;\n\n&lt;p&gt;For example at my previous Data Analyst positions I owned the SSRS and PowerBI reporting dashboards, the ETL processes to pull the data from the source systems onto our DW, the handling and prioritizing of report request tickets as they came in and integrating data from outside sources like legacy EHRs or flat CVS files into our DW.&lt;/p&gt;\n\n&lt;p&gt;Now that I am working as a Data Engineer my ownership includes everything and anything to do with SSIS or Azure Data Factory, spinning up test environments via either Docker or HyperV Linux VMs, management of our Git Hub Organization (we are currently on the free tier but are thinking of upgrading to the &amp;quot;teams&amp;quot; one if anyone has any feedback on this that would be awesome), QA processes for the data being ETL. And everyone&amp;#39;s favorite heavily documenting everything.&lt;/p&gt;\n\n&lt;p&gt;Things I DONT own in this position are any DBA task aside from the test environments. A lot of our work is done on external clients databases so it&amp;#39;s a constant game of tag asking their DBA for new permissions, creating specific DB roles, asking for more space etc. I also have nothing to do with the front end BI tool that our data warehouse connects to, and I don&amp;#39;t orchestrate gaining access to outside data systems. Some of them have us use a Ctirix Desktop , others have a VPN set up so we can still work on our native desktop and connect to their DBs via our locally installed SSMS and Visual Studio. &lt;/p&gt;\n\n&lt;p&gt;So what do you own ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxral1", "is_robot_indexable": true, "report_reasons": null, "author": "DrRedmondNYC", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxral1/what_are_some_of_the_things_you_own_in_your_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxral1/what_are_some_of_the_things_you_own_in_your_job/", "subreddit_subscribers": 80227, "created_utc": 1668694131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improve your first Airflow DAG for Beginners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_yxqu3h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/PbxI0Jyvlx0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Airflow DAG: Improving your first DAG for Beginners\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow DAG: Improving your first DAG for Beginners", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/PbxI0Jyvlx0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Airflow DAG: Improving your first DAG for Beginners\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/PbxI0Jyvlx0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/PbxI0Jyvlx0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Airflow DAG: Improving your first DAG for Beginners\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/yxqu3h", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Xn3WbAxnw799tDd9ijigLXGMdkE66MXWIr5Fn_WeJes.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668693043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/PbxI0Jyvlx0", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yLANk1T7rlyJ7SbNQWzMk5-TgMZUNaK55gIIyxg5_9g.jpg?auto=webp&amp;s=80cc9e1b851516520e19278f3f1ca46d85ee3cb5", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/yLANk1T7rlyJ7SbNQWzMk5-TgMZUNaK55gIIyxg5_9g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=37e4add9e698dfa4373452351ec8529b3f749208", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/yLANk1T7rlyJ7SbNQWzMk5-TgMZUNaK55gIIyxg5_9g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2700b9139284bbda6d9a3ae75e59fca2fefdbc7b", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/yLANk1T7rlyJ7SbNQWzMk5-TgMZUNaK55gIIyxg5_9g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e9f99bb280b7704c11ae4c9598f792d354f0ba6", "width": 320, "height": 240}], "variants": {}, "id": "3dfk3R-YGk3OQRrlA7g6DrrWa8GPsDh_-v1ao8Hj8x0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yxqu3h", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxqu3h/improve_your_first_airflow_dag_for_beginners/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/PbxI0Jyvlx0", "subreddit_subscribers": 80227, "created_utc": 1668693043.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow DAG: Improving your first DAG for Beginners", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/PbxI0Jyvlx0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Airflow DAG: Improving your first DAG for Beginners\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/PbxI0Jyvlx0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our team has been put in charge of creating a process to move customers from one product DB to another. \n\nThis is currently done using SSIS packages that are pretty much a black box to anyone wondering what logic they're using to move data.\n\nWe're building out a new workflow, I'd like to take a step back and consider another option.\n\nAs a software engineer I'd really like to be able to write test cases, do version control, all that good stuff while also documenting business logic of DB values and how they relate across products?", "author_fullname": "t2_exqc18fi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tool for creating a process to move customers from one product/DB to another", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxq1tz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668691157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our team has been put in charge of creating a process to move customers from one product DB to another. &lt;/p&gt;\n\n&lt;p&gt;This is currently done using SSIS packages that are pretty much a black box to anyone wondering what logic they&amp;#39;re using to move data.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re building out a new workflow, I&amp;#39;d like to take a step back and consider another option.&lt;/p&gt;\n\n&lt;p&gt;As a software engineer I&amp;#39;d really like to be able to write test cases, do version control, all that good stuff while also documenting business logic of DB values and how they relate across products?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxq1tz", "is_robot_indexable": true, "report_reasons": null, "author": "No-Swimming-3", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxq1tz/best_tool_for_creating_a_process_to_move/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxq1tz/best_tool_for_creating_a_process_to_move/", "subreddit_subscribers": 80227, "created_utc": 1668691157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm not interested in the cert,  just in the Content in the course.\n\n&amp;#x200B;\n\nDoes anyone know which is better?", "author_fullname": "t2_a9icd9le", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IBM Data Engineering Professional Certificate Or Meta Data Engineer Cert", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxnu68", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668684490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not interested in the cert,  just in the Content in the course.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Does anyone know which is better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxnu68", "is_robot_indexable": true, "report_reasons": null, "author": "Then_Landscape6474", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxnu68/ibm_data_engineering_professional_certificate_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxnu68/ibm_data_engineering_professional_certificate_or/", "subreddit_subscribers": 80227, "created_utc": 1668684490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. My company after two years of pandemic want to switch back to hybrid work (1-2 days in a week in office). \nFor my all team this change do not make sense. Tomorrow we have a call with boss of my boss. \n\nDo you have any ideas of how to argument that we want to work remotely?\n\nDid any of you have similar situation?", "author_fullname": "t2_50lq8gs1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching from remote to hybrid", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxm64i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668678681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. My company after two years of pandemic want to switch back to hybrid work (1-2 days in a week in office). \nFor my all team this change do not make sense. Tomorrow we have a call with boss of my boss. &lt;/p&gt;\n\n&lt;p&gt;Do you have any ideas of how to argument that we want to work remotely?&lt;/p&gt;\n\n&lt;p&gt;Did any of you have similar situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yxm64i", "is_robot_indexable": true, "report_reasons": null, "author": "truverol", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yxm64i/switching_from_remote_to_hybrid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yxm64i/switching_from_remote_to_hybrid/", "subreddit_subscribers": 80227, "created_utc": 1668678681.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}