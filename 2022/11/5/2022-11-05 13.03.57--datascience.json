{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all!\n\nAround 2 months back I was unfortunately a part of a mass layoff and had to start looking for a new job. I had come to this community for help in improving my resume (My post was removed as I had not used the weekly thread for resume review. Will keep that in mind next time). But even for the brief period of time that the post was up, I had received a lot of good advice which I incorporated.\n\nI start my new role next week. This community played a significant part in my success that I wanted to acknowledge. I'm not sure if these kinds of posts are allowed, if not they'll be removed anyway :p\n\nI'd like to personally thank the following users who gave very clear and concise advice that I was able to use to enhance my resume. u/abitofaLuna-tic, u/chunzilla, u/VacuousWaffle, u/denim_duck,  u/proverbialbunny. Thank you all!\n\n**P.S:** A common point that was suggested to me was to switch from a 2-column format to a 1-column format. This worked. When comparing the two resumes I saw how concise and easy to read my 1-column (single page) resume was. That being said, I've observed that my 2-column resume seemed to do really well with young startups, companies with smaller DS teams, or a more personal screening process. Just thought that this was interesting.", "author_fullname": "t2_40233m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thank you! An appreciation post of this DS community.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ym30il", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 178, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 178, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667578214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;Around 2 months back I was unfortunately a part of a mass layoff and had to start looking for a new job. I had come to this community for help in improving my resume (My post was removed as I had not used the weekly thread for resume review. Will keep that in mind next time). But even for the brief period of time that the post was up, I had received a lot of good advice which I incorporated.&lt;/p&gt;\n\n&lt;p&gt;I start my new role next week. This community played a significant part in my success that I wanted to acknowledge. I&amp;#39;m not sure if these kinds of posts are allowed, if not they&amp;#39;ll be removed anyway :p&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to personally thank the following users who gave very clear and concise advice that I was able to use to enhance my resume. &lt;a href=\"/u/abitofaLuna-tic\"&gt;u/abitofaLuna-tic&lt;/a&gt;, &lt;a href=\"/u/chunzilla\"&gt;u/chunzilla&lt;/a&gt;, &lt;a href=\"/u/VacuousWaffle\"&gt;u/VacuousWaffle&lt;/a&gt;, &lt;a href=\"/u/denim_duck\"&gt;u/denim_duck&lt;/a&gt;,  &lt;a href=\"/u/proverbialbunny\"&gt;u/proverbialbunny&lt;/a&gt;. Thank you all!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;P.S:&lt;/strong&gt; A common point that was suggested to me was to switch from a 2-column format to a 1-column format. This worked. When comparing the two resumes I saw how concise and easy to read my 1-column (single page) resume was. That being said, I&amp;#39;ve observed that my 2-column resume seemed to do really well with young startups, companies with smaller DS teams, or a more personal screening process. Just thought that this was interesting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 30, "id": "award_c4b2e438-16bb-4568-88e7-7893b7662944", "penny_donate": null, "award_sub_type": "PREMIUM", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=16&amp;height=16&amp;auto=webp&amp;s=1a331be5cf6d754b4cb7ed2ca3706f70d5260a57", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=32&amp;height=32&amp;auto=webp&amp;s=6d0a6351d4080286095df432f95a103cdf4188f2", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=48&amp;height=48&amp;auto=webp&amp;s=913e99a6f6688f26c08dcb411f043f71b17df931", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=64&amp;height=64&amp;auto=webp&amp;s=e3ad9900371bf1f91eb422b4d000b3a1c0d5a9c4", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=128&amp;height=128&amp;auto=webp&amp;s=4cc281fbace61e034477d2bdb7b158913457863d", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "A glittering stamp for a feel-good thing", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Wholesome Seal of Approval", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=16&amp;height=16&amp;auto=webp&amp;s=1a331be5cf6d754b4cb7ed2ca3706f70d5260a57", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=32&amp;height=32&amp;auto=webp&amp;s=6d0a6351d4080286095df432f95a103cdf4188f2", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=48&amp;height=48&amp;auto=webp&amp;s=913e99a6f6688f26c08dcb411f043f71b17df931", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=64&amp;height=64&amp;auto=webp&amp;s=e3ad9900371bf1f91eb422b4d000b3a1c0d5a9c4", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=128&amp;height=128&amp;auto=webp&amp;s=4cc281fbace61e034477d2bdb7b158913457863d", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym30il", "is_robot_indexable": true, "report_reasons": null, "author": "CrypticTac", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym30il/thank_you_an_appreciation_post_of_this_ds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym30il/thank_you_an_appreciation_post_of_this_ds/", "subreddit_subscribers": 817376, "created_utc": 1667578214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_646nndt4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with data correction ideas!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 63, "top_awarded_type": null, "hide_score": false, "name": "t3_ym7xf7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 110, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 110, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/XZQBoeYkHor8sauoGpE89qUu215Xelc49ZeTtjUHxi0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667590002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/07okhwaljzx91.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/07okhwaljzx91.png?auto=webp&amp;s=3abbb8edb7a79cb532fa70a6874deaea57ccdd19", "width": 1370, "height": 626}, "resolutions": [{"url": "https://preview.redd.it/07okhwaljzx91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f436c3a594c9d798aee4b0438879cd2d4fb3fd7e", "width": 108, "height": 49}, {"url": "https://preview.redd.it/07okhwaljzx91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c10c3c424f76c22eba507ad18593748a291aa2a", "width": 216, "height": 98}, {"url": "https://preview.redd.it/07okhwaljzx91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=564cfc465cbea7714cfa9b492005025718325bde", "width": 320, "height": 146}, {"url": "https://preview.redd.it/07okhwaljzx91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5da4e5cca6926a257ffd45e15e4b8ce6121ebfd3", "width": 640, "height": 292}, {"url": "https://preview.redd.it/07okhwaljzx91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa72b08f4171aa0c7169c26571f9f670e6f03473", "width": 960, "height": 438}, {"url": "https://preview.redd.it/07okhwaljzx91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16e622297f11f63958db60fc89410cdb8ac7669f", "width": 1080, "height": 493}], "variants": {}, "id": "4BAoQm9jbGgjC1B8VxdCqj3zYDmFb8ikr_ZRR5HiLdA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym7xf7", "is_robot_indexable": true, "report_reasons": null, "author": "vanslife4511", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym7xf7/need_help_with_data_correction_ideas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/07okhwaljzx91.png", "subreddit_subscribers": 817376, "created_utc": 1667590002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_eg4u9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most problems are summary stats problems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 103, "top_awarded_type": null, "hide_score": false, "name": "t3_ymjob8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JxGJ8m7GUUvasmtuTnjlk4cC8Ck3_2OzkK45s-dQrsk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667621210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgflip.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://imgflip.com/i/6zlhnz", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nZgz3bddyTZpxvP6RUwlynpBKDYzKtcFR1TefBtiIqM.jpg?auto=webp&amp;s=df9a71544da6509bf796178ac29d6e0883c9b1f1", "width": 675, "height": 499}, "resolutions": [{"url": "https://external-preview.redd.it/nZgz3bddyTZpxvP6RUwlynpBKDYzKtcFR1TefBtiIqM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1cd47f4c9896859de24407a48dde34c8b8abd7b", "width": 108, "height": 79}, {"url": "https://external-preview.redd.it/nZgz3bddyTZpxvP6RUwlynpBKDYzKtcFR1TefBtiIqM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8240f81c35c0279e67b6065c321d1478acb8f8d8", "width": 216, "height": 159}, {"url": "https://external-preview.redd.it/nZgz3bddyTZpxvP6RUwlynpBKDYzKtcFR1TefBtiIqM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4e22cc2a1e6434e9b6805c0eb60f3cdaa0eecb0c", "width": 320, "height": 236}, {"url": "https://external-preview.redd.it/nZgz3bddyTZpxvP6RUwlynpBKDYzKtcFR1TefBtiIqM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=45047658620bd32a6a7cde637bc05961df533ece", "width": 640, "height": 473}], "variants": {}, "id": "yWtS-vJuzYM_wblMJHn5uNjCKmsN53aq_EjGOFGbLfU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymjob8", "is_robot_indexable": true, "report_reasons": null, "author": "equivocal20", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymjob8/most_problems_are_summary_stats_problems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgflip.com/i/6zlhnz", "subreddit_subscribers": 817376, "created_utc": 1667621210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_3fi5o45p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which project are you the most proud of? Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymh10u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667613246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymh10u", "is_robot_indexable": true, "report_reasons": null, "author": "HairyProtection", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymh10u/which_project_are_you_the_most_proud_of_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymh10u/which_project_are_you_the_most_proud_of_why/", "subreddit_subscribers": 817376, "created_utc": 1667613246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Howdy Data folks,\n\nI'm in the retail space and trying to basically forecast sales for 2023. I took over the BI/data role after the guy previously in the role left earlier this year. He built a projection basically using previous sales from the last couple years (and I'm still trying to read through his python code to figure out how he came to the calculation btw), but I feel like with the economy and what not-things could be so up and down that maybe we shouldnt rely on previous years sales. \n\nAre there any data sources I should be considering looking at, in order to better verify sales/projections for next year? \n\nAny help or insight would be VASTLY appreciated.", "author_fullname": "t2_7meg6iov", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forecasting retail sales in 2023? Do you use anything in particular for insight?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ym4xxj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667582891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy Data folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in the retail space and trying to basically forecast sales for 2023. I took over the BI/data role after the guy previously in the role left earlier this year. He built a projection basically using previous sales from the last couple years (and I&amp;#39;m still trying to read through his python code to figure out how he came to the calculation btw), but I feel like with the economy and what not-things could be so up and down that maybe we shouldnt rely on previous years sales. &lt;/p&gt;\n\n&lt;p&gt;Are there any data sources I should be considering looking at, in order to better verify sales/projections for next year? &lt;/p&gt;\n\n&lt;p&gt;Any help or insight would be VASTLY appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym4xxj", "is_robot_indexable": true, "report_reasons": null, "author": "WhatsTheAnswerDude", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym4xxj/forecasting_retail_sales_in_2023_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym4xxj/forecasting_retail_sales_in_2023_do_you_use/", "subreddit_subscribers": 817376, "created_utc": 1667582891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am building a MC simulation in Python for a sample portfolio that contains various weightings of asset classes (think like 10% stocks, 5% bonds, 6% mutual funds, etc). The returns of these variables are all correlated with each other in some way. I have the mean and SD of each of their returns and the correlation matrix. The distribution of each individual return is normally distributed. \n\nI am currently using scipy.multivariate_normal but not sure if this is the right approach or not for simulating n number of years of returns. \n\nAny advice is appreciated!", "author_fullname": "t2_gzra5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to account for correlated variables in a Monte Carlo simulation in Python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymadxr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667599223.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667595833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am building a MC simulation in Python for a sample portfolio that contains various weightings of asset classes (think like 10% stocks, 5% bonds, 6% mutual funds, etc). The returns of these variables are all correlated with each other in some way. I have the mean and SD of each of their returns and the correlation matrix. The distribution of each individual return is normally distributed. &lt;/p&gt;\n\n&lt;p&gt;I am currently using scipy.multivariate_normal but not sure if this is the right approach or not for simulating n number of years of returns. &lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymadxr", "is_robot_indexable": true, "report_reasons": null, "author": "zferguson", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymadxr/how_to_account_for_correlated_variables_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymadxr/how_to_account_for_correlated_variables_in_a/", "subreddit_subscribers": 817376, "created_utc": 1667595833.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nI've been asked this question during an interview. \n\nI have currently 3 YOE as a Data Scientist in the finance industry and I have applied to this small tech startup (&lt;20 people, &lt;10 technical people). They have no DS/ML people on their team, so I would be their first, and I would be working on a subject very similar to what I have already implemented in my current company.\n\nThe interviewer asked me this question after I gave him my low range of salary expectations.\n\nHow do you know when you are ready to be the head of data science of a company ? What mandatory skills should you have ? Since it's a small company should I expect to be responsible of the data engineering part also ? What should I clarify with them in my next interview ?", "author_fullname": "t2_6b5q0a66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are you ready to lead a DS team and be the head of data science ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ym13he", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667573585.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been asked this question during an interview. &lt;/p&gt;\n\n&lt;p&gt;I have currently 3 YOE as a Data Scientist in the finance industry and I have applied to this small tech startup (&amp;lt;20 people, &amp;lt;10 technical people). They have no DS/ML people on their team, so I would be their first, and I would be working on a subject very similar to what I have already implemented in my current company.&lt;/p&gt;\n\n&lt;p&gt;The interviewer asked me this question after I gave him my low range of salary expectations.&lt;/p&gt;\n\n&lt;p&gt;How do you know when you are ready to be the head of data science of a company ? What mandatory skills should you have ? Since it&amp;#39;s a small company should I expect to be responsible of the data engineering part also ? What should I clarify with them in my next interview ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym13he", "is_robot_indexable": true, "report_reasons": null, "author": "Intrepid_Evening", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym13he/are_you_ready_to_lead_a_ds_team_and_be_the_head/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym13he/are_you_ready_to_lead_a_ds_team_and_be_the_head/", "subreddit_subscribers": 817376, "created_utc": 1667573585.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_91atae5g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Share your experiences using windows vs mac in data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylzrmj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667570256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylzrmj", "is_robot_indexable": true, "report_reasons": null, "author": "MalaysiaDankMeme", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylzrmj/share_your_experiences_using_windows_vs_mac_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylzrmj/share_your_experiences_using_windows_vs_mac_in/", "subreddit_subscribers": 817376, "created_utc": 1667570256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I  can't even remember where I picked this up, but my understanding is  that when building an ML model, and you want to know how well that model  really predicts on unseen (test) data - to give you an indication of  how well it will perform in the field, one doesn't just do a train test  split once and evaluate your model on the test set, but rather, one  should do this many times and take the mean performance from the test  set. Is this actually correct? Here is some code to indicate what I  mean, note that in this example I'm not including any data, but just  assume that X is an np.array of predictors and y is an np.array with a binary outcome:\n\n`from sklearn.linear_model import ElasticNetCV`\n\n`from sklearn.metrics import roc_curve, auc`\n\n`auc_list = []`\n\n`for i in range(30):`\n\n`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)`\n\n`train_means = X_train.mean(axis = 0)`\n\n`train_sd = X_train.std(axis = 0)`\n\n`X_train = (X_train - train_means)/train_sd`\n\n`X_test = (X_test - train_means)/train_sd`\n\n`lr_model = ElasticNetCV(cv=5)`\n\n`lr_model.fit(X_train, y_train)`\n\n`y_preds = lr_model.predict(X_test)`\n\n`fpr, tpr, thresholds = roc_curve(y_test, y_preds, pos_label=1)`\n\n`auc_roc = auc(fpr, tpr)`\n\n`auc_list.append(auc_roc)`\n\nSo  above I evaluate the AUC of my model on the testing set, and I do this  on 30 different train/test splits, and I then look at the mean AUC  across the 30 splits - np.mean(auc\\_list)\n\nIs  this a correct procedure? My intuition is that there may be some train  test splits that give particularly good or bad predictions due to the  randomness of the split, and so one must take the mean of many splits.\n\nHowever,  when I look at every single online tutorial or online course on how to  build an ml model, this aspect of it is never included, so much so to  the extent that I'm now wondering if I've been doing it wrong in taking  this approach? So does anyone know what is correct? And if my way is  correct, why is it that this is never included in online tutorials?\n\nMany thanks!", "author_fullname": "t2_lrovl6pi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The mean of multiple train/test splits is required in order to evaluate an ML model(?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yly3wd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667565995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I  can&amp;#39;t even remember where I picked this up, but my understanding is  that when building an ML model, and you want to know how well that model  really predicts on unseen (test) data - to give you an indication of  how well it will perform in the field, one doesn&amp;#39;t just do a train test  split once and evaluate your model on the test set, but rather, one  should do this many times and take the mean performance from the test  set. Is this actually correct? Here is some code to indicate what I  mean, note that in this example I&amp;#39;m not including any data, but just  assume that X is an np.array of predictors and y is an np.array with a binary outcome:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;from sklearn.linear_model import ElasticNetCV&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;from sklearn.metrics import roc_curve, auc&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;auc_list = []&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;for i in range(30):&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;train_means = X_train.mean(axis = 0)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;train_sd = X_train.std(axis = 0)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;X_train = (X_train - train_means)/train_sd&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;X_test = (X_test - train_means)/train_sd&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;lr_model = ElasticNetCV(cv=5)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;lr_model.fit(X_train, y_train)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;y_preds = lr_model.predict(X_test)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;fpr, tpr, thresholds = roc_curve(y_test, y_preds, pos_label=1)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;auc_roc = auc(fpr, tpr)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;auc_list.append(auc_roc)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;So  above I evaluate the AUC of my model on the testing set, and I do this  on 30 different train/test splits, and I then look at the mean AUC  across the 30 splits - np.mean(auc_list)&lt;/p&gt;\n\n&lt;p&gt;Is  this a correct procedure? My intuition is that there may be some train  test splits that give particularly good or bad predictions due to the  randomness of the split, and so one must take the mean of many splits.&lt;/p&gt;\n\n&lt;p&gt;However,  when I look at every single online tutorial or online course on how to  build an ml model, this aspect of it is never included, so much so to  the extent that I&amp;#39;m now wondering if I&amp;#39;ve been doing it wrong in taking  this approach? So does anyone know what is correct? And if my way is  correct, why is it that this is never included in online tutorials?&lt;/p&gt;\n\n&lt;p&gt;Many thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yly3wd", "is_robot_indexable": true, "report_reasons": null, "author": "likeamanyfacedgod", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yly3wd/the_mean_of_multiple_traintest_splits_is_required/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yly3wd/the_mean_of_multiple_traintest_splits_is_required/", "subreddit_subscribers": 817376, "created_utc": 1667565995.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello I am a newbie just starting to look into Data Science &amp; Analytics &amp; I am very overwhelmed by the amount of resources &amp; was wanting to find a community or just make friends with people in the field so I can get a good grasp of it as in my state tech is not popular at all here. I am trying to find more information about it but its hard to be absorb the info when there are different websites saying different things, I would love to just be able to actually talk to someone with experience.  I am trying to complete certifications on coursera but I know I will need to do much more. Thank you,", "author_fullname": "t2_hiivs8ny", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Communitys?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymowko", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667639539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello I am a newbie just starting to look into Data Science &amp;amp; Analytics &amp;amp; I am very overwhelmed by the amount of resources &amp;amp; was wanting to find a community or just make friends with people in the field so I can get a good grasp of it as in my state tech is not popular at all here. I am trying to find more information about it but its hard to be absorb the info when there are different websites saying different things, I would love to just be able to actually talk to someone with experience.  I am trying to complete certifications on coursera but I know I will need to do much more. Thank you,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymowko", "is_robot_indexable": true, "report_reasons": null, "author": "DirectionLogical3908", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymowko/communitys/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymowko/communitys/", "subreddit_subscribers": 817376, "created_utc": 1667639539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone I was wondering if anyone has experience building models that calculate liquidity score and also credibility scores? Is there any material / research articles or textbooks that you can share about tackling such problems?", "author_fullname": "t2_4y6pugvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Liquidity and credibility scores", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymnvfz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667635756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone I was wondering if anyone has experience building models that calculate liquidity score and also credibility scores? Is there any material / research articles or textbooks that you can share about tackling such problems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymnvfz", "is_robot_indexable": true, "report_reasons": null, "author": "Quick-Indication", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymnvfz/liquidity_and_credibility_scores/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymnvfz/liquidity_and_credibility_scores/", "subreddit_subscribers": 817376, "created_utc": 1667635756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello people, I'm working on a deep learning model and I have been facing an error which says input 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim = 4, found ndim = 3. Full shape received:(None, 176,208). This happened when I tried to deploy the model. Where exactly am i making the mistake? I have defined my input shape in the model as (176,208,3). I'm new to learning deep learning and I would love to learn as much as I can from my mistakes.", "author_fullname": "t2_ckzjt222", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to learn deep learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymmjdr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667632188.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667630868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello people, I&amp;#39;m working on a deep learning model and I have been facing an error which says input 0 of layer &amp;quot;conv2d&amp;quot; is incompatible with the layer: expected min_ndim = 4, found ndim = 3. Full shape received:(None, 176,208). This happened when I tried to deploy the model. Where exactly am i making the mistake? I have defined my input shape in the model as (176,208,3). I&amp;#39;m new to learning deep learning and I would love to learn as much as I can from my mistakes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymmjdr", "is_robot_indexable": true, "report_reasons": null, "author": "SubstanceNarrow2605", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymmjdr/trying_to_learn_deep_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymmjdr/trying_to_learn_deep_learning/", "subreddit_subscribers": 817376, "created_utc": 1667630868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "A few days ago, I came across this Reddit [post](https://www.reddit.com/r/datascience/comments/yi6sxu/is_there_any_metric_to_measure_difference_of_a/). It intrigued me since I've never heard some of the suggested answers given. Such as KL-divergence. I've tried reading up about it from this [article](https://machinelearningmastery.com/divergence-between-probability-distributions/). \n\n&amp;#x200B;\n\nWhat I can understand is that KL-divergence is a way to compare 2 different distributions from one another? I don't quite understand it. It doesn't calculate the distance between the data points but rather measures the divergence of one probability distribution from another? (This part is a bit blurry to me)  \n\n\nSo, let's say we're using a model, and we test it on 2 different samples. From there we are comparing how those 2 distributions are different with one another? Thus, what can we accomplish after this? I'm trying to wrap my head around this concept and see how beneficial it is.   \n\n\nOn top of that, in the original reddit post, there are some that suggested using chi squared test/p-value. I'm guessing this is under the topic of hypothesis testing? If anyone has any good references for this topic and how to actually apply it in a data science scenario, that would be highly appreciated. \n\nIf there is any confusion in my writing, do let me know and I'll try to clarify it. Many thanks. \n\n&amp;#x200B;\n\n\\##new2datascience", "author_fullname": "t2_46qj3zfd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Requesting for additional explanations/suggestions in regard to comparing different distributions of a given population.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymo862", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667637035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A few days ago, I came across this Reddit &lt;a href=\"https://www.reddit.com/r/datascience/comments/yi6sxu/is_there_any_metric_to_measure_difference_of_a/\"&gt;post&lt;/a&gt;. It intrigued me since I&amp;#39;ve never heard some of the suggested answers given. Such as KL-divergence. I&amp;#39;ve tried reading up about it from this &lt;a href=\"https://machinelearningmastery.com/divergence-between-probability-distributions/\"&gt;article&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What I can understand is that KL-divergence is a way to compare 2 different distributions from one another? I don&amp;#39;t quite understand it. It doesn&amp;#39;t calculate the distance between the data points but rather measures the divergence of one probability distribution from another? (This part is a bit blurry to me)  &lt;/p&gt;\n\n&lt;p&gt;So, let&amp;#39;s say we&amp;#39;re using a model, and we test it on 2 different samples. From there we are comparing how those 2 distributions are different with one another? Thus, what can we accomplish after this? I&amp;#39;m trying to wrap my head around this concept and see how beneficial it is.   &lt;/p&gt;\n\n&lt;p&gt;On top of that, in the original reddit post, there are some that suggested using chi squared test/p-value. I&amp;#39;m guessing this is under the topic of hypothesis testing? If anyone has any good references for this topic and how to actually apply it in a data science scenario, that would be highly appreciated. &lt;/p&gt;\n\n&lt;p&gt;If there is any confusion in my writing, do let me know and I&amp;#39;ll try to clarify it. Many thanks. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;##new2datascience&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/r8r46UG6uhW30tQCX7PLzcrsuEUDh8-6XogRORhWEjA.jpg?auto=webp&amp;s=29a43be06798dc9d2563aaa1c025e4d0fd7d1034", "width": 1280, "height": 960}, "resolutions": [{"url": "https://external-preview.redd.it/r8r46UG6uhW30tQCX7PLzcrsuEUDh8-6XogRORhWEjA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b268d44ea6f1eb9879f29037b9c0b6f2dfd7479d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/r8r46UG6uhW30tQCX7PLzcrsuEUDh8-6XogRORhWEjA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa1ba12483f6fc1a594d7afccff6863c5caff5e4", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/r8r46UG6uhW30tQCX7PLzcrsuEUDh8-6XogRORhWEjA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a0493fed18f4791ffe7d2e7f704b6d6c9a7726ec", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/r8r46UG6uhW30tQCX7PLzcrsuEUDh8-6XogRORhWEjA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8598856416b987a3fbe7c0acd122394f7f471de7", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/r8r46UG6uhW30tQCX7PLzcrsuEUDh8-6XogRORhWEjA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f6e47a5adeed8af618a6d47d5f9aa573b6fd1d04", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/r8r46UG6uhW30tQCX7PLzcrsuEUDh8-6XogRORhWEjA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=00a71c6c226b90edd8a55c44c7aa2be96f5af5b0", "width": 1080, "height": 810}], "variants": {}, "id": "gfBeQN8D4vB4QI7pWkGKSg9KRvvoebgU4_DMaAPJnv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymo862", "is_robot_indexable": true, "report_reasons": null, "author": "FlyingRaijinEX", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymo862/requesting_for_additional_explanationssuggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymo862/requesting_for_additional_explanationssuggestions/", "subreddit_subscribers": 817376, "created_utc": 1667637035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Some focus areas being the below for a hospital chain with over 10 branches.  \n\n\\#1 staff burnout \n\n\\#2 Unsatisfactory patient experience \n\n\\#3 inconsistencies of outcomes for high volume patients procedures done at different locations\n\nCould someone please help?", "author_fullname": "t2_6km84599", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some areas where AI/ML-based data science solutions can help solve problems and unlock efficiencies in healthcare?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymnp30", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667635127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some focus areas being the below for a hospital chain with over 10 branches.  &lt;/p&gt;\n\n&lt;p&gt;#1 staff burnout &lt;/p&gt;\n\n&lt;p&gt;#2 Unsatisfactory patient experience &lt;/p&gt;\n\n&lt;p&gt;#3 inconsistencies of outcomes for high volume patients procedures done at different locations&lt;/p&gt;\n\n&lt;p&gt;Could someone please help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymnp30", "is_robot_indexable": true, "report_reasons": null, "author": "Morphineonroids", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymnp30/what_are_some_areas_where_aimlbased_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymnp30/what_are_some_areas_where_aimlbased_data_science/", "subreddit_subscribers": 817376, "created_utc": 1667635127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've recently published an article on [medium](https://medium.com/mlearning-ai/use-servicefoundry-to-deploy-machine-learning-models-like-a-ninja-a912ca550333) explaining how we can deploy ML Models with super ease! The service I've mentioned is TrueFoundry and it is one the easiest platforms to get started with MLOps. Do check out the platform and share your thoughts!", "author_fullname": "t2_hcskz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying Machine Learning models like a Ninja!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymm604", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667629543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently published an article on &lt;a href=\"https://medium.com/mlearning-ai/use-servicefoundry-to-deploy-machine-learning-models-like-a-ninja-a912ca550333\"&gt;medium&lt;/a&gt; explaining how we can deploy ML Models with super ease! The service I&amp;#39;ve mentioned is TrueFoundry and it is one the easiest platforms to get started with MLOps. Do check out the platform and share your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iEySlyRJwyewYCeRyowlBSCOsQyVWLFBCs1VXRZuZCA.jpg?auto=webp&amp;s=77610147ab91d04ef94b914609714ea0d549b71a", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/iEySlyRJwyewYCeRyowlBSCOsQyVWLFBCs1VXRZuZCA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d12ee9847940ae2108569c75576e990f1d58ec6", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/iEySlyRJwyewYCeRyowlBSCOsQyVWLFBCs1VXRZuZCA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d7915e54e5509a8a928c7b8f531f3d43b3aec67f", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/iEySlyRJwyewYCeRyowlBSCOsQyVWLFBCs1VXRZuZCA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d0a0941feb3b8562811b0830909c1911bac9c144", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/iEySlyRJwyewYCeRyowlBSCOsQyVWLFBCs1VXRZuZCA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a7957bdd03936b42daf30467d6aa139cdd8be00e", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/iEySlyRJwyewYCeRyowlBSCOsQyVWLFBCs1VXRZuZCA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dcc054763e2e83c411c28b218e50dfdcd6b6d903", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/iEySlyRJwyewYCeRyowlBSCOsQyVWLFBCs1VXRZuZCA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=59658436cb5e46268297b89b1f3828a55f60a0e9", "width": 1080, "height": 720}], "variants": {}, "id": "_MP9AGypUJRslj14kc8UPlvs1UUdrFySSgt4CkyLNS4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymm604", "is_robot_indexable": true, "report_reasons": null, "author": "vishank97", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymm604/deploying_machine_learning_models_like_a_ninja/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymm604/deploying_machine_learning_models_like_a_ninja/", "subreddit_subscribers": 817376, "created_utc": 1667629543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For those of you that work freelance, how do you deal with requests to e.g. give a presentation or give advice at a workshop? I fairly often get requests such as these, typically they would like me to attend a workshop to discuss their work, give input and possibly make a short presentation on my area of expertise. Invariably, my time would not be paid. It can be from an hour up to a half day of time or more.\n\nBack when I was working in academia or in a research position, I would usually say yes to these requests because I would be interested to hear about new work, meet new people, etc. I was able to attend them during working hours so effectively it was part of my job.\n\nNow working freelance, if I spend half a day attending a workshop to help a project, that's half a day that I can't work on paid work (or else taking time off) so I'm losing out.\n\nProblem is, I think people who haven't worked freelance often don't appreciate this fact. So when I turn down such requests I'm probably coming across as unhelpful. If I ask to be paid I seem like a dick.\n\nAnyone else had this problem and have thoughts on how to tackle it? Am I just overthinking it?", "author_fullname": "t2_45ni7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with pro bono requests?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymbtyw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667599399.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those of you that work freelance, how do you deal with requests to e.g. give a presentation or give advice at a workshop? I fairly often get requests such as these, typically they would like me to attend a workshop to discuss their work, give input and possibly make a short presentation on my area of expertise. Invariably, my time would not be paid. It can be from an hour up to a half day of time or more.&lt;/p&gt;\n\n&lt;p&gt;Back when I was working in academia or in a research position, I would usually say yes to these requests because I would be interested to hear about new work, meet new people, etc. I was able to attend them during working hours so effectively it was part of my job.&lt;/p&gt;\n\n&lt;p&gt;Now working freelance, if I spend half a day attending a workshop to help a project, that&amp;#39;s half a day that I can&amp;#39;t work on paid work (or else taking time off) so I&amp;#39;m losing out.&lt;/p&gt;\n\n&lt;p&gt;Problem is, I think people who haven&amp;#39;t worked freelance often don&amp;#39;t appreciate this fact. So when I turn down such requests I&amp;#39;m probably coming across as unhelpful. If I ask to be paid I seem like a dick.&lt;/p&gt;\n\n&lt;p&gt;Anyone else had this problem and have thoughts on how to tackle it? Am I just overthinking it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymbtyw", "is_robot_indexable": true, "report_reasons": null, "author": "dr_chickolas", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymbtyw/how_to_deal_with_pro_bono_requests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymbtyw/how_to_deal_with_pro_bono_requests/", "subreddit_subscribers": 817376, "created_utc": 1667599399.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi!\n\nI've seen similar posts here, so maybe you will help me too since I have really no idea what figures to put for this project as it is my first one and I can't find any similar ones in some other posts like this one or even on Fiverr ( If it's even a good place to look for price estimation ).\n\nSo, project looks like this ( Sorry for the bad explanation, I don't know how to put it more simply):\n\nI need to automate a creation of a file containing forecasts data for a company ( multiple companies with a little bit different data to work with ), to be more precise:  \n\\- Each company has its own data in csv file, I need to extract the data from it for each part that we sell to them ( part, quantity, year, month )  \n\\- Then I create an excel file containing the extracted data, in the excel file are sheets for each part that shows new data in each row ( updated weekly ) and some basic calculation of total quantity, total sum, change in time and the excel file also contains sheets with each month of a certain year with data for all parts and similar work done as above, only with different formatting  \n\\- I have to also make a GUI for this as the end user won't always be me, but also my colleagues and boss that need to have it as simple as possible.\n\nFor this project I use Python, Pandas, OpenPyXL and plan to use Kivy for interface ( I will do it last ).\n\nAlso, I live in Europe and more specifically in Poland, it will have a great impact on the price I think.\n\nOne last thing - I don't think that in my case charging per hour makes sense, since I'm also learning new stuff on the way and don't think it would be fair.\n\nAny help would be much appreciated!", "author_fullname": "t2_2cd289b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[EU] How much should I charge for an automation project ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ym8oex", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667591819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen similar posts here, so maybe you will help me too since I have really no idea what figures to put for this project as it is my first one and I can&amp;#39;t find any similar ones in some other posts like this one or even on Fiverr ( If it&amp;#39;s even a good place to look for price estimation ).&lt;/p&gt;\n\n&lt;p&gt;So, project looks like this ( Sorry for the bad explanation, I don&amp;#39;t know how to put it more simply):&lt;/p&gt;\n\n&lt;p&gt;I need to automate a creation of a file containing forecasts data for a company ( multiple companies with a little bit different data to work with ), to be more precise:&lt;br/&gt;\n- Each company has its own data in csv file, I need to extract the data from it for each part that we sell to them ( part, quantity, year, month )&lt;br/&gt;\n- Then I create an excel file containing the extracted data, in the excel file are sheets for each part that shows new data in each row ( updated weekly ) and some basic calculation of total quantity, total sum, change in time and the excel file also contains sheets with each month of a certain year with data for all parts and similar work done as above, only with different formatting&lt;br/&gt;\n- I have to also make a GUI for this as the end user won&amp;#39;t always be me, but also my colleagues and boss that need to have it as simple as possible.&lt;/p&gt;\n\n&lt;p&gt;For this project I use Python, Pandas, OpenPyXL and plan to use Kivy for interface ( I will do it last ).&lt;/p&gt;\n\n&lt;p&gt;Also, I live in Europe and more specifically in Poland, it will have a great impact on the price I think.&lt;/p&gt;\n\n&lt;p&gt;One last thing - I don&amp;#39;t think that in my case charging per hour makes sense, since I&amp;#39;m also learning new stuff on the way and don&amp;#39;t think it would be fair.&lt;/p&gt;\n\n&lt;p&gt;Any help would be much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym8oex", "is_robot_indexable": true, "report_reasons": null, "author": "Hyalskavran", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym8oex/eu_how_much_should_i_charge_for_an_automation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym8oex/eu_how_much_should_i_charge_for_an_automation/", "subreddit_subscribers": 817376, "created_utc": 1667591819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I have met a problem with draw curve with seaborn. My first demand is to draw a curce with shadow, so I choose seaborn.lineplot. (see the fig following)\n\nhttps://preview.redd.it/0o97vwr3f3y91.png?width=898&amp;format=png&amp;auto=webp&amp;s=b276f199c14c04dec56d10760c0a35ab80bd8bb1\n\nwhile my second demand is to set style for the lines, like:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/gqnn93tef3y91.png?width=893&amp;format=png&amp;auto=webp&amp;s=caeb9fd8a8d514707f3ea771003e9b5132fd9aa2\n\nI have search some materials but have no idea how can I do it, could you please give some help?", "author_fullname": "t2_sonzi145", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "set line style in seaborn", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 101, "top_awarded_type": null, "hide_score": false, "media_metadata": {"0o97vwr3f3y91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 78, "x": 108, "u": "https://preview.redd.it/0o97vwr3f3y91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=309bf88602160504d3e8a7095119c92b610e96ec"}, {"y": 157, "x": 216, "u": "https://preview.redd.it/0o97vwr3f3y91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7106c2bd15464654b806cc64a03ee35c32e5fe5"}, {"y": 233, "x": 320, "u": "https://preview.redd.it/0o97vwr3f3y91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=18b963eb22d3b824283171faf6c78d504977a683"}, {"y": 466, "x": 640, "u": "https://preview.redd.it/0o97vwr3f3y91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=901f71a88afc19c8c30004590dd0815a5297a71b"}], "s": {"y": 654, "x": 898, "u": "https://preview.redd.it/0o97vwr3f3y91.png?width=898&amp;format=png&amp;auto=webp&amp;s=b276f199c14c04dec56d10760c0a35ab80bd8bb1"}, "id": "0o97vwr3f3y91"}, "gqnn93tef3y91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 78, "x": 108, "u": "https://preview.redd.it/gqnn93tef3y91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf55907daa4a2281cef240bc95be3c8a790beb52"}, {"y": 156, "x": 216, "u": "https://preview.redd.it/gqnn93tef3y91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=940f4b7aaf0f02aee1d3f79dd4f36a0de3d97d9c"}, {"y": 232, "x": 320, "u": "https://preview.redd.it/gqnn93tef3y91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab02d19ede3902cdbe1fa0300efdfe0b69f9bc80"}, {"y": 464, "x": 640, "u": "https://preview.redd.it/gqnn93tef3y91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae50cf2b51c993c73a4da2f7a64d85da879c1b09"}], "s": {"y": 648, "x": 893, "u": "https://preview.redd.it/gqnn93tef3y91.png?width=893&amp;format=png&amp;auto=webp&amp;s=caeb9fd8a8d514707f3ea771003e9b5132fd9aa2"}, "id": "gqnn93tef3y91"}}, "name": "t3_ymo8ez", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/r8lE8PVEd6KCWNWPUUiTX0mPPWHs5rB4ut1Rp671utc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667637061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have met a problem with draw curve with seaborn. My first demand is to draw a curce with shadow, so I choose seaborn.lineplot. (see the fig following)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0o97vwr3f3y91.png?width=898&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b276f199c14c04dec56d10760c0a35ab80bd8bb1\"&gt;https://preview.redd.it/0o97vwr3f3y91.png?width=898&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b276f199c14c04dec56d10760c0a35ab80bd8bb1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;while my second demand is to set style for the lines, like:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gqnn93tef3y91.png?width=893&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=caeb9fd8a8d514707f3ea771003e9b5132fd9aa2\"&gt;https://preview.redd.it/gqnn93tef3y91.png?width=893&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=caeb9fd8a8d514707f3ea771003e9b5132fd9aa2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I have search some materials but have no idea how can I do it, could you please give some help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymo8ez", "is_robot_indexable": true, "report_reasons": null, "author": "JoPrimer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymo8ez/set_line_style_in_seaborn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymo8ez/set_line_style_in_seaborn/", "subreddit_subscribers": 817376, "created_utc": 1667637061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/ymnk91)", "author_fullname": "t2_fo3smcik", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scrum vs Extreme Programming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymnk91", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667634621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/ymnk91\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "ymnk91", "is_robot_indexable": true, "report_reasons": null, "author": "useriogz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1667893821355, "options": [{"text": "Scrum", "id": "19611688"}, {"text": "Extreme Programming", "id": "19611689"}, {"text": "I don't know", "id": "19611690"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 122, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymnk91/scrum_vs_extreme_programming/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/datascience/comments/ymnk91/scrum_vs_extreme_programming/", "subreddit_subscribers": 817376, "created_utc": 1667634621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was curious if someone could help me figure out a method of determining growth rates for a data set with a large range.\n\nFor example, the growth rate over a year for 100 different cities. Some cities may start with 100 people and others may start with 100,000 or 1m. If a 100 person city grows by 100 people thats a 100% growth rate, however a city of 100,000's growth rate will be much lower.\n\nMy issue is that if I sort growth rates of all cities descending, I will get pretty much only the smallest cities and their crazy high growth rates.\n\nOne solution I thought of was to create \"bands\" of cities (IE: 0-100, 100-1000, 1000-10,000, etc) and test each of their growth rates, however, I believe there might be a better way to \"weight\" certain cities growth rates depending on their size?", "author_fullname": "t2_fc3cf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to sort or rank growth rates across large range data sets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ym74qp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667588115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was curious if someone could help me figure out a method of determining growth rates for a data set with a large range.&lt;/p&gt;\n\n&lt;p&gt;For example, the growth rate over a year for 100 different cities. Some cities may start with 100 people and others may start with 100,000 or 1m. If a 100 person city grows by 100 people thats a 100% growth rate, however a city of 100,000&amp;#39;s growth rate will be much lower.&lt;/p&gt;\n\n&lt;p&gt;My issue is that if I sort growth rates of all cities descending, I will get pretty much only the smallest cities and their crazy high growth rates.&lt;/p&gt;\n\n&lt;p&gt;One solution I thought of was to create &amp;quot;bands&amp;quot; of cities (IE: 0-100, 100-1000, 1000-10,000, etc) and test each of their growth rates, however, I believe there might be a better way to &amp;quot;weight&amp;quot; certain cities growth rates depending on their size?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym74qp", "is_robot_indexable": true, "report_reasons": null, "author": "johnnyhighschool", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym74qp/how_to_sort_or_rank_growth_rates_across_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym74qp/how_to_sort_or_rank_growth_rates_across_large/", "subreddit_subscribers": 817376, "created_utc": 1667588115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi there! I recently found out about data science as a profession and think it\u2019s so exciting and powerful!\n\nI\u2019m almost done building the MVP of my web app. Since I\u2019m bootstrapping and don\u2019t earn any money with it yet, I\u2018m not in a position to hire a data scientist to unleash its potential. \n\nHowever, I wanted to ask if there is the possibility to offer my web app for a case study to data science students where we can create a win win situation. i don\u2019t want to take advantage of anyone. The thing is, I don\u2019t know anything about the data science ecosystem or how data scientists work when working on their own projects and if they usually build something on top of an existing business or not. I also don\u2019t know if there are any risks by giving access to metadata to anyone. \n\nAny recommendation is welcome. I\u2018d love to have a starting point to open a conversation with data scientists and explore all the possibilities that exist. Thank you!", "author_fullname": "t2_aql2tl29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use my startup as a case study?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymo6ei", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667636864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there! I recently found out about data science as a profession and think it\u2019s so exciting and powerful!&lt;/p&gt;\n\n&lt;p&gt;I\u2019m almost done building the MVP of my web app. Since I\u2019m bootstrapping and don\u2019t earn any money with it yet, I\u2018m not in a position to hire a data scientist to unleash its potential. &lt;/p&gt;\n\n&lt;p&gt;However, I wanted to ask if there is the possibility to offer my web app for a case study to data science students where we can create a win win situation. i don\u2019t want to take advantage of anyone. The thing is, I don\u2019t know anything about the data science ecosystem or how data scientists work when working on their own projects and if they usually build something on top of an existing business or not. I also don\u2019t know if there are any risks by giving access to metadata to anyone. &lt;/p&gt;\n\n&lt;p&gt;Any recommendation is welcome. I\u2018d love to have a starting point to open a conversation with data scientists and explore all the possibilities that exist. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymo6ei", "is_robot_indexable": true, "report_reasons": null, "author": "Good-Half9818", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymo6ei/use_my_startup_as_a_case_study/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymo6ei/use_my_startup_as_a_case_study/", "subreddit_subscribers": 817376, "created_utc": 1667636864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_8azmn3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finally figured out a basic \"scraper\" in Google Sheets to refresh data from CDC's disconnected webpages!! Proud of myself, but also know there's a faster/better way to do this...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 107, "top_awarded_type": null, "hide_score": false, "name": "t3_ymad4r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/aXeOf1Z1pgx2ufcYllooEt_iBp0vyjPIUpJhwsFCL88.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667595780.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zzhpqstv00y91.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zzhpqstv00y91.png?auto=webp&amp;s=1230378ad0b23bcfb05a1557996ed0f311e1e66a", "width": 1310, "height": 1005}, "resolutions": [{"url": "https://preview.redd.it/zzhpqstv00y91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b780f4068479fa9a3f60772b41f54ce1ec2dbabd", "width": 108, "height": 82}, {"url": "https://preview.redd.it/zzhpqstv00y91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=429e1e814e4b18a90ac4418f350945a4bb2d5041", "width": 216, "height": 165}, {"url": "https://preview.redd.it/zzhpqstv00y91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0da61dcd83be9fc58c7b6e6b7e6f6248b3475924", "width": 320, "height": 245}, {"url": "https://preview.redd.it/zzhpqstv00y91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6cf50af632eb7108bb307842077a3dfff34b266", "width": 640, "height": 490}, {"url": "https://preview.redd.it/zzhpqstv00y91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9fb0c7ff45a6bf194404debef1cbd8a7ca8cfc00", "width": 960, "height": 736}, {"url": "https://preview.redd.it/zzhpqstv00y91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4cadec700dac7b9d709cfba680ed4ccaefd5bb1d", "width": 1080, "height": 828}], "variants": {}, "id": "PWZl6j4Y2GGVEfoR03Nq1B1ntNzjz4_sqUhMHlZP-E8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymad4r", "is_robot_indexable": true, "report_reasons": null, "author": "TimboCA", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymad4r/finally_figured_out_a_basic_scraper_in_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zzhpqstv00y91.png", "subreddit_subscribers": 817376, "created_utc": 1667595780.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}