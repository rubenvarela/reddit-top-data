{"kind": "Listing", "data": {"after": null, "dist": 16, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all!\n\nAround 2 months back I was unfortunately a part of a mass layoff and had to start looking for a new job. I had come to this community for help in improving my resume (My post was removed as I had not used the weekly thread for resume review. Will keep that in mind next time). But even for the brief period of time that the post was up, I had received a lot of good advice which I incorporated.\n\nI start my new role next week. This community played a significant part in my success that I wanted to acknowledge. I'm not sure if these kinds of posts are allowed, if not they'll be removed anyway :p\n\nI'd like to personally thank the following users who gave very clear and concise advice that I was able to use to enhance my resume. u/abitofaLuna-tic, u/chunzilla, u/VacuousWaffle, u/denim_duck,  u/proverbialbunny. Thank you all!\n\n**P.S:** A common point that was suggested to me was to switch from a 2-column format to a 1-column format. This worked. When comparing the two resumes I saw how concise and easy to read my 1-column (single page) resume was. That being said, I've observed that my 2-column resume seemed to do really well with young startups, companies with smaller DS teams, or a more personal screening process. Just thought that this was interesting.", "author_fullname": "t2_40233m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thank you! An appreciation post of this DS community.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ym30il", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 142, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 142, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667578214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;Around 2 months back I was unfortunately a part of a mass layoff and had to start looking for a new job. I had come to this community for help in improving my resume (My post was removed as I had not used the weekly thread for resume review. Will keep that in mind next time). But even for the brief period of time that the post was up, I had received a lot of good advice which I incorporated.&lt;/p&gt;\n\n&lt;p&gt;I start my new role next week. This community played a significant part in my success that I wanted to acknowledge. I&amp;#39;m not sure if these kinds of posts are allowed, if not they&amp;#39;ll be removed anyway :p&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to personally thank the following users who gave very clear and concise advice that I was able to use to enhance my resume. &lt;a href=\"/u/abitofaLuna-tic\"&gt;u/abitofaLuna-tic&lt;/a&gt;, &lt;a href=\"/u/chunzilla\"&gt;u/chunzilla&lt;/a&gt;, &lt;a href=\"/u/VacuousWaffle\"&gt;u/VacuousWaffle&lt;/a&gt;, &lt;a href=\"/u/denim_duck\"&gt;u/denim_duck&lt;/a&gt;,  &lt;a href=\"/u/proverbialbunny\"&gt;u/proverbialbunny&lt;/a&gt;. Thank you all!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;P.S:&lt;/strong&gt; A common point that was suggested to me was to switch from a 2-column format to a 1-column format. This worked. When comparing the two resumes I saw how concise and easy to read my 1-column (single page) resume was. That being said, I&amp;#39;ve observed that my 2-column resume seemed to do really well with young startups, companies with smaller DS teams, or a more personal screening process. Just thought that this was interesting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 30, "id": "award_c4b2e438-16bb-4568-88e7-7893b7662944", "penny_donate": null, "award_sub_type": "PREMIUM", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=16&amp;height=16&amp;auto=webp&amp;s=1a331be5cf6d754b4cb7ed2ca3706f70d5260a57", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=32&amp;height=32&amp;auto=webp&amp;s=6d0a6351d4080286095df432f95a103cdf4188f2", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=48&amp;height=48&amp;auto=webp&amp;s=913e99a6f6688f26c08dcb411f043f71b17df931", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=64&amp;height=64&amp;auto=webp&amp;s=e3ad9900371bf1f91eb422b4d000b3a1c0d5a9c4", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=128&amp;height=128&amp;auto=webp&amp;s=4cc281fbace61e034477d2bdb7b158913457863d", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "A glittering stamp for a feel-good thing", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Wholesome Seal of Approval", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=16&amp;height=16&amp;auto=webp&amp;s=1a331be5cf6d754b4cb7ed2ca3706f70d5260a57", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=32&amp;height=32&amp;auto=webp&amp;s=6d0a6351d4080286095df432f95a103cdf4188f2", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=48&amp;height=48&amp;auto=webp&amp;s=913e99a6f6688f26c08dcb411f043f71b17df931", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=64&amp;height=64&amp;auto=webp&amp;s=e3ad9900371bf1f91eb422b4d000b3a1c0d5a9c4", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png?width=128&amp;height=128&amp;auto=webp&amp;s=4cc281fbace61e034477d2bdb7b158913457863d", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/b9ks3a5k7jj41_WholesomeSealofApproval.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym30il", "is_robot_indexable": true, "report_reasons": null, "author": "CrypticTac", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym30il/thank_you_an_appreciation_post_of_this_ds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym30il/thank_you_an_appreciation_post_of_this_ds/", "subreddit_subscribers": 817326, "created_utc": 1667578214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_646nndt4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with data correction ideas!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 63, "top_awarded_type": null, "hide_score": false, "name": "t3_ym7xf7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 70, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 70, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/XZQBoeYkHor8sauoGpE89qUu215Xelc49ZeTtjUHxi0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667590002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/07okhwaljzx91.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/07okhwaljzx91.png?auto=webp&amp;s=3abbb8edb7a79cb532fa70a6874deaea57ccdd19", "width": 1370, "height": 626}, "resolutions": [{"url": "https://preview.redd.it/07okhwaljzx91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f436c3a594c9d798aee4b0438879cd2d4fb3fd7e", "width": 108, "height": 49}, {"url": "https://preview.redd.it/07okhwaljzx91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c10c3c424f76c22eba507ad18593748a291aa2a", "width": 216, "height": 98}, {"url": "https://preview.redd.it/07okhwaljzx91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=564cfc465cbea7714cfa9b492005025718325bde", "width": 320, "height": 146}, {"url": "https://preview.redd.it/07okhwaljzx91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5da4e5cca6926a257ffd45e15e4b8ce6121ebfd3", "width": 640, "height": 292}, {"url": "https://preview.redd.it/07okhwaljzx91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa72b08f4171aa0c7169c26571f9f670e6f03473", "width": 960, "height": 438}, {"url": "https://preview.redd.it/07okhwaljzx91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16e622297f11f63958db60fc89410cdb8ac7669f", "width": 1080, "height": 493}], "variants": {}, "id": "4BAoQm9jbGgjC1B8VxdCqj3zYDmFb8ikr_ZRR5HiLdA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym7xf7", "is_robot_indexable": true, "report_reasons": null, "author": "vanslife4511", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym7xf7/need_help_with_data_correction_ideas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/07okhwaljzx91.png", "subreddit_subscribers": 817326, "created_utc": 1667590002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Howdy Data folks,\n\nI'm in the retail space and trying to basically forecast sales for 2023. I took over the BI/data role after the guy previously in the role left earlier this year. He built a projection basically using previous sales from the last couple years (and I'm still trying to read through his python code to figure out how he came to the calculation btw), but I feel like with the economy and what not-things could be so up and down that maybe we shouldnt rely on previous years sales. \n\nAre there any data sources I should be considering looking at, in order to better verify sales/projections for next year? \n\nAny help or insight would be VASTLY appreciated.", "author_fullname": "t2_7meg6iov", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forecasting retail sales in 2023? Do you use anything in particular for insight?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ym4xxj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667582891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy Data folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in the retail space and trying to basically forecast sales for 2023. I took over the BI/data role after the guy previously in the role left earlier this year. He built a projection basically using previous sales from the last couple years (and I&amp;#39;m still trying to read through his python code to figure out how he came to the calculation btw), but I feel like with the economy and what not-things could be so up and down that maybe we shouldnt rely on previous years sales. &lt;/p&gt;\n\n&lt;p&gt;Are there any data sources I should be considering looking at, in order to better verify sales/projections for next year? &lt;/p&gt;\n\n&lt;p&gt;Any help or insight would be VASTLY appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym4xxj", "is_robot_indexable": true, "report_reasons": null, "author": "WhatsTheAnswerDude", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym4xxj/forecasting_retail_sales_in_2023_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym4xxj/forecasting_retail_sales_in_2023_do_you_use/", "subreddit_subscribers": 817326, "created_utc": 1667582891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am building a MC simulation in Python for a sample portfolio that contains various weightings of asset classes (think like 10% stocks, 5% bonds, 6% mutual funds, etc). The returns of these variables are all correlated with each other in some way. I have the mean and SD of each of their returns and the correlation matrix. The distribution of each individual return is normally distributed. \n\nI am currently using scipy.multivariate_normal but not sure if this is the right approach or not for simulating n number of years of returns. \n\nAny advice is appreciated!", "author_fullname": "t2_gzra5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to account for correlated variables in a Monte Carlo simulation in Python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymadxr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667599223.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667595833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am building a MC simulation in Python for a sample portfolio that contains various weightings of asset classes (think like 10% stocks, 5% bonds, 6% mutual funds, etc). The returns of these variables are all correlated with each other in some way. I have the mean and SD of each of their returns and the correlation matrix. The distribution of each individual return is normally distributed. &lt;/p&gt;\n\n&lt;p&gt;I am currently using scipy.multivariate_normal but not sure if this is the right approach or not for simulating n number of years of returns. &lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymadxr", "is_robot_indexable": true, "report_reasons": null, "author": "zferguson", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymadxr/how_to_account_for_correlated_variables_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymadxr/how_to_account_for_correlated_variables_in_a/", "subreddit_subscribers": 817326, "created_utc": 1667595833.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_3fi5o45p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which project are you the most proud of? Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymh10u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667613246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymh10u", "is_robot_indexable": true, "report_reasons": null, "author": "HairyProtection", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymh10u/which_project_are_you_the_most_proud_of_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymh10u/which_project_are_you_the_most_proud_of_why/", "subreddit_subscribers": 817326, "created_utc": 1667613246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nI've been asked this question during an interview. \n\nI have currently 3 YOE as a Data Scientist in the finance industry and I have applied to this small tech startup (&lt;20 people, &lt;10 technical people). They have no DS/ML people on their team, so I would be their first, and I would be working on a subject very similar to what I have already implemented in my current company.\n\nThe interviewer asked me this question after I gave him my low range of salary expectations.\n\nHow do you know when you are ready to be the head of data science of a company ? What mandatory skills should you have ? Since it's a small company should I expect to be responsible of the data engineering part also ? What should I clarify with them in my next interview ?", "author_fullname": "t2_6b5q0a66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are you ready to lead a DS team and be the head of data science ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ym13he", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667573585.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been asked this question during an interview. &lt;/p&gt;\n\n&lt;p&gt;I have currently 3 YOE as a Data Scientist in the finance industry and I have applied to this small tech startup (&amp;lt;20 people, &amp;lt;10 technical people). They have no DS/ML people on their team, so I would be their first, and I would be working on a subject very similar to what I have already implemented in my current company.&lt;/p&gt;\n\n&lt;p&gt;The interviewer asked me this question after I gave him my low range of salary expectations.&lt;/p&gt;\n\n&lt;p&gt;How do you know when you are ready to be the head of data science of a company ? What mandatory skills should you have ? Since it&amp;#39;s a small company should I expect to be responsible of the data engineering part also ? What should I clarify with them in my next interview ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym13he", "is_robot_indexable": true, "report_reasons": null, "author": "Intrepid_Evening", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym13he/are_you_ready_to_lead_a_ds_team_and_be_the_head/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym13he/are_you_ready_to_lead_a_ds_team_and_be_the_head/", "subreddit_subscribers": 817326, "created_utc": 1667573585.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In our company, we are mainly working on handling duplicate data. Not the exact matches, similar records, too. What are the common problems in your company and how you are handling these issues?", "author_fullname": "t2_5ddglx8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the most common data processing problem?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylv12d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667557544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In our company, we are mainly working on handling duplicate data. Not the exact matches, similar records, too. What are the common problems in your company and how you are handling these issues?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylv12d", "is_robot_indexable": true, "report_reasons": null, "author": "alka_irl", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylv12d/what_is_the_most_common_data_processing_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylv12d/what_is_the_most_common_data_processing_problem/", "subreddit_subscribers": 817326, "created_utc": 1667557544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_eg4u9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most problems are summary stats problems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 103, "top_awarded_type": null, "hide_score": false, "name": "t3_ymjob8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JxGJ8m7GUUvasmtuTnjlk4cC8Ck3_2OzkK45s-dQrsk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667621210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgflip.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://imgflip.com/i/6zlhnz", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nZgz3bddyTZpxvP6RUwlynpBKDYzKtcFR1TefBtiIqM.jpg?auto=webp&amp;s=df9a71544da6509bf796178ac29d6e0883c9b1f1", "width": 675, "height": 499}, "resolutions": [{"url": "https://external-preview.redd.it/nZgz3bddyTZpxvP6RUwlynpBKDYzKtcFR1TefBtiIqM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1cd47f4c9896859de24407a48dde34c8b8abd7b", "width": 108, "height": 79}, {"url": "https://external-preview.redd.it/nZgz3bddyTZpxvP6RUwlynpBKDYzKtcFR1TefBtiIqM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8240f81c35c0279e67b6065c321d1478acb8f8d8", "width": 216, "height": 159}, {"url": "https://external-preview.redd.it/nZgz3bddyTZpxvP6RUwlynpBKDYzKtcFR1TefBtiIqM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4e22cc2a1e6434e9b6805c0eb60f3cdaa0eecb0c", "width": 320, "height": 236}, {"url": "https://external-preview.redd.it/nZgz3bddyTZpxvP6RUwlynpBKDYzKtcFR1TefBtiIqM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=45047658620bd32a6a7cde637bc05961df533ece", "width": 640, "height": 473}], "variants": {}, "id": "yWtS-vJuzYM_wblMJHn5uNjCKmsN53aq_EjGOFGbLfU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymjob8", "is_robot_indexable": true, "report_reasons": null, "author": "equivocal20", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymjob8/most_problems_are_summary_stats_problems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgflip.com/i/6zlhnz", "subreddit_subscribers": 817326, "created_utc": 1667621210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I  can't even remember where I picked this up, but my understanding is  that when building an ML model, and you want to know how well that model  really predicts on unseen (test) data - to give you an indication of  how well it will perform in the field, one doesn't just do a train test  split once and evaluate your model on the test set, but rather, one  should do this many times and take the mean performance from the test  set. Is this actually correct? Here is some code to indicate what I  mean, note that in this example I'm not including any data, but just  assume that X is an np.array of predictors and y is an np.array with a binary outcome:\n\n`from sklearn.linear_model import ElasticNetCV`\n\n`from sklearn.metrics import roc_curve, auc`\n\n`auc_list = []`\n\n`for i in range(30):`\n\n`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)`\n\n`train_means = X_train.mean(axis = 0)`\n\n`train_sd = X_train.std(axis = 0)`\n\n`X_train = (X_train - train_means)/train_sd`\n\n`X_test = (X_test - train_means)/train_sd`\n\n`lr_model = ElasticNetCV(cv=5)`\n\n`lr_model.fit(X_train, y_train)`\n\n`y_preds = lr_model.predict(X_test)`\n\n`fpr, tpr, thresholds = roc_curve(y_test, y_preds, pos_label=1)`\n\n`auc_roc = auc(fpr, tpr)`\n\n`auc_list.append(auc_roc)`\n\nSo  above I evaluate the AUC of my model on the testing set, and I do this  on 30 different train/test splits, and I then look at the mean AUC  across the 30 splits - np.mean(auc\\_list)\n\nIs  this a correct procedure? My intuition is that there may be some train  test splits that give particularly good or bad predictions due to the  randomness of the split, and so one must take the mean of many splits.\n\nHowever,  when I look at every single online tutorial or online course on how to  build an ml model, this aspect of it is never included, so much so to  the extent that I'm now wondering if I've been doing it wrong in taking  this approach? So does anyone know what is correct? And if my way is  correct, why is it that this is never included in online tutorials?\n\nMany thanks!", "author_fullname": "t2_lrovl6pi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The mean of multiple train/test splits is required in order to evaluate an ML model(?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yly3wd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667565995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I  can&amp;#39;t even remember where I picked this up, but my understanding is  that when building an ML model, and you want to know how well that model  really predicts on unseen (test) data - to give you an indication of  how well it will perform in the field, one doesn&amp;#39;t just do a train test  split once and evaluate your model on the test set, but rather, one  should do this many times and take the mean performance from the test  set. Is this actually correct? Here is some code to indicate what I  mean, note that in this example I&amp;#39;m not including any data, but just  assume that X is an np.array of predictors and y is an np.array with a binary outcome:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;from sklearn.linear_model import ElasticNetCV&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;from sklearn.metrics import roc_curve, auc&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;auc_list = []&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;for i in range(30):&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;train_means = X_train.mean(axis = 0)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;train_sd = X_train.std(axis = 0)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;X_train = (X_train - train_means)/train_sd&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;X_test = (X_test - train_means)/train_sd&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;lr_model = ElasticNetCV(cv=5)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;lr_model.fit(X_train, y_train)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;y_preds = lr_model.predict(X_test)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;fpr, tpr, thresholds = roc_curve(y_test, y_preds, pos_label=1)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;auc_roc = auc(fpr, tpr)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;auc_list.append(auc_roc)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;So  above I evaluate the AUC of my model on the testing set, and I do this  on 30 different train/test splits, and I then look at the mean AUC  across the 30 splits - np.mean(auc_list)&lt;/p&gt;\n\n&lt;p&gt;Is  this a correct procedure? My intuition is that there may be some train  test splits that give particularly good or bad predictions due to the  randomness of the split, and so one must take the mean of many splits.&lt;/p&gt;\n\n&lt;p&gt;However,  when I look at every single online tutorial or online course on how to  build an ml model, this aspect of it is never included, so much so to  the extent that I&amp;#39;m now wondering if I&amp;#39;ve been doing it wrong in taking  this approach? So does anyone know what is correct? And if my way is  correct, why is it that this is never included in online tutorials?&lt;/p&gt;\n\n&lt;p&gt;Many thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yly3wd", "is_robot_indexable": true, "report_reasons": null, "author": "likeamanyfacedgod", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yly3wd/the_mean_of_multiple_traintest_splits_is_required/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yly3wd/the_mean_of_multiple_traintest_splits_is_required/", "subreddit_subscribers": 817326, "created_utc": 1667565995.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_91atae5g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Share your experiences using windows vs mac in data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylzrmj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667570256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylzrmj", "is_robot_indexable": true, "report_reasons": null, "author": "MalaysiaDankMeme", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylzrmj/share_your_experiences_using_windows_vs_mac_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylzrmj/share_your_experiences_using_windows_vs_mac_in/", "subreddit_subscribers": 817326, "created_utc": 1667570256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For those of you that work freelance, how do you deal with requests to e.g. give a presentation or give advice at a workshop? I fairly often get requests such as these, typically they would like me to attend a workshop to discuss their work, give input and possibly make a short presentation on my area of expertise. Invariably, my time would not be paid. It can be from an hour up to a half day of time or more.\n\nBack when I was working in academia or in a research position, I would usually say yes to these requests because I would be interested to hear about new work, meet new people, etc. I was able to attend them during working hours so effectively it was part of my job.\n\nNow working freelance, if I spend half a day attending a workshop to help a project, that's half a day that I can't work on paid work (or else taking time off) so I'm losing out.\n\nProblem is, I think people who haven't worked freelance often don't appreciate this fact. So when I turn down such requests I'm probably coming across as unhelpful. If I ask to be paid I seem like a dick.\n\nAnyone else had this problem and have thoughts on how to tackle it? Am I just overthinking it?", "author_fullname": "t2_45ni7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with pro bono requests?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymbtyw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667599399.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those of you that work freelance, how do you deal with requests to e.g. give a presentation or give advice at a workshop? I fairly often get requests such as these, typically they would like me to attend a workshop to discuss their work, give input and possibly make a short presentation on my area of expertise. Invariably, my time would not be paid. It can be from an hour up to a half day of time or more.&lt;/p&gt;\n\n&lt;p&gt;Back when I was working in academia or in a research position, I would usually say yes to these requests because I would be interested to hear about new work, meet new people, etc. I was able to attend them during working hours so effectively it was part of my job.&lt;/p&gt;\n\n&lt;p&gt;Now working freelance, if I spend half a day attending a workshop to help a project, that&amp;#39;s half a day that I can&amp;#39;t work on paid work (or else taking time off) so I&amp;#39;m losing out.&lt;/p&gt;\n\n&lt;p&gt;Problem is, I think people who haven&amp;#39;t worked freelance often don&amp;#39;t appreciate this fact. So when I turn down such requests I&amp;#39;m probably coming across as unhelpful. If I ask to be paid I seem like a dick.&lt;/p&gt;\n\n&lt;p&gt;Anyone else had this problem and have thoughts on how to tackle it? Am I just overthinking it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymbtyw", "is_robot_indexable": true, "report_reasons": null, "author": "dr_chickolas", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymbtyw/how_to_deal_with_pro_bono_requests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ymbtyw/how_to_deal_with_pro_bono_requests/", "subreddit_subscribers": 817326, "created_utc": 1667599399.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi!\n\nI've seen similar posts here, so maybe you will help me too since I have really no idea what figures to put for this project as it is my first one and I can't find any similar ones in some other posts like this one or even on Fiverr ( If it's even a good place to look for price estimation ).\n\nSo, project looks like this ( Sorry for the bad explanation, I don't know how to put it more simply):\n\nI need to automate a creation of a file containing forecasts data for a company ( multiple companies with a little bit different data to work with ), to be more precise:  \n\\- Each company has its own data in csv file, I need to extract the data from it for each part that we sell to them ( part, quantity, year, month )  \n\\- Then I create an excel file containing the extracted data, in the excel file are sheets for each part that shows new data in each row ( updated weekly ) and some basic calculation of total quantity, total sum, change in time and the excel file also contains sheets with each month of a certain year with data for all parts and similar work done as above, only with different formatting  \n\\- I have to also make a GUI for this as the end user won't always be me, but also my colleagues and boss that need to have it as simple as possible.\n\nFor this project I use Python, Pandas, OpenPyXL and plan to use Kivy for interface ( I will do it last ).\n\nAlso, I live in Europe and more specifically in Poland, it will have a great impact on the price I think.\n\nOne last thing - I don't think that in my case charging per hour makes sense, since I'm also learning new stuff on the way and don't think it would be fair.\n\nAny help would be much appreciated!", "author_fullname": "t2_2cd289b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[EU] How much should I charge for an automation project ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ym8oex", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667591819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen similar posts here, so maybe you will help me too since I have really no idea what figures to put for this project as it is my first one and I can&amp;#39;t find any similar ones in some other posts like this one or even on Fiverr ( If it&amp;#39;s even a good place to look for price estimation ).&lt;/p&gt;\n\n&lt;p&gt;So, project looks like this ( Sorry for the bad explanation, I don&amp;#39;t know how to put it more simply):&lt;/p&gt;\n\n&lt;p&gt;I need to automate a creation of a file containing forecasts data for a company ( multiple companies with a little bit different data to work with ), to be more precise:&lt;br/&gt;\n- Each company has its own data in csv file, I need to extract the data from it for each part that we sell to them ( part, quantity, year, month )&lt;br/&gt;\n- Then I create an excel file containing the extracted data, in the excel file are sheets for each part that shows new data in each row ( updated weekly ) and some basic calculation of total quantity, total sum, change in time and the excel file also contains sheets with each month of a certain year with data for all parts and similar work done as above, only with different formatting&lt;br/&gt;\n- I have to also make a GUI for this as the end user won&amp;#39;t always be me, but also my colleagues and boss that need to have it as simple as possible.&lt;/p&gt;\n\n&lt;p&gt;For this project I use Python, Pandas, OpenPyXL and plan to use Kivy for interface ( I will do it last ).&lt;/p&gt;\n\n&lt;p&gt;Also, I live in Europe and more specifically in Poland, it will have a great impact on the price I think.&lt;/p&gt;\n\n&lt;p&gt;One last thing - I don&amp;#39;t think that in my case charging per hour makes sense, since I&amp;#39;m also learning new stuff on the way and don&amp;#39;t think it would be fair.&lt;/p&gt;\n\n&lt;p&gt;Any help would be much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym8oex", "is_robot_indexable": true, "report_reasons": null, "author": "Hyalskavran", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym8oex/eu_how_much_should_i_charge_for_an_automation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym8oex/eu_how_much_should_i_charge_for_an_automation/", "subreddit_subscribers": 817326, "created_utc": 1667591819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was curious if someone could help me figure out a method of determining growth rates for a data set with a large range.\n\nFor example, the growth rate over a year for 100 different cities. Some cities may start with 100 people and others may start with 100,000 or 1m. If a 100 person city grows by 100 people thats a 100% growth rate, however a city of 100,000's growth rate will be much lower.\n\nMy issue is that if I sort growth rates of all cities descending, I will get pretty much only the smallest cities and their crazy high growth rates.\n\nOne solution I thought of was to create \"bands\" of cities (IE: 0-100, 100-1000, 1000-10,000, etc) and test each of their growth rates, however, I believe there might be a better way to \"weight\" certain cities growth rates depending on their size?", "author_fullname": "t2_fc3cf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to sort or rank growth rates across large range data sets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ym74qp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667588115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was curious if someone could help me figure out a method of determining growth rates for a data set with a large range.&lt;/p&gt;\n\n&lt;p&gt;For example, the growth rate over a year for 100 different cities. Some cities may start with 100 people and others may start with 100,000 or 1m. If a 100 person city grows by 100 people thats a 100% growth rate, however a city of 100,000&amp;#39;s growth rate will be much lower.&lt;/p&gt;\n\n&lt;p&gt;My issue is that if I sort growth rates of all cities descending, I will get pretty much only the smallest cities and their crazy high growth rates.&lt;/p&gt;\n\n&lt;p&gt;One solution I thought of was to create &amp;quot;bands&amp;quot; of cities (IE: 0-100, 100-1000, 1000-10,000, etc) and test each of their growth rates, however, I believe there might be a better way to &amp;quot;weight&amp;quot; certain cities growth rates depending on their size?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym74qp", "is_robot_indexable": true, "report_reasons": null, "author": "johnnyhighschool", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym74qp/how_to_sort_or_rank_growth_rates_across_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym74qp/how_to_sort_or_rank_growth_rates_across_large/", "subreddit_subscribers": 817326, "created_utc": 1667588115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7ao0u2en", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you apply to US based roles from an international country?? (applying for jobs from companies that sponsor visas). Would you even get a response?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yluqeg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667556581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "yluqeg", "is_robot_indexable": true, "report_reasons": null, "author": "the_scientist-7367", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yluqeg/can_you_apply_to_us_based_roles_from_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yluqeg/can_you_apply_to_us_based_roles_from_an/", "subreddit_subscribers": 817326, "created_utc": 1667556581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I'm a software engineer who is learning about deep learning + machine learning.\n\nI've been exploring time series prediction using TensorFlow, PyTorch, prophet etc.\n\nAssuming you can nicely split your data for training, test, validation is there a library or framework that lets you pass this exactly dataset to multiple or many different time series libraries and plot + evaluate the MAPE + other statistics?\n\nExample:\n\n* I nicely load, feature engineer and split my datasets\n* I then want to pass these into the following:\n   * prophet\n   * neuralprophet\n   * keras\n   * TensorFlow probability\n   * etc\n* And then I want to plot all of these results (MAPE etc) sorted from lowest to highest etc\n\nI presume (hope) that this has been solved for already and that I wouldn't have to roll my own version but just wondering what's out there?\n\nThis was the post that inspired me to ask the question\n\n[https://bytepawn.com/comparing-neuralprophet-and-prophet-for-timeseries-forecasting.html](https://bytepawn.com/comparing-neuralprophet-and-prophet-for-timeseries-forecasting.html)\n\nThanks", "author_fullname": "t2_sdb0o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Framework to compare TimeSeries models against a single set of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylrxxq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667547457.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m a software engineer who is learning about deep learning + machine learning.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been exploring time series prediction using TensorFlow, PyTorch, prophet etc.&lt;/p&gt;\n\n&lt;p&gt;Assuming you can nicely split your data for training, test, validation is there a library or framework that lets you pass this exactly dataset to multiple or many different time series libraries and plot + evaluate the MAPE + other statistics?&lt;/p&gt;\n\n&lt;p&gt;Example:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I nicely load, feature engineer and split my datasets&lt;/li&gt;\n&lt;li&gt;I then want to pass these into the following:\n\n&lt;ul&gt;\n&lt;li&gt;prophet&lt;/li&gt;\n&lt;li&gt;neuralprophet&lt;/li&gt;\n&lt;li&gt;keras&lt;/li&gt;\n&lt;li&gt;TensorFlow probability&lt;/li&gt;\n&lt;li&gt;etc&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;And then I want to plot all of these results (MAPE etc) sorted from lowest to highest etc&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I presume (hope) that this has been solved for already and that I wouldn&amp;#39;t have to roll my own version but just wondering what&amp;#39;s out there?&lt;/p&gt;\n\n&lt;p&gt;This was the post that inspired me to ask the question&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://bytepawn.com/comparing-neuralprophet-and-prophet-for-timeseries-forecasting.html\"&gt;https://bytepawn.com/comparing-neuralprophet-and-prophet-for-timeseries-forecasting.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylrxxq", "is_robot_indexable": true, "report_reasons": null, "author": "Javaguy44", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylrxxq/framework_to_compare_timeseries_models_against_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylrxxq/framework_to_compare_timeseries_models_against_a/", "subreddit_subscribers": 817326, "created_utc": 1667547457.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_8azmn3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finally figured out a basic \"scraper\" in Google Sheets to refresh data from CDC's disconnected webpages!! Proud of myself, but also know there's a faster/better way to do this...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 107, "top_awarded_type": null, "hide_score": false, "name": "t3_ymad4r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/aXeOf1Z1pgx2ufcYllooEt_iBp0vyjPIUpJhwsFCL88.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667595780.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zzhpqstv00y91.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zzhpqstv00y91.png?auto=webp&amp;s=1230378ad0b23bcfb05a1557996ed0f311e1e66a", "width": 1310, "height": 1005}, "resolutions": [{"url": "https://preview.redd.it/zzhpqstv00y91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b780f4068479fa9a3f60772b41f54ce1ec2dbabd", "width": 108, "height": 82}, {"url": "https://preview.redd.it/zzhpqstv00y91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=429e1e814e4b18a90ac4418f350945a4bb2d5041", "width": 216, "height": 165}, {"url": "https://preview.redd.it/zzhpqstv00y91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0da61dcd83be9fc58c7b6e6b7e6f6248b3475924", "width": 320, "height": 245}, {"url": "https://preview.redd.it/zzhpqstv00y91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6cf50af632eb7108bb307842077a3dfff34b266", "width": 640, "height": 490}, {"url": "https://preview.redd.it/zzhpqstv00y91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9fb0c7ff45a6bf194404debef1cbd8a7ca8cfc00", "width": 960, "height": 736}, {"url": "https://preview.redd.it/zzhpqstv00y91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4cadec700dac7b9d709cfba680ed4ccaefd5bb1d", "width": 1080, "height": 828}], "variants": {}, "id": "PWZl6j4Y2GGVEfoR03Nq1B1ntNzjz4_sqUhMHlZP-E8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ymad4r", "is_robot_indexable": true, "report_reasons": null, "author": "TimboCA", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ymad4r/finally_figured_out_a_basic_scraper_in_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zzhpqstv00y91.png", "subreddit_subscribers": 817326, "created_utc": 1667595780.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}