{"kind": "Listing", "data": {"after": "t3_ykkupe", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As of July 1st this year all health insurers in the US were required to publish files on their websites of all their negotiated prices they have for every possible medical procedure with every doctor in the country. In totality this data set equates to trillions of rows and hundreds of TB of data.\n\nI'm interested in building out a collaborative effort to aggregate all this data, but the cost of hosting seems to be a huge problem. What's the cheapest, effective way to host all this data in such a way that it's publicly accessible?\n\nEdit: so exciting to see how many people are interested in this problem! Thank you all for your thoughts.. time for me to step back and evaluate the best way forward based on all this new info. I'll be back =)", "author_fullname": "t2_457f1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help hosting trillions of rows of new health insurance public price data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yk9gye", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 212, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 212, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667447857.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667405767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As of July 1st this year all health insurers in the US were required to publish files on their websites of all their negotiated prices they have for every possible medical procedure with every doctor in the country. In totality this data set equates to trillions of rows and hundreds of TB of data.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in building out a collaborative effort to aggregate all this data, but the cost of hosting seems to be a huge problem. What&amp;#39;s the cheapest, effective way to host all this data in such a way that it&amp;#39;s publicly accessible?&lt;/p&gt;\n\n&lt;p&gt;Edit: so exciting to see how many people are interested in this problem! Thank you all for your thoughts.. time for me to step back and evaluate the best way forward based on all this new info. I&amp;#39;ll be back =)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yk9gye", "is_robot_indexable": true, "report_reasons": null, "author": "invisiblelemur88", "discussion_type": null, "num_comments": 91, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yk9gye/help_hosting_trillions_of_rows_of_new_health/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yk9gye/help_hosting_trillions_of_rows_of_new_health/", "subreddit_subscribers": 817003, "created_utc": 1667405767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is it just me or are the vast majority of job adverts on linked in right now for senior/lead/principal data scientists? (UK btw)\n\nI only saw a single advert for a junior role and this had over 200 applications in just a few a hours of being released.", "author_fullname": "t2_1rp1btfa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No one is hiring juniors/ mid-level data scientists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykyte6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 126, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 126, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667471938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it just me or are the vast majority of job adverts on linked in right now for senior/lead/principal data scientists? (UK btw)&lt;/p&gt;\n\n&lt;p&gt;I only saw a single advert for a junior role and this had over 200 applications in just a few a hours of being released.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykyte6", "is_robot_indexable": true, "report_reasons": null, "author": "nullspace1729", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykyte6/no_one_is_hiring_juniors_midlevel_data_scientists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykyte6/no_one_is_hiring_juniors_midlevel_data_scientists/", "subreddit_subscribers": 817003, "created_utc": 1667471938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been doing data sci for 4 years, but actually, I think that I'm pretty crap at it... I program really well in R, but no one wants it, my Python is intermediate. I'm pretty good at stats from my PhD days, but that doesn't really come in handy in practice. When I do am ML project it's always regression based and seems to do the trick.... I did some deep learning courses, but I'm not sure if there are really any use cases out there for that... I basically feel like I'm not particularly well specialized. What thing could i really work on to start doing more exciting work and improve my job outlook? For example, really killing it with the deep learning is my first intuition, but it seems that in practice this skill is not all that sought after or needed.... What do you think?", "author_fullname": "t2_lrovl6pi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What skills do I need to really work on?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykaxuh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667409178.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been doing data sci for 4 years, but actually, I think that I&amp;#39;m pretty crap at it... I program really well in R, but no one wants it, my Python is intermediate. I&amp;#39;m pretty good at stats from my PhD days, but that doesn&amp;#39;t really come in handy in practice. When I do am ML project it&amp;#39;s always regression based and seems to do the trick.... I did some deep learning courses, but I&amp;#39;m not sure if there are really any use cases out there for that... I basically feel like I&amp;#39;m not particularly well specialized. What thing could i really work on to start doing more exciting work and improve my job outlook? For example, really killing it with the deep learning is my first intuition, but it seems that in practice this skill is not all that sought after or needed.... What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykaxuh", "is_robot_indexable": true, "report_reasons": null, "author": "likeamanyfacedgod", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykaxuh/what_skills_do_i_need_to_really_work_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykaxuh/what_skills_do_i_need_to_really_work_on/", "subreddit_subscribers": 817003, "created_utc": 1667409178.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_bb4m08u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing OpenAI GPT3 in Airtable. Fine-tuning gpt3 even with a very small dataset (50 in this case) appears to work pretty well. Any interesting use cases that you'd recommend testing with?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_ykybpj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 36, "total_awards_received": 0, "media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Running OpenAI in Airtable. Extracting structured JSON data from unstructured resume text.&lt;br&gt;&lt;br&gt;Fine-tuning gpt3 even with a very small dataset (50 in this case) appears to work extremely well.&lt;br&gt;&lt;br&gt;Also tested converting/extracting data from bank transactions, logs, emails &amp;amp; other data &lt;a href=\"https://t.co/k2596TLvFc\"&gt;pic.twitter.com/k2596TLvFc&lt;/a&gt;&lt;/p&gt;&amp;mdash; Igor Nefedov (@igornefedovi) &lt;a href=\"https://twitter.com/igornefedovi/status/1588032734315704320?ref_src=twsrc%5Etfw\"&gt;November 3, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"oembed": {"provider_url": "https://twitter.com", "url": "https://twitter.com/igornefedovi/status/1588032734315704320", "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Running OpenAI in Airtable. Extracting structured JSON data from unstructured resume text.&lt;br&gt;&lt;br&gt;Fine-tuning gpt3 even with a very small dataset (50 in this case) appears to work extremely well.&lt;br&gt;&lt;br&gt;Also tested converting/extracting data from bank transactions, logs, emails &amp;amp; other data &lt;a href=\"https://t.co/k2596TLvFc\"&gt;pic.twitter.com/k2596TLvFc&lt;/a&gt;&lt;/p&gt;&amp;mdash; Igor Nefedov (@igornefedovi) &lt;a href=\"https://twitter.com/igornefedovi/status/1588032734315704320?ref_src=twsrc%5Etfw\"&gt;November 3, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_name": "Igor Nefedov", "height": null, "width": 350, "version": "1.0", "author_url": "https://twitter.com/igornefedovi", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}, "type": "twitter.com"}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Running OpenAI in Airtable. Extracting structured JSON data from unstructured resume text.&lt;br&gt;&lt;br&gt;Fine-tuning gpt3 even with a very small dataset (50 in this case) appears to work extremely well.&lt;br&gt;&lt;br&gt;Also tested converting/extracting data from bank transactions, logs, emails &amp;amp; other data &lt;a href=\"https://t.co/k2596TLvFc\"&gt;pic.twitter.com/k2596TLvFc&lt;/a&gt;&lt;/p&gt;&amp;mdash; Igor Nefedov (@igornefedovi) &lt;a href=\"https://twitter.com/igornefedovi/status/1588032734315704320?ref_src=twsrc%5Etfw\"&gt;November 3, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/ykybpj", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/H5M11fv0b8Wy11cjAmYQFWbHB-Y653u0jbpFjTdc5o4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667470649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "twitter.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://twitter.com/igornefedovi/status/1588032734315704320", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Q3_cAr_0AWvl2doPMq2oGYGlBrKQB6CXdpzOFQmnUBk.jpg?auto=webp&amp;s=438a341ce2e08c3d99c58319f7ff907ff8f90806", "width": 140, "height": 78}, "resolutions": [{"url": "https://external-preview.redd.it/Q3_cAr_0AWvl2doPMq2oGYGlBrKQB6CXdpzOFQmnUBk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0cf5cae1ce8dfd29d83b91454d5da4ef40e8288b", "width": 108, "height": 60}], "variants": {}, "id": "HYnQ8FaO96_yByBsMJgxlZn_e9b7QzNp8IGJjv5tEac"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykybpj", "is_robot_indexable": true, "report_reasons": null, "author": "igornefedovi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykybpj/testing_openai_gpt3_in_airtable_finetuning_gpt3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://twitter.com/igornefedovi/status/1588032734315704320", "subreddit_subscribers": 817003, "created_utc": 1667470649.0, "num_crossposts": 0, "media": {"oembed": {"provider_url": "https://twitter.com", "url": "https://twitter.com/igornefedovi/status/1588032734315704320", "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Running OpenAI in Airtable. Extracting structured JSON data from unstructured resume text.&lt;br&gt;&lt;br&gt;Fine-tuning gpt3 even with a very small dataset (50 in this case) appears to work extremely well.&lt;br&gt;&lt;br&gt;Also tested converting/extracting data from bank transactions, logs, emails &amp;amp; other data &lt;a href=\"https://t.co/k2596TLvFc\"&gt;pic.twitter.com/k2596TLvFc&lt;/a&gt;&lt;/p&gt;&amp;mdash; Igor Nefedov (@igornefedovi) &lt;a href=\"https://twitter.com/igornefedovi/status/1588032734315704320?ref_src=twsrc%5Etfw\"&gt;November 3, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_name": "Igor Nefedov", "height": null, "width": 350, "version": "1.0", "author_url": "https://twitter.com/igornefedovi", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}, "type": "twitter.com"}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi folks\n\nI was wondering if there are any free sentiment analysis tools that are pre-trained (on typical customer support quer), so that I can run some text through it to get a general idea of positivity negativity? It\u2019s not a whole lot of text, maybe several thousand paragraphs.\n\nThanks.", "author_fullname": "t2_4zxcnppv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sentiment analysis of customer support tickets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykmpgt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667434158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks&lt;/p&gt;\n\n&lt;p&gt;I was wondering if there are any free sentiment analysis tools that are pre-trained (on typical customer support quer), so that I can run some text through it to get a general idea of positivity negativity? It\u2019s not a whole lot of text, maybe several thousand paragraphs.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykmpgt", "is_robot_indexable": true, "report_reasons": null, "author": "enigmapaulns", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykmpgt/sentiment_analysis_of_customer_support_tickets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykmpgt/sentiment_analysis_of_customer_support_tickets/", "subreddit_subscribers": 817003, "created_utc": 1667434158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is it not almost always better to use probability output? So the business had a view of the model confidence for each prediction and can act on this information rather than a more black box output?", "author_fullname": "t2_gm8b3iu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In a business context, why would you ever use a binary output for a binary classifier? Its it almost always better to output class probability so the business can set their own threshold?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykytgo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667471944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it not almost always better to use probability output? So the business had a view of the model confidence for each prediction and can act on this information rather than a more black box output?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykytgo", "is_robot_indexable": true, "report_reasons": null, "author": "poppycocknbalderdash", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykytgo/in_a_business_context_why_would_you_ever_use_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykytgo/in_a_business_context_why_would_you_ever_use_a/", "subreddit_subscribers": 817003, "created_utc": 1667471944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to load some data and do the following tasks:\n\n1. load data from csv\n2. encode data types\n3. normalize data\n4. .... more to come...\n5. split data into two\n\nIs there a common Python design pattern approach for this type of pipeline data analysis?\n\nNot sure exactly what I need but it reminds me a little of a ***Builder*** pattern.\n\n    var myObject = myBuilder.addName(\"John Doe\").addAge(15).build()\n\nI've seen some packages that look to support it using decorators, but not sure if that's overcomplicating things or even a common approach.", "author_fullname": "t2_293ojlay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Data Analysis Patterns - A pipeline design?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yk8ski", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667404214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to load some data and do the following tasks:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;load data from csv&lt;/li&gt;\n&lt;li&gt;encode data types&lt;/li&gt;\n&lt;li&gt;normalize data&lt;/li&gt;\n&lt;li&gt;.... more to come...&lt;/li&gt;\n&lt;li&gt;split data into two&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Is there a common Python design pattern approach for this type of pipeline data analysis?&lt;/p&gt;\n\n&lt;p&gt;Not sure exactly what I need but it reminds me a little of a &lt;strong&gt;&lt;em&gt;Builder&lt;/em&gt;&lt;/strong&gt; pattern.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;var myObject = myBuilder.addName(&amp;quot;John Doe&amp;quot;).addAge(15).build()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;ve seen some packages that look to support it using decorators, but not sure if that&amp;#39;s overcomplicating things or even a common approach.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yk8ski", "is_robot_indexable": true, "report_reasons": null, "author": "chocolate_buttons", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yk8ski/python_data_analysis_patterns_a_pipeline_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yk8ski/python_data_analysis_patterns_a_pipeline_design/", "subreddit_subscribers": 817003, "created_utc": 1667404214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi! Recent BSc Biology grad here.\n\nI was thinking of doing an MDS to help train me how to work with big data. Would like to use the skills acquired to do bioinformatics or something related to analysis of healthcare data (I've been told by r/bioinformatics to go for an MDS instead of MSc Bioinf because you are taught similar skills but MDS will give broader job prospects). However, many of the MDS programs in my country are considered professional programs and thus funding is not provided by the university. This is a pretty significant downside for me. Would a masters in statistics provide me with similar skills? Thanks :)", "author_fullname": "t2_v7ofz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to Data Science Masters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykrj8n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667447641.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Recent BSc Biology grad here.&lt;/p&gt;\n\n&lt;p&gt;I was thinking of doing an MDS to help train me how to work with big data. Would like to use the skills acquired to do bioinformatics or something related to analysis of healthcare data (I&amp;#39;ve been told by &lt;a href=\"/r/bioinformatics\"&gt;r/bioinformatics&lt;/a&gt; to go for an MDS instead of MSc Bioinf because you are taught similar skills but MDS will give broader job prospects). However, many of the MDS programs in my country are considered professional programs and thus funding is not provided by the university. This is a pretty significant downside for me. Would a masters in statistics provide me with similar skills? Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykrj8n", "is_robot_indexable": true, "report_reasons": null, "author": "KwallahT", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykrj8n/alternative_to_data_science_masters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykrj8n/alternative_to_data_science_masters/", "subreddit_subscribers": 817003, "created_utc": 1667447641.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I'm not a data scientist, but I've been assigned a project somewhat in this category and I'm trying to improve my programs performance.\n\nI'm scanning through a 10TB file system with 800,000 directories and a ton of files in python. I'm storing some file meta data like: size, mtime, and user id in a pandas dataframe, and it's taking 20+ hours to complete. What's odd about this is if I just go through the file system and only record file paths, it only takes about 6 hours. Which I would say is pretty fair. \n\nI feel that my problem is a memory problem. The more data is store the slower the program gets. Im sure people in this field have to work through datasets this large or larger very frequently, and there are developed methods for doing so efficiently. \n\nIs there a better means to record file meta data than to just save it in a structure in memory?", "author_fullname": "t2_l8wunj9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices for storing Large Datasets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yk8yoi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667404614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m not a data scientist, but I&amp;#39;ve been assigned a project somewhat in this category and I&amp;#39;m trying to improve my programs performance.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m scanning through a 10TB file system with 800,000 directories and a ton of files in python. I&amp;#39;m storing some file meta data like: size, mtime, and user id in a pandas dataframe, and it&amp;#39;s taking 20+ hours to complete. What&amp;#39;s odd about this is if I just go through the file system and only record file paths, it only takes about 6 hours. Which I would say is pretty fair. &lt;/p&gt;\n\n&lt;p&gt;I feel that my problem is a memory problem. The more data is store the slower the program gets. Im sure people in this field have to work through datasets this large or larger very frequently, and there are developed methods for doing so efficiently. &lt;/p&gt;\n\n&lt;p&gt;Is there a better means to record file meta data than to just save it in a structure in memory?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yk8yoi", "is_robot_indexable": true, "report_reasons": null, "author": "CruderMermaid6", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yk8yoi/best_practices_for_storing_large_datasets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yk8yoi/best_practices_for_storing_large_datasets/", "subreddit_subscribers": 817003, "created_utc": 1667404614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_d7ung", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can someone ELI5 nested cross validation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "name": "t3_yksloq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/a5my7OPO1jSh0YZfIOUAcwuTB1HxBwyhagXb-lR_xIE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667451058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "scikit-learn.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p8UWe7P-YBJbKgqnfC_c6CCnbN_dt6MaKfe8oBmQpYo.jpg?auto=webp&amp;s=b27948a66bfc7b1fcce0868f365679abefd2dcb6", "width": 160, "height": 58}, "resolutions": [{"url": "https://external-preview.redd.it/p8UWe7P-YBJbKgqnfC_c6CCnbN_dt6MaKfe8oBmQpYo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ea009a13d87d058093cb4117dad12bf8f3d5e5ff", "width": 108, "height": 39}], "variants": {}, "id": "aRDTtkw0_GBCGb9E7JLBzDXeEp2pWa53pBC0AILtbPw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "yksloq", "is_robot_indexable": true, "report_reasons": null, "author": "o-rka", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yksloq/can_someone_eli5_nested_cross_validation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html", "subreddit_subscribers": 817003, "created_utc": 1667451058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_2qp4bbiy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Data Analyst looking to transition to Data Scientist after I finish my undergrad. Any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zly81vwzqmx91": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 139, "x": 108, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=607cb9b722e074f75dba2bb17f8989b9513cdcfc"}, {"y": 278, "x": 216, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2903d584528d9ee64ddc4deb997762b60e42d67d"}, {"y": 411, "x": 320, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fed11764dc4995eb1290bd4b2784b170daeb17a5"}, {"y": 823, "x": 640, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=360c499b83f4739d9973aea2a58f67b9c5906458"}, {"y": 1235, "x": 960, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6478855580c0475dfb06355ee0f5b26370e35d90"}, {"y": 1390, "x": 1080, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6871e11c57a167f9ea88ba1373ea60c9e4e9b839"}], "s": {"y": 1506, "x": 1170, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=3028fdc1fd65df157da93178373d3ab6565ed228"}, "id": "zly81vwzqmx91"}, "ai4hsuwzqmx91": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 138, "x": 108, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=148bb6cd952e5825dcc7a62223804cb7222b0fd0"}, {"y": 277, "x": 216, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4233a1a87b9666fcd41ad76f6424d0b99f121b1"}, {"y": 411, "x": 320, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a99f72d0d6332336ed6df6ac6efc71aeaa55ce18"}, {"y": 823, "x": 640, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce6cfa5f0fe5f9b03e3b384c04f3829a0e49aad9"}, {"y": 1234, "x": 960, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e31afbdc88eec2f7baad4a6e21ed1d4d68c0d79"}, {"y": 1389, "x": 1080, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f26b619fad82ac39a61da2f5a4c33f623da676ea"}], "s": {"y": 1505, "x": 1170, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=34790eac3971584fbcf3fd0ef2eb49da54889187"}, "id": "ai4hsuwzqmx91"}}, "name": "t3_ykn277", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 4, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "ai4hsuwzqmx91", "id": 204891057}, {"media_id": "zly81vwzqmx91", "id": 204891058}]}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Y4vC0HMhl0goDdnP4g4nQATGjVuRQYk39OWhDZvkbmA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667435058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/ykn277", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "ykn277", "is_robot_indexable": true, "report_reasons": null, "author": "Dont_know_wa_im_doin", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykn277/data_analyst_looking_to_transition_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/ykn277", "subreddit_subscribers": 817003, "created_utc": 1667435058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been working on building up my data science knowledge for a few months now. Started actually building and testing models a bit over a month ago and now I'm fairly at a comfortable place. Not winning Kaggle competitions but the scores are something I'm happy with...for now.\n\nI've realised that my Achilles' Heel at this point is feature engineering, which is one of the most important tasks to build a good model. Before beginning work, I do look at similar datasets and see how others have treated that data but somehow my lack of knowledge in that domain fails me.\n\nI think I have very good interpretability skills from a business perspective but I'm unable to translate that understanding into features.\n\nHow much time do you all spend researching the related industry/domain of the data before building models? Any tips/tricks that you think could work for me?\n\nRight now I'm reading [http://www.feat.engineering/](http://www.feat.engineering/) which I found on this sub", "author_fullname": "t2_55cjp0l4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature Engineering without domain knowledge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykk7j9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667428781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on building up my data science knowledge for a few months now. Started actually building and testing models a bit over a month ago and now I&amp;#39;m fairly at a comfortable place. Not winning Kaggle competitions but the scores are something I&amp;#39;m happy with...for now.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve realised that my Achilles&amp;#39; Heel at this point is feature engineering, which is one of the most important tasks to build a good model. Before beginning work, I do look at similar datasets and see how others have treated that data but somehow my lack of knowledge in that domain fails me.&lt;/p&gt;\n\n&lt;p&gt;I think I have very good interpretability skills from a business perspective but I&amp;#39;m unable to translate that understanding into features.&lt;/p&gt;\n\n&lt;p&gt;How much time do you all spend researching the related industry/domain of the data before building models? Any tips/tricks that you think could work for me?&lt;/p&gt;\n\n&lt;p&gt;Right now I&amp;#39;m reading &lt;a href=\"http://www.feat.engineering/\"&gt;http://www.feat.engineering/&lt;/a&gt; which I found on this sub&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykk7j9", "is_robot_indexable": true, "report_reasons": null, "author": "_Triggernometry_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykk7j9/feature_engineering_without_domain_knowledge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykk7j9/feature_engineering_without_domain_knowledge/", "subreddit_subscribers": 817003, "created_utc": 1667428781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_6ntqfru4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow, Poetry and Docker: Anyone get them to work together?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yksvjr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667451938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yksvjr", "is_robot_indexable": true, "report_reasons": null, "author": "Entire_Ambassador349", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yksvjr/airflow_poetry_and_docker_anyone_get_them_to_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yksvjr/airflow_poetry_and_docker_anyone_get_them_to_work/", "subreddit_subscribers": 817003, "created_utc": 1667451938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been tasked with creating a production grade scalable distribution network routing optimization model with some additional nuances. I'm using Rust as it will use very large datasets and has many constraints. I presume using python computation will be very slow and can't take that risk. I'm quite adept in Rust FYI but didn't develop any AI/ML projects with it.\n\nSince I'm developing such a large scale project for the first time, I'm unsure as to how to arrive at a reasonable implementation timeframe since I may have to write the entire custom AI/ML model from scratch to accommodate for the uncommon nuances.\n\nSuggestions on how to make my work a bit easier from all of you experienced peeps are also welcome.", "author_fullname": "t2_ra4tp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to report timelines to the management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yk8i01", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667403508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been tasked with creating a production grade scalable distribution network routing optimization model with some additional nuances. I&amp;#39;m using Rust as it will use very large datasets and has many constraints. I presume using python computation will be very slow and can&amp;#39;t take that risk. I&amp;#39;m quite adept in Rust FYI but didn&amp;#39;t develop any AI/ML projects with it.&lt;/p&gt;\n\n&lt;p&gt;Since I&amp;#39;m developing such a large scale project for the first time, I&amp;#39;m unsure as to how to arrive at a reasonable implementation timeframe since I may have to write the entire custom AI/ML model from scratch to accommodate for the uncommon nuances.&lt;/p&gt;\n\n&lt;p&gt;Suggestions on how to make my work a bit easier from all of you experienced peeps are also welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yk8i01", "is_robot_indexable": true, "report_reasons": null, "author": "a_aniq", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yk8i01/how_to_report_timelines_to_the_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yk8i01/how_to_report_timelines_to_the_management/", "subreddit_subscribers": 817003, "created_utc": 1667403508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been a ds for a number of years now on a decent size team and have worked on and lead handful of cool projects but I have zero formal education in the field.  I\u2019m self taught enough that I can keep up with anyone at this point.  Actually I help interns with problems frequently that come straight out of a masters program. I\u2019m surprised how little you know after graduating.\n\nI\u2019ve kinda worry about how many job posting require a masters/phd if I wanted to explore other opportunities.\n\nDo you think with the current state of the industry that it will be hard to get a position without the formal education backing me?", "author_fullname": "t2_hwogh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Current DS without credentials", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yl1jbo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667478493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been a ds for a number of years now on a decent size team and have worked on and lead handful of cool projects but I have zero formal education in the field.  I\u2019m self taught enough that I can keep up with anyone at this point.  Actually I help interns with problems frequently that come straight out of a masters program. I\u2019m surprised how little you know after graduating.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve kinda worry about how many job posting require a masters/phd if I wanted to explore other opportunities.&lt;/p&gt;\n\n&lt;p&gt;Do you think with the current state of the industry that it will be hard to get a position without the formal education backing me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yl1jbo", "is_robot_indexable": true, "report_reasons": null, "author": "tkpk5280", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yl1jbo/current_ds_without_credentials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yl1jbo/current_ds_without_credentials/", "subreddit_subscribers": 817003, "created_utc": 1667478493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\n&amp;#x200B;\n\nForgive me if this is not the right place to do this but I am studying data science myself and part of an assignment we have is to interview Data Scientist in the field regarding Data Ethics and create a summary of the results.\n\n&amp;#x200B;\n\nWould a few of you mind answering the following questions for me? I have tried getting in contact directly with people at banks, other universities and so forth but I am struggling to find people willing to conduct the \"Interview\". We need to attach proof that we conducted the interview, hence I need actual real data and am not about the business of \"fudging\" results.\n\nAssignment questions are as follows:\n\n\\--------\n\nDuring the interview, discuss the person\u2019s professional experience with ethics issues in their professional career on both the technical and personnel/workplace sides. This should include issues they directly experienced, and if they want, can include issues they heard about, as well. Pick two or three of the most memorable issues brought up by your interview, and ask some follow-up questions:\n\n* Do they feel the issue was handled well or not?\n* Were there situations that made it difficult to take the most ethical path?\n\n\\--------\n\nAny feedback would really be appreciated.\n\nThanks :)", "author_fullname": "t2_ntnaxuu2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykysox", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667471888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Forgive me if this is not the right place to do this but I am studying data science myself and part of an assignment we have is to interview Data Scientist in the field regarding Data Ethics and create a summary of the results.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Would a few of you mind answering the following questions for me? I have tried getting in contact directly with people at banks, other universities and so forth but I am struggling to find people willing to conduct the &amp;quot;Interview&amp;quot;. We need to attach proof that we conducted the interview, hence I need actual real data and am not about the business of &amp;quot;fudging&amp;quot; results.&lt;/p&gt;\n\n&lt;p&gt;Assignment questions are as follows:&lt;/p&gt;\n\n&lt;p&gt;--------&lt;/p&gt;\n\n&lt;p&gt;During the interview, discuss the person\u2019s professional experience with ethics issues in their professional career on both the technical and personnel/workplace sides. This should include issues they directly experienced, and if they want, can include issues they heard about, as well. Pick two or three of the most memorable issues brought up by your interview, and ask some follow-up questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Do they feel the issue was handled well or not?&lt;/li&gt;\n&lt;li&gt;Were there situations that made it difficult to take the most ethical path?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;--------&lt;/p&gt;\n\n&lt;p&gt;Any feedback would really be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykysox", "is_robot_indexable": true, "report_reasons": null, "author": "Eat-More-Brains", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykysox/data_science_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykysox/data_science_interview/", "subreddit_subscribers": 817003, "created_utc": 1667471888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, guys how would you approach the following problem?\n\n&amp;#x200B;\n\nI'm currently using an simple AutoRegressive model with the last N days to predit a sales outcome for the next day. I want to improve the model by using the first hours of the day that I'm trying to predict. \n\nThe problem is that the number of hours that I'll have available are note fixed. Maybe someone will use the prediction service at 8am and then at 4pm.  How would you add this feature if you want to use all the data available at the time?", "author_fullname": "t2_kguah", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Timeseries help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykesdc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667417883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, guys how would you approach the following problem?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using an simple AutoRegressive model with the last N days to predit a sales outcome for the next day. I want to improve the model by using the first hours of the day that I&amp;#39;m trying to predict. &lt;/p&gt;\n\n&lt;p&gt;The problem is that the number of hours that I&amp;#39;ll have available are note fixed. Maybe someone will use the prediction service at 8am and then at 4pm.  How would you add this feature if you want to use all the data available at the time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykesdc", "is_robot_indexable": true, "report_reasons": null, "author": "chocolate_dealer", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykesdc/timeseries_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykesdc/timeseries_help/", "subreddit_subscribers": 817003, "created_utc": 1667417883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Dear all,\n\nSuppose yo want to do a parametric analysis, what I mean is that you want to obtain an estimate of some parameter P, from a given dataset A. \nFor some reason, a college was doing the same exact thing, but in a different dataset. He is measuring P from dataset B.\nNow both colleagues want to obtain the most of both that datasets, they meet and they find that A and B are not independent, and the intersection C is not negligible. \n\nDo you know some references to deal with this kind of situation? Or do you know a procedure to get the most information of both datasets without discarding the intersection? How would you proceed?\n\nSorry in advance if this is not the right place to ask\nHace a nice day!", "author_fullname": "t2_a50rr8cj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analysis in intersected data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yl4bur", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667485815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear all,&lt;/p&gt;\n\n&lt;p&gt;Suppose yo want to do a parametric analysis, what I mean is that you want to obtain an estimate of some parameter P, from a given dataset A. \nFor some reason, a college was doing the same exact thing, but in a different dataset. He is measuring P from dataset B.\nNow both colleagues want to obtain the most of both that datasets, they meet and they find that A and B are not independent, and the intersection C is not negligible. &lt;/p&gt;\n\n&lt;p&gt;Do you know some references to deal with this kind of situation? Or do you know a procedure to get the most information of both datasets without discarding the intersection? How would you proceed?&lt;/p&gt;\n\n&lt;p&gt;Sorry in advance if this is not the right place to ask\nHace a nice day!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yl4bur", "is_robot_indexable": true, "report_reasons": null, "author": "SpiritedEagle7948", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yl4bur/analysis_in_intersected_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yl4bur/analysis_in_intersected_data/", "subreddit_subscribers": 817003, "created_utc": 1667485815.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m not sure exactly how to phrase this. Professional DS with 15 years experience. I have no problems doing everything locally or on a web server if someone sets one up for me. For example, my company has a process to deploy Python scripts via restful API. The engineer taught me how to deploy what in need but when something breaks I am lost. How would I learn to fix bugs or setup my own type of service? Another issue. Someone is sending me a huge data file across a subnet at 5MB per second. Often the job fails. I would like to try to fix this. How do I learn about this. I don\u2019t even know what it\u2019s called. I\u2019m tired of waiting on other people to solve my problems.", "author_fullname": "t2_kcl3tfwe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good resources for architecture and networks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yl1w7d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667479497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m not sure exactly how to phrase this. Professional DS with 15 years experience. I have no problems doing everything locally or on a web server if someone sets one up for me. For example, my company has a process to deploy Python scripts via restful API. The engineer taught me how to deploy what in need but when something breaks I am lost. How would I learn to fix bugs or setup my own type of service? Another issue. Someone is sending me a huge data file across a subnet at 5MB per second. Often the job fails. I would like to try to fix this. How do I learn about this. I don\u2019t even know what it\u2019s called. I\u2019m tired of waiting on other people to solve my problems.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yl1w7d", "is_robot_indexable": true, "report_reasons": null, "author": "NewEcho2940", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yl1w7d/good_resources_for_architecture_and_networks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yl1w7d/good_resources_for_architecture_and_networks/", "subreddit_subscribers": 817003, "created_utc": 1667479497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm finding parallelising an r task (which basically means assigning work to a separate r session) requires all data in my environment to be copied over to the cache of the new core. If I'm working with just 10gb of data, my machine is losing more time copying data than it's saving by parallelising. This means I can't even split up CV tasks on windows, which should be trivial to parallelise.\n\nIs there an obvious alternative to this approach? And am I right in saying running on Linux would avoid this bottleneck?", "author_fullname": "t2_1rwftqt3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does lack of fork parallelisation make windows an impractical OS for ML?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yl10to", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667477285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m finding parallelising an r task (which basically means assigning work to a separate r session) requires all data in my environment to be copied over to the cache of the new core. If I&amp;#39;m working with just 10gb of data, my machine is losing more time copying data than it&amp;#39;s saving by parallelising. This means I can&amp;#39;t even split up CV tasks on windows, which should be trivial to parallelise.&lt;/p&gt;\n\n&lt;p&gt;Is there an obvious alternative to this approach? And am I right in saying running on Linux would avoid this bottleneck?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yl10to", "is_robot_indexable": true, "report_reasons": null, "author": "theAbominablySlowMan", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yl10to/does_lack_of_fork_parallelisation_make_windows_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yl10to/does_lack_of_fork_parallelisation_make_windows_an/", "subreddit_subscribers": 817003, "created_utc": 1667477285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all - apologies if this has been asked but I haven\u2019t found a great answer. What do you all recommend to learn to translate datasets (and the concept behind the visuals) into something I can host on a website? I know there\u2019s a ton of tools/languages (Plotly dash, d3js, tableau, RShiny) out there, but not sure where to spend my time learning.\n\nMy aim is to be able to compare two athletes in a given sport (take American football for example) and provide a head to head analysis on certain talents. I know this kind of stuff already exists, but I\u2019d like to learn by doing.\n\nFor reference, I primarily work in Python but happy to learn a new language for this. Thanks!", "author_fullname": "t2_5tmeg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best interactive web-interface to show data/viz?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yl0kxk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667476259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all - apologies if this has been asked but I haven\u2019t found a great answer. What do you all recommend to learn to translate datasets (and the concept behind the visuals) into something I can host on a website? I know there\u2019s a ton of tools/languages (Plotly dash, d3js, tableau, RShiny) out there, but not sure where to spend my time learning.&lt;/p&gt;\n\n&lt;p&gt;My aim is to be able to compare two athletes in a given sport (take American football for example) and provide a head to head analysis on certain talents. I know this kind of stuff already exists, but I\u2019d like to learn by doing.&lt;/p&gt;\n\n&lt;p&gt;For reference, I primarily work in Python but happy to learn a new language for this. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yl0kxk", "is_robot_indexable": true, "report_reasons": null, "author": "dumper514", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yl0kxk/best_interactive_webinterface_to_show_dataviz/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yl0kxk/best_interactive_webinterface_to_show_dataviz/", "subreddit_subscribers": 817003, "created_utc": 1667476259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to pursue a MS Data Science degree from the UK and will try to find employment there post graduation. As the fees are really high I would have to sell off my assets to fund my education. However, what concerns me is that will I be able to find a job there provided I know my stuff and I will probably get an MS from a top ranked institution? How difficult would that be?\n\n[View Poll](https://www.reddit.com/poll/yksr9u)", "author_fullname": "t2_ywgj6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it advisable to get a MS DS degree keeping in mind the current recession?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yksr9u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667451578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to pursue a MS Data Science degree from the UK and will try to find employment there post graduation. As the fees are really high I would have to sell off my assets to fund my education. However, what concerns me is that will I be able to find a job there provided I know my stuff and I will probably get an MS from a top ranked institution? How difficult would that be?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/yksr9u\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "yksr9u", "is_robot_indexable": true, "report_reasons": null, "author": "musmas", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1667624378440, "options": [{"text": "Yes", "id": "19566071"}, {"text": "No", "id": "19566072"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 119, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yksr9u/is_it_advisable_to_get_a_ms_ds_degree_keeping_in/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/datascience/comments/yksr9u/is_it_advisable_to_get_a_ms_ds_degree_keeping_in/", "subreddit_subscribers": 817003, "created_utc": 1667451578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "A friend and I recently participated in a local datathon here in Australia and we're looking for further challenges either here or anywhere in the world (if it's open to international teams), however we're having a hard time finding websites or communities that are centered in promoting/advertising these events. We have something similar in Australia [for hackathons in general](https://www.hackathonsaustralia.com/), but they ceased comms over a year ago.\n\nAny suggestions about where to look for more data-driven challenges and competitions for international datathons or (ideally) in Australia? (Yes I *am* googling please don't send me there).", "author_fullname": "t2_71lk6sl4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any suggestions on communities (here on Reddit) or websites that announce/promote datathons?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykrt71", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Networking", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667448496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A friend and I recently participated in a local datathon here in Australia and we&amp;#39;re looking for further challenges either here or anywhere in the world (if it&amp;#39;s open to international teams), however we&amp;#39;re having a hard time finding websites or communities that are centered in promoting/advertising these events. We have something similar in Australia &lt;a href=\"https://www.hackathonsaustralia.com/\"&gt;for hackathons in general&lt;/a&gt;, but they ceased comms over a year ago.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions about where to look for more data-driven challenges and competitions for international datathons or (ideally) in Australia? (Yes I &lt;em&gt;am&lt;/em&gt; googling please don&amp;#39;t send me there).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?auto=webp&amp;s=11eea8bc1fb810dc41c39d03eb1ea8d9c1977777", "width": 2460, "height": 1403}, "resolutions": [{"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ef608d492647f3667beab7bfa9ab39f2e9d5dcc", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=abde207855145f367f630b2ee21b426c780c27be", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=327fcb10a130b311dbfd0b7defec642d8f3030d7", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d9a13d21b8d35521dd4ea5faacf7b598eff8fd4", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f93546129811ed8d4043fe3b38fcbf0b42e988d", "width": 960, "height": 547}, {"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8670867e42d602b1d0e593ca46be3d885ac8511f", "width": 1080, "height": 615}], "variants": {}, "id": "vM8d3m2aOfl5VOI6_MUCLK00ddGCAj8Fgsy5VTpH0Sw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykrt71", "is_robot_indexable": true, "report_reasons": null, "author": "William_Rosebud", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykrt71/any_suggestions_on_communities_here_on_reddit_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykrt71/any_suggestions_on_communities_here_on_reddit_or/", "subreddit_subscribers": 817003, "created_utc": 1667448496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I have a dataset that a client sent me. They are a furniture manufacturer company, specialized in making Dressers. Their \"added value\" are how economical their products are.\n\nThe dataset's rows are individual ordered products, which can repeat for a single customer depending on how many products they ordered in a single transaction/day (if they bought 10 dressers, a single customer has 10 rows, each row being 1 product bought). **I'm tasked with gathering actionable insights for the client based on the dataset; the issue is that the dataset is very limited in terms of columns/variables, and I think/feel I'm out of ideas already**. I'm part of a digital marketing business, but this is exercise is actually not directly related to exclusively/mandatorily generating digital marketing recommendations.\n\nThe dataset columns are:\n\n1. Order Date (dd/mm/yyy), only with data from January 2021 to October 2022\n2. Product Name\n3. Customer Name (mostly useless)\n4. Customer Gender (M or F)\n5. City (needs A LOT of cleaning to standardize. This was a manual input from whoever filled it out)\n6. State (from the United States)\n7. Zip Code\n8. Channel (whether they sell through Amazon, Walmart or other marketplaces, or directly through their website. This is only available for 2021 and not for 2022)\n\nI already looked at things like:\n\n1. Top/bottom states in terms of orders by 100k population (based on each state's population). \\[Bar charts\\]\n2. Total orders (Y axis) by month (X axis) by state to detect outliers/patterns (nothing to highlight here, they mostly follow the national, aggregated total orders). \\[Line charts\\]\n3. Most popular products by state and their avg. order share (from each state) \\[Bar charts\\]\n4. Top/bottom products just based on total orders \\[Bar charts\\]\n5. Top/bottom states just based on total orders \\[Bar charts\\]\n6. Reach by product (whether they are sold in all states: 100%, just in one (1/51), etc.) \\[Bar charts\\]\n7. How much of the total orders do Dressers represent specifically \\[Area chart\\]\n8. Top 5 products by month \\[Ribbon chart\\]\n9. Top 5 products by gender \\[Bar chart\\]\n10. Top/bottom products by their male-to-female ratio in terms of orders \\[Bar chart\\]\n11. Repurchases based on a \"composite key\" of customer name + zip code for a single day \\[Bar chart, but almost useless. Almost no repurchases with less than 0.5% of all transactions\\]\n\nThat's about it. I think having columns like \"revenue\" and data for more years could really bring a whole new world of insights and creativity to get real actionable insights. So far the client (for any reason) has been reluctant to provide this or more data apart from 2021 and 2022. **Any ideas? I would enormously appreciate your input on this**\n\n**TL;DR: I have a very limited dataset on orders for furniture, mostly Dressers. I need to gather actionable insights and the data is extremely limited in terms of date range (2021-2022) and variables (up to 8 variables, with just 4 or 5 being truly useful). Ideas on actionable insights?**\n\nThank you!", "author_fullname": "t2_ie26w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to add more creativity to this EDA to gather useful, actionable insights", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykp0fb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667440266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a dataset that a client sent me. They are a furniture manufacturer company, specialized in making Dressers. Their &amp;quot;added value&amp;quot; are how economical their products are.&lt;/p&gt;\n\n&lt;p&gt;The dataset&amp;#39;s rows are individual ordered products, which can repeat for a single customer depending on how many products they ordered in a single transaction/day (if they bought 10 dressers, a single customer has 10 rows, each row being 1 product bought). &lt;strong&gt;I&amp;#39;m tasked with gathering actionable insights for the client based on the dataset; the issue is that the dataset is very limited in terms of columns/variables, and I think/feel I&amp;#39;m out of ideas already&lt;/strong&gt;. I&amp;#39;m part of a digital marketing business, but this is exercise is actually not directly related to exclusively/mandatorily generating digital marketing recommendations.&lt;/p&gt;\n\n&lt;p&gt;The dataset columns are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Order Date (dd/mm/yyy), only with data from January 2021 to October 2022&lt;/li&gt;\n&lt;li&gt;Product Name&lt;/li&gt;\n&lt;li&gt;Customer Name (mostly useless)&lt;/li&gt;\n&lt;li&gt;Customer Gender (M or F)&lt;/li&gt;\n&lt;li&gt;City (needs A LOT of cleaning to standardize. This was a manual input from whoever filled it out)&lt;/li&gt;\n&lt;li&gt;State (from the United States)&lt;/li&gt;\n&lt;li&gt;Zip Code&lt;/li&gt;\n&lt;li&gt;Channel (whether they sell through Amazon, Walmart or other marketplaces, or directly through their website. This is only available for 2021 and not for 2022)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I already looked at things like:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Top/bottom states in terms of orders by 100k population (based on each state&amp;#39;s population). [Bar charts]&lt;/li&gt;\n&lt;li&gt;Total orders (Y axis) by month (X axis) by state to detect outliers/patterns (nothing to highlight here, they mostly follow the national, aggregated total orders). [Line charts]&lt;/li&gt;\n&lt;li&gt;Most popular products by state and their avg. order share (from each state) [Bar charts]&lt;/li&gt;\n&lt;li&gt;Top/bottom products just based on total orders [Bar charts]&lt;/li&gt;\n&lt;li&gt;Top/bottom states just based on total orders [Bar charts]&lt;/li&gt;\n&lt;li&gt;Reach by product (whether they are sold in all states: 100%, just in one (1/51), etc.) [Bar charts]&lt;/li&gt;\n&lt;li&gt;How much of the total orders do Dressers represent specifically [Area chart]&lt;/li&gt;\n&lt;li&gt;Top 5 products by month [Ribbon chart]&lt;/li&gt;\n&lt;li&gt;Top 5 products by gender [Bar chart]&lt;/li&gt;\n&lt;li&gt;Top/bottom products by their male-to-female ratio in terms of orders [Bar chart]&lt;/li&gt;\n&lt;li&gt;Repurchases based on a &amp;quot;composite key&amp;quot; of customer name + zip code for a single day [Bar chart, but almost useless. Almost no repurchases with less than 0.5% of all transactions]&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;That&amp;#39;s about it. I think having columns like &amp;quot;revenue&amp;quot; and data for more years could really bring a whole new world of insights and creativity to get real actionable insights. So far the client (for any reason) has been reluctant to provide this or more data apart from 2021 and 2022. &lt;strong&gt;Any ideas? I would enormously appreciate your input on this&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR: I have a very limited dataset on orders for furniture, mostly Dressers. I need to gather actionable insights and the data is extremely limited in terms of date range (2021-2022) and variables (up to 8 variables, with just 4 or 5 being truly useful). Ideas on actionable insights?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykp0fb", "is_robot_indexable": true, "report_reasons": null, "author": "MadGlobin", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykp0fb/how_to_add_more_creativity_to_this_eda_to_gather/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykp0fb/how_to_add_more_creativity_to_this_eda_to_gather/", "subreddit_subscribers": 817003, "created_utc": 1667440266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm interested in learning a new \"spoken language\" (idk how to call it), I tried to look it up but only got answers about programming languages.\n\nI already speak Spanish (native) and English (not perfectly) but don't have any idea where should I continue.", "author_fullname": "t2_5oj654vi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the most useful spoken languages for data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykkupe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667430075.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m interested in learning a new &amp;quot;spoken language&amp;quot; (idk how to call it), I tried to look it up but only got answers about programming languages.&lt;/p&gt;\n\n&lt;p&gt;I already speak Spanish (native) and English (not perfectly) but don&amp;#39;t have any idea where should I continue.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykkupe", "is_robot_indexable": true, "report_reasons": null, "author": "Pepe_Alpa", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykkupe/what_are_the_most_useful_spoken_languages_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykkupe/what_are_the_most_useful_spoken_languages_for/", "subreddit_subscribers": 817003, "created_utc": 1667430075.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}