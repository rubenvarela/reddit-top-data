{"kind": "Listing", "data": {"after": "t3_yl4bur", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is it just me or are the vast majority of job adverts on linked in right now for senior/lead/principal data scientists? (UK btw)\n\nI only saw a single advert for a junior role and this had over 200 applications in just a few a hours of being released.", "author_fullname": "t2_1rp1btfa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No one is hiring juniors/ mid-level data scientists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykyte6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 273, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 273, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667471938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it just me or are the vast majority of job adverts on linked in right now for senior/lead/principal data scientists? (UK btw)&lt;/p&gt;\n\n&lt;p&gt;I only saw a single advert for a junior role and this had over 200 applications in just a few a hours of being released.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykyte6", "is_robot_indexable": true, "report_reasons": null, "author": "nullspace1729", "discussion_type": null, "num_comments": 73, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykyte6/no_one_is_hiring_juniors_midlevel_data_scientists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykyte6/no_one_is_hiring_juniors_midlevel_data_scientists/", "subreddit_subscribers": 817069, "created_utc": 1667471938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_bb4m08u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing OpenAI GPT3 in Airtable. Fine-tuning gpt3 even with a very small dataset (50 in this case) appears to work pretty well. Any interesting use cases that you'd recommend testing with?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_ykybpj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 45, "total_awards_received": 0, "media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Running OpenAI in Airtable. Extracting structured JSON data from unstructured resume text.&lt;br&gt;&lt;br&gt;Fine-tuning gpt3 even with a very small dataset (50 in this case) appears to work extremely well.&lt;br&gt;&lt;br&gt;Also tested converting/extracting data from bank transactions, logs, emails &amp;amp; other data &lt;a href=\"https://t.co/k2596TLvFc\"&gt;pic.twitter.com/k2596TLvFc&lt;/a&gt;&lt;/p&gt;&amp;mdash; Igor Nefedov (@igornefedovi) &lt;a href=\"https://twitter.com/igornefedovi/status/1588032734315704320?ref_src=twsrc%5Etfw\"&gt;November 3, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/igornefedovi/status/1588032734315704320", "author_name": "Igor Nefedov", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Running OpenAI in Airtable. Extracting structured JSON data from unstructured resume text.&lt;br&gt;&lt;br&gt;Fine-tuning gpt3 even with a very small dataset (50 in this case) appears to work extremely well.&lt;br&gt;&lt;br&gt;Also tested converting/extracting data from bank transactions, logs, emails &amp;amp; other data &lt;a href=\"https://t.co/k2596TLvFc\"&gt;pic.twitter.com/k2596TLvFc&lt;/a&gt;&lt;/p&gt;&amp;mdash; Igor Nefedov (@igornefedovi) &lt;a href=\"https://twitter.com/igornefedovi/status/1588032734315704320?ref_src=twsrc%5Etfw\"&gt;November 3, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/igornefedovi", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Running OpenAI in Airtable. Extracting structured JSON data from unstructured resume text.&lt;br&gt;&lt;br&gt;Fine-tuning gpt3 even with a very small dataset (50 in this case) appears to work extremely well.&lt;br&gt;&lt;br&gt;Also tested converting/extracting data from bank transactions, logs, emails &amp;amp; other data &lt;a href=\"https://t.co/k2596TLvFc\"&gt;pic.twitter.com/k2596TLvFc&lt;/a&gt;&lt;/p&gt;&amp;mdash; Igor Nefedov (@igornefedovi) &lt;a href=\"https://twitter.com/igornefedovi/status/1588032734315704320?ref_src=twsrc%5Etfw\"&gt;November 3, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/ykybpj", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/H5M11fv0b8Wy11cjAmYQFWbHB-Y653u0jbpFjTdc5o4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667470649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "twitter.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://twitter.com/igornefedovi/status/1588032734315704320", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Q3_cAr_0AWvl2doPMq2oGYGlBrKQB6CXdpzOFQmnUBk.jpg?auto=webp&amp;s=438a341ce2e08c3d99c58319f7ff907ff8f90806", "width": 140, "height": 78}, "resolutions": [{"url": "https://external-preview.redd.it/Q3_cAr_0AWvl2doPMq2oGYGlBrKQB6CXdpzOFQmnUBk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0cf5cae1ce8dfd29d83b91454d5da4ef40e8288b", "width": 108, "height": 60}], "variants": {}, "id": "HYnQ8FaO96_yByBsMJgxlZn_e9b7QzNp8IGJjv5tEac"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykybpj", "is_robot_indexable": true, "report_reasons": null, "author": "igornefedovi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykybpj/testing_openai_gpt3_in_airtable_finetuning_gpt3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://twitter.com/igornefedovi/status/1588032734315704320", "subreddit_subscribers": 817069, "created_utc": 1667470649.0, "num_crossposts": 0, "media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/igornefedovi/status/1588032734315704320", "author_name": "Igor Nefedov", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Running OpenAI in Airtable. Extracting structured JSON data from unstructured resume text.&lt;br&gt;&lt;br&gt;Fine-tuning gpt3 even with a very small dataset (50 in this case) appears to work extremely well.&lt;br&gt;&lt;br&gt;Also tested converting/extracting data from bank transactions, logs, emails &amp;amp; other data &lt;a href=\"https://t.co/k2596TLvFc\"&gt;pic.twitter.com/k2596TLvFc&lt;/a&gt;&lt;/p&gt;&amp;mdash; Igor Nefedov (@igornefedovi) &lt;a href=\"https://twitter.com/igornefedovi/status/1588032734315704320?ref_src=twsrc%5Etfw\"&gt;November 3, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/igornefedovi", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi folks\n\nI was wondering if there are any free sentiment analysis tools that are pre-trained (on typical customer support quer), so that I can run some text through it to get a general idea of positivity negativity? It\u2019s not a whole lot of text, maybe several thousand paragraphs.\n\nThanks.", "author_fullname": "t2_4zxcnppv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sentiment analysis of customer support tickets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykmpgt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667434158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks&lt;/p&gt;\n\n&lt;p&gt;I was wondering if there are any free sentiment analysis tools that are pre-trained (on typical customer support quer), so that I can run some text through it to get a general idea of positivity negativity? It\u2019s not a whole lot of text, maybe several thousand paragraphs.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykmpgt", "is_robot_indexable": true, "report_reasons": null, "author": "enigmapaulns", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykmpgt/sentiment_analysis_of_customer_support_tickets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykmpgt/sentiment_analysis_of_customer_support_tickets/", "subreddit_subscribers": 817069, "created_utc": 1667434158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is it not almost always better to use probability output? So the business had a view of the model confidence for each prediction and can act on this information rather than a more black box output?", "author_fullname": "t2_gm8b3iu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In a business context, why would you ever use a binary output for a binary classifier? Its it almost always better to output class probability so the business can set their own threshold?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykytgo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667471944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it not almost always better to use probability output? So the business had a view of the model confidence for each prediction and can act on this information rather than a more black box output?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykytgo", "is_robot_indexable": true, "report_reasons": null, "author": "poppycocknbalderdash", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykytgo/in_a_business_context_why_would_you_ever_use_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykytgo/in_a_business_context_why_would_you_ever_use_a/", "subreddit_subscribers": 817069, "created_utc": 1667471944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi! Recent BSc Biology grad here.\n\nI was thinking of doing an MDS to help train me how to work with big data. Would like to use the skills acquired to do bioinformatics or something related to analysis of healthcare data (I've been told by r/bioinformatics to go for an MDS instead of MSc Bioinf because you are taught similar skills but MDS will give broader job prospects). However, many of the MDS programs in my country are considered professional programs and thus funding is not provided by the university. This is a pretty significant downside for me. Would a masters in statistics provide me with similar skills? Thanks :)", "author_fullname": "t2_v7ofz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to Data Science Masters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykrj8n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667447641.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Recent BSc Biology grad here.&lt;/p&gt;\n\n&lt;p&gt;I was thinking of doing an MDS to help train me how to work with big data. Would like to use the skills acquired to do bioinformatics or something related to analysis of healthcare data (I&amp;#39;ve been told by &lt;a href=\"/r/bioinformatics\"&gt;r/bioinformatics&lt;/a&gt; to go for an MDS instead of MSc Bioinf because you are taught similar skills but MDS will give broader job prospects). However, many of the MDS programs in my country are considered professional programs and thus funding is not provided by the university. This is a pretty significant downside for me. Would a masters in statistics provide me with similar skills? Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykrj8n", "is_robot_indexable": true, "report_reasons": null, "author": "KwallahT", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykrj8n/alternative_to_data_science_masters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykrj8n/alternative_to_data_science_masters/", "subreddit_subscribers": 817069, "created_utc": 1667447641.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_2qp4bbiy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Data Analyst looking to transition to Data Scientist after I finish my undergrad. Any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zly81vwzqmx91": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 139, "x": 108, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=607cb9b722e074f75dba2bb17f8989b9513cdcfc"}, {"y": 278, "x": 216, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2903d584528d9ee64ddc4deb997762b60e42d67d"}, {"y": 411, "x": 320, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fed11764dc4995eb1290bd4b2784b170daeb17a5"}, {"y": 823, "x": 640, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=360c499b83f4739d9973aea2a58f67b9c5906458"}, {"y": 1235, "x": 960, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6478855580c0475dfb06355ee0f5b26370e35d90"}, {"y": 1390, "x": 1080, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6871e11c57a167f9ea88ba1373ea60c9e4e9b839"}], "s": {"y": 1506, "x": 1170, "u": "https://preview.redd.it/zly81vwzqmx91.jpg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=3028fdc1fd65df157da93178373d3ab6565ed228"}, "id": "zly81vwzqmx91"}, "ai4hsuwzqmx91": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 138, "x": 108, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=148bb6cd952e5825dcc7a62223804cb7222b0fd0"}, {"y": 277, "x": 216, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4233a1a87b9666fcd41ad76f6424d0b99f121b1"}, {"y": 411, "x": 320, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a99f72d0d6332336ed6df6ac6efc71aeaa55ce18"}, {"y": 823, "x": 640, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce6cfa5f0fe5f9b03e3b384c04f3829a0e49aad9"}, {"y": 1234, "x": 960, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e31afbdc88eec2f7baad4a6e21ed1d4d68c0d79"}, {"y": 1389, "x": 1080, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f26b619fad82ac39a61da2f5a4c33f623da676ea"}], "s": {"y": 1505, "x": 1170, "u": "https://preview.redd.it/ai4hsuwzqmx91.jpg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=34790eac3971584fbcf3fd0ef2eb49da54889187"}, "id": "ai4hsuwzqmx91"}}, "name": "t3_ykn277", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "ups": 6, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "ai4hsuwzqmx91", "id": 204891057}, {"media_id": "zly81vwzqmx91", "id": 204891058}]}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Y4vC0HMhl0goDdnP4g4nQATGjVuRQYk39OWhDZvkbmA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667435058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/ykn277", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "ykn277", "is_robot_indexable": true, "report_reasons": null, "author": "Dont_know_wa_im_doin", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykn277/data_analyst_looking_to_transition_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/ykn277", "subreddit_subscribers": 817069, "created_utc": 1667435058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_6ntqfru4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow, Poetry and Docker: Anyone get them to work together?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yksvjr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667451938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yksvjr", "is_robot_indexable": true, "report_reasons": null, "author": "Entire_Ambassador349", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yksvjr/airflow_poetry_and_docker_anyone_get_them_to_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yksvjr/airflow_poetry_and_docker_anyone_get_them_to_work/", "subreddit_subscribers": 817069, "created_utc": 1667451938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been working on building up my data science knowledge for a few months now. Started actually building and testing models a bit over a month ago and now I'm fairly at a comfortable place. Not winning Kaggle competitions but the scores are something I'm happy with...for now.\n\nI've realised that my Achilles' Heel at this point is feature engineering, which is one of the most important tasks to build a good model. Before beginning work, I do look at similar datasets and see how others have treated that data but somehow my lack of knowledge in that domain fails me.\n\nI think I have very good interpretability skills from a business perspective but I'm unable to translate that understanding into features.\n\nHow much time do you all spend researching the related industry/domain of the data before building models? Any tips/tricks that you think could work for me?\n\nRight now I'm reading [http://www.feat.engineering/](http://www.feat.engineering/) which I found on this sub", "author_fullname": "t2_55cjp0l4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature Engineering without domain knowledge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykk7j9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667428781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on building up my data science knowledge for a few months now. Started actually building and testing models a bit over a month ago and now I&amp;#39;m fairly at a comfortable place. Not winning Kaggle competitions but the scores are something I&amp;#39;m happy with...for now.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve realised that my Achilles&amp;#39; Heel at this point is feature engineering, which is one of the most important tasks to build a good model. Before beginning work, I do look at similar datasets and see how others have treated that data but somehow my lack of knowledge in that domain fails me.&lt;/p&gt;\n\n&lt;p&gt;I think I have very good interpretability skills from a business perspective but I&amp;#39;m unable to translate that understanding into features.&lt;/p&gt;\n\n&lt;p&gt;How much time do you all spend researching the related industry/domain of the data before building models? Any tips/tricks that you think could work for me?&lt;/p&gt;\n\n&lt;p&gt;Right now I&amp;#39;m reading &lt;a href=\"http://www.feat.engineering/\"&gt;http://www.feat.engineering/&lt;/a&gt; which I found on this sub&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykk7j9", "is_robot_indexable": true, "report_reasons": null, "author": "_Triggernometry_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykk7j9/feature_engineering_without_domain_knowledge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykk7j9/feature_engineering_without_domain_knowledge/", "subreddit_subscribers": 817069, "created_utc": 1667428781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_dv1qy8c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Add it to the training set, Walmart", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_ylfpqx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/J8_0BpvFtCKSrQS42vGNBi1MYiGUfVlolDzuFR9rNbI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667511110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/irkcbvz41tx91.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/irkcbvz41tx91.jpg?auto=webp&amp;s=3eeebea51a0db8c7505d8dcedfa7a51d28a47eda", "width": 918, "height": 1530}, "resolutions": [{"url": "https://preview.redd.it/irkcbvz41tx91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=17681b92c04f89a5069d6060dcf9ec6bc5175953", "width": 108, "height": 180}, {"url": "https://preview.redd.it/irkcbvz41tx91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=36dca12dd4f1a82c7d4a698546271a198b5aae42", "width": 216, "height": 360}, {"url": "https://preview.redd.it/irkcbvz41tx91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=62c6512e64131b9b06c4448406eb5c256cda068e", "width": 320, "height": 533}, {"url": "https://preview.redd.it/irkcbvz41tx91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4c8f3b471181c4747b7a49eb847a1da22d3bb58", "width": 640, "height": 1066}], "variants": {}, "id": "VitcyIruHSc5Syx21UOmHxbI5Z06g7YMBREl7rTcvBQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylfpqx", "is_robot_indexable": true, "report_reasons": null, "author": "ljh78", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylfpqx/add_it_to_the_training_set_walmart/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/irkcbvz41tx91.jpg", "subreddit_subscribers": 817069, "created_utc": 1667511110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\n&amp;#x200B;\n\nForgive me if this is not the right place to do this but I am studying data science myself and part of an assignment we have is to interview Data Scientist in the field regarding Data Ethics and create a summary of the results.\n\n&amp;#x200B;\n\nWould a few of you mind answering the following questions for me? I have tried getting in contact directly with people at banks, other universities and so forth but I am struggling to find people willing to conduct the \"Interview\". We need to attach proof that we conducted the interview, hence I need actual real data and am not about the business of \"fudging\" results.\n\nAssignment questions are as follows:\n\n\\--------\n\nDuring the interview, discuss the person\u2019s professional experience with ethics issues in their professional career on both the technical and personnel/workplace sides. This should include issues they directly experienced, and if they want, can include issues they heard about, as well. Pick two or three of the most memorable issues brought up by your interview, and ask some follow-up questions:\n\n* Do they feel the issue was handled well or not?\n* Were there situations that made it difficult to take the most ethical path?\n\n\\--------\n\nAny feedback would really be appreciated.\n\nThanks :)", "author_fullname": "t2_ntnaxuu2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykysox", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667471888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Forgive me if this is not the right place to do this but I am studying data science myself and part of an assignment we have is to interview Data Scientist in the field regarding Data Ethics and create a summary of the results.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Would a few of you mind answering the following questions for me? I have tried getting in contact directly with people at banks, other universities and so forth but I am struggling to find people willing to conduct the &amp;quot;Interview&amp;quot;. We need to attach proof that we conducted the interview, hence I need actual real data and am not about the business of &amp;quot;fudging&amp;quot; results.&lt;/p&gt;\n\n&lt;p&gt;Assignment questions are as follows:&lt;/p&gt;\n\n&lt;p&gt;--------&lt;/p&gt;\n\n&lt;p&gt;During the interview, discuss the person\u2019s professional experience with ethics issues in their professional career on both the technical and personnel/workplace sides. This should include issues they directly experienced, and if they want, can include issues they heard about, as well. Pick two or three of the most memorable issues brought up by your interview, and ask some follow-up questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Do they feel the issue was handled well or not?&lt;/li&gt;\n&lt;li&gt;Were there situations that made it difficult to take the most ethical path?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;--------&lt;/p&gt;\n\n&lt;p&gt;Any feedback would really be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykysox", "is_robot_indexable": true, "report_reasons": null, "author": "Eat-More-Brains", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykysox/data_science_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykysox/data_science_interview/", "subreddit_subscribers": 817069, "created_utc": 1667471888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_d7ung", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can someone ELI5 nested cross validation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "name": "t3_yksloq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/a5my7OPO1jSh0YZfIOUAcwuTB1HxBwyhagXb-lR_xIE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667451058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "scikit-learn.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p8UWe7P-YBJbKgqnfC_c6CCnbN_dt6MaKfe8oBmQpYo.jpg?auto=webp&amp;s=b27948a66bfc7b1fcce0868f365679abefd2dcb6", "width": 160, "height": 58}, "resolutions": [{"url": "https://external-preview.redd.it/p8UWe7P-YBJbKgqnfC_c6CCnbN_dt6MaKfe8oBmQpYo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ea009a13d87d058093cb4117dad12bf8f3d5e5ff", "width": 108, "height": 39}], "variants": {}, "id": "aRDTtkw0_GBCGb9E7JLBzDXeEp2pWa53pBC0AILtbPw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "yksloq", "is_robot_indexable": true, "report_reasons": null, "author": "o-rka", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yksloq/can_someone_eli5_nested_cross_validation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html", "subreddit_subscribers": 817069, "created_utc": 1667451058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello everybody\n\nA long time ago I came here asking for guidance on the world of data from there to here I've been improving and practicing, without belting the internships. That's why I'm coming back here looking for an international opportunity (Especially with the instability that Brazil and Latin America are experiencing), any help is welcome and whatever I can help just call, I'm here to network.\n\n[Linkedin](https://www.linkedin.com/in/matheussbrandao/?locale=en_US)", "author_fullname": "t2_3bymdoy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an international network and guidance.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ylf3m2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667509839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everybody&lt;/p&gt;\n\n&lt;p&gt;A long time ago I came here asking for guidance on the world of data from there to here I&amp;#39;ve been improving and practicing, without belting the internships. That&amp;#39;s why I&amp;#39;m coming back here looking for an international opportunity (Especially with the instability that Brazil and Latin America are experiencing), any help is welcome and whatever I can help just call, I&amp;#39;m here to network.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.linkedin.com/in/matheussbrandao/?locale=en_US\"&gt;Linkedin&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylf3m2", "is_robot_indexable": true, "report_reasons": null, "author": "mathsugar", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylf3m2/looking_for_an_international_network_and_guidance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylf3m2/looking_for_an_international_network_and_guidance/", "subreddit_subscribers": 817069, "created_utc": 1667509839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I'm trying to reverse engineer in my mind the process of returning summary statistics for a large dataset very quickly. I am thinking for example, returning data to a front end to create some charts on a webpage.\n\nLet's say we have a database recording the results of 20,000 matches of a game. The matches have all sorts of data, for example damage done by a certain spell every 10 seconds. It's a lot of data.\n\nWe now want to return the total damage of the spell, \"fireball\", throughout all of the matches. A silly approach would be, every time a user visits a webpage, sum across all 20,000 matches find where spell = \"fireball\" and sum up each damage bit to get the total. This computation would take time regardless of overpaying for high levels of computational power.\n\nWhat I would consider doing in this case is create a new statistics portion of the database. It would include total damage for fireball. Each time you would finish a match, or at another time interval, you would simply add the damage to the total.\n\nWhen you want your statistics, you simply ping the fireball statistics and return the data. I think that this would work great because it would put minor computation requirements on your server frequently rather than massive computation whenever a user wants his stats.\n\nA problem comes up with this approach, what if you needed to report more complex statistics that you can't simply add to every time? If you wanted 95th quartile, or median, you would need to go through every game session and get yourself the entire series (let's call this a full database scan).\n\n**My question:** What sorts of approaches in data management are given for computing up to date complex statistics over large data sets? Would it be a batched process where we are not doing a full database scan every match, but perhaps every 500 matches? Is there any reading I can do to understand these types data/database management practices?", "author_fullname": "t2_4xizssma", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Serving Complex Statistics from Large Databases as Quickly As Possible", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ylewcw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667509424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to reverse engineer in my mind the process of returning summary statistics for a large dataset very quickly. I am thinking for example, returning data to a front end to create some charts on a webpage.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say we have a database recording the results of 20,000 matches of a game. The matches have all sorts of data, for example damage done by a certain spell every 10 seconds. It&amp;#39;s a lot of data.&lt;/p&gt;\n\n&lt;p&gt;We now want to return the total damage of the spell, &amp;quot;fireball&amp;quot;, throughout all of the matches. A silly approach would be, every time a user visits a webpage, sum across all 20,000 matches find where spell = &amp;quot;fireball&amp;quot; and sum up each damage bit to get the total. This computation would take time regardless of overpaying for high levels of computational power.&lt;/p&gt;\n\n&lt;p&gt;What I would consider doing in this case is create a new statistics portion of the database. It would include total damage for fireball. Each time you would finish a match, or at another time interval, you would simply add the damage to the total.&lt;/p&gt;\n\n&lt;p&gt;When you want your statistics, you simply ping the fireball statistics and return the data. I think that this would work great because it would put minor computation requirements on your server frequently rather than massive computation whenever a user wants his stats.&lt;/p&gt;\n\n&lt;p&gt;A problem comes up with this approach, what if you needed to report more complex statistics that you can&amp;#39;t simply add to every time? If you wanted 95th quartile, or median, you would need to go through every game session and get yourself the entire series (let&amp;#39;s call this a full database scan).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My question:&lt;/strong&gt; What sorts of approaches in data management are given for computing up to date complex statistics over large data sets? Would it be a batched process where we are not doing a full database scan every match, but perhaps every 500 matches? Is there any reading I can do to understand these types data/database management practices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylewcw", "is_robot_indexable": true, "report_reasons": null, "author": "gunnerydota", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylewcw/serving_complex_statistics_from_large_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylewcw/serving_complex_statistics_from_large_databases/", "subreddit_subscribers": 817069, "created_utc": 1667509424.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Even a tabular one could work, basically anything that can help!\n\nIn terms of tabular, I'm currently thinking off do an overall table per day of the week, then have another one under per day the hour. This can be a way but just wondering if there's an effective way to visualize it?\n\nI saw some examples like a heatmap but not sure if it can work, or how to do it.\n\nThe data is from ads data (clicks, impressions, installs, or etc.)", "author_fullname": "t2_7u37sy3v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Whats a good visual to analyze/validate a dayparting strategy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylb8dh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667501673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Even a tabular one could work, basically anything that can help!&lt;/p&gt;\n\n&lt;p&gt;In terms of tabular, I&amp;#39;m currently thinking off do an overall table per day of the week, then have another one under per day the hour. This can be a way but just wondering if there&amp;#39;s an effective way to visualize it?&lt;/p&gt;\n\n&lt;p&gt;I saw some examples like a heatmap but not sure if it can work, or how to do it.&lt;/p&gt;\n\n&lt;p&gt;The data is from ads data (clicks, impressions, installs, or etc.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylb8dh", "is_robot_indexable": true, "report_reasons": null, "author": "Aggressive-Pup-28", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylb8dh/whats_a_good_visual_to_analyzevalidate_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylb8dh/whats_a_good_visual_to_analyzevalidate_a/", "subreddit_subscribers": 817069, "created_utc": 1667501673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello there. I'm a university student who wants to learn data analytics and get a certificate online. I recently discovered 365 Data Science's free courses until November 21, and I was able to obtain coupon codes for a 60% discount on their annual subscription, which costs $174 per year.\n\nI'm interested in their [Data Analyst career track](https://learn.365datascience.com/career-tracks/data-analyst/). A total of 44 hours of course content plus exams is required for the certificate of completion.\n\nIs there anyone here who has used their learning platform? I'd be delighted to hear from some of you. I'd also like to hear from users of other platforms like DataQuest, DataCamp, and so on. I searched for the same question in this forum but only found spam promotional posts.\n\nI'm stuck between this course and the [Google Data Analytics Professional Certificate](https://www.coursera.org/professional-certificates/google-data-analytics). According to what I've read online, the Google one costs $39 per month and lasts 181 hours. Due to its lower cost and shorter duration, this makes me lean more toward the 365DS program.\n\nWhich certification has more credibility and is more useful? I'm looking for a more job-ready option and something to help me overcome my poor academic performance. Thank you,\u00a0any help would be greatly appreciated :)", "author_fullname": "t2_egf3g5uu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I take the 365DS Data Analyst Career Track?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yla99d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667499608.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there. I&amp;#39;m a university student who wants to learn data analytics and get a certificate online. I recently discovered 365 Data Science&amp;#39;s free courses until November 21, and I was able to obtain coupon codes for a 60% discount on their annual subscription, which costs $174 per year.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in their &lt;a href=\"https://learn.365datascience.com/career-tracks/data-analyst/\"&gt;Data Analyst career track&lt;/a&gt;. A total of 44 hours of course content plus exams is required for the certificate of completion.&lt;/p&gt;\n\n&lt;p&gt;Is there anyone here who has used their learning platform? I&amp;#39;d be delighted to hear from some of you. I&amp;#39;d also like to hear from users of other platforms like DataQuest, DataCamp, and so on. I searched for the same question in this forum but only found spam promotional posts.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m stuck between this course and the &lt;a href=\"https://www.coursera.org/professional-certificates/google-data-analytics\"&gt;Google Data Analytics Professional Certificate&lt;/a&gt;. According to what I&amp;#39;ve read online, the Google one costs $39 per month and lasts 181 hours. Due to its lower cost and shorter duration, this makes me lean more toward the 365DS program.&lt;/p&gt;\n\n&lt;p&gt;Which certification has more credibility and is more useful? I&amp;#39;m looking for a more job-ready option and something to help me overcome my poor academic performance. Thank you,\u00a0any help would be greatly appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B-U7YKipzRsegPjbhMQ2eMIFLFDTxNL0nOgNOEEBtlg.jpg?auto=webp&amp;s=2c79ec2c452bb4d01df675d2a739bd47f221a670", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/B-U7YKipzRsegPjbhMQ2eMIFLFDTxNL0nOgNOEEBtlg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=108658d3e7925b2e114ce4a4c04d5f4e4295487d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/B-U7YKipzRsegPjbhMQ2eMIFLFDTxNL0nOgNOEEBtlg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9561ba20aaf47ba07975c9cc270feb3b83108d8a", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/B-U7YKipzRsegPjbhMQ2eMIFLFDTxNL0nOgNOEEBtlg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=82e442618be3944d045e58fadd5034487d2d6642", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/B-U7YKipzRsegPjbhMQ2eMIFLFDTxNL0nOgNOEEBtlg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0afa3e8d4b806ab74b5d8d146baf8afd3fa7b37", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/B-U7YKipzRsegPjbhMQ2eMIFLFDTxNL0nOgNOEEBtlg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7fd653b4af836fe5c3f3e7ad731464fa007c47fb", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/B-U7YKipzRsegPjbhMQ2eMIFLFDTxNL0nOgNOEEBtlg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=070e674c45fe1ce071fffe889a90255b231bbe82", "width": 1080, "height": 564}], "variants": {}, "id": "t7A4BY9o-aX-XTyFjhKEkiEB0ueAqjclZI80A8Moy40"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yla99d", "is_robot_indexable": true, "report_reasons": null, "author": "nolettuceandtomatoes", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yla99d/should_i_take_the_365ds_data_analyst_career_track/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yla99d/should_i_take_the_365ds_data_analyst_career_track/", "subreddit_subscribers": 817069, "created_utc": 1667499608.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Understanding that r/datascience is potentially not the right place for this question but posting regardless. \n\nNew joiner to a process design workstream and our change log is kept manually in excel. This doc tracks all changes that has occurred of the lifecycle of the project and has gotten lengthy and ugly. I am looking for a software solution that visualizes the change log better and is more manageable than clicking through excel tabs and cells. \n\nThanks in advance!", "author_fullname": "t2_ar2k0q40", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best visualization software for a change log", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yleci5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667508290.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Understanding that &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt; is potentially not the right place for this question but posting regardless. &lt;/p&gt;\n\n&lt;p&gt;New joiner to a process design workstream and our change log is kept manually in excel. This doc tracks all changes that has occurred of the lifecycle of the project and has gotten lengthy and ugly. I am looking for a software solution that visualizes the change log better and is more manageable than clicking through excel tabs and cells. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yleci5", "is_robot_indexable": true, "report_reasons": null, "author": "Deep_Coyote_7509", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yleci5/best_visualization_software_for_a_change_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yleci5/best_visualization_software_for_a_change_log/", "subreddit_subscribers": 817069, "created_utc": 1667508290.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have what I think may be a simple request, but I don\u2019t know where to start. I\u2019d like to use a program to figure out how many unique groups I have by sorting two lists into a 3rd list. Let\u2019s say list A has 100 locations, and list B has 20 products. All the items in list A require some of the products from list B. What I\u2019m hoping to do is drag the items from List B over the items in List A thus creating a group, ie \u201citem 1 in List A contains these 14 products\u201d (and I need items in List B to be reusable across all the items in List A). Then I want the program to tell me how many locations in List A are receiving the same products (and which locations). Then afterwards I\u2019ll know how many unique groupings of locations and products exist. I don\u2019t know if card sorting would actually work here or if I have the wrong idea about what card sorting is. I\u2019ve been able to do this in Excel somewhat successfully, but it\u2019s more maintenance than help. Just hoping someone might be able to point me in the right direction.", "author_fullname": "t2_bk6kyll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A simple request from a simpleton", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylcoe3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667504789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have what I think may be a simple request, but I don\u2019t know where to start. I\u2019d like to use a program to figure out how many unique groups I have by sorting two lists into a 3rd list. Let\u2019s say list A has 100 locations, and list B has 20 products. All the items in list A require some of the products from list B. What I\u2019m hoping to do is drag the items from List B over the items in List A thus creating a group, ie \u201citem 1 in List A contains these 14 products\u201d (and I need items in List B to be reusable across all the items in List A). Then I want the program to tell me how many locations in List A are receiving the same products (and which locations). Then afterwards I\u2019ll know how many unique groupings of locations and products exist. I don\u2019t know if card sorting would actually work here or if I have the wrong idea about what card sorting is. I\u2019ve been able to do this in Excel somewhat successfully, but it\u2019s more maintenance than help. Just hoping someone might be able to point me in the right direction.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylcoe3", "is_robot_indexable": true, "report_reasons": null, "author": "DolphLundgrenMD", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylcoe3/a_simple_request_from_a_simpleton/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylcoe3/a_simple_request_from_a_simpleton/", "subreddit_subscribers": 817069, "created_utc": 1667504789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book Club - Fundamentals of Data Engineering by Joe Reis and Matthew Housley.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yl6qnq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_mh2cfxap", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Networking", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[Fundamentals of Data Engineering](https://www.amazon.com/Fundamentals-Data-Engineering-Robust-Systems/dp/1098108302) has received a lot of good reviews in the data world, and I\u2019ve been meaning to read it.Turns out lots of folks have been meaning to read it too, so I put together a book club!\n\nI'm not being paid to promote this book in any way, shape, or form. I have no business affiliation with the authors. I just want to read it in a book club.**Here\u2019s a brief overview of the book club:**\\- All participants read the book independently, and then we meet bi-weekly for 30 mins to discuss key takeaways, questions, hot takes, etc.**Schedule:**\n\n* [**November 18th**](https://calendly.com/jparkerrogers/oa-book-club?month=2022-11&amp;date=2022-11-18)**:** Discuss Chapters 1-3\n* [**December 2nd**](https://calendly.com/jparkerrogers/oa-book-club?month=2022-12&amp;date=2022-12-02)**:** Discuss Chapters 4-7\n* [**December 8th**](https://calendly.com/jparkerrogers/oa-book-club?month=2022-12&amp;date=2022-12-08)**:** Live AMA with Joe Reis, the author\n* [**December 16th**](https://calendly.com/jparkerrogers/oa-book-club?month=2022-12&amp;date=2022-12-16)**:** Discuss Chapters 8-11\n\nWe currently have 20+ people participating!If this sounds interesting to you, find additional details and schedule [here](https://calendly.com/jparkerrogers/oa-book-club?month=2022-11).", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book Club - Fundamentals of Data Engineering by Joe Reis and Matthew Housley.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykbcfo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 157, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 157, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667434000.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_1": 1}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667410085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.amazon.com/Fundamentals-Data-Engineering-Robust-Systems/dp/1098108302\"&gt;Fundamentals of Data Engineering&lt;/a&gt; has received a lot of good reviews in the data world, and I\u2019ve been meaning to read it.Turns out lots of folks have been meaning to read it too, so I put together a book club!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not being paid to promote this book in any way, shape, or form. I have no business affiliation with the authors. I just want to read it in a book club.&lt;strong&gt;Here\u2019s a brief overview of the book club:&lt;/strong&gt;- All participants read the book independently, and then we meet bi-weekly for 30 mins to discuss key takeaways, questions, hot takes, etc.&lt;strong&gt;Schedule:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://calendly.com/jparkerrogers/oa-book-club?month=2022-11&amp;amp;date=2022-11-18\"&gt;&lt;strong&gt;November 18th&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; Discuss Chapters 1-3&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://calendly.com/jparkerrogers/oa-book-club?month=2022-12&amp;amp;date=2022-12-02\"&gt;&lt;strong&gt;December 2nd&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; Discuss Chapters 4-7&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://calendly.com/jparkerrogers/oa-book-club?month=2022-12&amp;amp;date=2022-12-08\"&gt;&lt;strong&gt;December 8th&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; Live AMA with Joe Reis, the author&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://calendly.com/jparkerrogers/oa-book-club?month=2022-12&amp;amp;date=2022-12-16\"&gt;&lt;strong&gt;December 16th&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; Discuss Chapters 8-11&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We currently have 20+ people participating!If this sounds interesting to you, find additional details and schedule &lt;a href=\"https://calendly.com/jparkerrogers/oa-book-club?month=2022-11\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 100, "id": "gid_1", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Shows the Silver Award... and that's it.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Silver", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "ykbcfo", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ykbcfo/book_club_fundamentals_of_data_engineering_by_joe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ykbcfo/book_club_fundamentals_of_data_engineering_by_joe/", "subreddit_subscribers": 78837, "created_utc": 1667410085.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1667491606.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/dataengineering/comments/ykbcfo/book_club_fundamentals_of_data_engineering_by_joe/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yl6qnq", "is_robot_indexable": true, "report_reasons": null, "author": "whb2030", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_ykbcfo", "author_flair_text_color": null, "permalink": "/r/datascience/comments/yl6qnq/book_club_fundamentals_of_data_engineering_by_joe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/dataengineering/comments/ykbcfo/book_club_fundamentals_of_data_engineering_by_joe/", "subreddit_subscribers": 817069, "created_utc": 1667491606.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m not sure exactly how to phrase this. Professional DS with 15 years experience. I have no problems doing everything locally or on a web server if someone sets one up for me. For example, my company has a process to deploy Python scripts via restful API. The engineer taught me how to deploy what in need but when something breaks I am lost. How would I learn to fix bugs or setup my own type of service? Another issue. Someone is sending me a huge data file across a subnet at 5MB per second. Often the job fails. I would like to try to fix this. How do I learn about this. I don\u2019t even know what it\u2019s called. I\u2019m tired of waiting on other people to solve my problems.", "author_fullname": "t2_kcl3tfwe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good resources for architecture and networks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yl1w7d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667479497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m not sure exactly how to phrase this. Professional DS with 15 years experience. I have no problems doing everything locally or on a web server if someone sets one up for me. For example, my company has a process to deploy Python scripts via restful API. The engineer taught me how to deploy what in need but when something breaks I am lost. How would I learn to fix bugs or setup my own type of service? Another issue. Someone is sending me a huge data file across a subnet at 5MB per second. Often the job fails. I would like to try to fix this. How do I learn about this. I don\u2019t even know what it\u2019s called. I\u2019m tired of waiting on other people to solve my problems.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yl1w7d", "is_robot_indexable": true, "report_reasons": null, "author": "NewEcho2940", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yl1w7d/good_resources_for_architecture_and_networks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yl1w7d/good_resources_for_architecture_and_networks/", "subreddit_subscribers": 817069, "created_utc": 1667479497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm finding parallelising an r task (which basically means assigning work to a separate r session) requires all data in my environment to be copied over to the cache of the new core. If I'm working with just 10gb of data, my machine is losing more time copying data than it's saving by parallelising. This means I can't even split up CV tasks on windows, which should be trivial to parallelise.\n\nIs there an obvious alternative to this approach? And am I right in saying running on Linux would avoid this bottleneck?", "author_fullname": "t2_1rwftqt3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does lack of fork parallelisation make windows an impractical OS for ML?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yl10to", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667477285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m finding parallelising an r task (which basically means assigning work to a separate r session) requires all data in my environment to be copied over to the cache of the new core. If I&amp;#39;m working with just 10gb of data, my machine is losing more time copying data than it&amp;#39;s saving by parallelising. This means I can&amp;#39;t even split up CV tasks on windows, which should be trivial to parallelise.&lt;/p&gt;\n\n&lt;p&gt;Is there an obvious alternative to this approach? And am I right in saying running on Linux would avoid this bottleneck?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yl10to", "is_robot_indexable": true, "report_reasons": null, "author": "theAbominablySlowMan", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yl10to/does_lack_of_fork_parallelisation_make_windows_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yl10to/does_lack_of_fork_parallelisation_make_windows_an/", "subreddit_subscribers": 817069, "created_utc": 1667477285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "A friend and I recently participated in a local datathon here in Australia and we're looking for further challenges either here or anywhere in the world (if it's open to international teams), however we're having a hard time finding websites or communities that are centered in promoting/advertising these events. We have something similar in Australia [for hackathons in general](https://www.hackathonsaustralia.com/), but they ceased comms over a year ago.\n\nAny suggestions about where to look for more data-driven challenges and competitions for international datathons or (ideally) in Australia? (Yes I *am* googling please don't send me there).", "author_fullname": "t2_71lk6sl4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any suggestions on communities (here on Reddit) or websites that announce/promote datathons?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykrt71", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Networking", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667448496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A friend and I recently participated in a local datathon here in Australia and we&amp;#39;re looking for further challenges either here or anywhere in the world (if it&amp;#39;s open to international teams), however we&amp;#39;re having a hard time finding websites or communities that are centered in promoting/advertising these events. We have something similar in Australia &lt;a href=\"https://www.hackathonsaustralia.com/\"&gt;for hackathons in general&lt;/a&gt;, but they ceased comms over a year ago.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions about where to look for more data-driven challenges and competitions for international datathons or (ideally) in Australia? (Yes I &lt;em&gt;am&lt;/em&gt; googling please don&amp;#39;t send me there).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?auto=webp&amp;s=11eea8bc1fb810dc41c39d03eb1ea8d9c1977777", "width": 2460, "height": 1403}, "resolutions": [{"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ef608d492647f3667beab7bfa9ab39f2e9d5dcc", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=abde207855145f367f630b2ee21b426c780c27be", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=327fcb10a130b311dbfd0b7defec642d8f3030d7", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d9a13d21b8d35521dd4ea5faacf7b598eff8fd4", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f93546129811ed8d4043fe3b38fcbf0b42e988d", "width": 960, "height": 547}, {"url": "https://external-preview.redd.it/IXtgRs6Zobbj_lavvAd9PHWaZRdv_ASW5PMi7ACWVL0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8670867e42d602b1d0e593ca46be3d885ac8511f", "width": 1080, "height": 615}], "variants": {}, "id": "vM8d3m2aOfl5VOI6_MUCLK00ddGCAj8Fgsy5VTpH0Sw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykrt71", "is_robot_indexable": true, "report_reasons": null, "author": "William_Rosebud", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykrt71/any_suggestions_on_communities_here_on_reddit_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykrt71/any_suggestions_on_communities_here_on_reddit_or/", "subreddit_subscribers": 817069, "created_utc": 1667448496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I have a dataset that a client sent me. They are a furniture manufacturer company, specialized in making Dressers. Their \"added value\" are how economical their products are.\n\nThe dataset's rows are individual ordered products, which can repeat for a single customer depending on how many products they ordered in a single transaction/day (if they bought 10 dressers, a single customer has 10 rows, each row being 1 product bought). **I'm tasked with gathering actionable insights for the client based on the dataset; the issue is that the dataset is very limited in terms of columns/variables, and I think/feel I'm out of ideas already**. I'm part of a digital marketing business, but this is exercise is actually not directly related to exclusively/mandatorily generating digital marketing recommendations.\n\nThe dataset columns are:\n\n1. Order Date (dd/mm/yyy), only with data from January 2021 to October 2022\n2. Product Name\n3. Customer Name (mostly useless)\n4. Customer Gender (M or F)\n5. City (needs A LOT of cleaning to standardize. This was a manual input from whoever filled it out)\n6. State (from the United States)\n7. Zip Code\n8. Channel (whether they sell through Amazon, Walmart or other marketplaces, or directly through their website. This is only available for 2021 and not for 2022)\n\nI already looked at things like:\n\n1. Top/bottom states in terms of orders by 100k population (based on each state's population). \\[Bar charts\\]\n2. Total orders (Y axis) by month (X axis) by state to detect outliers/patterns (nothing to highlight here, they mostly follow the national, aggregated total orders). \\[Line charts\\]\n3. Most popular products by state and their avg. order share (from each state) \\[Bar charts\\]\n4. Top/bottom products just based on total orders \\[Bar charts\\]\n5. Top/bottom states just based on total orders \\[Bar charts\\]\n6. Reach by product (whether they are sold in all states: 100%, just in one (1/51), etc.) \\[Bar charts\\]\n7. How much of the total orders do Dressers represent specifically \\[Area chart\\]\n8. Top 5 products by month \\[Ribbon chart\\]\n9. Top 5 products by gender \\[Bar chart\\]\n10. Top/bottom products by their male-to-female ratio in terms of orders \\[Bar chart\\]\n11. Repurchases based on a \"composite key\" of customer name + zip code for a single day \\[Bar chart, but almost useless. Almost no repurchases with less than 0.5% of all transactions\\]\n\nThat's about it. I think having columns like \"revenue\" and data for more years could really bring a whole new world of insights and creativity to get real actionable insights. So far the client (for any reason) has been reluctant to provide this or more data apart from 2021 and 2022. **Any ideas? I would enormously appreciate your input on this**\n\n**TL;DR: I have a very limited dataset on orders for furniture, mostly Dressers. I need to gather actionable insights and the data is extremely limited in terms of date range (2021-2022) and variables (up to 8 variables, with just 4 or 5 being truly useful). Ideas on actionable insights?**\n\nThank you!", "author_fullname": "t2_ie26w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to add more creativity to this EDA to gather useful, actionable insights", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykp0fb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667440266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a dataset that a client sent me. They are a furniture manufacturer company, specialized in making Dressers. Their &amp;quot;added value&amp;quot; are how economical their products are.&lt;/p&gt;\n\n&lt;p&gt;The dataset&amp;#39;s rows are individual ordered products, which can repeat for a single customer depending on how many products they ordered in a single transaction/day (if they bought 10 dressers, a single customer has 10 rows, each row being 1 product bought). &lt;strong&gt;I&amp;#39;m tasked with gathering actionable insights for the client based on the dataset; the issue is that the dataset is very limited in terms of columns/variables, and I think/feel I&amp;#39;m out of ideas already&lt;/strong&gt;. I&amp;#39;m part of a digital marketing business, but this is exercise is actually not directly related to exclusively/mandatorily generating digital marketing recommendations.&lt;/p&gt;\n\n&lt;p&gt;The dataset columns are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Order Date (dd/mm/yyy), only with data from January 2021 to October 2022&lt;/li&gt;\n&lt;li&gt;Product Name&lt;/li&gt;\n&lt;li&gt;Customer Name (mostly useless)&lt;/li&gt;\n&lt;li&gt;Customer Gender (M or F)&lt;/li&gt;\n&lt;li&gt;City (needs A LOT of cleaning to standardize. This was a manual input from whoever filled it out)&lt;/li&gt;\n&lt;li&gt;State (from the United States)&lt;/li&gt;\n&lt;li&gt;Zip Code&lt;/li&gt;\n&lt;li&gt;Channel (whether they sell through Amazon, Walmart or other marketplaces, or directly through their website. This is only available for 2021 and not for 2022)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I already looked at things like:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Top/bottom states in terms of orders by 100k population (based on each state&amp;#39;s population). [Bar charts]&lt;/li&gt;\n&lt;li&gt;Total orders (Y axis) by month (X axis) by state to detect outliers/patterns (nothing to highlight here, they mostly follow the national, aggregated total orders). [Line charts]&lt;/li&gt;\n&lt;li&gt;Most popular products by state and their avg. order share (from each state) [Bar charts]&lt;/li&gt;\n&lt;li&gt;Top/bottom products just based on total orders [Bar charts]&lt;/li&gt;\n&lt;li&gt;Top/bottom states just based on total orders [Bar charts]&lt;/li&gt;\n&lt;li&gt;Reach by product (whether they are sold in all states: 100%, just in one (1/51), etc.) [Bar charts]&lt;/li&gt;\n&lt;li&gt;How much of the total orders do Dressers represent specifically [Area chart]&lt;/li&gt;\n&lt;li&gt;Top 5 products by month [Ribbon chart]&lt;/li&gt;\n&lt;li&gt;Top 5 products by gender [Bar chart]&lt;/li&gt;\n&lt;li&gt;Top/bottom products by their male-to-female ratio in terms of orders [Bar chart]&lt;/li&gt;\n&lt;li&gt;Repurchases based on a &amp;quot;composite key&amp;quot; of customer name + zip code for a single day [Bar chart, but almost useless. Almost no repurchases with less than 0.5% of all transactions]&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;That&amp;#39;s about it. I think having columns like &amp;quot;revenue&amp;quot; and data for more years could really bring a whole new world of insights and creativity to get real actionable insights. So far the client (for any reason) has been reluctant to provide this or more data apart from 2021 and 2022. &lt;strong&gt;Any ideas? I would enormously appreciate your input on this&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR: I have a very limited dataset on orders for furniture, mostly Dressers. I need to gather actionable insights and the data is extremely limited in terms of date range (2021-2022) and variables (up to 8 variables, with just 4 or 5 being truly useful). Ideas on actionable insights?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykp0fb", "is_robot_indexable": true, "report_reasons": null, "author": "MadGlobin", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykp0fb/how_to_add_more_creativity_to_this_eda_to_gather/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykp0fb/how_to_add_more_creativity_to_this_eda_to_gather/", "subreddit_subscribers": 817069, "created_utc": 1667440266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm interested in learning a new \"spoken language\" (idk how to call it), I tried to look it up but only got answers about programming languages.\n\nI already speak Spanish (native) and English (not perfectly) but don't have any idea where should I continue.", "author_fullname": "t2_5oj654vi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the most useful spoken languages for data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ykkupe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667430075.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m interested in learning a new &amp;quot;spoken language&amp;quot; (idk how to call it), I tried to look it up but only got answers about programming languages.&lt;/p&gt;\n\n&lt;p&gt;I already speak Spanish (native) and English (not perfectly) but don&amp;#39;t have any idea where should I continue.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ykkupe", "is_robot_indexable": true, "report_reasons": null, "author": "Pepe_Alpa", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ykkupe/what_are_the_most_useful_spoken_languages_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ykkupe/what_are_the_most_useful_spoken_languages_for/", "subreddit_subscribers": 817069, "created_utc": 1667430075.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As stated above, I work in Child Fatality Review. Every state in the US for the most part has a review team that draws conclusions from the data collected. I did not go to school for data analysis but I would like to be trained in graphing statistics as it would make the most impact on our annual reports.  Does anybody have any tips on how to develop this skill without going back to school for it?", "author_fullname": "t2_oxgbf5nq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Child Fatality Review - How can I develop this career further without going to school?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yl5rrp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667489374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As stated above, I work in Child Fatality Review. Every state in the US for the most part has a review team that draws conclusions from the data collected. I did not go to school for data analysis but I would like to be trained in graphing statistics as it would make the most impact on our annual reports.  Does anybody have any tips on how to develop this skill without going back to school for it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yl5rrp", "is_robot_indexable": true, "report_reasons": null, "author": "truecrimeavocado", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yl5rrp/child_fatality_review_how_can_i_develop_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yl5rrp/child_fatality_review_how_can_i_develop_this/", "subreddit_subscribers": 817069, "created_utc": 1667489374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Dear all,\n\nSuppose yo want to do a parametric analysis, what I mean is that you want to obtain an estimate of some parameter P, from a given dataset A. \nFor some reason, a college was doing the same exact thing, but in a different dataset. He is measuring P from dataset B.\nNow both colleagues want to obtain the most of both that datasets, they meet and they find that A and B are not independent, and the intersection C is not negligible. \n\nDo you know some references to deal with this kind of situation? Or do you know a procedure to get the most information of both datasets without discarding the intersection? How would you proceed?\n\nSorry in advance if this is not the right place to ask\nHace a nice day!", "author_fullname": "t2_a50rr8cj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analysis in intersected data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yl4bur", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667485815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear all,&lt;/p&gt;\n\n&lt;p&gt;Suppose yo want to do a parametric analysis, what I mean is that you want to obtain an estimate of some parameter P, from a given dataset A. \nFor some reason, a college was doing the same exact thing, but in a different dataset. He is measuring P from dataset B.\nNow both colleagues want to obtain the most of both that datasets, they meet and they find that A and B are not independent, and the intersection C is not negligible. &lt;/p&gt;\n\n&lt;p&gt;Do you know some references to deal with this kind of situation? Or do you know a procedure to get the most information of both datasets without discarding the intersection? How would you proceed?&lt;/p&gt;\n\n&lt;p&gt;Sorry in advance if this is not the right place to ask\nHace a nice day!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yl4bur", "is_robot_indexable": true, "report_reasons": null, "author": "SpiritedEagle7948", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yl4bur/analysis_in_intersected_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yl4bur/analysis_in_intersected_data/", "subreddit_subscribers": 817069, "created_utc": 1667485815.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}