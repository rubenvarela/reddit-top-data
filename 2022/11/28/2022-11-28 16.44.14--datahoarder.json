{"kind": "Listing", "data": {"after": "t3_z6obz8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8po7w9n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "In search of 2 old games, Military Madness (cellphones, ~2006) and Military Madness 2 / Neo Nectaris (iPhone, ~2010). Already tried every relevant site, so have to ask do anybody still have either of them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ron3kvc1pn2a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 72, "x": 108, "u": "https://preview.redd.it/ron3kvc1pn2a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e8f428d17d3152f9afb39e0edce00361c62af09"}, {"y": 144, "x": 216, "u": "https://preview.redd.it/ron3kvc1pn2a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e161b850d09f92ac8587b131d349b8dd0625fb34"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/ron3kvc1pn2a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce9b96417da35644cb78dac5009290a145299841"}], "s": {"y": 320, "x": 480, "u": "https://preview.redd.it/ron3kvc1pn2a1.jpg?width=480&amp;format=pjpg&amp;auto=webp&amp;s=c79c52b0c680f53fe2e7fbc74fae2619c68659e8"}, "id": "ron3kvc1pn2a1"}, "wkoffgj1pn2a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 72, "x": 108, "u": "https://preview.redd.it/wkoffgj1pn2a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3c8c36a431f7b5de0a497339215d631b20800a4"}, {"y": 144, "x": 216, "u": "https://preview.redd.it/wkoffgj1pn2a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f001963c396ac5631735e28a0bdccc81aa27179"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/wkoffgj1pn2a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a2ca94bca494f18ce46c243136a2096cfc878c3"}], "s": {"y": 320, "x": 480, "u": "https://preview.redd.it/wkoffgj1pn2a1.jpg?width=480&amp;format=pjpg&amp;auto=webp&amp;s=008e48f8876a2bf5049561f2ab34d35ff7e4988b"}, "id": "wkoffgj1pn2a1"}}, "name": "t3_z6rjug", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 75, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "map example of iPhone Neo Nectaris gameplay ", "media_id": "ron3kvc1pn2a1", "id": 213692469}, {"caption": "battle example of iPhone Neo Nectaris gameplay", "media_id": "wkoffgj1pn2a1", "id": 213692470}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 75, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ca7drkiqqxa9R5E0sx0hSdMJbApZCQcNGzoQR7Fmqb4.jpg", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669625748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/z6rjug", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "DVD", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6rjug", "is_robot_indexable": true, "report_reasons": null, "author": "TJ_six", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6rjug/in_search_of_2_old_games_military_madness/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/z6rjug", "subreddit_subscribers": 656595, "created_utc": 1669625748.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ud20v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you all monitor ambient temps for your drives? Cooking drives is no fun... I think I found a decent solution with these $12 Govee bluetooth thermometers and Home Assistant.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_z6yt5j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 60, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 60, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/x3Mb9g8NNWNcr8NdTGW4r5ePEQ7g6Gzfd7G18_NNB8U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669647819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "austinsnerdythings.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://austinsnerdythings.com/2021/12/27/using-the-govee-bluetooth-thermometer-with-home-assistant-python-and-mqtt/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?auto=webp&amp;s=b48a34bccb2954db8eb1e4c65492d55fd53a6469", "width": 797, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a4e406978acf38f4e1bc40cc05bcf511d8a97ea0", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38d7e5b765a309c12b1f74510985e49efeaaad5d", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=52144e94d62313773ce70d6ed7f890458c3e7d29", "width": 320, "height": 321}, {"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da6e2c281454349bd4a62444a47de41ac3cb6365", "width": 640, "height": 642}], "variants": {}, "id": "-Z7KWEbgh-aaJdZinEtRKgixRmRb1ywIvERkB5QovFs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6yt5j", "is_robot_indexable": true, "report_reasons": null, "author": "MzCWzL", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6yt5j/how_do_you_all_monitor_ambient_temps_for_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://austinsnerdythings.com/2021/12/27/using-the-govee-bluetooth-thermometer-with-home-assistant-python-and-mqtt/", "subreddit_subscribers": 656595, "created_utc": 1669647819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[This article](https://www.servethehome.com/wd-red-pro-20tb-launched-with-wickedly-weak-workload-rating/) on ServeTheHome got me thinking... in a RAID array where periodic \"scrubs\" or consistency checks are recommended to prevent bit rot, the entire disk has to be read. \n\nI did some quick math for my own array which uses HGST He10 10TB drives with 550TB/yr rated workload and the default MegaRAID settings of weekly consistency check AND patrol read, resulting in\n\n10TB * 2 * 52 = 1040TB/yr read\n\nWhile doing both of them monthly instead is\n\n10TB * 2 * 12 = 240TB/yr read, leaving 310TB/yr for actually using the drives\n\nI searched to see if anyone else had posted here or on other forums about this concern and couldn't find anything. Should I reduce the frequency to monthly to avoid 'wearing out' the drives, or is a weekly check needed to prevent bit rot on high capacity drives?\n\nEDIT: thanks for all the data points confirming monthly check is okay! I was going by some old threads like [this one](https://community.spiceworks.com/topic/1648419-lsi-megaraid-patrol-read-and-consistency-check-schedule-recommendations) which scared me into thinking I needed to keep the weekly default. I've switched mine to monthly now.", "author_fullname": "t2_10gstl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running weekly consistency checks on 10TB+ drives exceeds the rated annual workload", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z68j76", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669583861.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669573609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.servethehome.com/wd-red-pro-20tb-launched-with-wickedly-weak-workload-rating/\"&gt;This article&lt;/a&gt; on ServeTheHome got me thinking... in a RAID array where periodic &amp;quot;scrubs&amp;quot; or consistency checks are recommended to prevent bit rot, the entire disk has to be read. &lt;/p&gt;\n\n&lt;p&gt;I did some quick math for my own array which uses HGST He10 10TB drives with 550TB/yr rated workload and the default MegaRAID settings of weekly consistency check AND patrol read, resulting in&lt;/p&gt;\n\n&lt;p&gt;10TB * 2 * 52 = 1040TB/yr read&lt;/p&gt;\n\n&lt;p&gt;While doing both of them monthly instead is&lt;/p&gt;\n\n&lt;p&gt;10TB * 2 * 12 = 240TB/yr read, leaving 310TB/yr for actually using the drives&lt;/p&gt;\n\n&lt;p&gt;I searched to see if anyone else had posted here or on other forums about this concern and couldn&amp;#39;t find anything. Should I reduce the frequency to monthly to avoid &amp;#39;wearing out&amp;#39; the drives, or is a weekly check needed to prevent bit rot on high capacity drives?&lt;/p&gt;\n\n&lt;p&gt;EDIT: thanks for all the data points confirming monthly check is okay! I was going by some old threads like &lt;a href=\"https://community.spiceworks.com/topic/1648419-lsi-megaraid-patrol-read-and-consistency-check-schedule-recommendations\"&gt;this one&lt;/a&gt; which scared me into thinking I needed to keep the weekly default. I&amp;#39;ve switched mine to monthly now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BUZXwSx4FYKZmj-VU6fkiJFn3ZbX_w4URhT0fUwD-mg.jpg?auto=webp&amp;s=cb351bd56b93ca009b377beba589db481b6848f8", "width": 800, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/BUZXwSx4FYKZmj-VU6fkiJFn3ZbX_w4URhT0fUwD-mg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=35cecc1f6cc5bf3533dd0bb979ed4aaf207b5921", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/BUZXwSx4FYKZmj-VU6fkiJFn3ZbX_w4URhT0fUwD-mg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=19630e261c1b90ff636bb0adcb95f3573d512bfb", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/BUZXwSx4FYKZmj-VU6fkiJFn3ZbX_w4URhT0fUwD-mg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d302121db869389decf9fd03a6cc3e66db8d9c6c", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/BUZXwSx4FYKZmj-VU6fkiJFn3ZbX_w4URhT0fUwD-mg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd1a46d03245e6f7771d376e4f1768bd0befa817", "width": 640, "height": 480}], "variants": {}, "id": "Db_RhGlIp4EmXb0BNwixBz21lbTamvNQ-ZavgIbxi0E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z68j76", "is_robot_indexable": true, "report_reasons": null, "author": "denpa_", "discussion_type": null, "num_comments": 17, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z68j76/running_weekly_consistency_checks_on_10tb_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z68j76/running_weekly_consistency_checks_on_10tb_drives/", "subreddit_subscribers": 656595, "created_utc": 1669573609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "You download your information from Facebook, and just thousands of files you need to open one by one. Is there a way to make this actually useful and accessible?", "author_fullname": "t2_4rtg6gja", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can you actually use your facebook information downloaded?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6hro4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669595653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You download your information from Facebook, and just thousands of files you need to open one by one. Is there a way to make this actually useful and accessible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z6hro4", "is_robot_indexable": true, "report_reasons": null, "author": "stackshockprism", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6hro4/how_can_you_actually_use_your_facebook/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6hro4/how_can_you_actually_use_your_facebook/", "subreddit_subscribers": 656595, "created_utc": 1669595653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good morning everyone. Like most people studying, I got fucked by the pandemic and became part of the Zoom Academy, and most of the materials we used are digital only. I wanted to know if there's a way to download 3 years' worth of documents, assignments, and such on Google Classroom to my computer. My access to these documents is coming close, so I wanted to store them for future use. Is there anything I can do, or should I download everything manually?\n\nA random guy on Discord said I should come to this sub to search for answers, so sorry if this is not the correct place", "author_fullname": "t2_2g2sxfio", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download of Google Classroom Materials", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6vrk8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669639855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good morning everyone. Like most people studying, I got fucked by the pandemic and became part of the Zoom Academy, and most of the materials we used are digital only. I wanted to know if there&amp;#39;s a way to download 3 years&amp;#39; worth of documents, assignments, and such on Google Classroom to my computer. My access to these documents is coming close, so I wanted to store them for future use. Is there anything I can do, or should I download everything manually?&lt;/p&gt;\n\n&lt;p&gt;A random guy on Discord said I should come to this sub to search for answers, so sorry if this is not the correct place&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6vrk8", "is_robot_indexable": true, "report_reasons": null, "author": "Sanslution", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6vrk8/download_of_google_classroom_materials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6vrk8/download_of_google_classroom_materials/", "subreddit_subscribers": 656595, "created_utc": 1669639855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have seen a website similar to Wayback Machine / archive.org that used to crawl and index HTML code of  every webpage / website. I forgot the website. Does anyone know anything similar?\n\nFor example, we could search for some term like \"&lt;h1&gt;twitter&lt;/h1&gt;\" and it used to return all webpages that has a twitter inside head tag in their html code.\n\nThanks in advance", "author_fullname": "t2_9s1kb9su", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web Archive - HTML Code dump of websites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6w06z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669640499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen a website similar to Wayback Machine / archive.org that used to crawl and index HTML code of  every webpage / website. I forgot the website. Does anyone know anything similar?&lt;/p&gt;\n\n&lt;p&gt;For example, we could search for some term like &amp;quot;&amp;lt;h1&amp;gt;twitter&amp;lt;/h1&amp;gt;&amp;quot; and it used to return all webpages that has a twitter inside head tag in their html code.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6w06z", "is_robot_indexable": true, "report_reasons": null, "author": "karthiksudhan-wild", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6w06z/web_archive_html_code_dump_of_websites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6w06z/web_archive_html_code_dump_of_websites/", "subreddit_subscribers": 656595, "created_utc": 1669640499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Most here have at least 1TB+ my self included. I\u2019m a Mac user so I haven\u2019t found out how to scan a drive or a folder for corrupt files but so far I haven\u2019t encountered any issues. What are your experiences? \n\nAll my drives are stationary so nothings ever been dropped or exposed to water or anything which helps my case for sure.", "author_fullname": "t2_2o8t3n2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How often have you experienced a corrupt file(s)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6e88z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669586931.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most here have at least 1TB+ my self included. I\u2019m a Mac user so I haven\u2019t found out how to scan a drive or a folder for corrupt files but so far I haven\u2019t encountered any issues. What are your experiences? &lt;/p&gt;\n\n&lt;p&gt;All my drives are stationary so nothings ever been dropped or exposed to water or anything which helps my case for sure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6e88z", "is_robot_indexable": true, "report_reasons": null, "author": "QualitySound96", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6e88z/how_often_have_you_experienced_a_corrupt_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6e88z/how_often_have_you_experienced_a_corrupt_files/", "subreddit_subscribers": 656595, "created_utc": 1669586931.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_32nu9tpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Elements 12TB: $175 after $40 off promo code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 53, "top_awarded_type": null, "hide_score": true, "name": "t3_z6z4xe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EecN7D0NOmxars9JZSQaWp7TArdt0eQiuhqdKq7dSgU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669648651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/qet2t0j2lp2a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?auto=webp&amp;s=c40c3b7a7ad094899748d2ea343232f8cad7833e", "width": 1828, "height": 696}, "resolutions": [{"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=716870203a3f2c715dd5a406f2ae998b612aeae6", "width": 108, "height": 41}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=83bcf71c8b11fdf7eb628a1e7371846e6e7edb76", "width": 216, "height": 82}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d78414140ee2eaee7f515e0dbb9a54a95f3959b", "width": 320, "height": 121}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bcd4582e1b12b84c9816b614091553bd2e8bf24", "width": 640, "height": 243}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=25ac76353e7c9e33dd5b0cb3b21c87b6d227a98a", "width": 960, "height": 365}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=412d2b11079b810832e50aaa9433344ebe0feaaf", "width": 1080, "height": 411}], "variants": {}, "id": "vLd_u51BuwpxsUA6Owsmqe0DJSQ32OjxAvsPEhxGye0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "z6z4xe", "is_robot_indexable": true, "report_reasons": null, "author": "Kosofkors", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6z4xe/wd_elements_12tb_175_after_40_off_promo_code/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/qet2t0j2lp2a1.png", "subreddit_subscribers": 656595, "created_utc": 1669648651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am planning to build a NAS with unraid which will have 2x14, 2x8 TB,  for now. This NAS will be a standalone and act as NAS without Plex or  any other services. I want to have another system working as the main  server. I currently have few optoins at my disposal and was wondering  what would be the best use of current resources. Some are already built  systems and some need to be built.\n\nExpand-ability: slowly but not sure to what storage.\n\nPower consumption: Preferred low, as it might be idle 70% at least for the first year.\n\n&amp;#x200B;\n\nOption1 (Preferred option):\n\n[i5-2400](https://www.intel.com/content/www/us/en/products/sku/52207/intel-core-i52400-processor-6m-cache-up-to-3-40-ghz/specifications.html), 8GB (not expandable) ram as NAS and[ i5-8500T](https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html), 16GB as the main server\n\nOption2:\n\n[i3-10105F](https://www.intel.com/content/www/us/en/products/sku/203474/intel-core-i310105f-processor-6m-cache-up-to-4-40-ghz/specifications.html), xGB (need to be built) and [i5-8500T](https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html), 16GB - either can be used for either purpose\n\nOption3:\n\n[i7-6700](https://www.intel.com/content/www/us/en/products/sku/88195/intel-core-i76700k-processor-8m-cache-up-to-4-20-ghz/specifications.html), xGB (need to be built) and [i5-8500T](https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html), 16GB - either can be used for either purpose\n\nOption4:\n\nUse either of the above systems to do the whole server in one massive system, which has all the HDD's and the main server.\n\n&amp;#x200B;\n\nComparison of CPU's  \n[https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F](https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F)\n\n&amp;#x200B;\n\nAlso, have a [1060](https://www.evga.com/products/specs/gpu.aspx?pn=e466f076-3e8f-4825-b4b0-f1863e893972), 6GB nvidia if that can be leveraged.\n\nAs mentioned would prefer to use the existing builds but wondering if 2nd gen would be sufficient enough.", "author_fullname": "t2_2lkmjxkk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[BUILD HELP] Choosing right CPU and RAM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6ojsg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669615353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning to build a NAS with unraid which will have 2x14, 2x8 TB,  for now. This NAS will be a standalone and act as NAS without Plex or  any other services. I want to have another system working as the main  server. I currently have few optoins at my disposal and was wondering  what would be the best use of current resources. Some are already built  systems and some need to be built.&lt;/p&gt;\n\n&lt;p&gt;Expand-ability: slowly but not sure to what storage.&lt;/p&gt;\n\n&lt;p&gt;Power consumption: Preferred low, as it might be idle 70% at least for the first year.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Option1 (Preferred option):&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.intel.com/content/www/us/en/products/sku/52207/intel-core-i52400-processor-6m-cache-up-to-3-40-ghz/specifications.html\"&gt;i5-2400&lt;/a&gt;, 8GB (not expandable) ram as NAS and&lt;a href=\"https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html\"&gt; i5-8500T&lt;/a&gt;, 16GB as the main server&lt;/p&gt;\n\n&lt;p&gt;Option2:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.intel.com/content/www/us/en/products/sku/203474/intel-core-i310105f-processor-6m-cache-up-to-4-40-ghz/specifications.html\"&gt;i3-10105F&lt;/a&gt;, xGB (need to be built) and &lt;a href=\"https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html\"&gt;i5-8500T&lt;/a&gt;, 16GB - either can be used for either purpose&lt;/p&gt;\n\n&lt;p&gt;Option3:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.intel.com/content/www/us/en/products/sku/88195/intel-core-i76700k-processor-8m-cache-up-to-4-20-ghz/specifications.html\"&gt;i7-6700&lt;/a&gt;, xGB (need to be built) and &lt;a href=\"https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html\"&gt;i5-8500T&lt;/a&gt;, 16GB - either can be used for either purpose&lt;/p&gt;\n\n&lt;p&gt;Option4:&lt;/p&gt;\n\n&lt;p&gt;Use either of the above systems to do the whole server in one massive system, which has all the HDD&amp;#39;s and the main server.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Comparison of CPU&amp;#39;s&lt;br/&gt;\n&lt;a href=\"https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F\"&gt;https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Also, have a &lt;a href=\"https://www.evga.com/products/specs/gpu.aspx?pn=e466f076-3e8f-4825-b4b0-f1863e893972\"&gt;1060&lt;/a&gt;, 6GB nvidia if that can be leveraged.&lt;/p&gt;\n\n&lt;p&gt;As mentioned would prefer to use the existing builds but wondering if 2nd gen would be sufficient enough.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?auto=webp&amp;s=0402e358a1b5b71d7e7e3840908c20761a156712", "width": 586, "height": 387}, "resolutions": [{"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a95ee4184562ea6e68359542a9ba4fac4f67356", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db96825d26a40d3ff36bd9249696f6589a6c12f4", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=09df17da18177e610ef5f2a653e8af6a754ba14d", "width": 320, "height": 211}], "variants": {}, "id": "7wlnlsb1p8kqETD6MDhVPbujRAXC14mk8BJ3S2O8fCg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6ojsg", "is_robot_indexable": true, "report_reasons": null, "author": "batmaniac77", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6ojsg/build_help_choosing_right_cpu_and_ram/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6ojsg/build_help_choosing_right_cpu_and_ram/", "subreddit_subscribers": 656595, "created_utc": 1669615353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been ripping some cds using exact audio copy with accurate rip setup. I tried to submit my results to the database and it says it successful however when I try to rip the CD again it says tracks not in accurate rip database. I tried disabling my firewall and using another computer but results are the same. Is there anything I could to so that my results get uploaded? No issues with the cds as well all ripped accurately according to cue tools.", "author_fullname": "t2_4evp94do", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EAC accurate rip results wont upload to database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6ik0v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669597777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been ripping some cds using exact audio copy with accurate rip setup. I tried to submit my results to the database and it says it successful however when I try to rip the CD again it says tracks not in accurate rip database. I tried disabling my firewall and using another computer but results are the same. Is there anything I could to so that my results get uploaded? No issues with the cds as well all ripped accurately according to cue tools.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6ik0v", "is_robot_indexable": true, "report_reasons": null, "author": "atitann", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6ik0v/eac_accurate_rip_results_wont_upload_to_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6ik0v/eac_accurate_rip_results_wont_upload_to_database/", "subreddit_subscribers": 656595, "created_utc": 1669597777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! I have been backing up games for well over a decade now and have something like 60TB including artwork. I know a RAID isn\u2019t a backup and I had a bit of a scare the other day and I\u2019ve decided to get serious about making a backup. I was looking into tape drives and I never realized how expensive they were, but something needs done. So\u2026 what do you all recommend? I\u2019ve been looking at an IBM 3592-E08\n\nThank you.", "author_fullname": "t2_14mq9m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "60+TB Gaming NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6aglm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669578136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I have been backing up games for well over a decade now and have something like 60TB including artwork. I know a RAID isn\u2019t a backup and I had a bit of a scare the other day and I\u2019ve decided to get serious about making a backup. I was looking into tape drives and I never realized how expensive they were, but something needs done. So\u2026 what do you all recommend? I\u2019ve been looking at an IBM 3592-E08&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6aglm", "is_robot_indexable": true, "report_reasons": null, "author": "Sasquatters", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6aglm/60tb_gaming_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6aglm/60tb_gaming_nas/", "subreddit_subscribers": 656595, "created_utc": 1669578136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If I understand it correctly these contain multiple bdxls, so if I take them out can I write and read them in a regular bdxl compatible drive instead of using the whole cartridge in $10000 readers? It seems cheaper than buying 100 or 128gb bdxls by themselves.", "author_fullname": "t2_50fpwuu0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can discs be removed from from optical disc archival cartridges?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6wm22", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669642158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I understand it correctly these contain multiple bdxls, so if I take them out can I write and read them in a regular bdxl compatible drive instead of using the whole cartridge in $10000 readers? It seems cheaper than buying 100 or 128gb bdxls by themselves.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6wm22", "is_robot_indexable": true, "report_reasons": null, "author": "Voldy256", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6wm22/can_discs_be_removed_from_from_optical_disc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6wm22/can_discs_be_removed_from_from_optical_disc/", "subreddit_subscribers": 656595, "created_utc": 1669642158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi reddit,\n\n&amp;#x200B;\n\nI need to download a specific file type from my dropbox account. In this case only all .mp4-files. Any suggestion how I can do it? \n\n&amp;#x200B;\n\nIt's not possible to use the \"Multi file organize/Custom filters\" to move all the files to a new directory. All directories needs to be kept as they are.", "author_fullname": "t2_jakf6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download specific file types from Dropbox", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6vqqz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669639795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi reddit,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I need to download a specific file type from my dropbox account. In this case only all .mp4-files. Any suggestion how I can do it? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not possible to use the &amp;quot;Multi file organize/Custom filters&amp;quot; to move all the files to a new directory. All directories needs to be kept as they are.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6vqqz", "is_robot_indexable": true, "report_reasons": null, "author": "blowmycool", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6vqqz/download_specific_file_types_from_dropbox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6vqqz/download_specific_file_types_from_dropbox/", "subreddit_subscribers": 656595, "created_utc": 1669639795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I was looking to buy a 2.5'' 5Tb external hard drive, however after some searching, it seems like the WD drives don't have an external SATA connector or something like that, so I was wondering if in the case of a possible disk failure, that should be something I should have in mind (like buying a Seagate disk that has the SATA connector). I am not interested in shucking the hard drive though.\n\nSeagate disks seemed like a good option too, but however from what I've searched, they seemed to have a higher failure rate compared to other brands...\n\nWith all this in mind, what would be your suggestions? Should I care about the SATA connector I'll probably never use (unless it helps recovering disk content, if it fails one day), or should I look into other brands?\n\nThanks in advance!", "author_fullname": "t2_ycqmv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2.5\" 5TB external HDD - WD vs Seagate vs others?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6p8jr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669617607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I was looking to buy a 2.5&amp;#39;&amp;#39; 5Tb external hard drive, however after some searching, it seems like the WD drives don&amp;#39;t have an external SATA connector or something like that, so I was wondering if in the case of a possible disk failure, that should be something I should have in mind (like buying a Seagate disk that has the SATA connector). I am not interested in shucking the hard drive though.&lt;/p&gt;\n\n&lt;p&gt;Seagate disks seemed like a good option too, but however from what I&amp;#39;ve searched, they seemed to have a higher failure rate compared to other brands...&lt;/p&gt;\n\n&lt;p&gt;With all this in mind, what would be your suggestions? Should I care about the SATA connector I&amp;#39;ll probably never use (unless it helps recovering disk content, if it fails one day), or should I look into other brands?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6p8jr", "is_robot_indexable": true, "report_reasons": null, "author": "supercar1x", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6p8jr/25_5tb_external_hdd_wd_vs_seagate_vs_others/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6p8jr/25_5tb_external_hdd_wd_vs_seagate_vs_others/", "subreddit_subscribers": 656595, "created_utc": 1669617607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have no idea if this is the right place to ask, but I\u2019m absolutely lost and I figured y\u2019all might be able help. I\u2019ve been working on a project that involves some image files from around 1997. The only issue is that (almost) all of the files I was given seem to be corrupted and unusable. They don\u2019t have any file extensions, and adding extensions doesn\u2019t work either. The most important images still seem to take up a decent bit of space, so is there any chance something like this could be salvaged? Figured I might as well shoot out this last cry for help, if you think you can help I\u2019d be glad to send over some of the problem files to look at. Thanks!", "author_fullname": "t2_8rjlxo5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Corrupted image files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6kfc5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669602979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have no idea if this is the right place to ask, but I\u2019m absolutely lost and I figured y\u2019all might be able help. I\u2019ve been working on a project that involves some image files from around 1997. The only issue is that (almost) all of the files I was given seem to be corrupted and unusable. They don\u2019t have any file extensions, and adding extensions doesn\u2019t work either. The most important images still seem to take up a decent bit of space, so is there any chance something like this could be salvaged? Figured I might as well shoot out this last cry for help, if you think you can help I\u2019d be glad to send over some of the problem files to look at. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6kfc5", "is_robot_indexable": true, "report_reasons": null, "author": "Pokeballersprime2", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6kfc5/corrupted_image_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6kfc5/corrupted_image_files/", "subreddit_subscribers": 656595, "created_utc": 1669602979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_h3zc4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD 16TB Elements External @ $230, 20tb @$320", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_z6uljs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XQeO52zNtio13p2nbLbQSnEKOTDkng2PtQV7Ij4piiA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669636453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bhphotovideo.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.bhphotovideo.com/c/product/1604996-REG/wd_wdbwlg0160hbk_nesn_wd_elements_desktop_16tb.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?auto=webp&amp;s=d533c09e4e1c69de0bd50bc25ae8da9e7dbafee7", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6754091ac5eb6a439260facfe4e2f85cfc9f9260", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c8cd696a54908ce6e149eb7f17ba706715b5992", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee30d5223ed5c1434c89d0fd0000d85515ced8e0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=08f43d5bc65c353767346d1847f8bd9e62526bf3", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8cec47c342d745aa908a41fe83669752d0c890b0", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=638ff173faccb4ac7c5e3898d30b6c71d30e31d0", "width": 1080, "height": 567}], "variants": {}, "id": "MLLs0tCS3MSYCsXnYaXuEEuXfYT7OImhc-S9BT9FlNI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "z6uljs", "is_robot_indexable": true, "report_reasons": null, "author": "Anzial", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6uljs/wd_16tb_elements_external_230_20tb_320/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.bhphotovideo.com/c/product/1604996-REG/wd_wdbwlg0160hbk_nesn_wd_elements_desktop_16tb.html", "subreddit_subscribers": 656595, "created_utc": 1669636453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Greetings friends,\n\nI'm sitting on a pile of PDFs I'd like to organize and searchable. I've come across paperless, but building a NAS or a raspberry pi just to tag and search some PDFs seems overkill, and with 2 young kids and 2 more on the way, I don't have a huge amount of time to fiddle around with tech.\n\nCurrently, all that stuff is handled in Evernote, but if possible, I'd like to leave behind the world of proprietary systems for that task.\n\nAny advice?", "author_fullname": "t2_1233ak", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PDF library: Solution for OCR and tagging", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6a1nd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669577146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings friends,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sitting on a pile of PDFs I&amp;#39;d like to organize and searchable. I&amp;#39;ve come across paperless, but building a NAS or a raspberry pi just to tag and search some PDFs seems overkill, and with 2 young kids and 2 more on the way, I don&amp;#39;t have a huge amount of time to fiddle around with tech.&lt;/p&gt;\n\n&lt;p&gt;Currently, all that stuff is handled in Evernote, but if possible, I&amp;#39;d like to leave behind the world of proprietary systems for that task.&lt;/p&gt;\n\n&lt;p&gt;Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6a1nd", "is_robot_indexable": true, "report_reasons": null, "author": "Gilgeam", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6a1nd/pdf_library_solution_for_ocr_and_tagging/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6a1nd/pdf_library_solution_for_ocr_and_tagging/", "subreddit_subscribers": 656595, "created_utc": 1669577146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to set up a self hosted paperless instance and digitize all my physical documents.  I'd like to get a scanner than can scan a small stack of papers at a time, on both sides, and deposit them vita FTP/NFS/SMB/SMTP to a directory to be consumed by paperless.  I've heard there is also some detection feature for accidently scanning two pages stuck together, I probably want that as well.  I have no need to run this printer independently of a PC but I don't mind if it does.  Has anyone been through this and have a scanner that they like?  I'm happy to spend whatever budget to buy it for life and hit that price to performance sweet spot.  Better than replacing is down the line when it has problems..", "author_fullname": "t2_zjagd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a document scanner for Paperless-ng", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z65tem", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669567326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to set up a self hosted paperless instance and digitize all my physical documents.  I&amp;#39;d like to get a scanner than can scan a small stack of papers at a time, on both sides, and deposit them vita FTP/NFS/SMB/SMTP to a directory to be consumed by paperless.  I&amp;#39;ve heard there is also some detection feature for accidently scanning two pages stuck together, I probably want that as well.  I have no need to run this printer independently of a PC but I don&amp;#39;t mind if it does.  Has anyone been through this and have a scanner that they like?  I&amp;#39;m happy to spend whatever budget to buy it for life and hit that price to performance sweet spot.  Better than replacing is down the line when it has problems..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z65tem", "is_robot_indexable": true, "report_reasons": null, "author": "jswervedizzle", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z65tem/looking_for_a_document_scanner_for_paperlessng/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z65tem/looking_for_a_document_scanner_for_paperlessng/", "subreddit_subscribers": 656595, "created_utc": 1669567326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "$13.62/TB\n\nAmazon: https://www.amazon.com/dp/B09KMGQG5Y\n\nWestern Digital: https://www.westerndigital.com/products/internal-drives/wd-blue-desktop-sata-hdd#WD80EAZZ\n\nSeems Amazon ships pretty much immediately, WD Store indicates available 3-4 weeks.", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Blue 3.5\" 8TB CMR 5640 RPM HDD $109.99 (USA) Cyber Monday Deal (Amazon and WD Store)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z6z5zl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669648728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;$13.62/TB&lt;/p&gt;\n\n&lt;p&gt;Amazon: &lt;a href=\"https://www.amazon.com/dp/B09KMGQG5Y\"&gt;https://www.amazon.com/dp/B09KMGQG5Y&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Western Digital: &lt;a href=\"https://www.westerndigital.com/products/internal-drives/wd-blue-desktop-sata-hdd#WD80EAZZ\"&gt;https://www.westerndigital.com/products/internal-drives/wd-blue-desktop-sata-hdd#WD80EAZZ&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Seems Amazon ships pretty much immediately, WD Store indicates available 3-4 weeks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z6z5zl", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6z5zl/wd_blue_35_8tb_cmr_5640_rpm_hdd_10999_usa_cyber/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6z5zl/wd_blue_35_8tb_cmr_5640_rpm_hdd_10999_usa_cyber/", "subreddit_subscribers": 656595, "created_utc": 1669648728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I intend to download the entirety of a popular fandom wiki (funkipedia) using WikiTeam's scripts. It's fandom so iirc it's based off MediaWiki should it should be compatible with the scripts. My question is what kind of size will the wiki be if I just do a full text dump + all images.\n\nI know this wiki also hosts music but I probably won't download that. Same goes for all revisions and comments to articles. I'm just going to get the pages and maybe the images. If it's just text I'm estimating it will be at least less than 5gb but with images I don't know. I ask because I'm going to be running this on VPS with limited space so I want to be sure I don't have to restart because I ran out of room. \n\nI'm also curious as to how the script displays the completed wiki download. Does it give you some kind of directory structure and make it easily navigatable like if I downloaded it with HTTrack or Wget (with --convert-links)? I see it downloads the site as XML files; do I need to make something to parse all the XML? \n\nAnyone have any experience using these tools that could help? I've looked through the wiki. It doesn't seem like these guys offer a full site download.", "author_fullname": "t2_d3o3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What kind of sizes should I expect downloading a fandom wiki? And has anyone have experience using WikiTeam's tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6kgua", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669603106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I intend to download the entirety of a popular fandom wiki (funkipedia) using WikiTeam&amp;#39;s scripts. It&amp;#39;s fandom so iirc it&amp;#39;s based off MediaWiki should it should be compatible with the scripts. My question is what kind of size will the wiki be if I just do a full text dump + all images.&lt;/p&gt;\n\n&lt;p&gt;I know this wiki also hosts music but I probably won&amp;#39;t download that. Same goes for all revisions and comments to articles. I&amp;#39;m just going to get the pages and maybe the images. If it&amp;#39;s just text I&amp;#39;m estimating it will be at least less than 5gb but with images I don&amp;#39;t know. I ask because I&amp;#39;m going to be running this on VPS with limited space so I want to be sure I don&amp;#39;t have to restart because I ran out of room. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also curious as to how the script displays the completed wiki download. Does it give you some kind of directory structure and make it easily navigatable like if I downloaded it with HTTrack or Wget (with --convert-links)? I see it downloads the site as XML files; do I need to make something to parse all the XML? &lt;/p&gt;\n\n&lt;p&gt;Anyone have any experience using these tools that could help? I&amp;#39;ve looked through the wiki. It doesn&amp;#39;t seem like these guys offer a full site download.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "LP-Archive", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6kgua", "is_robot_indexable": true, "report_reasons": null, "author": "StormGaza", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6kgua/what_kind_of_sizes_should_i_expect_downloading_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6kgua/what_kind_of_sizes_should_i_expect_downloading_a/", "subreddit_subscribers": 656595, "created_utc": 1669603106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there,\n\nI'm Tom and I am a DataHoarder :)\n\nI'm based in Europe and I've just setup a small cloud storage service.\n\n\nA short story about my initial thoughts on this project:\n\nI've been Hoarding Data for over two decades now, going through all phases (losing data, having multiple copies of them, 3-2-1 rules etc), and I've noticed that it is getting very expensive after a while (well, you can notice that very fast nowadays hah).\n\nAll these started when I wanted to grab a couple of extra TBs of online storage to backup some secondary data. There aren't many affordable (mind you, $5-$7/TB might be cheap for some) options out there and all I wanted was to share a box with someone.\n\nSo I've decided to make a little project to \"fix\" this. Maybe there are more people out there who think the same (you never know until you try?). I've had my ups and downs but this is something I was really excited to code. This is also the first project I created after a long time (so please be kind :)). Hopefully it will make me get back to the game.\n\nI'm basically splitting up the servers, no overselling or anything like that, and you can use (S)FTP, SCP, rsync, Rclone, Duplicati, BorgBackup to create your backups.\n\n\nRequesting Feedback:\n\nI'm offering 100GB Trials for a week for you to play around and provide any kind of feedback so I can improve either the quality of the service or the website itself. I know it requires a signup and email verification but I tried to make it as painless as possible :(\n\nYou can check it out [here](https://www.extralayer.eu/).\n\nI'm already working on the next phase (the first one wasn't anything special, just a basic build :)) which I think it can do wonders to decrease the cost of online storage even further. Hopefully it will be ready sometime in Q1 2023. I will let you know :)\n\nThank you for your time! \n\nTom\n\nPS: I've got approval from the moderators to post this thread.\n\nPS2: I've made a similar post on Hacker News earlier today, just a small FYI.", "author_fullname": "t2_uf5afmca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Cloud Storage Service - Requesting Feedback", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6hu3q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Feedback", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669597382.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669595837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m Tom and I am a DataHoarder :)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m based in Europe and I&amp;#39;ve just setup a small cloud storage service.&lt;/p&gt;\n\n&lt;p&gt;A short story about my initial thoughts on this project:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been Hoarding Data for over two decades now, going through all phases (losing data, having multiple copies of them, 3-2-1 rules etc), and I&amp;#39;ve noticed that it is getting very expensive after a while (well, you can notice that very fast nowadays hah).&lt;/p&gt;\n\n&lt;p&gt;All these started when I wanted to grab a couple of extra TBs of online storage to backup some secondary data. There aren&amp;#39;t many affordable (mind you, $5-$7/TB might be cheap for some) options out there and all I wanted was to share a box with someone.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;ve decided to make a little project to &amp;quot;fix&amp;quot; this. Maybe there are more people out there who think the same (you never know until you try?). I&amp;#39;ve had my ups and downs but this is something I was really excited to code. This is also the first project I created after a long time (so please be kind :)). Hopefully it will make me get back to the game.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m basically splitting up the servers, no overselling or anything like that, and you can use (S)FTP, SCP, rsync, Rclone, Duplicati, BorgBackup to create your backups.&lt;/p&gt;\n\n&lt;p&gt;Requesting Feedback:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m offering 100GB Trials for a week for you to play around and provide any kind of feedback so I can improve either the quality of the service or the website itself. I know it requires a signup and email verification but I tried to make it as painless as possible :(&lt;/p&gt;\n\n&lt;p&gt;You can check it out &lt;a href=\"https://www.extralayer.eu/\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m already working on the next phase (the first one wasn&amp;#39;t anything special, just a basic build :)) which I think it can do wonders to decrease the cost of online storage even further. Hopefully it will be ready sometime in Q1 2023. I will let you know :)&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time! &lt;/p&gt;\n\n&lt;p&gt;Tom&lt;/p&gt;\n\n&lt;p&gt;PS: I&amp;#39;ve got approval from the moderators to post this thread.&lt;/p&gt;\n\n&lt;p&gt;PS2: I&amp;#39;ve made a similar post on Hacker News earlier today, just a small FYI.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z6hu3q", "is_robot_indexable": true, "report_reasons": null, "author": "ExtraLayer_eu", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6hu3q/new_cloud_storage_service_requesting_feedback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6hu3q/new_cloud_storage_service_requesting_feedback/", "subreddit_subscribers": 656595, "created_utc": 1669595837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If I want to download, say, an album from [here](https://archive.org/download/progressive.rock2020/), I have to open each mp3 and download it (I'd rather have lossless files, but I'll take what I can get). Is there a tool that lets me download entire directories from this site? If not, do y'all know where I can get high quality downloads of music for free? Thanks for any help :)", "author_fullname": "t2_13uums", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download entire directories from archive.org?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z6zxed", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669650560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I want to download, say, an album from &lt;a href=\"https://archive.org/download/progressive.rock2020/\"&gt;here&lt;/a&gt;, I have to open each mp3 and download it (I&amp;#39;d rather have lossless files, but I&amp;#39;ll take what I can get). Is there a tool that lets me download entire directories from this site? If not, do y&amp;#39;all know where I can get high quality downloads of music for free? Thanks for any help :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6zxed", "is_robot_indexable": true, "report_reasons": null, "author": "Star_Wolf64", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6zxed/download_entire_directories_from_archiveorg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6zxed/download_entire_directories_from_archiveorg/", "subreddit_subscribers": 656595, "created_utc": 1669650560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I prefer like download whole profile. The extensions from browsers are broken as of now", "author_fullname": "t2_s8evm0k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good software of downloading instagram posts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6xes9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669644205.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I prefer like download whole profile. The extensions from browsers are broken as of now&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z6xes9", "is_robot_indexable": true, "report_reasons": null, "author": "PM_ME_UR_GALLOWB00BS", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6xes9/any_good_software_of_downloading_instagram_posts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6xes9/any_good_software_of_downloading_instagram_posts/", "subreddit_subscribers": 656595, "created_utc": 1669644205.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey folks!\n\nCurrently running 8x10TB raidz2, considering to grab some 14-20TB drives during CyberMonday if I find some good deals. I don't have a set budget, but I prefer to keep it cheap/cost efficient. Primary usage is media storage and streaming. \n\nMy server have up to 18 bays available. What I've come up with so far are:\n\n1) 2x8 raidz2 vdevs for one zpool  - Largest cost and lowest Capacity Efficiency, but highest performance\n\n2) 1x8 to 1x10 raidz2 vdev for one zpool - Smallest cost, best Capacity Efficiency\n\n3) 1x10 to 1x16 raidz3 vdev for one zpool - Medium cost, medium Capacity Efficiency, lowest performance, possibly largest vdev\n\nCan anyone help me decide? =)", "author_fullname": "t2_4qo0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Input wanted on expanding storage: keywords zfs and vdev widths", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6syoz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669630794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks!&lt;/p&gt;\n\n&lt;p&gt;Currently running 8x10TB raidz2, considering to grab some 14-20TB drives during CyberMonday if I find some good deals. I don&amp;#39;t have a set budget, but I prefer to keep it cheap/cost efficient. Primary usage is media storage and streaming. &lt;/p&gt;\n\n&lt;p&gt;My server have up to 18 bays available. What I&amp;#39;ve come up with so far are:&lt;/p&gt;\n\n&lt;p&gt;1) 2x8 raidz2 vdevs for one zpool  - Largest cost and lowest Capacity Efficiency, but highest performance&lt;/p&gt;\n\n&lt;p&gt;2) 1x8 to 1x10 raidz2 vdev for one zpool - Smallest cost, best Capacity Efficiency&lt;/p&gt;\n\n&lt;p&gt;3) 1x10 to 1x16 raidz3 vdev for one zpool - Medium cost, medium Capacity Efficiency, lowest performance, possibly largest vdev&lt;/p&gt;\n\n&lt;p&gt;Can anyone help me decide? =)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "128TB+6TB++ (raw)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6syoz", "is_robot_indexable": true, "report_reasons": null, "author": "Griznah", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6syoz/input_wanted_on_expanding_storage_keywords_zfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6syoz/input_wanted_on_expanding_storage_keywords_zfs/", "subreddit_subscribers": 656595, "created_utc": 1669630794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[This](https://www.whyp.it/tracks/47235/ff4m-your-face-reveal-extremely-harsh-fdom-sph-cei-joi-with-emmafielder?token=QYqTk) for example.", "author_fullname": "t2_fxa6t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have any idea how to download audio off whyp.it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6obz8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.46, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669614677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.whyp.it/tracks/47235/ff4m-your-face-reveal-extremely-harsh-fdom-sph-cei-joi-with-emmafielder?token=QYqTk\"&gt;This&lt;/a&gt; for example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?auto=webp&amp;s=82a0f834b43394e4fee20a2cc41fe00e125258a9", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ab775b4b7793dfa1b93d3386a8ebc60a2b9e080", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=52800673f8193dec4d7a141ef80e90373a30e585", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2070fa9a1c3efd3db1f3ee42c77a10dd9d6eaabf", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=caaed099ba79e0457c7b6bb678fee3d3456de086", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd72a7a67ac1bf061a7ee91eff122226a5959341", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8a07b0fff76863a31cec1f235028a8a4feb38f32", "width": 1080, "height": 564}], "variants": {}, "id": "WhUaEduY2rnlf0g05TXrTe9kWyllkLZGWuyRJaTvepc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6obz8", "is_robot_indexable": true, "report_reasons": null, "author": "onlytoask", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6obz8/anyone_have_any_idea_how_to_download_audio_off/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6obz8/anyone_have_any_idea_how_to_download_audio_off/", "subreddit_subscribers": 656595, "created_utc": 1669614677.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}