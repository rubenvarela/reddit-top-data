{"kind": "Listing", "data": {"after": "t3_z77nit", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ud20v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you all monitor ambient temps for your drives? Cooking drives is no fun... I think I found a decent solution with these $12 Govee bluetooth thermometers and Home Assistant.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_z6yt5j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 178, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 178, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/x3Mb9g8NNWNcr8NdTGW4r5ePEQ7g6Gzfd7G18_NNB8U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669647819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "austinsnerdythings.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://austinsnerdythings.com/2021/12/27/using-the-govee-bluetooth-thermometer-with-home-assistant-python-and-mqtt/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?auto=webp&amp;s=b48a34bccb2954db8eb1e4c65492d55fd53a6469", "width": 797, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a4e406978acf38f4e1bc40cc05bcf511d8a97ea0", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38d7e5b765a309c12b1f74510985e49efeaaad5d", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=52144e94d62313773ce70d6ed7f890458c3e7d29", "width": 320, "height": 321}, {"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da6e2c281454349bd4a62444a47de41ac3cb6365", "width": 640, "height": 642}], "variants": {}, "id": "-Z7KWEbgh-aaJdZinEtRKgixRmRb1ywIvERkB5QovFs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6yt5j", "is_robot_indexable": true, "report_reasons": null, "author": "MzCWzL", "discussion_type": null, "num_comments": 83, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6yt5j/how_do_you_all_monitor_ambient_temps_for_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://austinsnerdythings.com/2021/12/27/using-the-govee-bluetooth-thermometer-with-home-assistant-python-and-mqtt/", "subreddit_subscribers": 656635, "created_utc": 1669647819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_32nu9tpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Elements 12TB: $175 after $40 off promo code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 53, "top_awarded_type": null, "hide_score": false, "name": "t3_z6z4xe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 120, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 120, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EecN7D0NOmxars9JZSQaWp7TArdt0eQiuhqdKq7dSgU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669648651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/qet2t0j2lp2a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?auto=webp&amp;s=c40c3b7a7ad094899748d2ea343232f8cad7833e", "width": 1828, "height": 696}, "resolutions": [{"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=716870203a3f2c715dd5a406f2ae998b612aeae6", "width": 108, "height": 41}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=83bcf71c8b11fdf7eb628a1e7371846e6e7edb76", "width": 216, "height": 82}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d78414140ee2eaee7f515e0dbb9a54a95f3959b", "width": 320, "height": 121}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bcd4582e1b12b84c9816b614091553bd2e8bf24", "width": 640, "height": 243}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=25ac76353e7c9e33dd5b0cb3b21c87b6d227a98a", "width": 960, "height": 365}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=412d2b11079b810832e50aaa9433344ebe0feaaf", "width": 1080, "height": 411}], "variants": {}, "id": "vLd_u51BuwpxsUA6Owsmqe0DJSQ32OjxAvsPEhxGye0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "z6z4xe", "is_robot_indexable": true, "report_reasons": null, "author": "Kosofkors", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6z4xe/wd_elements_12tb_175_after_40_off_promo_code/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/qet2t0j2lp2a1.png", "subreddit_subscribers": 656635, "created_utc": 1669648651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "You download your information from Facebook, and just thousands of files you need to open one by one. Is there a way to make this actually useful and accessible?", "author_fullname": "t2_4rtg6gja", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can you actually use your facebook information downloaded?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6hro4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669595653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You download your information from Facebook, and just thousands of files you need to open one by one. Is there a way to make this actually useful and accessible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z6hro4", "is_robot_indexable": true, "report_reasons": null, "author": "stackshockprism", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6hro4/how_can_you_actually_use_your_facebook/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6hro4/how_can_you_actually_use_your_facebook/", "subreddit_subscribers": 656635, "created_utc": 1669595653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good morning everyone. Like most people studying, I got fucked by the pandemic and became part of the Zoom Academy, and most of the materials we used are digital only. I wanted to know if there's a way to download 3 years' worth of documents, assignments, and such on Google Classroom to my computer. My access to these documents is coming close, so I wanted to store them for future use. Is there anything I can do, or should I download everything manually?\n\nA random guy on Discord said I should come to this sub to search for answers, so sorry if this is not the correct place", "author_fullname": "t2_2g2sxfio", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download of Google Classroom Materials", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6vrk8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669639855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good morning everyone. Like most people studying, I got fucked by the pandemic and became part of the Zoom Academy, and most of the materials we used are digital only. I wanted to know if there&amp;#39;s a way to download 3 years&amp;#39; worth of documents, assignments, and such on Google Classroom to my computer. My access to these documents is coming close, so I wanted to store them for future use. Is there anything I can do, or should I download everything manually?&lt;/p&gt;\n\n&lt;p&gt;A random guy on Discord said I should come to this sub to search for answers, so sorry if this is not the correct place&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6vrk8", "is_robot_indexable": true, "report_reasons": null, "author": "Sanslution", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6vrk8/download_of_google_classroom_materials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6vrk8/download_of_google_classroom_materials/", "subreddit_subscribers": 656635, "created_utc": 1669639855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have seen a website similar to Wayback Machine / archive.org that used to crawl and index HTML code of  every webpage / website. I forgot the website. Does anyone know anything similar?\n\nFor example, we could search for some term like \"&lt;h1&gt;twitter&lt;/h1&gt;\" and it used to return all webpages that has a twitter inside head tag in their html code.\n\nThanks in advance", "author_fullname": "t2_9s1kb9su", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web Archive - HTML Code dump of websites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6w06z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669640499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen a website similar to Wayback Machine / archive.org that used to crawl and index HTML code of  every webpage / website. I forgot the website. Does anyone know anything similar?&lt;/p&gt;\n\n&lt;p&gt;For example, we could search for some term like &amp;quot;&amp;lt;h1&amp;gt;twitter&amp;lt;/h1&amp;gt;&amp;quot; and it used to return all webpages that has a twitter inside head tag in their html code.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6w06z", "is_robot_indexable": true, "report_reasons": null, "author": "karthiksudhan-wild", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6w06z/web_archive_html_code_dump_of_websites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6w06z/web_archive_html_code_dump_of_websites/", "subreddit_subscribers": 656635, "created_utc": 1669640499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Most here have at least 1TB+ my self included. I\u2019m a Mac user so I haven\u2019t found out how to scan a drive or a folder for corrupt files but so far I haven\u2019t encountered any issues. What are your experiences? \n\nAll my drives are stationary so nothings ever been dropped or exposed to water or anything which helps my case for sure.", "author_fullname": "t2_2o8t3n2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How often have you experienced a corrupt file(s)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6e88z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669586931.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most here have at least 1TB+ my self included. I\u2019m a Mac user so I haven\u2019t found out how to scan a drive or a folder for corrupt files but so far I haven\u2019t encountered any issues. What are your experiences? &lt;/p&gt;\n\n&lt;p&gt;All my drives are stationary so nothings ever been dropped or exposed to water or anything which helps my case for sure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6e88z", "is_robot_indexable": true, "report_reasons": null, "author": "QualitySound96", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6e88z/how_often_have_you_experienced_a_corrupt_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6e88z/how_often_have_you_experienced_a_corrupt_files/", "subreddit_subscribers": 656635, "created_utc": 1669586931.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_h3zc4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD 16TB Elements External @ $230, 20tb @$320", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_z6uljs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XQeO52zNtio13p2nbLbQSnEKOTDkng2PtQV7Ij4piiA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669636453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bhphotovideo.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.bhphotovideo.com/c/product/1604996-REG/wd_wdbwlg0160hbk_nesn_wd_elements_desktop_16tb.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?auto=webp&amp;s=d533c09e4e1c69de0bd50bc25ae8da9e7dbafee7", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6754091ac5eb6a439260facfe4e2f85cfc9f9260", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c8cd696a54908ce6e149eb7f17ba706715b5992", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee30d5223ed5c1434c89d0fd0000d85515ced8e0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=08f43d5bc65c353767346d1847f8bd9e62526bf3", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8cec47c342d745aa908a41fe83669752d0c890b0", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=638ff173faccb4ac7c5e3898d30b6c71d30e31d0", "width": 1080, "height": 567}], "variants": {}, "id": "MLLs0tCS3MSYCsXnYaXuEEuXfYT7OImhc-S9BT9FlNI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "z6uljs", "is_robot_indexable": true, "report_reasons": null, "author": "Anzial", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6uljs/wd_16tb_elements_external_230_20tb_320/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.bhphotovideo.com/c/product/1604996-REG/wd_wdbwlg0160hbk_nesn_wd_elements_desktop_16tb.html", "subreddit_subscribers": 656635, "created_utc": 1669636453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If I understand it correctly these contain multiple bdxls, so if I take them out can I write and read them in a regular bdxl compatible drive instead of using the whole cartridge in $10000 readers? It seems cheaper than buying 100 or 128gb bdxls by themselves.", "author_fullname": "t2_50fpwuu0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can discs be removed from from optical disc archival cartridges?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6wm22", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669642158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I understand it correctly these contain multiple bdxls, so if I take them out can I write and read them in a regular bdxl compatible drive instead of using the whole cartridge in $10000 readers? It seems cheaper than buying 100 or 128gb bdxls by themselves.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6wm22", "is_robot_indexable": true, "report_reasons": null, "author": "Voldy256", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6wm22/can_discs_be_removed_from_from_optical_disc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6wm22/can_discs_be_removed_from_from_optical_disc/", "subreddit_subscribers": 656635, "created_utc": 1669642158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "$13.62/TB\n\nAmazon: https://www.amazon.com/dp/B09KMGQG5Y\n\nWestern Digital: https://www.westerndigital.com/products/internal-drives/wd-blue-desktop-sata-hdd#WD80EAZZ\n\nSeems Amazon ships pretty much immediately, WD Store indicates available 3-4 weeks.", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Blue 3.5\" 8TB CMR 5640 RPM HDD $109.99 (USA) Cyber Monday Deal (Amazon and WD Store)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6z5zl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669648728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;$13.62/TB&lt;/p&gt;\n\n&lt;p&gt;Amazon: &lt;a href=\"https://www.amazon.com/dp/B09KMGQG5Y\"&gt;https://www.amazon.com/dp/B09KMGQG5Y&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Western Digital: &lt;a href=\"https://www.westerndigital.com/products/internal-drives/wd-blue-desktop-sata-hdd#WD80EAZZ\"&gt;https://www.westerndigital.com/products/internal-drives/wd-blue-desktop-sata-hdd#WD80EAZZ&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Seems Amazon ships pretty much immediately, WD Store indicates available 3-4 weeks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z6z5zl", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6z5zl/wd_blue_35_8tb_cmr_5640_rpm_hdd_10999_usa_cyber/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6z5zl/wd_blue_35_8tb_cmr_5640_rpm_hdd_10999_usa_cyber/", "subreddit_subscribers": 656635, "created_utc": 1669648728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi reddit,\n\n&amp;#x200B;\n\nI need to download a specific file type from my dropbox account. In this case only all .mp4-files. Any suggestion how I can do it? \n\n&amp;#x200B;\n\nIt's not possible to use the \"Multi file organize/Custom filters\" to move all the files to a new directory. All directories needs to be kept as they are.", "author_fullname": "t2_jakf6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download specific file types from Dropbox", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6vqqz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669639795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi reddit,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I need to download a specific file type from my dropbox account. In this case only all .mp4-files. Any suggestion how I can do it? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not possible to use the &amp;quot;Multi file organize/Custom filters&amp;quot; to move all the files to a new directory. All directories needs to be kept as they are.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6vqqz", "is_robot_indexable": true, "report_reasons": null, "author": "blowmycool", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6vqqz/download_specific_file_types_from_dropbox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6vqqz/download_specific_file_types_from_dropbox/", "subreddit_subscribers": 656635, "created_utc": 1669639795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am planning to build a NAS with unraid which will have 2x14, 2x8 TB,  for now. This NAS will be a standalone and act as NAS without Plex or  any other services. I want to have another system working as the main  server. I currently have few optoins at my disposal and was wondering  what would be the best use of current resources. Some are already built  systems and some need to be built.\n\nExpand-ability: slowly but not sure to what storage.\n\nPower consumption: Preferred low, as it might be idle 70% at least for the first year.\n\n&amp;#x200B;\n\nOption1 (Preferred option):\n\n[i5-2400](https://www.intel.com/content/www/us/en/products/sku/52207/intel-core-i52400-processor-6m-cache-up-to-3-40-ghz/specifications.html), 8GB (not expandable) ram as NAS and[ i5-8500T](https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html), 16GB as the main server\n\nOption2:\n\n[i3-10105F](https://www.intel.com/content/www/us/en/products/sku/203474/intel-core-i310105f-processor-6m-cache-up-to-4-40-ghz/specifications.html), xGB (need to be built) and [i5-8500T](https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html), 16GB - either can be used for either purpose\n\nOption3:\n\n[i7-6700](https://www.intel.com/content/www/us/en/products/sku/88195/intel-core-i76700k-processor-8m-cache-up-to-4-20-ghz/specifications.html), xGB (need to be built) and [i5-8500T](https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html), 16GB - either can be used for either purpose\n\nOption4:\n\nUse either of the above systems to do the whole server in one massive system, which has all the HDD's and the main server.\n\n&amp;#x200B;\n\nComparison of CPU's  \n[https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F](https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F)\n\n&amp;#x200B;\n\nAlso, have a [1060](https://www.evga.com/products/specs/gpu.aspx?pn=e466f076-3e8f-4825-b4b0-f1863e893972), 6GB nvidia if that can be leveraged.\n\nAs mentioned would prefer to use the existing builds but wondering if 2nd gen would be sufficient enough.", "author_fullname": "t2_2lkmjxkk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[BUILD HELP] Choosing right CPU and RAM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6ojsg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669615353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning to build a NAS with unraid which will have 2x14, 2x8 TB,  for now. This NAS will be a standalone and act as NAS without Plex or  any other services. I want to have another system working as the main  server. I currently have few optoins at my disposal and was wondering  what would be the best use of current resources. Some are already built  systems and some need to be built.&lt;/p&gt;\n\n&lt;p&gt;Expand-ability: slowly but not sure to what storage.&lt;/p&gt;\n\n&lt;p&gt;Power consumption: Preferred low, as it might be idle 70% at least for the first year.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Option1 (Preferred option):&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.intel.com/content/www/us/en/products/sku/52207/intel-core-i52400-processor-6m-cache-up-to-3-40-ghz/specifications.html\"&gt;i5-2400&lt;/a&gt;, 8GB (not expandable) ram as NAS and&lt;a href=\"https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html\"&gt; i5-8500T&lt;/a&gt;, 16GB as the main server&lt;/p&gt;\n\n&lt;p&gt;Option2:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.intel.com/content/www/us/en/products/sku/203474/intel-core-i310105f-processor-6m-cache-up-to-4-40-ghz/specifications.html\"&gt;i3-10105F&lt;/a&gt;, xGB (need to be built) and &lt;a href=\"https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html\"&gt;i5-8500T&lt;/a&gt;, 16GB - either can be used for either purpose&lt;/p&gt;\n\n&lt;p&gt;Option3:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.intel.com/content/www/us/en/products/sku/88195/intel-core-i76700k-processor-8m-cache-up-to-4-20-ghz/specifications.html\"&gt;i7-6700&lt;/a&gt;, xGB (need to be built) and &lt;a href=\"https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html\"&gt;i5-8500T&lt;/a&gt;, 16GB - either can be used for either purpose&lt;/p&gt;\n\n&lt;p&gt;Option4:&lt;/p&gt;\n\n&lt;p&gt;Use either of the above systems to do the whole server in one massive system, which has all the HDD&amp;#39;s and the main server.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Comparison of CPU&amp;#39;s&lt;br/&gt;\n&lt;a href=\"https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F\"&gt;https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Also, have a &lt;a href=\"https://www.evga.com/products/specs/gpu.aspx?pn=e466f076-3e8f-4825-b4b0-f1863e893972\"&gt;1060&lt;/a&gt;, 6GB nvidia if that can be leveraged.&lt;/p&gt;\n\n&lt;p&gt;As mentioned would prefer to use the existing builds but wondering if 2nd gen would be sufficient enough.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?auto=webp&amp;s=0402e358a1b5b71d7e7e3840908c20761a156712", "width": 586, "height": 387}, "resolutions": [{"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a95ee4184562ea6e68359542a9ba4fac4f67356", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db96825d26a40d3ff36bd9249696f6589a6c12f4", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=09df17da18177e610ef5f2a653e8af6a754ba14d", "width": 320, "height": 211}], "variants": {}, "id": "7wlnlsb1p8kqETD6MDhVPbujRAXC14mk8BJ3S2O8fCg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6ojsg", "is_robot_indexable": true, "report_reasons": null, "author": "batmaniac77", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6ojsg/build_help_choosing_right_cpu_and_ram/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6ojsg/build_help_choosing_right_cpu_and_ram/", "subreddit_subscribers": 656635, "created_utc": 1669615353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been ripping some cds using exact audio copy with accurate rip setup. I tried to submit my results to the database and it says it successful however when I try to rip the CD again it says tracks not in accurate rip database. I tried disabling my firewall and using another computer but results are the same. Is there anything I could to so that my results get uploaded? No issues with the cds as well all ripped accurately according to cue tools.", "author_fullname": "t2_4evp94do", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EAC accurate rip results wont upload to database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6ik0v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669597777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been ripping some cds using exact audio copy with accurate rip setup. I tried to submit my results to the database and it says it successful however when I try to rip the CD again it says tracks not in accurate rip database. I tried disabling my firewall and using another computer but results are the same. Is there anything I could to so that my results get uploaded? No issues with the cds as well all ripped accurately according to cue tools.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6ik0v", "is_robot_indexable": true, "report_reasons": null, "author": "atitann", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6ik0v/eac_accurate_rip_results_wont_upload_to_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6ik0v/eac_accurate_rip_results_wont_upload_to_database/", "subreddit_subscribers": 656635, "created_utc": 1669597777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I was looking to buy a 2.5'' 5Tb external hard drive, however after some searching, it seems like the WD drives don't have an external SATA connector or something like that, so I was wondering if in the case of a possible disk failure, that should be something I should have in mind (like buying a Seagate disk that has the SATA connector). I am not interested in shucking the hard drive though.\n\nSeagate disks seemed like a good option too, but however from what I've searched, they seemed to have a higher failure rate compared to other brands...\n\nWith all this in mind, what would be your suggestions? Should I care about the SATA connector I'll probably never use (unless it helps recovering disk content, if it fails one day), or should I look into other brands?\n\nThanks in advance!", "author_fullname": "t2_ycqmv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2.5\" 5TB external HDD - WD vs Seagate vs others?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6p8jr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669617607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I was looking to buy a 2.5&amp;#39;&amp;#39; 5Tb external hard drive, however after some searching, it seems like the WD drives don&amp;#39;t have an external SATA connector or something like that, so I was wondering if in the case of a possible disk failure, that should be something I should have in mind (like buying a Seagate disk that has the SATA connector). I am not interested in shucking the hard drive though.&lt;/p&gt;\n\n&lt;p&gt;Seagate disks seemed like a good option too, but however from what I&amp;#39;ve searched, they seemed to have a higher failure rate compared to other brands...&lt;/p&gt;\n\n&lt;p&gt;With all this in mind, what would be your suggestions? Should I care about the SATA connector I&amp;#39;ll probably never use (unless it helps recovering disk content, if it fails one day), or should I look into other brands?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6p8jr", "is_robot_indexable": true, "report_reasons": null, "author": "supercar1x", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6p8jr/25_5tb_external_hdd_wd_vs_seagate_vs_others/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6p8jr/25_5tb_external_hdd_wd_vs_seagate_vs_others/", "subreddit_subscribers": 656635, "created_utc": 1669617607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have no idea if this is the right place to ask, but I\u2019m absolutely lost and I figured y\u2019all might be able help. I\u2019ve been working on a project that involves some image files from around 1997. The only issue is that (almost) all of the files I was given seem to be corrupted and unusable. They don\u2019t have any file extensions, and adding extensions doesn\u2019t work either. The most important images still seem to take up a decent bit of space, so is there any chance something like this could be salvaged? Figured I might as well shoot out this last cry for help, if you think you can help I\u2019d be glad to send over some of the problem files to look at. Thanks!", "author_fullname": "t2_8rjlxo5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Corrupted image files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6kfc5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669602979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have no idea if this is the right place to ask, but I\u2019m absolutely lost and I figured y\u2019all might be able help. I\u2019ve been working on a project that involves some image files from around 1997. The only issue is that (almost) all of the files I was given seem to be corrupted and unusable. They don\u2019t have any file extensions, and adding extensions doesn\u2019t work either. The most important images still seem to take up a decent bit of space, so is there any chance something like this could be salvaged? Figured I might as well shoot out this last cry for help, if you think you can help I\u2019d be glad to send over some of the problem files to look at. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6kfc5", "is_robot_indexable": true, "report_reasons": null, "author": "Pokeballersprime2", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6kfc5/corrupted_image_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6kfc5/corrupted_image_files/", "subreddit_subscribers": 656635, "created_utc": 1669602979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there,\n\nI'm Tom and I am a DataHoarder :)\n\nI'm based in Europe and I've just setup a small cloud storage service.\n\n\nA short story about my initial thoughts on this project:\n\nI've been Hoarding Data for over two decades now, going through all phases (losing data, having multiple copies of them, 3-2-1 rules etc), and I've noticed that it is getting very expensive after a while (well, you can notice that very fast nowadays hah).\n\nAll these started when I wanted to grab a couple of extra TBs of online storage to backup some secondary data. There aren't many affordable (mind you, $5-$7/TB might be cheap for some) options out there and all I wanted was to share a box with someone.\n\nSo I've decided to make a little project to \"fix\" this. Maybe there are more people out there who think the same (you never know until you try?). I've had my ups and downs but this is something I was really excited to code. This is also the first project I created after a long time (so please be kind :)). Hopefully it will make me get back to the game.\n\nI'm basically splitting up the servers, no overselling or anything like that, and you can use (S)FTP, SCP, rsync, Rclone, Duplicati, BorgBackup to create your backups.\n\n\nRequesting Feedback:\n\nI'm offering 100GB Trials for a week for you to play around and provide any kind of feedback so I can improve either the quality of the service or the website itself. I know it requires a signup and email verification but I tried to make it as painless as possible :(\n\nYou can check it out [here](https://www.extralayer.eu/).\n\nI'm already working on the next phase (the first one wasn't anything special, just a basic build :)) which I think it can do wonders to decrease the cost of online storage even further. Hopefully it will be ready sometime in Q1 2023. I will let you know :)\n\nThank you for your time! \n\nTom\n\nPS: I've got approval from the moderators to post this thread.\n\nPS2: I've made a similar post on Hacker News earlier today, just a small FYI.", "author_fullname": "t2_uf5afmca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Cloud Storage Service - Requesting Feedback", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6hu3q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Feedback", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669597382.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669595837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m Tom and I am a DataHoarder :)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m based in Europe and I&amp;#39;ve just setup a small cloud storage service.&lt;/p&gt;\n\n&lt;p&gt;A short story about my initial thoughts on this project:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been Hoarding Data for over two decades now, going through all phases (losing data, having multiple copies of them, 3-2-1 rules etc), and I&amp;#39;ve noticed that it is getting very expensive after a while (well, you can notice that very fast nowadays hah).&lt;/p&gt;\n\n&lt;p&gt;All these started when I wanted to grab a couple of extra TBs of online storage to backup some secondary data. There aren&amp;#39;t many affordable (mind you, $5-$7/TB might be cheap for some) options out there and all I wanted was to share a box with someone.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;ve decided to make a little project to &amp;quot;fix&amp;quot; this. Maybe there are more people out there who think the same (you never know until you try?). I&amp;#39;ve had my ups and downs but this is something I was really excited to code. This is also the first project I created after a long time (so please be kind :)). Hopefully it will make me get back to the game.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m basically splitting up the servers, no overselling or anything like that, and you can use (S)FTP, SCP, rsync, Rclone, Duplicati, BorgBackup to create your backups.&lt;/p&gt;\n\n&lt;p&gt;Requesting Feedback:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m offering 100GB Trials for a week for you to play around and provide any kind of feedback so I can improve either the quality of the service or the website itself. I know it requires a signup and email verification but I tried to make it as painless as possible :(&lt;/p&gt;\n\n&lt;p&gt;You can check it out &lt;a href=\"https://www.extralayer.eu/\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m already working on the next phase (the first one wasn&amp;#39;t anything special, just a basic build :)) which I think it can do wonders to decrease the cost of online storage even further. Hopefully it will be ready sometime in Q1 2023. I will let you know :)&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time! &lt;/p&gt;\n\n&lt;p&gt;Tom&lt;/p&gt;\n\n&lt;p&gt;PS: I&amp;#39;ve got approval from the moderators to post this thread.&lt;/p&gt;\n\n&lt;p&gt;PS2: I&amp;#39;ve made a similar post on Hacker News earlier today, just a small FYI.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z6hu3q", "is_robot_indexable": true, "report_reasons": null, "author": "ExtraLayer_eu", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6hu3q/new_cloud_storage_service_requesting_feedback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6hu3q/new_cloud_storage_service_requesting_feedback/", "subreddit_subscribers": 656635, "created_utc": 1669595837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I found a 20tb wd elements drive for around \u20ac20/tb and was thinking of getting it so I wanna know if these drives work well for hoarding", "author_fullname": "t2_15ts6z21", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are wd elements external hard drives reliable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z7552p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669662235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found a 20tb wd elements drive for around \u20ac20/tb and was thinking of getting it so I wanna know if these drives work well for hoarding&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z7552p", "is_robot_indexable": true, "report_reasons": null, "author": "neonvolta", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z7552p/are_wd_elements_external_hard_drives_reliable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z7552p/are_wd_elements_external_hard_drives_reliable/", "subreddit_subscribers": 656635, "created_utc": 1669662235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "And im running long generic test to check it but i have question in my head does it delete my data on my hdd?\n\nEdit: I said term insead of generic lol ", "author_fullname": "t2_5mmw7qf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have 2tb seagate drive that works weird", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z7380h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669658751.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669658083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;And im running long generic test to check it but i have question in my head does it delete my data on my hdd?&lt;/p&gt;\n\n&lt;p&gt;Edit: I said term insead of generic lol &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "5TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z7380h", "is_robot_indexable": true, "report_reasons": null, "author": "orkinos2", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z7380h/i_have_2tb_seagate_drive_that_works_weird/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z7380h/i_have_2tb_seagate_drive_that_works_weird/", "subreddit_subscribers": 656635, "created_utc": 1669658083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If I want to download, say, an album from [here](https://archive.org/download/progressive.rock2020/), I have to open each mp3 and download it (I'd rather have lossless files, but I'll take what I can get). Is there a tool that lets me download entire directories from this site? If not, do y'all know where I can get high quality downloads of music for free? Thanks for any help :)", "author_fullname": "t2_13uums", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download entire directories from archive.org?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6zxed", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669650560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I want to download, say, an album from &lt;a href=\"https://archive.org/download/progressive.rock2020/\"&gt;here&lt;/a&gt;, I have to open each mp3 and download it (I&amp;#39;d rather have lossless files, but I&amp;#39;ll take what I can get). Is there a tool that lets me download entire directories from this site? If not, do y&amp;#39;all know where I can get high quality downloads of music for free? Thanks for any help :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6zxed", "is_robot_indexable": true, "report_reasons": null, "author": "Star_Wolf64", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6zxed/download_entire_directories_from_archiveorg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6zxed/download_entire_directories_from_archiveorg/", "subreddit_subscribers": 656635, "created_utc": 1669650560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey folks!\n\nCurrently running 8x10TB raidz2, considering to grab some 14-20TB drives during CyberMonday if I find some good deals. I don't have a set budget, but I prefer to keep it cheap/cost efficient. Primary usage is media storage and streaming. \n\nMy server have up to 18 bays available. What I've come up with so far are:\n\n1) 2x8 raidz2 vdevs for one zpool  - Largest cost and lowest Capacity Efficiency, but highest performance\n\n2) 1x8 to 1x10 raidz2 vdev for one zpool - Smallest cost, best Capacity Efficiency\n\n3) 1x10 to 1x16 raidz3 vdev for one zpool - Medium cost, medium Capacity Efficiency, lowest performance, possibly largest vdev\n\nCan anyone help me decide? =)", "author_fullname": "t2_4qo0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Input wanted on expanding storage: keywords zfs and vdev widths", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6syoz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669630794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks!&lt;/p&gt;\n\n&lt;p&gt;Currently running 8x10TB raidz2, considering to grab some 14-20TB drives during CyberMonday if I find some good deals. I don&amp;#39;t have a set budget, but I prefer to keep it cheap/cost efficient. Primary usage is media storage and streaming. &lt;/p&gt;\n\n&lt;p&gt;My server have up to 18 bays available. What I&amp;#39;ve come up with so far are:&lt;/p&gt;\n\n&lt;p&gt;1) 2x8 raidz2 vdevs for one zpool  - Largest cost and lowest Capacity Efficiency, but highest performance&lt;/p&gt;\n\n&lt;p&gt;2) 1x8 to 1x10 raidz2 vdev for one zpool - Smallest cost, best Capacity Efficiency&lt;/p&gt;\n\n&lt;p&gt;3) 1x10 to 1x16 raidz3 vdev for one zpool - Medium cost, medium Capacity Efficiency, lowest performance, possibly largest vdev&lt;/p&gt;\n\n&lt;p&gt;Can anyone help me decide? =)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "128TB+6TB++ (raw)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6syoz", "is_robot_indexable": true, "report_reasons": null, "author": "Griznah", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6syoz/input_wanted_on_expanding_storage_keywords_zfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6syoz/input_wanted_on_expanding_storage_keywords_zfs/", "subreddit_subscribers": 656635, "created_utc": 1669630794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, due to the massive rocket fire from the russian federation and the lack of light for almost half a day, I am trying to find an additional opportunity to have a backup copy and use it\n One of the options is a smartphone, but it does not support ext4/btrfs, only fat32.\n Is there any way to solve this problem?", "author_fullname": "t2_lo8e4f55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am trying to find an additional option to have a backup and use it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6obui", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669614665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, due to the massive rocket fire from the russian federation and the lack of light for almost half a day, I am trying to find an additional opportunity to have a backup copy and use it\n One of the options is a smartphone, but it does not support ext4/btrfs, only fat32.\n Is there any way to solve this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "russian military ship, go to hell", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6obui", "is_robot_indexable": true, "report_reasons": null, "author": "kovach_ua", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6obui/i_am_trying_to_find_an_additional_option_to_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6obui/i_am_trying_to_find_an_additional_option_to_have/", "subreddit_subscribers": 656635, "created_utc": 1669614665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am using Syncovery and it has an option to compress each file it uploads. I am debating pros cons.\n\nMy initial (first time) upload will be 1.5 TB and about 160k files. I was thinking to do ultra compression since it is one time. That way, when I have to download/restore, it is less to download. But the initial backup will take 2-3 times longer with compression than if I do no compression.\n\nI'm hoping for some good insights from the community.\n\n[View Poll](https://www.reddit.com/poll/z6mw4w)", "author_fullname": "t2_5wpob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Poll: Do folks compress their file level backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6mw4w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669610277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using Syncovery and it has an option to compress each file it uploads. I am debating pros cons.&lt;/p&gt;\n\n&lt;p&gt;My initial (first time) upload will be 1.5 TB and about 160k files. I was thinking to do ultra compression since it is one time. That way, when I have to download/restore, it is less to download. But the initial backup will take 2-3 times longer with compression than if I do no compression.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping for some good insights from the community.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/z6mw4w\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6mw4w", "is_robot_indexable": true, "report_reasons": null, "author": "imthenachoman", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1669869477674, "options": [{"text": "Yes, I compress", "id": "20063642"}, {"text": "No, I do not compress", "id": "20063643"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 136, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6mw4w/poll_do_folks_compress_their_file_level_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/DataHoarder/comments/z6mw4w/poll_do_folks_compress_their_file_level_backups/", "subreddit_subscribers": 656635, "created_utc": 1669610277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2000+ .fbx mo-cap animation library for free.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6kppk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_2b9r7ngd", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free Data", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "gamedev", "selftext": "**TLDR**: Over 2,000 free 3d humanoid character animations that can be used commercially. The only thing you can't do with them, is sell them, although you can redistribute them if they're free. **Download link is below the discalimer**, which you should at least read the top part of if you plan to use these.\n\n[Sample](https://reddit.com/link/z6djuc/video/34um1xs99k2a1/player)\n\n# Disclaimer and boring stuff:\n\n**I did not create the animations/assets used in this**. The data used in this project was obtained from [mocap.cs.cmu.edu](https://mocap.cs.cmu.edu).  The database was created with funding from NSF EIA-0196217. I converted it to .fbx \\*~~and .glTF~~ from .bvh files I got from [cgspeed.com](https://cgspeed.com) with the use of Blender. I also re-targeted it to an example CC0 model with the use of a Blender add-on, [Auto-Rig Pro](https://blendermarket.com/products/auto-rig-pro)(unfortunetly not free, but you don't need it to use these). The model was made by [Quaternius](https://quaternius.itch.io/). The animations are free to use/modify/redistribute, so long as you don't just sell them as animations.\n\n**I have no affiliation with any of these mentioned parties and  just because I'm allowed to use these assets and distribute them,  doesn't mean any of them necessarily endorse this use. Having read the  terms of use though, I feel like this is something they were intended for.** \n\nThis library has been converted several times, by several people. You might have seen it around. If you wanted to, you could convert it and distribute a \"competing\" version(so long as you don't charge anything). I've downloaded both Unreal and Unity versions, but neither of which were able to be opened in Blender(.uasset files and the Unity .fbx files weren't compatible with Blender), which is a problem because they're not quite game ready as is, and editing animations in either engine is not ideal. I've found a bunch of dead links to other versions, like a different one that was supposed to be .fbx, but since it was a dead link, it wasn't very helpful. I actually started converting these with Godot in mind because it's still newer and there aren't all the assets available like there are for Unreal/Unity. I also initially tried to convert to both .fbx and .glTF because .glTF is a little better for Godot. The .glTF file was somehow including small pieces of data left over from previous conversions no matter how much I tried cleaning up Blender between them. Basically, each conversion would be slightly larger than the previous. It was pretty small, but that does add up when you're iterating over 2,000 files. I improved the conversion script a bunch over the process of converting the library, so if for some reason people needed .glTF versions, I can actually efficiently convert to it now. All 3 of those engines take .fbx and even if you're using other 3D software than Blender, .fbx originates from AutoDesk, so it's perfectly compatible with Maya/3DMax(Or should be, those programs are too rich for my blood so I didn't test it).\n\n# Link:\n\n[https://rancidmilk.itch.io/free-character-animations](https://rancidmilk.itch.io/free-character-animations)\n\nI wasn't sure of the best way to distribute these. I chose [itch.io](https://itch.io) because it's intended for games/game assets and let's you upload a gb before you have to start bugging them for more space. I completely turned off any payment methods. If you wanted to thank me in any way, you could help me improve the animations for everyone to use. It would be nice to cobble together a game ready pack for people to use that's just plug and play and free.\n\n# More info:\n\nThere's a lot more info on the itch page. If you have questions, I'm happy to answer them, please look there before asking though.\n\n# Bonus Content:\n\nI've included my conversion script and added a control rig(which I double checked I'm also allowed to distribute) for easy animation editing. \n\n# Summary:\n\nSo, while I have a lot of effort/time into converting this library, I literally only made the included Blender script and assembled everything.", "author_fullname": "t2_rylxysdt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I converted a massive library of mo-cap animations to .fbx which you can use freely with ALMOST no restrictions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/gamedev", "hidden": false, "pwls": 6, "link_flair_css_class": "assets cat-event", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"34um1xs99k2a1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/z6djuc/asset/34um1xs99k2a1/DASHPlaylist.mpd?a=1672263552%2CNzYxYjJiNmJmNmVhMTFhN2JkNDhmNjRlMWQ0NGQ4NzlmODU5NTYwNTZmZjBjNzhjN2JmYjU2ODk0YTdlZmU0NQ%3D%3D&amp;v=1&amp;f=sd", "x": 1280, "y": 720, "hlsUrl": "https://v.redd.it/link/z6djuc/asset/34um1xs99k2a1/HLSPlaylist.m3u8?a=1672263552%2CZTY4ZjAwYzU1Nzk0NzQ0NTQxNDEzOTc2YzQzYjhlODAyMGEyMmViNjc0NGYzODNmM2Q4ZjUxYzZhMjUwMWJjMQ%3D%3D&amp;v=1&amp;f=sd", "id": "34um1xs99k2a1", "isGif": false}}, "name": "t3_z6djuc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1459, "total_awards_received": 3, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Assets", "can_mod_post": false, "score": 1459, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669585404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.gamedev", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: Over 2,000 free 3d humanoid character animations that can be used commercially. The only thing you can&amp;#39;t do with them, is sell them, although you can redistribute them if they&amp;#39;re free. &lt;strong&gt;Download link is below the discalimer&lt;/strong&gt;, which you should at least read the top part of if you plan to use these.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/z6djuc/video/34um1xs99k2a1/player\"&gt;Sample&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Disclaimer and boring stuff:&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;I did not create the animations/assets used in this&lt;/strong&gt;. The data used in this project was obtained from &lt;a href=\"https://mocap.cs.cmu.edu\"&gt;mocap.cs.cmu.edu&lt;/a&gt;.  The database was created with funding from NSF EIA-0196217. I converted it to .fbx *&lt;del&gt;and .glTF&lt;/del&gt; from .bvh files I got from &lt;a href=\"https://cgspeed.com\"&gt;cgspeed.com&lt;/a&gt; with the use of Blender. I also re-targeted it to an example CC0 model with the use of a Blender add-on, &lt;a href=\"https://blendermarket.com/products/auto-rig-pro\"&gt;Auto-Rig Pro&lt;/a&gt;(unfortunetly not free, but you don&amp;#39;t need it to use these). The model was made by &lt;a href=\"https://quaternius.itch.io/\"&gt;Quaternius&lt;/a&gt;. The animations are free to use/modify/redistribute, so long as you don&amp;#39;t just sell them as animations.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I have no affiliation with any of these mentioned parties and  just because I&amp;#39;m allowed to use these assets and distribute them,  doesn&amp;#39;t mean any of them necessarily endorse this use. Having read the  terms of use though, I feel like this is something they were intended for.&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;This library has been converted several times, by several people. You might have seen it around. If you wanted to, you could convert it and distribute a &amp;quot;competing&amp;quot; version(so long as you don&amp;#39;t charge anything). I&amp;#39;ve downloaded both Unreal and Unity versions, but neither of which were able to be opened in Blender(.uasset files and the Unity .fbx files weren&amp;#39;t compatible with Blender), which is a problem because they&amp;#39;re not quite game ready as is, and editing animations in either engine is not ideal. I&amp;#39;ve found a bunch of dead links to other versions, like a different one that was supposed to be .fbx, but since it was a dead link, it wasn&amp;#39;t very helpful. I actually started converting these with Godot in mind because it&amp;#39;s still newer and there aren&amp;#39;t all the assets available like there are for Unreal/Unity. I also initially tried to convert to both .fbx and .glTF because .glTF is a little better for Godot. The .glTF file was somehow including small pieces of data left over from previous conversions no matter how much I tried cleaning up Blender between them. Basically, each conversion would be slightly larger than the previous. It was pretty small, but that does add up when you&amp;#39;re iterating over 2,000 files. I improved the conversion script a bunch over the process of converting the library, so if for some reason people needed .glTF versions, I can actually efficiently convert to it now. All 3 of those engines take .fbx and even if you&amp;#39;re using other 3D software than Blender, .fbx originates from AutoDesk, so it&amp;#39;s perfectly compatible with Maya/3DMax(Or should be, those programs are too rich for my blood so I didn&amp;#39;t test it).&lt;/p&gt;\n\n&lt;h1&gt;Link:&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://rancidmilk.itch.io/free-character-animations\"&gt;https://rancidmilk.itch.io/free-character-animations&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I wasn&amp;#39;t sure of the best way to distribute these. I chose &lt;a href=\"https://itch.io\"&gt;itch.io&lt;/a&gt; because it&amp;#39;s intended for games/game assets and let&amp;#39;s you upload a gb before you have to start bugging them for more space. I completely turned off any payment methods. If you wanted to thank me in any way, you could help me improve the animations for everyone to use. It would be nice to cobble together a game ready pack for people to use that&amp;#39;s just plug and play and free.&lt;/p&gt;\n\n&lt;h1&gt;More info:&lt;/h1&gt;\n\n&lt;p&gt;There&amp;#39;s a lot more info on the itch page. If you have questions, I&amp;#39;m happy to answer them, please look there before asking though.&lt;/p&gt;\n\n&lt;h1&gt;Bonus Content:&lt;/h1&gt;\n\n&lt;p&gt;I&amp;#39;ve included my conversion script and added a control rig(which I double checked I&amp;#39;m also allowed to distribute) for easy animation editing. &lt;/p&gt;\n\n&lt;h1&gt;Summary:&lt;/h1&gt;\n\n&lt;p&gt;So, while I have a lot of effort/time into converting this library, I literally only made the included Blender script and assembled everything.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 150, "id": "award_f44611f1-b89e-46dc-97fe-892280b13b82", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Thank you stranger. Shows the award.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 2, "static_icon_height": 2048, "name": "Helpful", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png"}, {"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 125, "id": "award_5f123e3d-4f48-42f4-9c11-e98b566d5897", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "When you come across a feel-good thing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Wholesome", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "7272f776-ba6a-11e5-a76d-0eb130e0b479", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qi0a", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z6djuc", "is_robot_indexable": true, "report_reasons": null, "author": "RancidMilkGames", "discussion_type": null, "num_comments": 67, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/gamedev/comments/z6djuc/i_converted_a_massive_library_of_mocap_animations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/gamedev/comments/z6djuc/i_converted_a_massive_library_of_mocap_animations/", "subreddit_subscribers": 927702, "created_utc": 1669585404.0, "num_crossposts": 3, "media": null, "is_video": false}], "created": 1669603832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.gamedev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/gamedev/comments/z6djuc/i_converted_a_massive_library_of_mocap_animations/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z6kppk", "is_robot_indexable": true, "report_reasons": null, "author": "RustedBlade7", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_z6djuc", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6kppk/2000_fbx_mocap_animation_library_for_free/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/gamedev/comments/z6djuc/i_converted_a_massive_library_of_mocap_animations/", "subreddit_subscribers": 656635, "created_utc": 1669603832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I intend to download the entirety of a popular fandom wiki (funkipedia) using WikiTeam's scripts. It's fandom so iirc it's based off MediaWiki should it should be compatible with the scripts. My question is what kind of size will the wiki be if I just do a full text dump + all images.\n\nI know this wiki also hosts music but I probably won't download that. Same goes for all revisions and comments to articles. I'm just going to get the pages and maybe the images. If it's just text I'm estimating it will be at least less than 5gb but with images I don't know. I ask because I'm going to be running this on VPS with limited space so I want to be sure I don't have to restart because I ran out of room. \n\nI'm also curious as to how the script displays the completed wiki download. Does it give you some kind of directory structure and make it easily navigatable like if I downloaded it with HTTrack or Wget (with --convert-links)? I see it downloads the site as XML files; do I need to make something to parse all the XML? \n\nAnyone have any experience using these tools that could help? I've looked through the wiki. It doesn't seem like these guys offer a full site download.", "author_fullname": "t2_d3o3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What kind of sizes should I expect downloading a fandom wiki? And has anyone have experience using WikiTeam's tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6kgua", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669603106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I intend to download the entirety of a popular fandom wiki (funkipedia) using WikiTeam&amp;#39;s scripts. It&amp;#39;s fandom so iirc it&amp;#39;s based off MediaWiki should it should be compatible with the scripts. My question is what kind of size will the wiki be if I just do a full text dump + all images.&lt;/p&gt;\n\n&lt;p&gt;I know this wiki also hosts music but I probably won&amp;#39;t download that. Same goes for all revisions and comments to articles. I&amp;#39;m just going to get the pages and maybe the images. If it&amp;#39;s just text I&amp;#39;m estimating it will be at least less than 5gb but with images I don&amp;#39;t know. I ask because I&amp;#39;m going to be running this on VPS with limited space so I want to be sure I don&amp;#39;t have to restart because I ran out of room. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also curious as to how the script displays the completed wiki download. Does it give you some kind of directory structure and make it easily navigatable like if I downloaded it with HTTrack or Wget (with --convert-links)? I see it downloads the site as XML files; do I need to make something to parse all the XML? &lt;/p&gt;\n\n&lt;p&gt;Anyone have any experience using these tools that could help? I&amp;#39;ve looked through the wiki. It doesn&amp;#39;t seem like these guys offer a full site download.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "LP-Archive", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6kgua", "is_robot_indexable": true, "report_reasons": null, "author": "StormGaza", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6kgua/what_kind_of_sizes_should_i_expect_downloading_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6kgua/what_kind_of_sizes_should_i_expect_downloading_a/", "subreddit_subscribers": 656635, "created_utc": 1669603106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,  \n \n\nI updated my computer to Fedora 37 and installed FreeFileSync v11.28 from FlatHub.  \n \n\nI've been using FFS for many years for backing up my files to a NAS Share.  \nI do not keep the share permanently mounted in Fedora, I only mount it when I want to do a backup. I'd like to keep this practice.  \n \n\nIn the past, I would first mount the share in Nautilus file manager,  then open FFS and it would appear as a destination. Now, even if I have  mounted the share in Nautilus, the share does not appear in FFS at all.  \n \n\nThe NAS has the IP address 10.10.10.6 and the share is called \"backup\",  so I assume I could enter any of the following and one would work:  \n \n\n    /run/user/1000/gvfs/smb-share:server=10.10.10.6,share=backup\n \n\nor  \n \n\n    //10.10.10.6/backup\n \n\nor  \n \n\n    smb://10.10.10.6/backup\n \n\nbut none of these works.  \n \n\nHas something changed?  \n Is there a way to manually add the share's full path to FFS?", "author_fullname": "t2_tcs76", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FreeFileSync cannot see mounted NAS share in Fedora 37", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z77tj5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669668061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,  &lt;/p&gt;\n\n&lt;p&gt;I updated my computer to Fedora 37 and installed FreeFileSync v11.28 from FlatHub.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using FFS for many years for backing up my files to a NAS Share.&lt;br/&gt;\nI do not keep the share permanently mounted in Fedora, I only mount it when I want to do a backup. I&amp;#39;d like to keep this practice.  &lt;/p&gt;\n\n&lt;p&gt;In the past, I would first mount the share in Nautilus file manager,  then open FFS and it would appear as a destination. Now, even if I have  mounted the share in Nautilus, the share does not appear in FFS at all.  &lt;/p&gt;\n\n&lt;p&gt;The NAS has the IP address 10.10.10.6 and the share is called &amp;quot;backup&amp;quot;,  so I assume I could enter any of the following and one would work:  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;/run/user/1000/gvfs/smb-share:server=10.10.10.6,share=backup\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;or  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;//10.10.10.6/backup\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;or  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;smb://10.10.10.6/backup\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;but none of these works.  &lt;/p&gt;\n\n&lt;p&gt;Has something changed?&lt;br/&gt;\n Is there a way to manually add the share&amp;#39;s full path to FFS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z77tj5", "is_robot_indexable": true, "report_reasons": null, "author": "Idiots-R-Invincible", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z77tj5/freefilesync_cannot_see_mounted_nas_share_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z77tj5/freefilesync_cannot_see_mounted_nas_share_in/", "subreddit_subscribers": 656635, "created_utc": 1669668061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey there! \n\nI have an older external 4tb HDD. Listed as the LaCie 4TB F.A. Porsche Designed Desktop Hard Drive.\n\nI want to install a 14 or 16 TB internal HDD (Western Digital, or Seagate) but when I do, my Macs both state that the HDD is unrecognized and offers me option to format the HDD. I am using the USB connection. I think the controller on the external HDD enclosure does not recognize HDDs of such high capacities. The reason I want do this is for aesthetics. I like the look of the enclosures.\n\nIs it possible to fix this problem? \n\nThanks for any help.", "author_fullname": "t2_n08tdizp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to update firmware on an external HDD enclosure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z77nit", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669667701.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there! &lt;/p&gt;\n\n&lt;p&gt;I have an older external 4tb HDD. Listed as the LaCie 4TB F.A. Porsche Designed Desktop Hard Drive.&lt;/p&gt;\n\n&lt;p&gt;I want to install a 14 or 16 TB internal HDD (Western Digital, or Seagate) but when I do, my Macs both state that the HDD is unrecognized and offers me option to format the HDD. I am using the USB connection. I think the controller on the external HDD enclosure does not recognize HDDs of such high capacities. The reason I want do this is for aesthetics. I like the look of the enclosures.&lt;/p&gt;\n\n&lt;p&gt;Is it possible to fix this problem? &lt;/p&gt;\n\n&lt;p&gt;Thanks for any help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z77nit", "is_robot_indexable": true, "report_reasons": null, "author": "Mister_Splendid", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z77nit/is_it_possible_to_update_firmware_on_an_external/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z77nit/is_it_possible_to_update_firmware_on_an_external/", "subreddit_subscribers": 656635, "created_utc": 1669667701.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}