{"kind": "Listing", "data": {"after": "t3_z6obui", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_gsacrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unsuccessful CD-backup with 10mb loss. I wonder why\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_z64gd0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 947, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 947, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_CON2lLLw0s8T5D3bxq7cqazpOKZhqqx1gf7_YYcQYo.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669564095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/s4zogvi83k2a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/s4zogvi83k2a1.jpg?auto=webp&amp;s=edbc4691e6896c87fcd34484fe6896ee4e45650d", "width": 3024, "height": 3024}, "resolutions": [{"url": "https://preview.redd.it/s4zogvi83k2a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d4ed397ad1dcc5d91f6c3e70affad3f239165628", "width": 108, "height": 108}, {"url": "https://preview.redd.it/s4zogvi83k2a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=57e1837b619c4da987f9fa3d2887f7995f0deec3", "width": 216, "height": 216}, {"url": "https://preview.redd.it/s4zogvi83k2a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f98933c1b181c164ee80d30eb2152b041077c90", "width": 320, "height": 320}, {"url": "https://preview.redd.it/s4zogvi83k2a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=256ea7add3805389ea97ac85f3ee2825e08fecbd", "width": 640, "height": 640}, {"url": "https://preview.redd.it/s4zogvi83k2a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b3cff8c089c13fba3a058d5e72cbe081c697b93c", "width": 960, "height": 960}, {"url": "https://preview.redd.it/s4zogvi83k2a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fe95df3f98adfa1831a5ea09fc7b964b87d39765", "width": 1080, "height": 1080}], "variants": {}, "id": "cbOQEqKMBU6GQ73doidW97WM4t5AAcHovRhUhY4jpYk"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "15TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z64gd0", "is_robot_indexable": true, "report_reasons": null, "author": "alkoka", "discussion_type": null, "num_comments": 130, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z64gd0/unsuccessful_cdbackup_with_10mb_loss_i_wonder_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/s4zogvi83k2a1.jpg", "subreddit_subscribers": 656587, "created_utc": 1669564095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8po7w9n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "In search of 2 old games, Military Madness (cellphones, ~2006) and Military Madness 2 / Neo Nectaris (iPhone, ~2010). Already tried every relevant site, so have to ask do anybody still have either of them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ron3kvc1pn2a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 72, "x": 108, "u": "https://preview.redd.it/ron3kvc1pn2a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e8f428d17d3152f9afb39e0edce00361c62af09"}, {"y": 144, "x": 216, "u": "https://preview.redd.it/ron3kvc1pn2a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e161b850d09f92ac8587b131d349b8dd0625fb34"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/ron3kvc1pn2a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce9b96417da35644cb78dac5009290a145299841"}], "s": {"y": 320, "x": 480, "u": "https://preview.redd.it/ron3kvc1pn2a1.jpg?width=480&amp;format=pjpg&amp;auto=webp&amp;s=c79c52b0c680f53fe2e7fbc74fae2619c68659e8"}, "id": "ron3kvc1pn2a1"}, "wkoffgj1pn2a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 72, "x": 108, "u": "https://preview.redd.it/wkoffgj1pn2a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c3c8c36a431f7b5de0a497339215d631b20800a4"}, {"y": 144, "x": 216, "u": "https://preview.redd.it/wkoffgj1pn2a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f001963c396ac5631735e28a0bdccc81aa27179"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/wkoffgj1pn2a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a2ca94bca494f18ce46c243136a2096cfc878c3"}], "s": {"y": 320, "x": 480, "u": "https://preview.redd.it/wkoffgj1pn2a1.jpg?width=480&amp;format=pjpg&amp;auto=webp&amp;s=008e48f8876a2bf5049561f2ab34d35ff7e4988b"}, "id": "wkoffgj1pn2a1"}}, "name": "t3_z6rjug", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 68, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "map example of iPhone Neo Nectaris gameplay ", "media_id": "ron3kvc1pn2a1", "id": 213692469}, {"caption": "battle example of iPhone Neo Nectaris gameplay", "media_id": "wkoffgj1pn2a1", "id": 213692470}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 68, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ca7drkiqqxa9R5E0sx0hSdMJbApZCQcNGzoQR7Fmqb4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669625748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/z6rjug", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6rjug", "is_robot_indexable": true, "report_reasons": null, "author": "TJ_six", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6rjug/in_search_of_2_old_games_military_madness/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/z6rjug", "subreddit_subscribers": 656587, "created_utc": 1669625748.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m going to be using UnRAID. I bought a couple 18tb exos x18 manufacturer recertified drives (1 for data and 1 for parity). Should I test them? If so, how, which tests, and how long will it take? I\u2019m gonna be eager to use them when I get em haha. Thank you!", "author_fullname": "t2_dahbpb7p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I have to run any tests on my hard drives before I use them? If so, which tests?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z64fxo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669564066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m going to be using UnRAID. I bought a couple 18tb exos x18 manufacturer recertified drives (1 for data and 1 for parity). Should I test them? If so, how, which tests, and how long will it take? I\u2019m gonna be eager to use them when I get em haha. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z64fxo", "is_robot_indexable": true, "report_reasons": null, "author": "v-a-g", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z64fxo/do_i_have_to_run_any_tests_on_my_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z64fxo/do_i_have_to_run_any_tests_on_my_hard_drives/", "subreddit_subscribers": 656587, "created_utc": 1669564066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[This article](https://www.servethehome.com/wd-red-pro-20tb-launched-with-wickedly-weak-workload-rating/) on ServeTheHome got me thinking... in a RAID array where periodic \"scrubs\" or consistency checks are recommended to prevent bit rot, the entire disk has to be read. \n\nI did some quick math for my own array which uses HGST He10 10TB drives with 550TB/yr rated workload and the default MegaRAID settings of weekly consistency check AND patrol read, resulting in\n\n10TB * 2 * 52 = 1040TB/yr read\n\nWhile doing both of them monthly instead is\n\n10TB * 2 * 12 = 240TB/yr read, leaving 310TB/yr for actually using the drives\n\nI searched to see if anyone else had posted here or on other forums about this concern and couldn't find anything. Should I reduce the frequency to monthly to avoid 'wearing out' the drives, or is a weekly check needed to prevent bit rot on high capacity drives?\n\nEDIT: thanks for all the data points confirming monthly check is okay! I was going by some old threads like [this one](https://community.spiceworks.com/topic/1648419-lsi-megaraid-patrol-read-and-consistency-check-schedule-recommendations) which scared me into thinking I needed to keep the weekly default. I've switched mine to monthly now.", "author_fullname": "t2_10gstl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running weekly consistency checks on 10TB+ drives exceeds the rated annual workload", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z68j76", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669583861.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669573609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.servethehome.com/wd-red-pro-20tb-launched-with-wickedly-weak-workload-rating/\"&gt;This article&lt;/a&gt; on ServeTheHome got me thinking... in a RAID array where periodic &amp;quot;scrubs&amp;quot; or consistency checks are recommended to prevent bit rot, the entire disk has to be read. &lt;/p&gt;\n\n&lt;p&gt;I did some quick math for my own array which uses HGST He10 10TB drives with 550TB/yr rated workload and the default MegaRAID settings of weekly consistency check AND patrol read, resulting in&lt;/p&gt;\n\n&lt;p&gt;10TB * 2 * 52 = 1040TB/yr read&lt;/p&gt;\n\n&lt;p&gt;While doing both of them monthly instead is&lt;/p&gt;\n\n&lt;p&gt;10TB * 2 * 12 = 240TB/yr read, leaving 310TB/yr for actually using the drives&lt;/p&gt;\n\n&lt;p&gt;I searched to see if anyone else had posted here or on other forums about this concern and couldn&amp;#39;t find anything. Should I reduce the frequency to monthly to avoid &amp;#39;wearing out&amp;#39; the drives, or is a weekly check needed to prevent bit rot on high capacity drives?&lt;/p&gt;\n\n&lt;p&gt;EDIT: thanks for all the data points confirming monthly check is okay! I was going by some old threads like &lt;a href=\"https://community.spiceworks.com/topic/1648419-lsi-megaraid-patrol-read-and-consistency-check-schedule-recommendations\"&gt;this one&lt;/a&gt; which scared me into thinking I needed to keep the weekly default. I&amp;#39;ve switched mine to monthly now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BUZXwSx4FYKZmj-VU6fkiJFn3ZbX_w4URhT0fUwD-mg.jpg?auto=webp&amp;s=cb351bd56b93ca009b377beba589db481b6848f8", "width": 800, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/BUZXwSx4FYKZmj-VU6fkiJFn3ZbX_w4URhT0fUwD-mg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=35cecc1f6cc5bf3533dd0bb979ed4aaf207b5921", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/BUZXwSx4FYKZmj-VU6fkiJFn3ZbX_w4URhT0fUwD-mg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=19630e261c1b90ff636bb0adcb95f3573d512bfb", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/BUZXwSx4FYKZmj-VU6fkiJFn3ZbX_w4URhT0fUwD-mg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d302121db869389decf9fd03a6cc3e66db8d9c6c", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/BUZXwSx4FYKZmj-VU6fkiJFn3ZbX_w4URhT0fUwD-mg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd1a46d03245e6f7771d376e4f1768bd0befa817", "width": 640, "height": 480}], "variants": {}, "id": "Db_RhGlIp4EmXb0BNwixBz21lbTamvNQ-ZavgIbxi0E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z68j76", "is_robot_indexable": true, "report_reasons": null, "author": "denpa_", "discussion_type": null, "num_comments": 17, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z68j76/running_weekly_consistency_checks_on_10tb_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z68j76/running_weekly_consistency_checks_on_10tb_drives/", "subreddit_subscribers": 656587, "created_utc": 1669573609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "You download your information from Facebook, and just thousands of files you need to open one by one. Is there a way to make this actually useful and accessible?", "author_fullname": "t2_4rtg6gja", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can you actually use your facebook information downloaded?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6hro4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669595653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You download your information from Facebook, and just thousands of files you need to open one by one. Is there a way to make this actually useful and accessible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z6hro4", "is_robot_indexable": true, "report_reasons": null, "author": "stackshockprism", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6hro4/how_can_you_actually_use_your_facebook/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6hro4/how_can_you_actually_use_your_facebook/", "subreddit_subscribers": 656587, "created_utc": 1669595653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have seen a website similar to Wayback Machine / archive.org that used to crawl and index HTML code of  every webpage / website. I forgot the website. Does anyone know anything similar?\n\nFor example, we could search for some term like \"&lt;h1&gt;twitter&lt;/h1&gt;\" and it used to return all webpages that has a twitter inside head tag in their html code.\n\nThanks in advance", "author_fullname": "t2_9s1kb9su", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web Archive - HTML Code dump of websites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6w06z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669640499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen a website similar to Wayback Machine / archive.org that used to crawl and index HTML code of  every webpage / website. I forgot the website. Does anyone know anything similar?&lt;/p&gt;\n\n&lt;p&gt;For example, we could search for some term like &amp;quot;&amp;lt;h1&amp;gt;twitter&amp;lt;/h1&amp;gt;&amp;quot; and it used to return all webpages that has a twitter inside head tag in their html code.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6w06z", "is_robot_indexable": true, "report_reasons": null, "author": "karthiksudhan-wild", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6w06z/web_archive_html_code_dump_of_websites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6w06z/web_archive_html_code_dump_of_websites/", "subreddit_subscribers": 656587, "created_utc": 1669640499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good morning everyone. Like most people studying, I got fucked by the pandemic and became part of the Zoom Academy, and most of the materials we used are digital only. I wanted to know if there's a way to download 3 years' worth of documents, assignments, and such on Google Classroom to my computer. My access to these documents is coming close, so I wanted to store them for future use. Is there anything I can do, or should I download everything manually?\n\nA random guy on Discord said I should come to this sub to search for answers, so sorry if this is not the correct place", "author_fullname": "t2_2g2sxfio", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download of Google Classroom Materials", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6vrk8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669639855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good morning everyone. Like most people studying, I got fucked by the pandemic and became part of the Zoom Academy, and most of the materials we used are digital only. I wanted to know if there&amp;#39;s a way to download 3 years&amp;#39; worth of documents, assignments, and such on Google Classroom to my computer. My access to these documents is coming close, so I wanted to store them for future use. Is there anything I can do, or should I download everything manually?&lt;/p&gt;\n\n&lt;p&gt;A random guy on Discord said I should come to this sub to search for answers, so sorry if this is not the correct place&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6vrk8", "is_robot_indexable": true, "report_reasons": null, "author": "Sanslution", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6vrk8/download_of_google_classroom_materials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6vrk8/download_of_google_classroom_materials/", "subreddit_subscribers": 656587, "created_utc": 1669639855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Most here have at least 1TB+ my self included. I\u2019m a Mac user so I haven\u2019t found out how to scan a drive or a folder for corrupt files but so far I haven\u2019t encountered any issues. What are your experiences? \n\nAll my drives are stationary so nothings ever been dropped or exposed to water or anything which helps my case for sure.", "author_fullname": "t2_2o8t3n2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How often have you experienced a corrupt file(s)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6e88z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669586931.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most here have at least 1TB+ my self included. I\u2019m a Mac user so I haven\u2019t found out how to scan a drive or a folder for corrupt files but so far I haven\u2019t encountered any issues. What are your experiences? &lt;/p&gt;\n\n&lt;p&gt;All my drives are stationary so nothings ever been dropped or exposed to water or anything which helps my case for sure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6e88z", "is_robot_indexable": true, "report_reasons": null, "author": "QualitySound96", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6e88z/how_often_have_you_experienced_a_corrupt_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6e88z/how_often_have_you_experienced_a_corrupt_files/", "subreddit_subscribers": 656587, "created_utc": 1669586931.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am planning to build a NAS with unraid which will have 2x14, 2x8 TB,  for now. This NAS will be a standalone and act as NAS without Plex or  any other services. I want to have another system working as the main  server. I currently have few optoins at my disposal and was wondering  what would be the best use of current resources. Some are already built  systems and some need to be built.\n\nExpand-ability: slowly but not sure to what storage.\n\nPower consumption: Preferred low, as it might be idle 70% at least for the first year.\n\n&amp;#x200B;\n\nOption1 (Preferred option):\n\n[i5-2400](https://www.intel.com/content/www/us/en/products/sku/52207/intel-core-i52400-processor-6m-cache-up-to-3-40-ghz/specifications.html), 8GB (not expandable) ram as NAS and[ i5-8500T](https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html), 16GB as the main server\n\nOption2:\n\n[i3-10105F](https://www.intel.com/content/www/us/en/products/sku/203474/intel-core-i310105f-processor-6m-cache-up-to-4-40-ghz/specifications.html), xGB (need to be built) and [i5-8500T](https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html), 16GB - either can be used for either purpose\n\nOption3:\n\n[i7-6700](https://www.intel.com/content/www/us/en/products/sku/88195/intel-core-i76700k-processor-8m-cache-up-to-4-20-ghz/specifications.html), xGB (need to be built) and [i5-8500T](https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html), 16GB - either can be used for either purpose\n\nOption4:\n\nUse either of the above systems to do the whole server in one massive system, which has all the HDD's and the main server.\n\n&amp;#x200B;\n\nComparison of CPU's  \n[https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F](https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F)\n\n&amp;#x200B;\n\nAlso, have a [1060](https://www.evga.com/products/specs/gpu.aspx?pn=e466f076-3e8f-4825-b4b0-f1863e893972), 6GB nvidia if that can be leveraged.\n\nAs mentioned would prefer to use the existing builds but wondering if 2nd gen would be sufficient enough.", "author_fullname": "t2_2lkmjxkk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[BUILD HELP] Choosing right CPU and RAM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6ojsg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669615353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning to build a NAS with unraid which will have 2x14, 2x8 TB,  for now. This NAS will be a standalone and act as NAS without Plex or  any other services. I want to have another system working as the main  server. I currently have few optoins at my disposal and was wondering  what would be the best use of current resources. Some are already built  systems and some need to be built.&lt;/p&gt;\n\n&lt;p&gt;Expand-ability: slowly but not sure to what storage.&lt;/p&gt;\n\n&lt;p&gt;Power consumption: Preferred low, as it might be idle 70% at least for the first year.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Option1 (Preferred option):&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.intel.com/content/www/us/en/products/sku/52207/intel-core-i52400-processor-6m-cache-up-to-3-40-ghz/specifications.html\"&gt;i5-2400&lt;/a&gt;, 8GB (not expandable) ram as NAS and&lt;a href=\"https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html\"&gt; i5-8500T&lt;/a&gt;, 16GB as the main server&lt;/p&gt;\n\n&lt;p&gt;Option2:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.intel.com/content/www/us/en/products/sku/203474/intel-core-i310105f-processor-6m-cache-up-to-4-40-ghz/specifications.html\"&gt;i3-10105F&lt;/a&gt;, xGB (need to be built) and &lt;a href=\"https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html\"&gt;i5-8500T&lt;/a&gt;, 16GB - either can be used for either purpose&lt;/p&gt;\n\n&lt;p&gt;Option3:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.intel.com/content/www/us/en/products/sku/88195/intel-core-i76700k-processor-8m-cache-up-to-4-20-ghz/specifications.html\"&gt;i7-6700&lt;/a&gt;, xGB (need to be built) and &lt;a href=\"https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html\"&gt;i5-8500T&lt;/a&gt;, 16GB - either can be used for either purpose&lt;/p&gt;\n\n&lt;p&gt;Option4:&lt;/p&gt;\n\n&lt;p&gt;Use either of the above systems to do the whole server in one massive system, which has all the HDD&amp;#39;s and the main server.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Comparison of CPU&amp;#39;s&lt;br/&gt;\n&lt;a href=\"https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F\"&gt;https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Also, have a &lt;a href=\"https://www.evga.com/products/specs/gpu.aspx?pn=e466f076-3e8f-4825-b4b0-f1863e893972\"&gt;1060&lt;/a&gt;, 6GB nvidia if that can be leveraged.&lt;/p&gt;\n\n&lt;p&gt;As mentioned would prefer to use the existing builds but wondering if 2nd gen would be sufficient enough.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?auto=webp&amp;s=0402e358a1b5b71d7e7e3840908c20761a156712", "width": 586, "height": 387}, "resolutions": [{"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a95ee4184562ea6e68359542a9ba4fac4f67356", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db96825d26a40d3ff36bd9249696f6589a6c12f4", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=09df17da18177e610ef5f2a653e8af6a754ba14d", "width": 320, "height": 211}], "variants": {}, "id": "7wlnlsb1p8kqETD6MDhVPbujRAXC14mk8BJ3S2O8fCg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6ojsg", "is_robot_indexable": true, "report_reasons": null, "author": "batmaniac77", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6ojsg/build_help_choosing_right_cpu_and_ram/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6ojsg/build_help_choosing_right_cpu_and_ram/", "subreddit_subscribers": 656587, "created_utc": 1669615353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been ripping some cds using exact audio copy with accurate rip setup. I tried to submit my results to the database and it says it successful however when I try to rip the CD again it says tracks not in accurate rip database. I tried disabling my firewall and using another computer but results are the same. Is there anything I could to so that my results get uploaded? No issues with the cds as well all ripped accurately according to cue tools.", "author_fullname": "t2_4evp94do", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EAC accurate rip results wont upload to database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6ik0v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669597777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been ripping some cds using exact audio copy with accurate rip setup. I tried to submit my results to the database and it says it successful however when I try to rip the CD again it says tracks not in accurate rip database. I tried disabling my firewall and using another computer but results are the same. Is there anything I could to so that my results get uploaded? No issues with the cds as well all ripped accurately according to cue tools.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6ik0v", "is_robot_indexable": true, "report_reasons": null, "author": "atitann", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6ik0v/eac_accurate_rip_results_wont_upload_to_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6ik0v/eac_accurate_rip_results_wont_upload_to_database/", "subreddit_subscribers": 656587, "created_utc": 1669597777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! I have been backing up games for well over a decade now and have something like 60TB including artwork. I know a RAID isn\u2019t a backup and I had a bit of a scare the other day and I\u2019ve decided to get serious about making a backup. I was looking into tape drives and I never realized how expensive they were, but something needs done. So\u2026 what do you all recommend? I\u2019ve been looking at an IBM 3592-E08\n\nThank you.", "author_fullname": "t2_14mq9m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "60+TB Gaming NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6aglm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669578136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I have been backing up games for well over a decade now and have something like 60TB including artwork. I know a RAID isn\u2019t a backup and I had a bit of a scare the other day and I\u2019ve decided to get serious about making a backup. I was looking into tape drives and I never realized how expensive they were, but something needs done. So\u2026 what do you all recommend? I\u2019ve been looking at an IBM 3592-E08&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6aglm", "is_robot_indexable": true, "report_reasons": null, "author": "Sasquatters", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6aglm/60tb_gaming_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6aglm/60tb_gaming_nas/", "subreddit_subscribers": 656587, "created_utc": 1669578136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If I understand it correctly these contain multiple bdxls, so if I take them out can I write and read them in a regular bdxl compatible drive instead of using the whole cartridge in $10000 readers? It seems cheaper than buying 100 or 128gb bdxls by themselves.", "author_fullname": "t2_50fpwuu0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can discs be removed from from optical disc archival cartridges?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6wm22", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669642158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I understand it correctly these contain multiple bdxls, so if I take them out can I write and read them in a regular bdxl compatible drive instead of using the whole cartridge in $10000 readers? It seems cheaper than buying 100 or 128gb bdxls by themselves.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6wm22", "is_robot_indexable": true, "report_reasons": null, "author": "Voldy256", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6wm22/can_discs_be_removed_from_from_optical_disc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6wm22/can_discs_be_removed_from_from_optical_disc/", "subreddit_subscribers": 656587, "created_utc": 1669642158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi reddit,\n\n&amp;#x200B;\n\nI need to download a specific file type from my dropbox account. In this case only all .mp4-files. Any suggestion how I can do it? \n\n&amp;#x200B;\n\nIt's not possible to use the \"Multi file organize/Custom filters\" to move all the files to a new directory. All directories needs to be kept as they are.", "author_fullname": "t2_jakf6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download specific file types from Dropbox", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6vqqz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669639795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi reddit,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I need to download a specific file type from my dropbox account. In this case only all .mp4-files. Any suggestion how I can do it? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not possible to use the &amp;quot;Multi file organize/Custom filters&amp;quot; to move all the files to a new directory. All directories needs to be kept as they are.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6vqqz", "is_robot_indexable": true, "report_reasons": null, "author": "blowmycool", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6vqqz/download_specific_file_types_from_dropbox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6vqqz/download_specific_file_types_from_dropbox/", "subreddit_subscribers": 656587, "created_utc": 1669639795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I was looking to buy a 2.5'' 5Tb external hard drive, however after some searching, it seems like the WD drives don't have an external SATA connector or something like that, so I was wondering if in the case of a possible disk failure, that should be something I should have in mind (like buying a Seagate disk that has the SATA connector). I am not interested in shucking the hard drive though.\n\nSeagate disks seemed like a good option too, but however from what I've searched, they seemed to have a higher failure rate compared to other brands...\n\nWith all this in mind, what would be your suggestions? Should I care about the SATA connector I'll probably never use (unless it helps recovering disk content, if it fails one day), or should I look into other brands?\n\nThanks in advance!", "author_fullname": "t2_ycqmv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2.5\" 5TB external HDD - WD vs Seagate vs others?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6p8jr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669617607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I was looking to buy a 2.5&amp;#39;&amp;#39; 5Tb external hard drive, however after some searching, it seems like the WD drives don&amp;#39;t have an external SATA connector or something like that, so I was wondering if in the case of a possible disk failure, that should be something I should have in mind (like buying a Seagate disk that has the SATA connector). I am not interested in shucking the hard drive though.&lt;/p&gt;\n\n&lt;p&gt;Seagate disks seemed like a good option too, but however from what I&amp;#39;ve searched, they seemed to have a higher failure rate compared to other brands...&lt;/p&gt;\n\n&lt;p&gt;With all this in mind, what would be your suggestions? Should I care about the SATA connector I&amp;#39;ll probably never use (unless it helps recovering disk content, if it fails one day), or should I look into other brands?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6p8jr", "is_robot_indexable": true, "report_reasons": null, "author": "supercar1x", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6p8jr/25_5tb_external_hdd_wd_vs_seagate_vs_others/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6p8jr/25_5tb_external_hdd_wd_vs_seagate_vs_others/", "subreddit_subscribers": 656587, "created_utc": 1669617607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have no idea if this is the right place to ask, but I\u2019m absolutely lost and I figured y\u2019all might be able help. I\u2019ve been working on a project that involves some image files from around 1997. The only issue is that (almost) all of the files I was given seem to be corrupted and unusable. They don\u2019t have any file extensions, and adding extensions doesn\u2019t work either. The most important images still seem to take up a decent bit of space, so is there any chance something like this could be salvaged? Figured I might as well shoot out this last cry for help, if you think you can help I\u2019d be glad to send over some of the problem files to look at. Thanks!", "author_fullname": "t2_8rjlxo5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Corrupted image files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6kfc5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669602979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have no idea if this is the right place to ask, but I\u2019m absolutely lost and I figured y\u2019all might be able help. I\u2019ve been working on a project that involves some image files from around 1997. The only issue is that (almost) all of the files I was given seem to be corrupted and unusable. They don\u2019t have any file extensions, and adding extensions doesn\u2019t work either. The most important images still seem to take up a decent bit of space, so is there any chance something like this could be salvaged? Figured I might as well shoot out this last cry for help, if you think you can help I\u2019d be glad to send over some of the problem files to look at. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6kfc5", "is_robot_indexable": true, "report_reasons": null, "author": "Pokeballersprime2", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6kfc5/corrupted_image_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6kfc5/corrupted_image_files/", "subreddit_subscribers": 656587, "created_utc": 1669602979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Greetings friends,\n\nI'm sitting on a pile of PDFs I'd like to organize and searchable. I've come across paperless, but building a NAS or a raspberry pi just to tag and search some PDFs seems overkill, and with 2 young kids and 2 more on the way, I don't have a huge amount of time to fiddle around with tech.\n\nCurrently, all that stuff is handled in Evernote, but if possible, I'd like to leave behind the world of proprietary systems for that task.\n\nAny advice?", "author_fullname": "t2_1233ak", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PDF library: Solution for OCR and tagging", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6a1nd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669577146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings friends,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sitting on a pile of PDFs I&amp;#39;d like to organize and searchable. I&amp;#39;ve come across paperless, but building a NAS or a raspberry pi just to tag and search some PDFs seems overkill, and with 2 young kids and 2 more on the way, I don&amp;#39;t have a huge amount of time to fiddle around with tech.&lt;/p&gt;\n\n&lt;p&gt;Currently, all that stuff is handled in Evernote, but if possible, I&amp;#39;d like to leave behind the world of proprietary systems for that task.&lt;/p&gt;\n\n&lt;p&gt;Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6a1nd", "is_robot_indexable": true, "report_reasons": null, "author": "Gilgeam", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6a1nd/pdf_library_solution_for_ocr_and_tagging/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6a1nd/pdf_library_solution_for_ocr_and_tagging/", "subreddit_subscribers": 656587, "created_utc": 1669577146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to set up a self hosted paperless instance and digitize all my physical documents.  I'd like to get a scanner than can scan a small stack of papers at a time, on both sides, and deposit them vita FTP/NFS/SMB/SMTP to a directory to be consumed by paperless.  I've heard there is also some detection feature for accidently scanning two pages stuck together, I probably want that as well.  I have no need to run this printer independently of a PC but I don't mind if it does.  Has anyone been through this and have a scanner that they like?  I'm happy to spend whatever budget to buy it for life and hit that price to performance sweet spot.  Better than replacing is down the line when it has problems..", "author_fullname": "t2_zjagd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a document scanner for Paperless-ng", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z65tem", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669567326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to set up a self hosted paperless instance and digitize all my physical documents.  I&amp;#39;d like to get a scanner than can scan a small stack of papers at a time, on both sides, and deposit them vita FTP/NFS/SMB/SMTP to a directory to be consumed by paperless.  I&amp;#39;ve heard there is also some detection feature for accidently scanning two pages stuck together, I probably want that as well.  I have no need to run this printer independently of a PC but I don&amp;#39;t mind if it does.  Has anyone been through this and have a scanner that they like?  I&amp;#39;m happy to spend whatever budget to buy it for life and hit that price to performance sweet spot.  Better than replacing is down the line when it has problems..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z65tem", "is_robot_indexable": true, "report_reasons": null, "author": "jswervedizzle", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z65tem/looking_for_a_document_scanner_for_paperlessng/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z65tem/looking_for_a_document_scanner_for_paperlessng/", "subreddit_subscribers": 656587, "created_utc": 1669567326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello Redditors,\n\nI have a novice question.  \n\nDo 3.5\" drives that are used to write once and accessed once every blue moon because they are stored elsewhere have more durability than one inside a system that has daily usage or it does not matter?", "author_fullname": "t2_3luaynwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD longevity when not used", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z64ff3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669564033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Redditors,&lt;/p&gt;\n\n&lt;p&gt;I have a novice question.  &lt;/p&gt;\n\n&lt;p&gt;Do 3.5&amp;quot; drives that are used to write once and accessed once every blue moon because they are stored elsewhere have more durability than one inside a system that has daily usage or it does not matter?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z64ff3", "is_robot_indexable": true, "report_reasons": null, "author": "LightDarkCloud", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z64ff3/hdd_longevity_when_not_used/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z64ff3/hdd_longevity_when_not_used/", "subreddit_subscribers": 656587, "created_utc": 1669564033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[This](https://www.whyp.it/tracks/47235/ff4m-your-face-reveal-extremely-harsh-fdom-sph-cei-joi-with-emmafielder?token=QYqTk) for example.", "author_fullname": "t2_fxa6t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have any idea how to download audio off whyp.it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6obz8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669614677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.whyp.it/tracks/47235/ff4m-your-face-reveal-extremely-harsh-fdom-sph-cei-joi-with-emmafielder?token=QYqTk\"&gt;This&lt;/a&gt; for example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?auto=webp&amp;s=82a0f834b43394e4fee20a2cc41fe00e125258a9", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ab775b4b7793dfa1b93d3386a8ebc60a2b9e080", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=52800673f8193dec4d7a141ef80e90373a30e585", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2070fa9a1c3efd3db1f3ee42c77a10dd9d6eaabf", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=caaed099ba79e0457c7b6bb678fee3d3456de086", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd72a7a67ac1bf061a7ee91eff122226a5959341", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/qzx7M7NalO6EsVsBJ_lA_DAkHDDP9CQNl68m6eGF-7g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8a07b0fff76863a31cec1f235028a8a4feb38f32", "width": 1080, "height": 564}], "variants": {}, "id": "WhUaEduY2rnlf0g05TXrTe9kWyllkLZGWuyRJaTvepc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6obz8", "is_robot_indexable": true, "report_reasons": null, "author": "onlytoask", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6obz8/anyone_have_any_idea_how_to_download_audio_off/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6obz8/anyone_have_any_idea_how_to_download_audio_off/", "subreddit_subscribers": 656587, "created_utc": 1669614677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there,\n\nI'm Tom and I am a DataHoarder :)\n\nI'm based in Europe and I've just setup a small cloud storage service.\n\n\nA short story about my initial thoughts on this project:\n\nI've been Hoarding Data for over two decades now, going through all phases (losing data, having multiple copies of them, 3-2-1 rules etc), and I've noticed that it is getting very expensive after a while (well, you can notice that very fast nowadays hah).\n\nAll these started when I wanted to grab a couple of extra TBs of online storage to backup some secondary data. There aren't many affordable (mind you, $5-$7/TB might be cheap for some) options out there and all I wanted was to share a box with someone.\n\nSo I've decided to make a little project to \"fix\" this. Maybe there are more people out there who think the same (you never know until you try?). I've had my ups and downs but this is something I was really excited to code. This is also the first project I created after a long time (so please be kind :)). Hopefully it will make me get back to the game.\n\nI'm basically splitting up the servers, no overselling or anything like that, and you can use (S)FTP, SCP, rsync, Rclone, Duplicati, BorgBackup to create your backups.\n\n\nRequesting Feedback:\n\nI'm offering 100GB Trials for a week for you to play around and provide any kind of feedback so I can improve either the quality of the service or the website itself. I know it requires a signup and email verification but I tried to make it as painless as possible :(\n\nYou can check it out [here](https://www.extralayer.eu/).\n\nI'm already working on the next phase (the first one wasn't anything special, just a basic build :)) which I think it can do wonders to decrease the cost of online storage even further. Hopefully it will be ready sometime in Q1 2023. I will let you know :)\n\nThank you for your time! \n\nTom\n\nPS: I've got approval from the moderators to post this thread.\n\nPS2: I've made a similar post on Hacker News earlier today, just a small FYI.", "author_fullname": "t2_uf5afmca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Cloud Storage Service - Requesting Feedback", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6hu3q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Feedback", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669597382.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669595837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m Tom and I am a DataHoarder :)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m based in Europe and I&amp;#39;ve just setup a small cloud storage service.&lt;/p&gt;\n\n&lt;p&gt;A short story about my initial thoughts on this project:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been Hoarding Data for over two decades now, going through all phases (losing data, having multiple copies of them, 3-2-1 rules etc), and I&amp;#39;ve noticed that it is getting very expensive after a while (well, you can notice that very fast nowadays hah).&lt;/p&gt;\n\n&lt;p&gt;All these started when I wanted to grab a couple of extra TBs of online storage to backup some secondary data. There aren&amp;#39;t many affordable (mind you, $5-$7/TB might be cheap for some) options out there and all I wanted was to share a box with someone.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;ve decided to make a little project to &amp;quot;fix&amp;quot; this. Maybe there are more people out there who think the same (you never know until you try?). I&amp;#39;ve had my ups and downs but this is something I was really excited to code. This is also the first project I created after a long time (so please be kind :)). Hopefully it will make me get back to the game.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m basically splitting up the servers, no overselling or anything like that, and you can use (S)FTP, SCP, rsync, Rclone, Duplicati, BorgBackup to create your backups.&lt;/p&gt;\n\n&lt;p&gt;Requesting Feedback:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m offering 100GB Trials for a week for you to play around and provide any kind of feedback so I can improve either the quality of the service or the website itself. I know it requires a signup and email verification but I tried to make it as painless as possible :(&lt;/p&gt;\n\n&lt;p&gt;You can check it out &lt;a href=\"https://www.extralayer.eu/\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m already working on the next phase (the first one wasn&amp;#39;t anything special, just a basic build :)) which I think it can do wonders to decrease the cost of online storage even further. Hopefully it will be ready sometime in Q1 2023. I will let you know :)&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time! &lt;/p&gt;\n\n&lt;p&gt;Tom&lt;/p&gt;\n\n&lt;p&gt;PS: I&amp;#39;ve got approval from the moderators to post this thread.&lt;/p&gt;\n\n&lt;p&gt;PS2: I&amp;#39;ve made a similar post on Hacker News earlier today, just a small FYI.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z6hu3q", "is_robot_indexable": true, "report_reasons": null, "author": "ExtraLayer_eu", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6hu3q/new_cloud_storage_service_requesting_feedback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6hu3q/new_cloud_storage_service_requesting_feedback/", "subreddit_subscribers": 656587, "created_utc": 1669595837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_h3zc4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD 16TB Elements External @ $230, 20tb @$320", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_z6uljs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XQeO52zNtio13p2nbLbQSnEKOTDkng2PtQV7Ij4piiA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669636453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bhphotovideo.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.bhphotovideo.com/c/product/1604996-REG/wd_wdbwlg0160hbk_nesn_wd_elements_desktop_16tb.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?auto=webp&amp;s=d533c09e4e1c69de0bd50bc25ae8da9e7dbafee7", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6754091ac5eb6a439260facfe4e2f85cfc9f9260", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c8cd696a54908ce6e149eb7f17ba706715b5992", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee30d5223ed5c1434c89d0fd0000d85515ced8e0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=08f43d5bc65c353767346d1847f8bd9e62526bf3", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8cec47c342d745aa908a41fe83669752d0c890b0", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=638ff173faccb4ac7c5e3898d30b6c71d30e31d0", "width": 1080, "height": 567}], "variants": {}, "id": "MLLs0tCS3MSYCsXnYaXuEEuXfYT7OImhc-S9BT9FlNI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "z6uljs", "is_robot_indexable": true, "report_reasons": null, "author": "Anzial", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6uljs/wd_16tb_elements_external_230_20tb_320/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.bhphotovideo.com/c/product/1604996-REG/wd_wdbwlg0160hbk_nesn_wd_elements_desktop_16tb.html", "subreddit_subscribers": 656587, "created_utc": 1669636453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey folks!\n\nCurrently running 8x10TB raidz2, considering to grab some 14-20TB drives during CyberMonday if I find some good deals. I don't have a set budget, but I prefer to keep it cheap/cost efficient. Primary usage is media storage and streaming. \n\nMy server have up to 18 bays available. What I've come up with so far are:\n\n1) 2x8 raidz2 vdevs for one zpool  - Largest cost and lowest Capacity Efficiency, but highest performance\n\n2) 1x8 to 1x10 raidz2 vdev for one zpool - Smallest cost, best Capacity Efficiency\n\n3) 1x10 to 1x16 raidz3 vdev for one zpool - Medium cost, medium Capacity Efficiency, lowest performance, possibly largest vdev\n\nCan anyone help me decide? =)", "author_fullname": "t2_4qo0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Input wanted on expanding storage: keywords zfs and vdev widths", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6syoz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669630794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks!&lt;/p&gt;\n\n&lt;p&gt;Currently running 8x10TB raidz2, considering to grab some 14-20TB drives during CyberMonday if I find some good deals. I don&amp;#39;t have a set budget, but I prefer to keep it cheap/cost efficient. Primary usage is media storage and streaming. &lt;/p&gt;\n\n&lt;p&gt;My server have up to 18 bays available. What I&amp;#39;ve come up with so far are:&lt;/p&gt;\n\n&lt;p&gt;1) 2x8 raidz2 vdevs for one zpool  - Largest cost and lowest Capacity Efficiency, but highest performance&lt;/p&gt;\n\n&lt;p&gt;2) 1x8 to 1x10 raidz2 vdev for one zpool - Smallest cost, best Capacity Efficiency&lt;/p&gt;\n\n&lt;p&gt;3) 1x10 to 1x16 raidz3 vdev for one zpool - Medium cost, medium Capacity Efficiency, lowest performance, possibly largest vdev&lt;/p&gt;\n\n&lt;p&gt;Can anyone help me decide? =)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "128TB+6TB++ (raw)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6syoz", "is_robot_indexable": true, "report_reasons": null, "author": "Griznah", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6syoz/input_wanted_on_expanding_storage_keywords_zfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6syoz/input_wanted_on_expanding_storage_keywords_zfs/", "subreddit_subscribers": 656587, "created_utc": 1669630794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "'Sup Hoarders.  \n\n\nSo, I'm redoing my data setup with backups and backdowns and open-source knockdowns and multiple copies knockups and sideways syncs and whatnot and what have you. That said, I can tell that every time with storage there's this painful sensation that I don't get what's going on. I can extend an LVM with virtual disk... But what is a volume? What is a logical volume? What is a \"disk\"? What is a file? What is a filesystem? How does file indexing work? Etc.  \n\n\n(Sure, there's material that treats how a single disk works in isolation, fine, but I haven't been able to find a single source that  cover the breadth of common enterprise solutions like FC to what ext4 is.)   \n\n\nWould anyone of you happen to know of that? An expanded intro or primer on data and storage technology (Something like a 'Computer Networking a Top-Down Approach' but for storage)?  \nWhether it's text, youtube-series etc. etc. make no difference to me.  \n\n\nThanks for reading. :)", "author_fullname": "t2_i1c1s4em", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Primer on data and storage technology", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6om64", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669615574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#39;Sup Hoarders.  &lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m redoing my data setup with backups and backdowns and open-source knockdowns and multiple copies knockups and sideways syncs and whatnot and what have you. That said, I can tell that every time with storage there&amp;#39;s this painful sensation that I don&amp;#39;t get what&amp;#39;s going on. I can extend an LVM with virtual disk... But what is a volume? What is a logical volume? What is a &amp;quot;disk&amp;quot;? What is a file? What is a filesystem? How does file indexing work? Etc.  &lt;/p&gt;\n\n&lt;p&gt;(Sure, there&amp;#39;s material that treats how a single disk works in isolation, fine, but I haven&amp;#39;t been able to find a single source that  cover the breadth of common enterprise solutions like FC to what ext4 is.)   &lt;/p&gt;\n\n&lt;p&gt;Would anyone of you happen to know of that? An expanded intro or primer on data and storage technology (Something like a &amp;#39;Computer Networking a Top-Down Approach&amp;#39; but for storage)?&lt;br/&gt;\nWhether it&amp;#39;s text, youtube-series etc. etc. make no difference to me.  &lt;/p&gt;\n\n&lt;p&gt;Thanks for reading. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6om64", "is_robot_indexable": true, "report_reasons": null, "author": "manpagebob", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6om64/primer_on_data_and_storage_technology/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6om64/primer_on_data_and_storage_technology/", "subreddit_subscribers": 656587, "created_utc": 1669615574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_11dj0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate IronWolf Pro 20TB NAS Hard Drive 7200 RPM 256MB Cache CMR 340 / 20 TB, 17 a TB but 20TB drive.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6og5b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.46, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1669615063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "newegg.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.newegg.com/seagate-ironwolf-pro-st20000ne000-20tb/p/N82E16822185007?Item=N82E16822185007&amp;ignorebbr=1&amp;cm_mmc=EMC-Automation112722-_-EMC-112722-Index-_-Desktop%20Internal%20Hard%20Drives-_-N82E16822185007&amp;ignorebbr=1&amp;cvtc=10647527", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "30TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z6og5b", "is_robot_indexable": true, "report_reasons": null, "author": "nando1969", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6og5b/seagate_ironwolf_pro_20tb_nas_hard_drive_7200_rpm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.newegg.com/seagate-ironwolf-pro-st20000ne000-20tb/p/N82E16822185007?Item=N82E16822185007&amp;ignorebbr=1&amp;cm_mmc=EMC-Automation112722-_-EMC-112722-Index-_-Desktop%20Internal%20Hard%20Drives-_-N82E16822185007&amp;ignorebbr=1&amp;cvtc=10647527", "subreddit_subscribers": 656587, "created_utc": 1669615063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, due to the massive rocket fire from the russian federation and the lack of light for almost half a day, I am trying to find an additional opportunity to have a backup copy and use it\n One of the options is a smartphone, but it does not support ext4/btrfs, only fat32.\n Is there any way to solve this problem?", "author_fullname": "t2_lo8e4f55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am trying to find an additional option to have a backup and use it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6obui", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669614665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, due to the massive rocket fire from the russian federation and the lack of light for almost half a day, I am trying to find an additional opportunity to have a backup copy and use it\n One of the options is a smartphone, but it does not support ext4/btrfs, only fat32.\n Is there any way to solve this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "russian military ship, go to hell", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6obui", "is_robot_indexable": true, "report_reasons": null, "author": "kovach_ua", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6obui/i_am_trying_to_find_an_additional_option_to_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6obui/i_am_trying_to_find_an_additional_option_to_have/", "subreddit_subscribers": 656587, "created_utc": 1669614665.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}