{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_dv1qy8c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Add it to the training set, Walmart", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_ylfpqx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 1471, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 1471, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/J8_0BpvFtCKSrQS42vGNBi1MYiGUfVlolDzuFR9rNbI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667511110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/irkcbvz41tx91.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/irkcbvz41tx91.jpg?auto=webp&amp;s=3eeebea51a0db8c7505d8dcedfa7a51d28a47eda", "width": 918, "height": 1530}, "resolutions": [{"url": "https://preview.redd.it/irkcbvz41tx91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=17681b92c04f89a5069d6060dcf9ec6bc5175953", "width": 108, "height": 180}, {"url": "https://preview.redd.it/irkcbvz41tx91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=36dca12dd4f1a82c7d4a698546271a198b5aae42", "width": 216, "height": 360}, {"url": "https://preview.redd.it/irkcbvz41tx91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=62c6512e64131b9b06c4448406eb5c256cda068e", "width": 320, "height": 533}, {"url": "https://preview.redd.it/irkcbvz41tx91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4c8f3b471181c4747b7a49eb847a1da22d3bb58", "width": 640, "height": 1066}], "variants": {}, "id": "VitcyIruHSc5Syx21UOmHxbI5Z06g7YMBREl7rTcvBQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylfpqx", "is_robot_indexable": true, "report_reasons": null, "author": "ljh78", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylfpqx/add_it_to_the_training_set_walmart/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/irkcbvz41tx91.jpg", "subreddit_subscribers": 817254, "created_utc": 1667511110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all!\n\nAround 2 months back I was unfortunately a part of a mass layoff and had to start looking for a new job. I had come to this community for help in improving my resume (My post was removed as I had not used the weekly thread for resume review. Will keep that in mind next time). But even for the brief period of time that the post was up, I had received a lot of good advice which I incorporated.\n\nI start my new role next week. This community played a significant part in my success that I wanted to acknowledge. I'm not sure if these kinds of posts are allowed, if not they'll be removed anyway :p\n\nI'd like to personally thank the following users who gave very clear and concise advice that I was able to use to enhance my resume. u/abitofaLuna-tic, u/chunzilla, u/VacuousWaffle, u/denim_duck,  u/proverbialbunny. Thank you all!\n\n**P.S:** A common point that was suggested to me was to switch from a 2-column format to a 1-column format. This worked. When comparing the two resumes I saw how concise and easy to read my 1-column (single page) resume was. That being said, I've observed that my 2-column resume seemed to do really well with young startups, companies with smaller DS teams, or a more personal screening process. Just thought that this was interesting.", "author_fullname": "t2_40233m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thank you! An appreciation post of this DS community.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ym30il", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667578214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;Around 2 months back I was unfortunately a part of a mass layoff and had to start looking for a new job. I had come to this community for help in improving my resume (My post was removed as I had not used the weekly thread for resume review. Will keep that in mind next time). But even for the brief period of time that the post was up, I had received a lot of good advice which I incorporated.&lt;/p&gt;\n\n&lt;p&gt;I start my new role next week. This community played a significant part in my success that I wanted to acknowledge. I&amp;#39;m not sure if these kinds of posts are allowed, if not they&amp;#39;ll be removed anyway :p&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to personally thank the following users who gave very clear and concise advice that I was able to use to enhance my resume. &lt;a href=\"/u/abitofaLuna-tic\"&gt;u/abitofaLuna-tic&lt;/a&gt;, &lt;a href=\"/u/chunzilla\"&gt;u/chunzilla&lt;/a&gt;, &lt;a href=\"/u/VacuousWaffle\"&gt;u/VacuousWaffle&lt;/a&gt;, &lt;a href=\"/u/denim_duck\"&gt;u/denim_duck&lt;/a&gt;,  &lt;a href=\"/u/proverbialbunny\"&gt;u/proverbialbunny&lt;/a&gt;. Thank you all!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;P.S:&lt;/strong&gt; A common point that was suggested to me was to switch from a 2-column format to a 1-column format. This worked. When comparing the two resumes I saw how concise and easy to read my 1-column (single page) resume was. That being said, I&amp;#39;ve observed that my 2-column resume seemed to do really well with young startups, companies with smaller DS teams, or a more personal screening process. Just thought that this was interesting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym30il", "is_robot_indexable": true, "report_reasons": null, "author": "CrypticTac", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym30il/thank_you_an_appreciation_post_of_this_ds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym30il/thank_you_an_appreciation_post_of_this_ds/", "subreddit_subscribers": 817254, "created_utc": 1667578214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_dfcot0zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It's great to see huge corporations like CVS trying to low-ball their senior data scientist . How do you even justify that low end of the salary range ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_ylir6o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5dgAaM76PdXrozNx2Y8LkrJrYorb71JgXsY_dZvz7gM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667518922.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/mmiiac2w5vx91.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/mmiiac2w5vx91.png?auto=webp&amp;s=13a694b51dec1728a6affcada829d4916b11abfd", "width": 1080, "height": 1841}, "resolutions": [{"url": "https://preview.redd.it/mmiiac2w5vx91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e4872ed5dec112ad6dcd89260e92e65e10ecd01", "width": 108, "height": 184}, {"url": "https://preview.redd.it/mmiiac2w5vx91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a71d2f44c9ac53e75c267c9cd0047143bd59f07", "width": 216, "height": 368}, {"url": "https://preview.redd.it/mmiiac2w5vx91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4eebacb8a62b560d494e00072fcb7b3094c14042", "width": 320, "height": 545}, {"url": "https://preview.redd.it/mmiiac2w5vx91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=272139c312067d8bc5cd800a88fa13946f5e2031", "width": 640, "height": 1090}, {"url": "https://preview.redd.it/mmiiac2w5vx91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=69ad657cce5748effb5e8fce7f1fe7c5af1ef9ed", "width": 960, "height": 1636}, {"url": "https://preview.redd.it/mmiiac2w5vx91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cf795e5cf6dd250ab34c77f79c579e1703e19b7c", "width": 1080, "height": 1841}], "variants": {}, "id": "w2Nb8mRHA2mwiamI1lTgc7BiyVY0KwF5aSD449DeM3I"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylir6o", "is_robot_indexable": true, "report_reasons": null, "author": "DrRedmondNYC", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylir6o/its_great_to_see_huge_corporations_like_cvs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/mmiiac2w5vx91.png", "subreddit_subscribers": 817254, "created_utc": 1667518922.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "When do you guys think FAANG will start rehiring for mid level data science/MLE roles with 2+ yoe?\n\n6 months?\n\n12 months?\n\n2 years?", "author_fullname": "t2_1ns77nex", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FAANG Hiring Again Timeframe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylk4na", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": "", "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "seniorflair", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667522587.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When do you guys think FAANG will start rehiring for mid level data science/MLE roles with 2+ yoe?&lt;/p&gt;\n\n&lt;p&gt;6 months?&lt;/p&gt;\n\n&lt;p&gt;12 months?&lt;/p&gt;\n\n&lt;p&gt;2 years?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Scientist MS|MBA ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylk4na", "is_robot_indexable": true, "report_reasons": null, "author": "DJAlaskaAndrew", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/datascience/comments/ylk4na/faang_hiring_again_timeframe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylk4na/faang_hiring_again_timeframe/", "subreddit_subscribers": 817254, "created_utc": 1667522587.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As my title suggests. I'm quite worried about the lay-offs happening, and I've overheard that data people are the first ones to let go. Is it true? Also do data engineers have more job security compared maybe data scientists/analysts?\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_tsoiffje", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it true that when tech companies lay off people, usually data scientists/analysts/engineers are the first ones to let go? If it's true, does it mean that we have less job security compared with usual SWEs writing codes and building products?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yli49w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667517367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As my title suggests. I&amp;#39;m quite worried about the lay-offs happening, and I&amp;#39;ve overheard that data people are the first ones to let go. Is it true? Also do data engineers have more job security compared maybe data scientists/analysts?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yli49w", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Maintenance-1871", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yli49w/is_it_true_that_when_tech_companies_lay_off/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yli49w/is_it_true_that_when_tech_companies_lay_off/", "subreddit_subscribers": 817254, "created_utc": 1667517367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a few years of analytics experience but have recently been looking to make a change. \n\nOver the course of my career I've had many interviews. I've done many case studies (some live, some take homes). I've answered lots of various technical and behavioral questions. You know the drill.\n\nBut not once had I been asked to literally calculate probabilities and shit live over an interview, until today. This seems like *such* bad practice to me. The question was a word problem, like something on a stats 101 homework assignment. There is quiet literally almost no value in asking questions like this.\n\nIt seems like these questions aren't all that uncommon in interviews...but they definitely ought to be!\n\nHow often have you been asked to do actual calculations during an interview?", "author_fullname": "t2_2s0os6oi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Asking Data Scientists to do calculations live in an interview is nuts.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yljrkq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667521599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a few years of analytics experience but have recently been looking to make a change. &lt;/p&gt;\n\n&lt;p&gt;Over the course of my career I&amp;#39;ve had many interviews. I&amp;#39;ve done many case studies (some live, some take homes). I&amp;#39;ve answered lots of various technical and behavioral questions. You know the drill.&lt;/p&gt;\n\n&lt;p&gt;But not once had I been asked to literally calculate probabilities and shit live over an interview, until today. This seems like &lt;em&gt;such&lt;/em&gt; bad practice to me. The question was a word problem, like something on a stats 101 homework assignment. There is quiet literally almost no value in asking questions like this.&lt;/p&gt;\n\n&lt;p&gt;It seems like these questions aren&amp;#39;t all that uncommon in interviews...but they definitely ought to be!&lt;/p&gt;\n\n&lt;p&gt;How often have you been asked to do actual calculations during an interview?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yljrkq", "is_robot_indexable": true, "report_reasons": null, "author": "randoma1231vd", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yljrkq/asking_data_scientists_to_do_calculations_live_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yljrkq/asking_data_scientists_to_do_calculations_live_in/", "subreddit_subscribers": 817254, "created_utc": 1667521599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In our company, we are mainly working on handling duplicate data. Not the exact matches, similar records, too. What are the common problems in your company and how you are handling these issues?", "author_fullname": "t2_5ddglx8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the most common data processing problem?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylv12d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667557544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In our company, we are mainly working on handling duplicate data. Not the exact matches, similar records, too. What are the common problems in your company and how you are handling these issues?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylv12d", "is_robot_indexable": true, "report_reasons": null, "author": "alka_irl", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylv12d/what_is_the_most_common_data_processing_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylv12d/what_is_the_most_common_data_processing_problem/", "subreddit_subscribers": 817254, "created_utc": 1667557544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nI've been asked this question during an interview. \n\nI have currently 3 YOE as a Data Scientist in the finance industry and I have applied to this small tech startup (&lt;20 people, &lt;10 technical people). They have no DS/ML people on their team, so I would be their first, and I would be working on a subject very similar to what I have already implemented in my current company.\n\nThe interviewer asked me this question after I gave him my low range of salary expectations.\n\nHow do you know when you are ready to be the head of data science of a company ? What mandatory skills should you have ? Since it's a small company should I expect to be responsible of the data engineering part also ? What should I clarify with them in my next interview ?", "author_fullname": "t2_6b5q0a66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are you ready to lead a DS team and be the head of data science ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ym13he", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667573585.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been asked this question during an interview. &lt;/p&gt;\n\n&lt;p&gt;I have currently 3 YOE as a Data Scientist in the finance industry and I have applied to this small tech startup (&amp;lt;20 people, &amp;lt;10 technical people). They have no DS/ML people on their team, so I would be their first, and I would be working on a subject very similar to what I have already implemented in my current company.&lt;/p&gt;\n\n&lt;p&gt;The interviewer asked me this question after I gave him my low range of salary expectations.&lt;/p&gt;\n\n&lt;p&gt;How do you know when you are ready to be the head of data science of a company ? What mandatory skills should you have ? Since it&amp;#39;s a small company should I expect to be responsible of the data engineering part also ? What should I clarify with them in my next interview ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym13he", "is_robot_indexable": true, "report_reasons": null, "author": "Intrepid_Evening", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym13he/are_you_ready_to_lead_a_ds_team_and_be_the_head/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym13he/are_you_ready_to_lead_a_ds_team_and_be_the_head/", "subreddit_subscribers": 817254, "created_utc": 1667573585.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have what I think may be a simple request, but I don\u2019t know where to start. I\u2019d like to use a program to figure out how many unique groups I have by sorting two lists into a 3rd list. Let\u2019s say list A has 100 locations, and list B has 20 products. All the items in list A require some of the products from list B. What I\u2019m hoping to do is drag the items from List B over the items in List A thus creating a group, ie \u201citem 1 in List A contains these 14 products\u201d (and I need items in List B to be reusable across all the items in List A). Then I want the program to tell me how many locations in List A are receiving the same products (and which locations). Then afterwards I\u2019ll know how many unique groupings of locations and products exist. I don\u2019t know if card sorting would actually work here or if I have the wrong idea about what card sorting is. I\u2019ve been able to do this in Excel somewhat successfully, but it\u2019s more maintenance than help. Just hoping someone might be able to point me in the right direction.", "author_fullname": "t2_bk6kyll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A simple request from a simpleton", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylcoe3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667504789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have what I think may be a simple request, but I don\u2019t know where to start. I\u2019d like to use a program to figure out how many unique groups I have by sorting two lists into a 3rd list. Let\u2019s say list A has 100 locations, and list B has 20 products. All the items in list A require some of the products from list B. What I\u2019m hoping to do is drag the items from List B over the items in List A thus creating a group, ie \u201citem 1 in List A contains these 14 products\u201d (and I need items in List B to be reusable across all the items in List A). Then I want the program to tell me how many locations in List A are receiving the same products (and which locations). Then afterwards I\u2019ll know how many unique groupings of locations and products exist. I don\u2019t know if card sorting would actually work here or if I have the wrong idea about what card sorting is. I\u2019ve been able to do this in Excel somewhat successfully, but it\u2019s more maintenance than help. Just hoping someone might be able to point me in the right direction.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylcoe3", "is_robot_indexable": true, "report_reasons": null, "author": "DolphLundgrenMD", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylcoe3/a_simple_request_from_a_simpleton/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylcoe3/a_simple_request_from_a_simpleton/", "subreddit_subscribers": 817254, "created_utc": 1667504789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was curious if someone could help me figure out a method of determining growth rates for a data set with a large range.\n\nFor example, the growth rate over a year for 100 different cities. Some cities may start with 100 people and others may start with 100,000 or 1m. If a 100 person city grows by 100 people thats a 100% growth rate, however a city of 100,000's growth rate will be much lower.\n\nMy issue is that if I sort growth rates of all cities descending, I will get pretty much only the smallest cities and their crazy high growth rates.\n\nOne solution I thought of was to create \"bands\" of cities (IE: 0-100, 100-1000, 1000-10,000, etc) and test each of their growth rates, however, I believe there might be a better way to \"weight\" certain cities growth rates depending on their size?", "author_fullname": "t2_fc3cf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to sort or rank growth rates across large range data sets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ym74qp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667588115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was curious if someone could help me figure out a method of determining growth rates for a data set with a large range.&lt;/p&gt;\n\n&lt;p&gt;For example, the growth rate over a year for 100 different cities. Some cities may start with 100 people and others may start with 100,000 or 1m. If a 100 person city grows by 100 people thats a 100% growth rate, however a city of 100,000&amp;#39;s growth rate will be much lower.&lt;/p&gt;\n\n&lt;p&gt;My issue is that if I sort growth rates of all cities descending, I will get pretty much only the smallest cities and their crazy high growth rates.&lt;/p&gt;\n\n&lt;p&gt;One solution I thought of was to create &amp;quot;bands&amp;quot; of cities (IE: 0-100, 100-1000, 1000-10,000, etc) and test each of their growth rates, however, I believe there might be a better way to &amp;quot;weight&amp;quot; certain cities growth rates depending on their size?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym74qp", "is_robot_indexable": true, "report_reasons": null, "author": "johnnyhighschool", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym74qp/how_to_sort_or_rank_growth_rates_across_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym74qp/how_to_sort_or_rank_growth_rates_across_large/", "subreddit_subscribers": 817254, "created_utc": 1667588115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Howdy Data folks,\n\nI'm in the retail space and trying to basically forecast sales for 2023. I took over the BI/data role after the guy previously in the role left earlier this year. He built a projection basically using previous sales from the last couple years (and I'm still trying to read through his python code to figure out how he came to the calculation btw), but I feel like with the economy and what not-things could be so up and down that maybe we shouldnt rely on previous years sales. \n\nAre there any data sources I should be considering looking at, in order to better verify sales/projections for next year? \n\nAny help or insight would be VASTLY appreciated.", "author_fullname": "t2_7meg6iov", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forecasting retail sales in 2023? Do you use anything in particular for insight?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ym4xxj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667582891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy Data folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in the retail space and trying to basically forecast sales for 2023. I took over the BI/data role after the guy previously in the role left earlier this year. He built a projection basically using previous sales from the last couple years (and I&amp;#39;m still trying to read through his python code to figure out how he came to the calculation btw), but I feel like with the economy and what not-things could be so up and down that maybe we shouldnt rely on previous years sales. &lt;/p&gt;\n\n&lt;p&gt;Are there any data sources I should be considering looking at, in order to better verify sales/projections for next year? &lt;/p&gt;\n\n&lt;p&gt;Any help or insight would be VASTLY appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ym4xxj", "is_robot_indexable": true, "report_reasons": null, "author": "WhatsTheAnswerDude", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ym4xxj/forecasting_retail_sales_in_2023_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ym4xxj/forecasting_retail_sales_in_2023_do_you_use/", "subreddit_subscribers": 817254, "created_utc": 1667582891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I  can't even remember where I picked this up, but my understanding is  that when building an ML model, and you want to know how well that model  really predicts on unseen (test) data - to give you an indication of  how well it will perform in the field, one doesn't just do a train test  split once and evaluate your model on the test set, but rather, one  should do this many times and take the mean performance from the test  set. Is this actually correct? Here is some code to indicate what I  mean, note that in this example I'm not including any data, but just  assume that X is an np.array of predictors and y is an np.array with a binary outcome:\n\n`from sklearn.linear_model import ElasticNetCV`\n\n`from sklearn.metrics import roc_curve, auc`\n\n`auc_list = []`\n\n`for i in range(30):`\n\n`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)`\n\n`train_means = X_train.mean(axis = 0)`\n\n`train_sd = X_train.std(axis = 0)`\n\n`X_train = (X_train - train_means)/train_sd`\n\n`X_test = (X_test - train_means)/train_sd`\n\n`lr_model = ElasticNetCV(cv=5)`\n\n`lr_model.fit(X_train, y_train)`\n\n`y_preds = lr_model.predict(X_test)`\n\n`fpr, tpr, thresholds = roc_curve(y_test, y_preds, pos_label=1)`\n\n`auc_roc = auc(fpr, tpr)`\n\n`auc_list.append(auc_roc)`\n\nSo  above I evaluate the AUC of my model on the testing set, and I do this  on 30 different train/test splits, and I then look at the mean AUC  across the 30 splits - np.mean(auc\\_list)\n\nIs  this a correct procedure? My intuition is that there may be some train  test splits that give particularly good or bad predictions due to the  randomness of the split, and so one must take the mean of many splits.\n\nHowever,  when I look at every single online tutorial or online course on how to  build an ml model, this aspect of it is never included, so much so to  the extent that I'm now wondering if I've been doing it wrong in taking  this approach? So does anyone know what is correct? And if my way is  correct, why is it that this is never included in online tutorials?\n\nMany thanks!", "author_fullname": "t2_lrovl6pi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The mean of multiple train/test splits is required in order to evaluate an ML model(?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yly3wd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667565995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I  can&amp;#39;t even remember where I picked this up, but my understanding is  that when building an ML model, and you want to know how well that model  really predicts on unseen (test) data - to give you an indication of  how well it will perform in the field, one doesn&amp;#39;t just do a train test  split once and evaluate your model on the test set, but rather, one  should do this many times and take the mean performance from the test  set. Is this actually correct? Here is some code to indicate what I  mean, note that in this example I&amp;#39;m not including any data, but just  assume that X is an np.array of predictors and y is an np.array with a binary outcome:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;from sklearn.linear_model import ElasticNetCV&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;from sklearn.metrics import roc_curve, auc&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;auc_list = []&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;for i in range(30):&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;train_means = X_train.mean(axis = 0)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;train_sd = X_train.std(axis = 0)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;X_train = (X_train - train_means)/train_sd&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;X_test = (X_test - train_means)/train_sd&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;lr_model = ElasticNetCV(cv=5)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;lr_model.fit(X_train, y_train)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;y_preds = lr_model.predict(X_test)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;fpr, tpr, thresholds = roc_curve(y_test, y_preds, pos_label=1)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;auc_roc = auc(fpr, tpr)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;auc_list.append(auc_roc)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;So  above I evaluate the AUC of my model on the testing set, and I do this  on 30 different train/test splits, and I then look at the mean AUC  across the 30 splits - np.mean(auc_list)&lt;/p&gt;\n\n&lt;p&gt;Is  this a correct procedure? My intuition is that there may be some train  test splits that give particularly good or bad predictions due to the  randomness of the split, and so one must take the mean of many splits.&lt;/p&gt;\n\n&lt;p&gt;However,  when I look at every single online tutorial or online course on how to  build an ml model, this aspect of it is never included, so much so to  the extent that I&amp;#39;m now wondering if I&amp;#39;ve been doing it wrong in taking  this approach? So does anyone know what is correct? And if my way is  correct, why is it that this is never included in online tutorials?&lt;/p&gt;\n\n&lt;p&gt;Many thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yly3wd", "is_robot_indexable": true, "report_reasons": null, "author": "likeamanyfacedgod", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yly3wd/the_mean_of_multiple_traintest_splits_is_required/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yly3wd/the_mean_of_multiple_traintest_splits_is_required/", "subreddit_subscribers": 817254, "created_utc": 1667565995.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "We are looking for such platform and I am trying to see what others think of these products or what else is out there ?", "author_fullname": "t2_he02w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone evaluated third party platforms such as panalgo or aetion or other ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylj3nj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667519852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are looking for such platform and I am trying to see what others think of these products or what else is out there ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylj3nj", "is_robot_indexable": true, "report_reasons": null, "author": "Vervain7", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylj3nj/has_anyone_evaluated_third_party_platforms_such/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylj3nj/has_anyone_evaluated_third_party_platforms_such/", "subreddit_subscribers": 817254, "created_utc": 1667519852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello friends!\n\nI'm trying to build a book similarity model to evaluate how similar two books are. I have multiple text fields such as:\n\n* title\n* author\n* subject\n* description (long text)\n* table of content (long text)\n\nUntil now I was concatenating everything into one string and producing one TF-IDF vector for each book. Then I was simply running cosine similarity.\n\nNow I came to realize some features are more important than others. For example title similarity is more important than subject similarity, but I don't know how to incorporate that into the model. I was thinking of constructing a TF-IDF vector for each column and then calculating similarities separately, then weighting the similarity scores themselves. Is there a better way to achieve this?\n\nThank you!", "author_fullname": "t2_286kcv5s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiple Text Features", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylhg0b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667515967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello friends!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to build a book similarity model to evaluate how similar two books are. I have multiple text fields such as:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;title&lt;/li&gt;\n&lt;li&gt;author&lt;/li&gt;\n&lt;li&gt;subject&lt;/li&gt;\n&lt;li&gt;description (long text)&lt;/li&gt;\n&lt;li&gt;table of content (long text)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Until now I was concatenating everything into one string and producing one TF-IDF vector for each book. Then I was simply running cosine similarity.&lt;/p&gt;\n\n&lt;p&gt;Now I came to realize some features are more important than others. For example title similarity is more important than subject similarity, but I don&amp;#39;t know how to incorporate that into the model. I was thinking of constructing a TF-IDF vector for each column and then calculating similarities separately, then weighting the similarity scores themselves. Is there a better way to achieve this?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylhg0b", "is_robot_indexable": true, "report_reasons": null, "author": "thecrixus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylhg0b/multiple_text_features/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylhg0b/multiple_text_features/", "subreddit_subscribers": 817254, "created_utc": 1667515967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey just wondering if anyone has some good resources to dive deep into quantitative analytics(books, websites, videos, etc.)", "author_fullname": "t2_9mx4u3n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best resources for quantitative analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylhcsv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667515795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey just wondering if anyone has some good resources to dive deep into quantitative analytics(books, websites, videos, etc.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylhcsv", "is_robot_indexable": true, "report_reasons": null, "author": "Vnix7", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylhcsv/best_resources_for_quantitative_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylhcsv/best_resources_for_quantitative_analytics/", "subreddit_subscribers": 817254, "created_utc": 1667515795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I'm trying to reverse engineer in my mind the process of returning summary statistics for a large dataset very quickly. I am thinking for example, returning data to a front end to create some charts on a webpage.\n\nLet's say we have a database recording the results of 20,000 matches of a game. The matches have all sorts of data, for example damage done by a certain spell every 10 seconds. It's a lot of data.\n\nWe now want to return the total damage of the spell, \"fireball\", throughout all of the matches. A silly approach would be, every time a user visits a webpage, sum across all 20,000 matches find where spell = \"fireball\" and sum up each damage bit to get the total. This computation would take time regardless of overpaying for high levels of computational power.\n\nWhat I would consider doing in this case is create a new statistics portion of the database. It would include total damage for fireball. Each time you would finish a match, or at another time interval, you would simply add the damage to the total.\n\nWhen you want your statistics, you simply ping the fireball statistics and return the data. I think that this would work great because it would put minor computation requirements on your server frequently rather than massive computation whenever a user wants his stats.\n\nA problem comes up with this approach, what if you needed to report more complex statistics that you can't simply add to every time? If you wanted 95th quartile, or median, you would need to go through every game session and get yourself the entire series (let's call this a full database scan).\n\n**My question:** What sorts of approaches in data management are given for computing up to date complex statistics over large data sets? Would it be a batched process where we are not doing a full database scan every match, but perhaps every 500 matches? Is there any reading I can do to understand these types data/database management practices?", "author_fullname": "t2_4xizssma", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Serving Complex Statistics from Large Databases as Quickly As Possible", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylewcw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667509424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to reverse engineer in my mind the process of returning summary statistics for a large dataset very quickly. I am thinking for example, returning data to a front end to create some charts on a webpage.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say we have a database recording the results of 20,000 matches of a game. The matches have all sorts of data, for example damage done by a certain spell every 10 seconds. It&amp;#39;s a lot of data.&lt;/p&gt;\n\n&lt;p&gt;We now want to return the total damage of the spell, &amp;quot;fireball&amp;quot;, throughout all of the matches. A silly approach would be, every time a user visits a webpage, sum across all 20,000 matches find where spell = &amp;quot;fireball&amp;quot; and sum up each damage bit to get the total. This computation would take time regardless of overpaying for high levels of computational power.&lt;/p&gt;\n\n&lt;p&gt;What I would consider doing in this case is create a new statistics portion of the database. It would include total damage for fireball. Each time you would finish a match, or at another time interval, you would simply add the damage to the total.&lt;/p&gt;\n\n&lt;p&gt;When you want your statistics, you simply ping the fireball statistics and return the data. I think that this would work great because it would put minor computation requirements on your server frequently rather than massive computation whenever a user wants his stats.&lt;/p&gt;\n\n&lt;p&gt;A problem comes up with this approach, what if you needed to report more complex statistics that you can&amp;#39;t simply add to every time? If you wanted 95th quartile, or median, you would need to go through every game session and get yourself the entire series (let&amp;#39;s call this a full database scan).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My question:&lt;/strong&gt; What sorts of approaches in data management are given for computing up to date complex statistics over large data sets? Would it be a batched process where we are not doing a full database scan every match, but perhaps every 500 matches? Is there any reading I can do to understand these types data/database management practices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylewcw", "is_robot_indexable": true, "report_reasons": null, "author": "gunnerydota", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylewcw/serving_complex_statistics_from_large_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylewcw/serving_complex_statistics_from_large_databases/", "subreddit_subscribers": 817254, "created_utc": 1667509424.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_91atae5g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Share your experiences using windows vs mac in data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylzrmj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667570256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylzrmj", "is_robot_indexable": true, "report_reasons": null, "author": "MalaysiaDankMeme", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylzrmj/share_your_experiences_using_windows_vs_mac_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylzrmj/share_your_experiences_using_windows_vs_mac_in/", "subreddit_subscribers": 817254, "created_utc": 1667570256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I'm a software engineer who is learning about deep learning + machine learning.\n\nI've been exploring time series prediction using TensorFlow, PyTorch, prophet etc.\n\nAssuming you can nicely split your data for training, test, validation is there a library or framework that lets you pass this exactly dataset to multiple or many different time series libraries and plot + evaluate the MAPE + other statistics?\n\nExample:\n\n* I nicely load, feature engineer and split my datasets\n* I then want to pass these into the following:\n   * prophet\n   * neuralprophet\n   * keras\n   * TensorFlow probability\n   * etc\n* And then I want to plot all of these results (MAPE etc) sorted from lowest to highest etc\n\nI presume (hope) that this has been solved for already and that I wouldn't have to roll my own version but just wondering what's out there?\n\nThis was the post that inspired me to ask the question\n\n[https://bytepawn.com/comparing-neuralprophet-and-prophet-for-timeseries-forecasting.html](https://bytepawn.com/comparing-neuralprophet-and-prophet-for-timeseries-forecasting.html)\n\nThanks", "author_fullname": "t2_sdb0o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Framework to compare TimeSeries models against a single set of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylrxxq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667547457.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m a software engineer who is learning about deep learning + machine learning.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been exploring time series prediction using TensorFlow, PyTorch, prophet etc.&lt;/p&gt;\n\n&lt;p&gt;Assuming you can nicely split your data for training, test, validation is there a library or framework that lets you pass this exactly dataset to multiple or many different time series libraries and plot + evaluate the MAPE + other statistics?&lt;/p&gt;\n\n&lt;p&gt;Example:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I nicely load, feature engineer and split my datasets&lt;/li&gt;\n&lt;li&gt;I then want to pass these into the following:\n\n&lt;ul&gt;\n&lt;li&gt;prophet&lt;/li&gt;\n&lt;li&gt;neuralprophet&lt;/li&gt;\n&lt;li&gt;keras&lt;/li&gt;\n&lt;li&gt;TensorFlow probability&lt;/li&gt;\n&lt;li&gt;etc&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;And then I want to plot all of these results (MAPE etc) sorted from lowest to highest etc&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I presume (hope) that this has been solved for already and that I wouldn&amp;#39;t have to roll my own version but just wondering what&amp;#39;s out there?&lt;/p&gt;\n\n&lt;p&gt;This was the post that inspired me to ask the question&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://bytepawn.com/comparing-neuralprophet-and-prophet-for-timeseries-forecasting.html\"&gt;https://bytepawn.com/comparing-neuralprophet-and-prophet-for-timeseries-forecasting.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylrxxq", "is_robot_indexable": true, "report_reasons": null, "author": "Javaguy44", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylrxxq/framework_to_compare_timeseries_models_against_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylrxxq/framework_to_compare_timeseries_models_against_a/", "subreddit_subscribers": 817254, "created_utc": 1667547457.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7ao0u2en", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you apply to US based roles from an international country?? (applying for jobs from companies that sponsor visas). Would you even get a response?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yluqeg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667556581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "yluqeg", "is_robot_indexable": true, "report_reasons": null, "author": "the_scientist-7367", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yluqeg/can_you_apply_to_us_based_roles_from_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yluqeg/can_you_apply_to_us_based_roles_from_an/", "subreddit_subscribers": 817254, "created_utc": 1667556581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Are they a scam? Offering 75.00 but can\u2019t find them on LinkedIn but can view website.", "author_fullname": "t2_tyd4eyda", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blue light expertise survey", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yllfgf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667526436.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667526179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are they a scam? Offering 75.00 but can\u2019t find them on LinkedIn but can view website.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yllfgf", "is_robot_indexable": true, "report_reasons": null, "author": "chefry54", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yllfgf/blue_light_expertise_survey/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yllfgf/blue_light_expertise_survey/", "subreddit_subscribers": 817254, "created_utc": 1667526179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_1s89t2xm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I\u2019ve got data on the top 50 subreddits for the past 6 months (top 100 posts in each and all the comments made within 24 hours). \u2014 what would you do with it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ylm8te", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667528486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ylm8te", "is_robot_indexable": true, "report_reasons": null, "author": "mfb1274", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ylm8te/ive_got_data_on_the_top_50_subreddits_for_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ylm8te/ive_got_data_on_the_top_50_subreddits_for_the/", "subreddit_subscribers": 817254, "created_utc": 1667528486.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}