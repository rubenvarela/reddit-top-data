{"kind": "Listing", "data": {"after": "t3_z8n0rg", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_hbwoyw04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "20tb Sata/SAS $269.99-$304.99 -or- 18tb Sata $189.99 Seagate recertified with 2 year warranty, wish I had the money, thought I'd share here!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_z8irvw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 131, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 131, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/O4bB82ABNHVdXXdfMBZQnBvA5Mk6ksZ9Ns88tZbB3rw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669786924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "serverpartdeals.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://serverpartdeals.com/collections/recertified-seagate-exos", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?auto=webp&amp;s=4c1db49fea90382fef14d5213d071fac818998ff", "width": 2000, "height": 2000}, "resolutions": [{"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e7ff784b044c5a132792a51783e12546aa096933", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ebb5efa12a9e165a566f9c8d169e9861369acd2f", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=763ac82ff67bfc932ed93a299e8d40c5271a9258", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=487c4ebc47f1af49ca7b4ed1e032a4d96672a443", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=32fed97e301ba788e5b9661d6052a5e41a4a23c1", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f4c9097a5b110531b40d5950876ae2530c195a25", "width": 1080, "height": 1080}], "variants": {}, "id": "_D0H66BxLDdRjM-Vxtc776wegerPH5Vt6qcZASPwe_c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "z8irvw", "is_robot_indexable": true, "report_reasons": null, "author": "igmyeongui", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8irvw/20tb_satasas_2699930499_or_18tb_sata_18999/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://serverpartdeals.com/collections/recertified-seagate-exos", "subreddit_subscribers": 656859, "created_utc": 1669786924.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought a new, faster NVMe drive and cloned my Windows system drive onto it using Macrium\n\nHowever, there's a small discrepancy in the total files/file sizes.\n\nIs there a tool to analyze both drives and show me which files it finds that differ between the two?\n\nEverything seems to be (mostly) in order, but there's a 6GB difference in used space on the two drives.\n\n(Also worth noting that my taskbar flashes for a few seconds while the desktop loads on the cloned drive, which doesn't happen if I boot from the old drive. I am using an app called TaskbarX, but it isn't working properly since cloning and I don't think that's the issue as the flashing was happening prior to re-configuring that program. In any case, this isn't the proper subreddit for that particular issue)", "author_fullname": "t2_7d7au", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tools to show the difference in files between two cloned drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8l12y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669793579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a new, faster NVMe drive and cloned my Windows system drive onto it using Macrium&lt;/p&gt;\n\n&lt;p&gt;However, there&amp;#39;s a small discrepancy in the total files/file sizes.&lt;/p&gt;\n\n&lt;p&gt;Is there a tool to analyze both drives and show me which files it finds that differ between the two?&lt;/p&gt;\n\n&lt;p&gt;Everything seems to be (mostly) in order, but there&amp;#39;s a 6GB difference in used space on the two drives.&lt;/p&gt;\n\n&lt;p&gt;(Also worth noting that my taskbar flashes for a few seconds while the desktop loads on the cloned drive, which doesn&amp;#39;t happen if I boot from the old drive. I am using an app called TaskbarX, but it isn&amp;#39;t working properly since cloning and I don&amp;#39;t think that&amp;#39;s the issue as the flashing was happening prior to re-configuring that program. In any case, this isn&amp;#39;t the proper subreddit for that particular issue)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z8l12y", "is_robot_indexable": true, "report_reasons": null, "author": "MiguelLancaster", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8l12y/best_tools_to_show_the_difference_in_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8l12y/best_tools_to_show_the_difference_in_files/", "subreddit_subscribers": 656859, "created_utc": 1669793579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI want Parity Based server/nas/external HDD for ONE PC 10Gb direct connection and each Pc is also connected to router with slower link\n\n&amp;#x200B;\n\nUnraid 120USD\n\nWindows 2022 Server 9$ \\[found online shop, works for me\\]\n\nHow parity works in windows spaces? I have HDDs of many sizes 16,14, 12 \\[all my HDDs totaling near 370Tb, but I dont need so much and ill be using all 16 and 14tb, no 12 or 10, maybe 250ish Tb\\]\n\n&amp;#x200B;\n\nI know on Unraid you chose Parity drives yourself so you can just set the biggest HDD's manually.\n\nBut from the videos i seen about windows, you create a pool of all drives and it creates parity across all of them there isnt  \"parity HDD\" by itself.\n\nSo how its going to work with different sized drives?\n\n&amp;#x200B;\n\nI prefer to use windows for ease of use, and so i can use that PC as backup PC for just you know PC stuff\n\nNo need for virtualization, maybe torrenting but i torrent to SSD, HDD wont work for that with my 2.5Gb fiber connection\n\nI do have bunch of different SSDs, that i want to utilize for cache thou, 2Tb Samsung EVO and bunch of 500Gb Samsung pros \\[which better suited to be cache drives\\]\n\nAlso im a windows guy, afraid Unraid will be confusing for me", "author_fullname": "t2_onbly768", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unraid or Server 2022? [NAS with Parity]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8hcj1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669782832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I want Parity Based server/nas/external HDD for ONE PC 10Gb direct connection and each Pc is also connected to router with slower link&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Unraid 120USD&lt;/p&gt;\n\n&lt;p&gt;Windows 2022 Server 9$ [found online shop, works for me]&lt;/p&gt;\n\n&lt;p&gt;How parity works in windows spaces? I have HDDs of many sizes 16,14, 12 [all my HDDs totaling near 370Tb, but I dont need so much and ill be using all 16 and 14tb, no 12 or 10, maybe 250ish Tb]&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I know on Unraid you chose Parity drives yourself so you can just set the biggest HDD&amp;#39;s manually.&lt;/p&gt;\n\n&lt;p&gt;But from the videos i seen about windows, you create a pool of all drives and it creates parity across all of them there isnt  &amp;quot;parity HDD&amp;quot; by itself.&lt;/p&gt;\n\n&lt;p&gt;So how its going to work with different sized drives?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I prefer to use windows for ease of use, and so i can use that PC as backup PC for just you know PC stuff&lt;/p&gt;\n\n&lt;p&gt;No need for virtualization, maybe torrenting but i torrent to SSD, HDD wont work for that with my 2.5Gb fiber connection&lt;/p&gt;\n\n&lt;p&gt;I do have bunch of different SSDs, that i want to utilize for cache thou, 2Tb Samsung EVO and bunch of 500Gb Samsung pros [which better suited to be cache drives]&lt;/p&gt;\n\n&lt;p&gt;Also im a windows guy, afraid Unraid will be confusing for me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8hcj1", "is_robot_indexable": true, "report_reasons": null, "author": "-Hexenhammer-", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8hcj1/unraid_or_server_2022_nas_with_parity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8hcj1/unraid_or_server_2022_nas_with_parity/", "subreddit_subscribers": 656859, "created_utc": 1669782832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nI'm trying to download some videos from this site - [https://www.skatehype.com](https://www.skatehype.com/) \\- most of the browser extensions (in Edge) I've tried haven't been able to detect the video, the one that can is unable to download because it continutally pauses after getting too many errors.\n\nIf I inspect, network, media etc it looks like the video file is [https://www.skatehype.com/s/v/12/3/12383.mp4](https://www.skatehype.com/s/v/12/3/12383.mp4) \\- but I can find a way of using this to download the video.\n\nAny pointers in the right directions would be greatly appreciated, thanks.", "author_fullname": "t2_liqtl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help hoarding video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z825og", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669747171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to download some videos from this site - &lt;a href=\"https://www.skatehype.com/\"&gt;https://www.skatehype.com&lt;/a&gt; - most of the browser extensions (in Edge) I&amp;#39;ve tried haven&amp;#39;t been able to detect the video, the one that can is unable to download because it continutally pauses after getting too many errors.&lt;/p&gt;\n\n&lt;p&gt;If I inspect, network, media etc it looks like the video file is &lt;a href=\"https://www.skatehype.com/s/v/12/3/12383.mp4\"&gt;https://www.skatehype.com/s/v/12/3/12383.mp4&lt;/a&gt; - but I can find a way of using this to download the video.&lt;/p&gt;\n\n&lt;p&gt;Any pointers in the right directions would be greatly appreciated, thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z825og", "is_robot_indexable": true, "report_reasons": null, "author": "gorillabankrolls", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z825og/help_hoarding_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z825og/help_hoarding_video/", "subreddit_subscribers": 656859, "created_utc": 1669747171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm new to this and debating getting a NAS for general backup.\n\nI understand that if a drive fails you can just swap it out and the NAS will repair/repopulate data from the other drive, but I don't know what would happen if the NAS itself failed? Do you just connect the drives to PC and copy/paste the files inside, or do you have to get another NAS of the same brand and hook the drives up to the new NAS to access those files?\n\nI'm planning on getting a 2 bay Synology and set up RAID 1.", "author_fullname": "t2_5cwbkrur", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recovering data when NAS fails (the actual NAS not the disks)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z86qo5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669757242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to this and debating getting a NAS for general backup.&lt;/p&gt;\n\n&lt;p&gt;I understand that if a drive fails you can just swap it out and the NAS will repair/repopulate data from the other drive, but I don&amp;#39;t know what would happen if the NAS itself failed? Do you just connect the drives to PC and copy/paste the files inside, or do you have to get another NAS of the same brand and hook the drives up to the new NAS to access those files?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning on getting a 2 bay Synology and set up RAID 1.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z86qo5", "is_robot_indexable": true, "report_reasons": null, "author": "khoa-gritson", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z86qo5/recovering_data_when_nas_fails_the_actual_nas_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z86qo5/recovering_data_when_nas_fails_the_actual_nas_not/", "subreddit_subscribers": 656859, "created_utc": 1669757242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I want to ask the community for some emotional help. I've lurked for a long time but made an account today just for this post(and hope to stop by occasionally).\n\nI'm not exactly a major datahoarder but I'm definitely a sentimentalist. Recently thanks to some dumb decisions on my part and not paying attention while working on hard drives, and some circumstances that led to me losing my backups, I lost 2TB of super important sentimental data spanning 18 years almost. I feel like an idiot for screwing this up so badly. It's been three weeks and I'm horribly depressed to say the least. I feel like I lost a loved one.\n\nThose who can share experiences and give advice, please do, I want to stop feeling so much intense grief. Or at least reduce it.\n\n(Before anyone brings it up, no, data recovery is impossible)", "author_fullname": "t2_uo1fs1kg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coping with dataloss?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8u78p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669821733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I want to ask the community for some emotional help. I&amp;#39;ve lurked for a long time but made an account today just for this post(and hope to stop by occasionally).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not exactly a major datahoarder but I&amp;#39;m definitely a sentimentalist. Recently thanks to some dumb decisions on my part and not paying attention while working on hard drives, and some circumstances that led to me losing my backups, I lost 2TB of super important sentimental data spanning 18 years almost. I feel like an idiot for screwing this up so badly. It&amp;#39;s been three weeks and I&amp;#39;m horribly depressed to say the least. I feel like I lost a loved one.&lt;/p&gt;\n\n&lt;p&gt;Those who can share experiences and give advice, please do, I want to stop feeling so much intense grief. Or at least reduce it.&lt;/p&gt;\n\n&lt;p&gt;(Before anyone brings it up, no, data recovery is impossible)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z8u78p", "is_robot_indexable": true, "report_reasons": null, "author": "Scramatic", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8u78p/coping_with_dataloss/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8u78p/coping_with_dataloss/", "subreddit_subscribers": 656859, "created_utc": 1669821733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Well it finally happened to me, one of my 14TB's in my Synology failed. It's one of 2 14TB's that I shucked and I kept everything (box, plastic shells, cables, etc) and opened it without breaking any clips; HOWEVER it doesn't look like I marked which internal drive came from which enclosure.\n\nI read through some threads here and it seems very hit or miss either way (sending bare shucked drive or reassembling). I'm guessing there's no way for us to tell which internal drive belongs in which shell?", "author_fullname": "t2_p5ds8v6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD 14TB HDD - RMA bare shucked drive or in enclosure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8rsqx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669816861.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669815685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Well it finally happened to me, one of my 14TB&amp;#39;s in my Synology failed. It&amp;#39;s one of 2 14TB&amp;#39;s that I shucked and I kept everything (box, plastic shells, cables, etc) and opened it without breaking any clips; HOWEVER it doesn&amp;#39;t look like I marked which internal drive came from which enclosure.&lt;/p&gt;\n\n&lt;p&gt;I read through some threads here and it seems very hit or miss either way (sending bare shucked drive or reassembling). I&amp;#39;m guessing there&amp;#39;s no way for us to tell which internal drive belongs in which shell?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8rsqx", "is_robot_indexable": true, "report_reasons": null, "author": "CiViCKiDD", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8rsqx/wd_14tb_hdd_rma_bare_shucked_drive_or_in_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8rsqx/wd_14tb_hdd_rma_bare_shucked_drive_or_in_enclosure/", "subreddit_subscribers": 656859, "created_utc": 1669815685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a complete noob when it comes to technology, and I desperately need your help.\n\nI want a storage solution for family photos and videos kept on over 20 separate CDs and 2-3 USB drives.\n\nI found this HDD in a local store and was wondering whether it could be used for storage (connected to a docking station) from time to time.\n\nI read that this is a surveillance HDD that can be used 24/7, so my question is if it can be used occasionally, say once every two months, and then stored somewhere else without power?", "author_fullname": "t2_kh69bbbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can SEAGATE SkyHawk be used for storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8qsqv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669813269.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669812953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a complete noob when it comes to technology, and I desperately need your help.&lt;/p&gt;\n\n&lt;p&gt;I want a storage solution for family photos and videos kept on over 20 separate CDs and 2-3 USB drives.&lt;/p&gt;\n\n&lt;p&gt;I found this HDD in a local store and was wondering whether it could be used for storage (connected to a docking station) from time to time.&lt;/p&gt;\n\n&lt;p&gt;I read that this is a surveillance HDD that can be used 24/7, so my question is if it can be used occasionally, say once every two months, and then stored somewhere else without power?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8qsqv", "is_robot_indexable": true, "report_reasons": null, "author": "the-emotional-emu", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8qsqv/can_seagate_skyhawk_be_used_for_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8qsqv/can_seagate_skyhawk_be_used_for_storage/", "subreddit_subscribers": 656859, "created_utc": 1669812953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I planning to use 30 hdds in hba mode no software raid just direct attached drives. Are Adaptec HBA Ultra 1200p-32i is a good choice.  \nhttps://www.microchip.com/en-us/product/HBA-1200up-32i", "author_fullname": "t2_x5b73", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adaptec HBA Ultra 1200p-32i", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8mhqq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669798477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I planning to use 30 hdds in hba mode no software raid just direct attached drives. Are Adaptec HBA Ultra 1200p-32i is a good choice.&lt;br/&gt;\n&lt;a href=\"https://www.microchip.com/en-us/product/HBA-1200up-32i\"&gt;https://www.microchip.com/en-us/product/HBA-1200up-32i&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8mhqq", "is_robot_indexable": true, "report_reasons": null, "author": "BlastSD", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8mhqq/adaptec_hba_ultra_1200p32i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8mhqq/adaptec_hba_ultra_1200p32i/", "subreddit_subscribers": 656859, "created_utc": 1669798477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Long story short I'm pretty sure this isn't possible but y'all would know if there's some weird way to do it so I figured I would ask. Basically I have ultra critical stuff on M-disks, but I know there are those 100gb mdisks. Is it possible to burn a few gigs to an M-disk, and then come back later, do a file compare and add more files to the same disk? Keeping everything that's already on the disk just adding more data. Thank you!", "author_fullname": "t2_tfp74", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding files to already burnt M-Disk?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z82bni", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669747538.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I&amp;#39;m pretty sure this isn&amp;#39;t possible but y&amp;#39;all would know if there&amp;#39;s some weird way to do it so I figured I would ask. Basically I have ultra critical stuff on M-disks, but I know there are those 100gb mdisks. Is it possible to burn a few gigs to an M-disk, and then come back later, do a file compare and add more files to the same disk? Keeping everything that&amp;#39;s already on the disk just adding more data. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z82bni", "is_robot_indexable": true, "report_reasons": null, "author": "rickyh7", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z82bni/adding_files_to_already_burnt_mdisk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z82bni/adding_files_to_already_burnt_mdisk/", "subreddit_subscribers": 656859, "created_utc": 1669747538.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm not sure if this is allowed here. Let me also preface this by saying this is for personal use. I also tried to search the sub but mainly came across Bluray and DVD ripping.\n\nOn some sites like Mubi, there are very interesting films which can be hard to come across in other places. I have discovered many rare films this way and when they remove them from the catalog they are very hard to find. I work in film stuff and would love to be able to revisit many of these later on.\n\nCan anyone point me to how I could do this? Any sites, videos, posts, etc, which explain this would be greatly appreciated.\n\nThank you", "author_fullname": "t2_90me57qk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ripping from streaming sites?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8ug9c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669822347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not sure if this is allowed here. Let me also preface this by saying this is for personal use. I also tried to search the sub but mainly came across Bluray and DVD ripping.&lt;/p&gt;\n\n&lt;p&gt;On some sites like Mubi, there are very interesting films which can be hard to come across in other places. I have discovered many rare films this way and when they remove them from the catalog they are very hard to find. I work in film stuff and would love to be able to revisit many of these later on.&lt;/p&gt;\n\n&lt;p&gt;Can anyone point me to how I could do this? Any sites, videos, posts, etc, which explain this would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8ug9c", "is_robot_indexable": true, "report_reasons": null, "author": "Powerful-Employer-20", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8ug9c/ripping_from_streaming_sites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8ug9c/ripping_from_streaming_sites/", "subreddit_subscribers": 656859, "created_utc": 1669822347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to archive all of my personal data onto M-Disks (around 500gb). I want to get 5 100GB Blu-Ray M-Disks for a long-term cold storage, but I've heard in passing that DVD's are more reliable, but they hold a lot less. Is the difference negligible or for long-term storage is one better than the other?", "author_fullname": "t2_r17l20lj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DVD or Blu Ray M-Disks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8t4cb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669819113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to archive all of my personal data onto M-Disks (around 500gb). I want to get 5 100GB Blu-Ray M-Disks for a long-term cold storage, but I&amp;#39;ve heard in passing that DVD&amp;#39;s are more reliable, but they hold a lot less. Is the difference negligible or for long-term storage is one better than the other?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8t4cb", "is_robot_indexable": true, "report_reasons": null, "author": "PassportNerd", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8t4cb/dvd_or_blu_ray_mdisks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8t4cb/dvd_or_blu_ray_mdisks/", "subreddit_subscribers": 656859, "created_utc": 1669819113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had an old server with an 8-drive RAIDZ-2 configuration for my data storage. I have created a new server and my intention was to plug in my PCI to 6 SATA port card and do an import on the ZFS pool. When I booted everything up I discovered the PCI card I was using was not recognized by my motherboard. I did some digging and it looks like the common recommendation is to get an LSI card flashed in \"IT mode\" to see the disks recognized. The card I purchased was this one:\n\n[https://www.newegg.com/p/14G-0006-001G5?Item=9SIAWFGJP29168](https://www.newegg.com/p/14G-0006-001G5?Item=9SIAWFGJP29168)\n\nI received the card today and plugged everything in. After booting into Ubuntu and running lsblk I discovered the drives are not being recognized. Running lspci reveals the LSI card is being recognized. I rebooted the computer and jumped into the WebBIOS for the card to see what was going on. The card is recognizing all 8 drives labeled as:\n\n    Slot:0,SATA,HDD,7.277TB, JBOD\n    Slot:1,SATA,HDD,7.277TB, JBOD\n    Slot:2,SATA,HDD,7.277TB, JBOD\n    Slot:3,SATA,HDD,7.277TB, JBOD\n    Slot:4,SATA,HDD,7.277TB, JBOD\n    Slot:5,SATA,HDD,7.277TB, JBOD\n    Slot:6,SATA,HDD,7.277TB, JBOD\n    Slot:7,SATA,HDD,7.277TB, JBOD\n\nThis looks correct to me. Why can't I see these drives in Ubuntu? Do I need to do something in WebBIOS in order to pass-through the information? Is there a way for me to check if this was actually flashed in IT mode?\n\nThe big thing I am scared of is losing the data on the drives. I don't have the ability to make a backup before messing with these drives and I'm terrified the card is going to try and initialize something.\n\nAny help would be greatly appreciated.", "author_fullname": "t2_68ots9ni", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating RAIDZ to new computer - How to make sure I don't lose data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8fmhw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669778313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had an old server with an 8-drive RAIDZ-2 configuration for my data storage. I have created a new server and my intention was to plug in my PCI to 6 SATA port card and do an import on the ZFS pool. When I booted everything up I discovered the PCI card I was using was not recognized by my motherboard. I did some digging and it looks like the common recommendation is to get an LSI card flashed in &amp;quot;IT mode&amp;quot; to see the disks recognized. The card I purchased was this one:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.newegg.com/p/14G-0006-001G5?Item=9SIAWFGJP29168\"&gt;https://www.newegg.com/p/14G-0006-001G5?Item=9SIAWFGJP29168&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I received the card today and plugged everything in. After booting into Ubuntu and running lsblk I discovered the drives are not being recognized. Running lspci reveals the LSI card is being recognized. I rebooted the computer and jumped into the WebBIOS for the card to see what was going on. The card is recognizing all 8 drives labeled as:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Slot:0,SATA,HDD,7.277TB, JBOD\nSlot:1,SATA,HDD,7.277TB, JBOD\nSlot:2,SATA,HDD,7.277TB, JBOD\nSlot:3,SATA,HDD,7.277TB, JBOD\nSlot:4,SATA,HDD,7.277TB, JBOD\nSlot:5,SATA,HDD,7.277TB, JBOD\nSlot:6,SATA,HDD,7.277TB, JBOD\nSlot:7,SATA,HDD,7.277TB, JBOD\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This looks correct to me. Why can&amp;#39;t I see these drives in Ubuntu? Do I need to do something in WebBIOS in order to pass-through the information? Is there a way for me to check if this was actually flashed in IT mode?&lt;/p&gt;\n\n&lt;p&gt;The big thing I am scared of is losing the data on the drives. I don&amp;#39;t have the ability to make a backup before messing with these drives and I&amp;#39;m terrified the card is going to try and initialize something.&lt;/p&gt;\n\n&lt;p&gt;Any help would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8fmhw", "is_robot_indexable": true, "report_reasons": null, "author": "tomsrobots", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8fmhw/migrating_raidz_to_new_computer_how_to_make_sure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8fmhw/migrating_raidz_to_new_computer_how_to_make_sure/", "subreddit_subscribers": 656859, "created_utc": 1669778313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys, \n\nI'm having a hard time finding a quality 6+ port USB hub to hook temporarily (4-5 months) six MyBooks (all 14TB) to a mini-PC on a Type-C USB 3.1 Gen 2 port. Typical usage would be copy/move stuff between the internal SSD drive to any one of the six MyBooks (one operation at a time). The important thing is the presence of individual on/off switches, so the drives would only run on demand (I need each one only 2-3 times a week and don't want them spinning up randomly or at boot/shutdown).\n\nThe only semi-legit device I can find is [this hub by StarTech](https://www.startech.com/en-eu/cards-adapters/5g7aibs-usb-hub-eu), however, most people in the sub have a pretty bad opinion about that brand and the price seems a bit steep, to be honest. That being said, the last thing I want is for my data to become corrupted because of some alphabet soup brand's bad hardware... (speaking of which, any recommendations for a solid USB-B to Type-C data cable are much appreciated!)\n\nWhat I like about StarTech's device is the use of actual switches, and not buttons, which usually don't remember the last state after power-cycling...\n\nThanks for reading and have a good one!", "author_fullname": "t2_r8d6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which USB hub with individual on/off switches?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z85s5u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669755129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having a hard time finding a quality 6+ port USB hub to hook temporarily (4-5 months) six MyBooks (all 14TB) to a mini-PC on a Type-C USB 3.1 Gen 2 port. Typical usage would be copy/move stuff between the internal SSD drive to any one of the six MyBooks (one operation at a time). The important thing is the presence of individual on/off switches, so the drives would only run on demand (I need each one only 2-3 times a week and don&amp;#39;t want them spinning up randomly or at boot/shutdown).&lt;/p&gt;\n\n&lt;p&gt;The only semi-legit device I can find is &lt;a href=\"https://www.startech.com/en-eu/cards-adapters/5g7aibs-usb-hub-eu\"&gt;this hub by StarTech&lt;/a&gt;, however, most people in the sub have a pretty bad opinion about that brand and the price seems a bit steep, to be honest. That being said, the last thing I want is for my data to become corrupted because of some alphabet soup brand&amp;#39;s bad hardware... (speaking of which, any recommendations for a solid USB-B to Type-C data cable are much appreciated!)&lt;/p&gt;\n\n&lt;p&gt;What I like about StarTech&amp;#39;s device is the use of actual switches, and not buttons, which usually don&amp;#39;t remember the last state after power-cycling...&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading and have a good one!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WWUhFm1UH1d76VeFn5Jq1cUqG3TT3n3wMI2Dar76WPY.jpg?auto=webp&amp;s=0d14aa9fae0a561c95e7ee557dceb55abc4fafa9", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/WWUhFm1UH1d76VeFn5Jq1cUqG3TT3n3wMI2Dar76WPY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=102bc651816f4608d640676c682a5cc3eac686b1", "width": 108, "height": 108}], "variants": {}, "id": "Yoe5OAAoT-eQrXAUU7Zn4hlPTdiNYSyG2maWRB4c7Ts"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z85s5u", "is_robot_indexable": true, "report_reasons": null, "author": "kraddock", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z85s5u/which_usb_hub_with_individual_onoff_switches/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z85s5u/which_usb_hub_with_individual_onoff_switches/", "subreddit_subscribers": 656859, "created_utc": 1669755129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there !\n\nI just received 4 ironwolf 4TB drives (ST4000VN006) and started checking them for potential damage from shipping.So i did a smart check on all of them ([https://file.io/0e2x4hA6sV8m](https://file.io/0e2x4hA6sV8m))\n\n    smartctl /dev/sdX -x\n\nand discovered that one of them (Disk 2) is having \"Pending Defects log (GP Log 0x0c)\" unlike the 3 others.\n\nIs that something i should be worried about ? Should i send it back for replacement ?I can't seem to find what those infos means.\n\nI also started a badblocks and a long smart test on all 4 drives, but it's gonna take time.\n\n    badblocks -v /dev/sdX\n    smartctl /dev/sdX -t long\n\nThank you in advance.", "author_fullname": "t2_lcwx2c6y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting \"Pending defects log\" on brand new drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z83kas", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669750310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there !&lt;/p&gt;\n\n&lt;p&gt;I just received 4 ironwolf 4TB drives (ST4000VN006) and started checking them for potential damage from shipping.So i did a smart check on all of them (&lt;a href=\"https://file.io/0e2x4hA6sV8m\"&gt;https://file.io/0e2x4hA6sV8m&lt;/a&gt;)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;smartctl /dev/sdX -x\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;and discovered that one of them (Disk 2) is having &amp;quot;Pending Defects log (GP Log 0x0c)&amp;quot; unlike the 3 others.&lt;/p&gt;\n\n&lt;p&gt;Is that something i should be worried about ? Should i send it back for replacement ?I can&amp;#39;t seem to find what those infos means.&lt;/p&gt;\n\n&lt;p&gt;I also started a badblocks and a long smart test on all 4 drives, but it&amp;#39;s gonna take time.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;badblocks -v /dev/sdX\nsmartctl /dev/sdX -t long\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z83kas", "is_robot_indexable": true, "report_reasons": null, "author": "UsrnameBetween3and20", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z83kas/getting_pending_defects_log_on_brand_new_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z83kas/getting_pending_defects_log_on_brand_new_drives/", "subreddit_subscribers": 656859, "created_utc": 1669750310.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Henlo.\n\nI've finished ripping the Blu-Rays for Avatar: The Last Airbender. And while I'm very happy with the result, each episode clocks in at around 5.5GB which isn't exactly ideal for streaming.\n\nI will of course be keeping these original files, but I've recently built a new PC with a 5700x and a 6700xt so I can once again re-encode things (hooray).\n\nThe BD of ATLA is an upscaled version of the original 480p broadcasts (might've been 480i, unsure). I'm looking for \"visually lossless\", which should be easy as many scenes already look not the best. I'd have to apply lots sharpening filters and such to get a better look, something which I don't have experience with.\n\nAnyhow, using ffmpeg, how do you guys usually re-encode media for smaller sizes? Doom9 mentions a few x265 options specifically, but ofc many clients don't support direct h265 streams.\n\nBonus question: blu ray subs are not fun, what's the best way to turn them into actual subtitles?", "author_fullname": "t2_1hutcmww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Media Server] Best practices for video compression?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8317z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669749112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Henlo.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve finished ripping the Blu-Rays for Avatar: The Last Airbender. And while I&amp;#39;m very happy with the result, each episode clocks in at around 5.5GB which isn&amp;#39;t exactly ideal for streaming.&lt;/p&gt;\n\n&lt;p&gt;I will of course be keeping these original files, but I&amp;#39;ve recently built a new PC with a 5700x and a 6700xt so I can once again re-encode things (hooray).&lt;/p&gt;\n\n&lt;p&gt;The BD of ATLA is an upscaled version of the original 480p broadcasts (might&amp;#39;ve been 480i, unsure). I&amp;#39;m looking for &amp;quot;visually lossless&amp;quot;, which should be easy as many scenes already look not the best. I&amp;#39;d have to apply lots sharpening filters and such to get a better look, something which I don&amp;#39;t have experience with.&lt;/p&gt;\n\n&lt;p&gt;Anyhow, using ffmpeg, how do you guys usually re-encode media for smaller sizes? Doom9 mentions a few x265 options specifically, but ofc many clients don&amp;#39;t support direct h265 streams.&lt;/p&gt;\n\n&lt;p&gt;Bonus question: blu ray subs are not fun, what&amp;#39;s the best way to turn them into actual subtitles?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Spent $330 for a single show", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8317z", "is_robot_indexable": true, "report_reasons": null, "author": "General-Stryker", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z8317z/media_server_best_practices_for_video_compression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8317z/media_server_best_practices_for_video_compression/", "subreddit_subscribers": 656859, "created_utc": 1669749112.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Thank you for your input.  My current SSD just failed on me.", "author_fullname": "t2_11dj0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a good price for SAMSUNG 870 EVO 4TB 2.5 that is brand new?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z81uny", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669746516.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thank you for your input.  My current SSD just failed on me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "30TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z81uny", "is_robot_indexable": true, "report_reasons": null, "author": "nando1969", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z81uny/what_is_a_good_price_for_samsung_870_evo_4tb_25/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z81uny/what_is_a_good_price_for_samsung_870_evo_4tb_25/", "subreddit_subscribers": 656859, "created_utc": 1669746516.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nWhen I copy files from a source to iDrive e2 using rclone, I am getting a lot of errors from e2:\n\n\"upload corrupted: Etag differ:\"\n\nrclone then retries and succeeds.\n\nIs that normal?", "author_fullname": "t2_lwumyho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "iDrive e2 (S3-compatible storage): lots of errors \"upload corrupted: Etag differ:\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z81oge", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669746121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;When I copy files from a source to iDrive e2 using rclone, I am getting a lot of errors from e2:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;upload corrupted: Etag differ:&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;rclone then retries and succeeds.&lt;/p&gt;\n\n&lt;p&gt;Is that normal?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z81oge", "is_robot_indexable": true, "report_reasons": null, "author": "TedBob99", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z81oge/idrive_e2_s3compatible_storage_lots_of_errors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z81oge/idrive_e2_s3compatible_storage_lots_of_errors/", "subreddit_subscribers": 656859, "created_utc": 1669746121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for a solution for cloning images of my drives that works for Linux systems and supports incremental backups. My current solution is Clonezilla. Works really well but requires me to clone the entire drive every time.\n\nI've combed through the community posts. Many seem to recommend Macrium Reflect. The posts are a bit on the older side, though. Is Macrium still a good option? \n\nAlso, not strictly necessary; but is there an option that allows me to keep using my system as it does the backup? I don't think even Macrium allows that on Linux systems.\n\nI'd really appreciate any guidance.", "author_fullname": "t2_40b70wjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental drive imaging solutions for Linux.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z80szt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669744183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a solution for cloning images of my drives that works for Linux systems and supports incremental backups. My current solution is Clonezilla. Works really well but requires me to clone the entire drive every time.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve combed through the community posts. Many seem to recommend Macrium Reflect. The posts are a bit on the older side, though. Is Macrium still a good option? &lt;/p&gt;\n\n&lt;p&gt;Also, not strictly necessary; but is there an option that allows me to keep using my system as it does the backup? I don&amp;#39;t think even Macrium allows that on Linux systems.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d really appreciate any guidance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z80szt", "is_robot_indexable": true, "report_reasons": null, "author": "Ushahin_Ceann", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z80szt/incremental_drive_imaging_solutions_for_linux/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z80szt/incremental_drive_imaging_solutions_for_linux/", "subreddit_subscribers": 656859, "created_utc": 1669744183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to add data to one device in one location and have it sync nightly or hourly with another NAS in a different location on another private network without creating a VPN tunnel.", "author_fullname": "t2_9vbi3ot", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I mirror sync two Synology DS220 across two physical locations without creating a WAN tunnel?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z8uvjc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669823380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to add data to one device in one location and have it sync nightly or hourly with another NAS in a different location on another private network without creating a VPN tunnel.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8uvjc", "is_robot_indexable": true, "report_reasons": null, "author": "ambient_whooshing", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8uvjc/can_i_mirror_sync_two_synology_ds220_across_two/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8uvjc/can_i_mirror_sync_two_synology_ds220_across_two/", "subreddit_subscribers": 656859, "created_utc": 1669823380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I run Linux Mint. I'm currently doing sync with help of my custom Python script developed to get rid of duplicates and manual operations. As of now I mostly add data to say couple of backup disks (add to one or the other, sync one to other with checksum control), connected via USB (local disks). Two-way sync is important for me, I tend to modify not one disk.\n\nI want to perform large re-shuffle (move/rename folders/files) of my data (and be able to do so in the future efficiently and easily). I want to be able to do it in GUI for one disk/location and be able to quickly propagate changes to other disk, both ways. Is there any software that can do that?\n\nWhat I've looked at so far:\n\n`rsync` - that classic does not identify moves/renames, I recall reading developers talk about it but it did not get into mainstream as of now AFAIK.\n\n`Syncthing` does not do local per FAQ.\n\n`Unison` docs say \"Unison sees the rename as a delete and a separate create\", I've tried it and indeed looks so.\n\n`btrfs` snapshots. Not sure yet, but in https://www.reddit.com/r/btrfs/comments/odpybp/btrfs_to_share_data_between_machines/ : \"Two-way syncing is not possible using btrfs snapshots and btrfs send/receive\"; \"I would not recommend them for backup purposes however; relying on filesystem-specific structures is not sustainable.\". \n\nIf there none is found, how difficult it is to use `inotify()` to get list of moves of inodes? Does `inotify()` provide information same as in `mv path1 path2` command?\n\nBack to `Syncthing`. If somebody has extensive experience, sees it can do what I want and can explain how to set up one PC so that it runs two `Syncthing` instances that will communicate and sync - you are welcome.\n\nTIA", "author_fullname": "t2_o727buez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any free Linux local file syncing software that efficiently handles moves and do checksum?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8ti34", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669821088.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669820074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run Linux Mint. I&amp;#39;m currently doing sync with help of my custom Python script developed to get rid of duplicates and manual operations. As of now I mostly add data to say couple of backup disks (add to one or the other, sync one to other with checksum control), connected via USB (local disks). Two-way sync is important for me, I tend to modify not one disk.&lt;/p&gt;\n\n&lt;p&gt;I want to perform large re-shuffle (move/rename folders/files) of my data (and be able to do so in the future efficiently and easily). I want to be able to do it in GUI for one disk/location and be able to quickly propagate changes to other disk, both ways. Is there any software that can do that?&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;ve looked at so far:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;rsync&lt;/code&gt; - that classic does not identify moves/renames, I recall reading developers talk about it but it did not get into mainstream as of now AFAIK.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Syncthing&lt;/code&gt; does not do local per FAQ.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Unison&lt;/code&gt; docs say &amp;quot;Unison sees the rename as a delete and a separate create&amp;quot;, I&amp;#39;ve tried it and indeed looks so.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;btrfs&lt;/code&gt; snapshots. Not sure yet, but in &lt;a href=\"https://www.reddit.com/r/btrfs/comments/odpybp/btrfs_to_share_data_between_machines/\"&gt;https://www.reddit.com/r/btrfs/comments/odpybp/btrfs_to_share_data_between_machines/&lt;/a&gt; : &amp;quot;Two-way syncing is not possible using btrfs snapshots and btrfs send/receive&amp;quot;; &amp;quot;I would not recommend them for backup purposes however; relying on filesystem-specific structures is not sustainable.&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;If there none is found, how difficult it is to use &lt;code&gt;inotify()&lt;/code&gt; to get list of moves of inodes? Does &lt;code&gt;inotify()&lt;/code&gt; provide information same as in &lt;code&gt;mv path1 path2&lt;/code&gt; command?&lt;/p&gt;\n\n&lt;p&gt;Back to &lt;code&gt;Syncthing&lt;/code&gt;. If somebody has extensive experience, sees it can do what I want and can explain how to set up one PC so that it runs two &lt;code&gt;Syncthing&lt;/code&gt; instances that will communicate and sync - you are welcome.&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8ti34", "is_robot_indexable": true, "report_reasons": null, "author": "UncertainAboutIt", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8ti34/any_free_linux_local_file_syncing_software_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8ti34/any_free_linux_local_file_syncing_software_that/", "subreddit_subscribers": 656859, "created_utc": 1669820074.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I managed to save a huge treasure trove of rare audio content that could be deleted by the site at any time. To be able to rip the site, I injected simple javascript that replaced the audio player with chronologically listing the mp3-files from the player as &lt;a&gt; tags.\n\nUnfortunately, the audio files are quite randomly named, and often not ID3-tagged at all. \n\nHere are the two relevant parts of a podcast HTML page:\n\n**Metadata part:**\n\n    &lt;script type=\"application/ld+json\"&gt;\n        {\n            \"@context\": \"https://schema.org/\",\n            \"@type\": \"Podcast\",\n            \"@id\": \"https://podcast.com/podcast/sidsel-dalen-21-netter/#player\",\n            \"abridged\": \"false\",\n            \"requiresSubscription\": \"false\",\n            \"author\": \"Author Name\",\n            \"name\": \"21 netter\",\n            \"description\": \"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\",\n            \"datePublished\": \"2010\",\n            \"character\": \"Mia Piasen\",\n            \"keywords\": \"Mia Piasen\",\n            \"duration\": \"PT10H46M00S\",\n            \"genre\": \"Krimpodcast\",\n            \"readBy\": \"Hanna Jakobsen\"\n            }\n        }\n    &lt;/script&gt;\n\n**File link part:**\n\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6771.mp3\" &gt;track_6771.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6772.mp3\" &gt;track_6772.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6773.mp3\" &gt;track_6773.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6774.mp3\" &gt;track_6774.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6775.mp3\" &gt;track_6775.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6776.mp3\" &gt;track_6776.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6777.mp3\" &gt;track_6777.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6778.mp3\" &gt;track_6778.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6779.mp3\" &gt;track_6779.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6780.mp3\" &gt;track_6780.mp3&lt;/a&gt;&lt;br&gt;\n\nHow do I extract that metadata info from the HTML page and use it to rename, move and ID3-tag the linked MP3 files in the right order?", "author_fullname": "t2_3dqo0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to rename, move and ID3-tag MP3 files with info and order from HTML pages that link to them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8p2md", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669807530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I managed to save a huge treasure trove of rare audio content that could be deleted by the site at any time. To be able to rip the site, I injected simple javascript that replaced the audio player with chronologically listing the mp3-files from the player as &amp;lt;a&amp;gt; tags.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, the audio files are quite randomly named, and often not ID3-tagged at all. &lt;/p&gt;\n\n&lt;p&gt;Here are the two relevant parts of a podcast HTML page:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Metadata part:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;lt;script type=&amp;quot;application/ld+json&amp;quot;&amp;gt;\n    {\n        &amp;quot;@context&amp;quot;: &amp;quot;https://schema.org/&amp;quot;,\n        &amp;quot;@type&amp;quot;: &amp;quot;Podcast&amp;quot;,\n        &amp;quot;@id&amp;quot;: &amp;quot;https://podcast.com/podcast/sidsel-dalen-21-netter/#player&amp;quot;,\n        &amp;quot;abridged&amp;quot;: &amp;quot;false&amp;quot;,\n        &amp;quot;requiresSubscription&amp;quot;: &amp;quot;false&amp;quot;,\n        &amp;quot;author&amp;quot;: &amp;quot;Author Name&amp;quot;,\n        &amp;quot;name&amp;quot;: &amp;quot;21 netter&amp;quot;,\n        &amp;quot;description&amp;quot;: &amp;quot;&amp;quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&amp;quot;&amp;quot;,\n        &amp;quot;datePublished&amp;quot;: &amp;quot;2010&amp;quot;,\n        &amp;quot;character&amp;quot;: &amp;quot;Mia Piasen&amp;quot;,\n        &amp;quot;keywords&amp;quot;: &amp;quot;Mia Piasen&amp;quot;,\n        &amp;quot;duration&amp;quot;: &amp;quot;PT10H46M00S&amp;quot;,\n        &amp;quot;genre&amp;quot;: &amp;quot;Krimpodcast&amp;quot;,\n        &amp;quot;readBy&amp;quot;: &amp;quot;Hanna Jakobsen&amp;quot;\n        }\n    }\n&amp;lt;/script&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;File link part:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6771.mp3&amp;quot; &amp;gt;track_6771.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6772.mp3&amp;quot; &amp;gt;track_6772.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6773.mp3&amp;quot; &amp;gt;track_6773.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6774.mp3&amp;quot; &amp;gt;track_6774.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6775.mp3&amp;quot; &amp;gt;track_6775.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6776.mp3&amp;quot; &amp;gt;track_6776.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6777.mp3&amp;quot; &amp;gt;track_6777.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6778.mp3&amp;quot; &amp;gt;track_6778.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6779.mp3&amp;quot; &amp;gt;track_6779.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6780.mp3&amp;quot; &amp;gt;track_6780.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;How do I extract that metadata info from the HTML page and use it to rename, move and ID3-tag the linked MP3 files in the right order?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8p2md", "is_robot_indexable": true, "report_reasons": null, "author": "kris33", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8p2md/how_to_rename_move_and_id3tag_mp3_files_with_info/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8p2md/how_to_rename_move_and_id3tag_mp3_files_with_info/", "subreddit_subscribers": 656859, "created_utc": 1669807530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I\u2019m trying to sort a portable RAID for my ancient. I\u2019ve bought a pair of 2.5\u201d 5GB Seagate drives and a Rugged Raid Pro to swap them into, only to find out, the damn thing won\u2019t fit 15mm disks - massive research error on my end.\n\nDoes anyone know a similarly small factor external RAID drive or RAID enclosure I could fit with these bad boys? I\u2019ve looked at the Akitio side-by-side enclosure (SK-2520 U3.1) but I can\u2019t get hold of it in the UK. Just asked a retailer about specs for a Raidon unit too, waiting for their reply. If anyone has any suggestions, I\u2019d be ever so grateful. Ty!", "author_fullname": "t2_4klbdouw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for portable external raid enclosure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8og7a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669805418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I\u2019m trying to sort a portable RAID for my ancient. I\u2019ve bought a pair of 2.5\u201d 5GB Seagate drives and a Rugged Raid Pro to swap them into, only to find out, the damn thing won\u2019t fit 15mm disks - massive research error on my end.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know a similarly small factor external RAID drive or RAID enclosure I could fit with these bad boys? I\u2019ve looked at the Akitio side-by-side enclosure (SK-2520 U3.1) but I can\u2019t get hold of it in the UK. Just asked a retailer about specs for a Raidon unit too, waiting for their reply. If anyone has any suggestions, I\u2019d be ever so grateful. Ty!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8og7a", "is_robot_indexable": true, "report_reasons": null, "author": "zolbear", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8og7a/looking_for_portable_external_raid_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8og7a/looking_for_portable_external_raid_enclosure/", "subreddit_subscribers": 656859, "created_utc": 1669805418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently struggling to come up with a good data integrity check for my backups. I am using \\[restic\\]([https://restic.net/](https://restic.net/)) (which has a lot of the same features as borg) as a backup engine. It already includes \\[integrity and consistency checks\\]([https://restic.readthedocs.io/en/stable/045\\_working\\_with\\_repos.html#checking-integrity-and-consistency](https://restic.readthedocs.io/en/stable/045_working_with_repos.html#checking-integrity-and-consistency)) but I feel like the described \\`check\\` command still leaves room for silently corrupted data, even with its \\`--read-data\\` flag : the integrity seems to be checked against hashes of already-packed data, but what if something happened during the packing process ?\n\nI'm looking for a integrity check scheme that checks restored data against checksums computed from the originally backed up data, such as described in \\[this issue I opened on the restic repo\\]([https://github.com/restic/restic/issues/4057](https://github.com/restic/restic/issues/4057)). I'm aware this kind of scheme must take into account the possibility of the data mutating on the source disk while being backed up.\n\nIs the Restic integrity check enough ? Am I being paranoid ? Are you aware of any way to achieve my goal ?\n\nThank you to anyone who takes the time to answer, even with an incomplete answer. I'm usually decent at googling, but I'm not having luck with this one.", "author_fullname": "t2_2lq2eg3f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What makes a good backup integrity check ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8ncss", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669801625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently struggling to come up with a good data integrity check for my backups. I am using [restic](&lt;a href=\"https://restic.net/\"&gt;https://restic.net/&lt;/a&gt;) (which has a lot of the same features as borg) as a backup engine. It already includes [integrity and consistency checks](&lt;a href=\"https://restic.readthedocs.io/en/stable/045_working_with_repos.html#checking-integrity-and-consistency\"&gt;https://restic.readthedocs.io/en/stable/045_working_with_repos.html#checking-integrity-and-consistency&lt;/a&gt;) but I feel like the described `check` command still leaves room for silently corrupted data, even with its `--read-data` flag : the integrity seems to be checked against hashes of already-packed data, but what if something happened during the packing process ?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a integrity check scheme that checks restored data against checksums computed from the originally backed up data, such as described in [this issue I opened on the restic repo](&lt;a href=\"https://github.com/restic/restic/issues/4057\"&gt;https://github.com/restic/restic/issues/4057&lt;/a&gt;). I&amp;#39;m aware this kind of scheme must take into account the possibility of the data mutating on the source disk while being backed up.&lt;/p&gt;\n\n&lt;p&gt;Is the Restic integrity check enough ? Am I being paranoid ? Are you aware of any way to achieve my goal ?&lt;/p&gt;\n\n&lt;p&gt;Thank you to anyone who takes the time to answer, even with an incomplete answer. I&amp;#39;m usually decent at googling, but I&amp;#39;m not having luck with this one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8ncss", "is_robot_indexable": true, "report_reasons": null, "author": "pcouy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8ncss/what_makes_a_good_backup_integrity_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8ncss/what_makes_a_good_backup_integrity_check/", "subreddit_subscribers": 656859, "created_utc": 1669801625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI've been working on a project that requires to convert a website to static HTML.\n\nSo far I've been working with Heritrix and I love it.\nAfter that, I use Warcat to concatenate every .warc.gz files into one,\u00a0and then I use\u00a0warc2html to convert that warc.gz into static HTML files.\n\nHowever, the warc2html project doesn't always give the best results.\nDoes anyone have any feedback/tools that might be useful for me?\n\nThanks!", "author_fullname": "t2_1naagq16", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Warc to Static HTML | Heritrix -&gt; Warcat -&gt; warc2html", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8n0rg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669800393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working on a project that requires to convert a website to static HTML.&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve been working with Heritrix and I love it.\nAfter that, I use Warcat to concatenate every .warc.gz files into one,\u00a0and then I use\u00a0warc2html to convert that warc.gz into static HTML files.&lt;/p&gt;\n\n&lt;p&gt;However, the warc2html project doesn&amp;#39;t always give the best results.\nDoes anyone have any feedback/tools that might be useful for me?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8n0rg", "is_robot_indexable": true, "report_reasons": null, "author": "xhicoBala", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8n0rg/warc_to_static_html_heritrix_warcat_warc2html/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8n0rg/warc_to_static_html_heritrix_warcat_warc2html/", "subreddit_subscribers": 656859, "created_utc": 1669800393.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}