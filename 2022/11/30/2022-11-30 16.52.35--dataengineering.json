{"kind": "Listing", "data": {"after": "t3_z8uv35", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Don't get me wrong, I know how hard things are for H1bs employees, and how hard they work. My spouse went through the whole F1, H1b situation. I've worked with a ton of people on H1bs, and a lot of my family friends have gone through/are in the situation. \n\nI'm just a bit frustrated because it feels like I've encountered some hiring managers who immigrated to the US whose entire teams are H1b employees. I've done a ton of interviews recently, and the data is heavily skewed towards rejection when this is the case, and I do fine/get offers in other cases. \n\nHas anyone had a similar experience? \n\nBy the way - this is in no way speaking negatively about anyone, and I know that being a US citizen gives me a ton of unfair advantageous.", "author_fullname": "t2_2djqd7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you noticed that some tech companies have a disproportionate amount of H1bs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8cvtv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 62, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 62, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669771610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Don&amp;#39;t get me wrong, I know how hard things are for H1bs employees, and how hard they work. My spouse went through the whole F1, H1b situation. I&amp;#39;ve worked with a ton of people on H1bs, and a lot of my family friends have gone through/are in the situation. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just a bit frustrated because it feels like I&amp;#39;ve encountered some hiring managers who immigrated to the US whose entire teams are H1b employees. I&amp;#39;ve done a ton of interviews recently, and the data is heavily skewed towards rejection when this is the case, and I do fine/get offers in other cases. &lt;/p&gt;\n\n&lt;p&gt;Has anyone had a similar experience? &lt;/p&gt;\n\n&lt;p&gt;By the way - this is in no way speaking negatively about anyone, and I know that being a US citizen gives me a ton of unfair advantageous.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8cvtv", "is_robot_indexable": true, "report_reasons": null, "author": "aznpersuazion", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8cvtv/have_you_noticed_that_some_tech_companies_have_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8cvtv/have_you_noticed_that_some_tech_companies_have_a/", "subreddit_subscribers": 81449, "created_utc": 1669771610.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_f7rx2o8s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "7 SQL Concepts You Should Know For Data Science - KDnuggets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_z8cen5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/paYLEadHXsFPRII12Dv5XsdYxjnm1uAcrcHMPf10V_g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669770449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kdnuggets.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.kdnuggets.com/2022/11/7-sql-concepts-needed-data-science.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QH9u-rzCZwrPb6832HFNTR0s99cCDKbXOufENA1myBc.jpg?auto=webp&amp;s=54b7d22b64d8464da18f8d7b8935411661d0d102", "width": 1000, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/QH9u-rzCZwrPb6832HFNTR0s99cCDKbXOufENA1myBc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e9ec91e12b022e3204de0923d8e5477e58775905", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/QH9u-rzCZwrPb6832HFNTR0s99cCDKbXOufENA1myBc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2fba71f65b78de007621731146aa18015724407b", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/QH9u-rzCZwrPb6832HFNTR0s99cCDKbXOufENA1myBc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ea700a89015dce36b15eb8aabec7eadbc485d6ab", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/QH9u-rzCZwrPb6832HFNTR0s99cCDKbXOufENA1myBc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=191ff9f3fa21e2f22f1b1666cc1b7ddbe4c4d0a7", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/QH9u-rzCZwrPb6832HFNTR0s99cCDKbXOufENA1myBc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e9d05210c64f9ddf9f9273532fe2bd85fc7879ce", "width": 960, "height": 576}], "variants": {}, "id": "xfS9I3SxennKVAgTfIDGPR-4hTpErjjKeDvjMrw9jTE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z8cen5", "is_robot_indexable": true, "report_reasons": null, "author": "No_Molasses_3859", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8cen5/7_sql_concepts_you_should_know_for_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.kdnuggets.com/2022/11/7-sql-concepts-needed-data-science.html", "subreddit_subscribers": 81449, "created_utc": 1669770449.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r6aazfpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Memgraph vs. Neo4j: A Performance Comparison", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_z8t2yh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pGCuKmdGp672BJqSaE0OQtFYrRLBUYQyLpy0DvrLlZY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669819012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memgraph.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memgraph.com/blog/memgraph-vs-neo4j-performance-benchmark-comparison", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?auto=webp&amp;s=c95f5fb4d03e8ee611fece6ca16ee53e9319cd47", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=34dd1c0af0ca1a777fe9c0b7a2eea3b8b7ca7ebe", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d4e40aa2cf0bbdfd747722ca56e6594c404b2b5", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d1b55a81f746a581c5337c562f40a4d95b3dd7b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=32bd87722c79868fc64f921f5df724e38061f45e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c019ea13d6df50a75187792bac5d77ef50aaf7ca", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2edd5a8066f88e8dc8843d382e27c80d6d4e85b3", "width": 1080, "height": 540}], "variants": {}, "id": "xy6fojG3rvgz9t12Plo1ohQk81wf46np_f7PqRZPdAw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z8t2yh", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Cap6526", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8t2yh/memgraph_vs_neo4j_a_performance_comparison/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memgraph.com/blog/memgraph-vs-neo4j-performance-benchmark-comparison", "subreddit_subscribers": 81449, "created_utc": 1669819012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I believe C-level and Data Teams alike are facing an ROI problem regarding Data-usage :   \n\n\nC-levels want to monitor the cost of their data platform at a granular level (i.e. \"Are the new business capabilities unlocked by our Data Platform worth the price ?\")   \n\n\nData Engineering teams want to monitor the usage of the tables and models they put in the hands of business units (i.e. \"Are we working for nothing ?\")   \n\n\nIf you are / were in any of those positions, what metrics would you like to follow to understand data usage and cost withing your company ?", "author_fullname": "t2_dfq2nv8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you measure Data Usage &amp; Cost ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z809cl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669742968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I believe C-level and Data Teams alike are facing an ROI problem regarding Data-usage :   &lt;/p&gt;\n\n&lt;p&gt;C-levels want to monitor the cost of their data platform at a granular level (i.e. &amp;quot;Are the new business capabilities unlocked by our Data Platform worth the price ?&amp;quot;)   &lt;/p&gt;\n\n&lt;p&gt;Data Engineering teams want to monitor the usage of the tables and models they put in the hands of business units (i.e. &amp;quot;Are we working for nothing ?&amp;quot;)   &lt;/p&gt;\n\n&lt;p&gt;If you are / were in any of those positions, what metrics would you like to follow to understand data usage and cost withing your company ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z809cl", "is_robot_indexable": true, "report_reasons": null, "author": "Kind-Philosophy-9612", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z809cl/how_would_you_measure_data_usage_cost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z809cl/how_would_you_measure_data_usage_cost/", "subreddit_subscribers": 81449, "created_utc": 1669742968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3kxbd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hey Snowflake, send me an email", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_z84nmu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": "#46d160", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/i2IC1rRzLqVNpz5-_a6c3r1j9BqPL1kxEQ7ctKYDN2Y.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669752700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hoffa.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://hoffa.medium.com/hey-snowflake-send-me-an-email-243741a0fe3", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BBYGNlChqSwqoZAHtti_IFEUrXWnZkAfW1i0i0isDuU.jpg?auto=webp&amp;s=fc17dbb04093fd5b00325689b39b0537bde1fceb", "width": 576, "height": 384}, "resolutions": [{"url": "https://external-preview.redd.it/BBYGNlChqSwqoZAHtti_IFEUrXWnZkAfW1i0i0isDuU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4228863116462f40dd4356310fd6457f108b9f8", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/BBYGNlChqSwqoZAHtti_IFEUrXWnZkAfW1i0i0isDuU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7d5f9d28990c34834bb782b9019afc45a8e9014", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/BBYGNlChqSwqoZAHtti_IFEUrXWnZkAfW1i0i0isDuU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b58692b030be9f064606805e9fcc75791ba9a8b6", "width": 320, "height": 213}], "variants": {}, "id": "Hp2eKZv_as4Ck3asWrPQz-vgEMJ1brMwQh6sN_piWo4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "honorary mod | Snowflake", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z84nmu", "is_robot_indexable": true, "report_reasons": null, "author": "fhoffa", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/z84nmu/hey_snowflake_send_me_an_email/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://hoffa.medium.com/hey-snowflake-send-me-an-email-243741a0fe3", "subreddit_subscribers": 81449, "created_utc": 1669752700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Forrester Wave just released a report on \"Translytical\" databases. You could also probably refer to this category as [HTAP](https://en.wikipedia.org/wiki/Hybrid_transactional/analytical_processing) with possible exceptions. \n\nIMO, only a few in this report are actually designed for this category (and Oracle scoring perfect for \"product strategy\" is questionable)...but that's neither here nor there.\n\nI'm curious what you all think about this database type. \n\nWith ETL being most often used for transforming OLTP data into OLAP data into a separate data serving layer altogether, what would it take for a hybrid database to circumvent the need for a lot of that ETL? Granted, it's unlikely ETL would ever truly be replaced but there are certainly some use cases that lend themselves to this.\n\nThoughts?", "author_fullname": "t2_3wpf9ifb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Translytical/HTAP databases: What does DE need from these?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z850vy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669753493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Forrester Wave just released a report on &amp;quot;Translytical&amp;quot; databases. You could also probably refer to this category as &lt;a href=\"https://en.wikipedia.org/wiki/Hybrid_transactional/analytical_processing\"&gt;HTAP&lt;/a&gt; with possible exceptions. &lt;/p&gt;\n\n&lt;p&gt;IMO, only a few in this report are actually designed for this category (and Oracle scoring perfect for &amp;quot;product strategy&amp;quot; is questionable)...but that&amp;#39;s neither here nor there.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious what you all think about this database type. &lt;/p&gt;\n\n&lt;p&gt;With ETL being most often used for transforming OLTP data into OLAP data into a separate data serving layer altogether, what would it take for a hybrid database to circumvent the need for a lot of that ETL? Granted, it&amp;#39;s unlikely ETL would ever truly be replaced but there are certainly some use cases that lend themselves to this.&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z850vy", "is_robot_indexable": true, "report_reasons": null, "author": "samhld", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z850vy/translyticalhtap_databases_what_does_de_need_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z850vy/translyticalhtap_databases_what_does_de_need_from/", "subreddit_subscribers": 81449, "created_utc": 1669753493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "  \n\nDear Data Engineers, \n\nI am a sociologist doing my doctorate in sociology at the University of Potsdam. In the context of my doctoral thesis, I am investigating the personal understanding of work and the work practice of Data Engineers.\n\nFor my study I am looking for people who are professionally active as Data Engineers, whom I can interview about their daily work routine. I am particularly interested in your personal work practices, i.e. \"HOW\" you do it in your professional work. I am particularly interested in your approach to problem solving and negotiation processes for finding solutions. I would like to conduct an interview with you, which should take about one hour. The interview can be conducted in presence or digitally, as desired. In both cases, an audio recording will be made for empirical analysis. All personal data will be anonymized.\n\nThe increasing number of users and companies using AI-based solutions makes your field particularly interesting for a sociological analysis. Therefore, I would be very pleased if you would be interested and have the time. \n\nWith kind regards", "author_fullname": "t2_jpt9q8kf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Phd Interview Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8qbd6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669811512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear Data Engineers, &lt;/p&gt;\n\n&lt;p&gt;I am a sociologist doing my doctorate in sociology at the University of Potsdam. In the context of my doctoral thesis, I am investigating the personal understanding of work and the work practice of Data Engineers.&lt;/p&gt;\n\n&lt;p&gt;For my study I am looking for people who are professionally active as Data Engineers, whom I can interview about their daily work routine. I am particularly interested in your personal work practices, i.e. &amp;quot;HOW&amp;quot; you do it in your professional work. I am particularly interested in your approach to problem solving and negotiation processes for finding solutions. I would like to conduct an interview with you, which should take about one hour. The interview can be conducted in presence or digitally, as desired. In both cases, an audio recording will be made for empirical analysis. All personal data will be anonymized.&lt;/p&gt;\n\n&lt;p&gt;The increasing number of users and companies using AI-based solutions makes your field particularly interesting for a sociological analysis. Therefore, I would be very pleased if you would be interested and have the time. &lt;/p&gt;\n\n&lt;p&gt;With kind regards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "z8qbd6", "is_robot_indexable": true, "report_reasons": null, "author": "SozUngl", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8qbd6/phd_interview_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8qbd6/phd_interview_data_engineers/", "subreddit_subscribers": 81449, "created_utc": 1669811512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have any of you guys moved abroad and kept your US salary, Im getting ready to do this for a Major Investment Bank with many offices across the world in about a year or two.\n\nCancun Mexico, Perth Australia or Portugal seem nice and Im at about 160k USD with 3 years exp.\n\nI know some folks did this during the pandemic and they liked it and saved a ton of cash and bought properties just curious to see if any of you did a similar route and how did you convert your USD in a local currency.", "author_fullname": "t2_225yi2s2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving out of the US but keeping US salary", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z88tl8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669762087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have any of you guys moved abroad and kept your US salary, Im getting ready to do this for a Major Investment Bank with many offices across the world in about a year or two.&lt;/p&gt;\n\n&lt;p&gt;Cancun Mexico, Perth Australia or Portugal seem nice and Im at about 160k USD with 3 years exp.&lt;/p&gt;\n\n&lt;p&gt;I know some folks did this during the pandemic and they liked it and saved a ton of cash and bought properties just curious to see if any of you did a similar route and how did you convert your USD in a local currency.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z88tl8", "is_robot_indexable": true, "report_reasons": null, "author": "Analyst214", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z88tl8/moving_out_of_the_us_but_keeping_us_salary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z88tl8/moving_out_of_the_us_but_keeping_us_salary/", "subreddit_subscribers": 81449, "created_utc": 1669762087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "(Hopefully this is the right sub - but please feel free to direct me elsewhere if not!)\n\nExciting times are happening at my company -leadership has finally recognized that it's time to move away from excel and use tools that are way better for processing large datasets - enough so they are putting company resources towards it (time, staffing, $$, etc.). Yay!\n\nWe have spent the last few months piloting R &amp; Python - which essentially means that teams have been re-writing  standard business rules and data processes that use to live in excel using both of these languages.  (The reason we used these languages was bc of the skill sets and experiences that different people had on each team).\n\nIt's been going great so far (albeit some obvious ways we need to improve and scale up).  However, we have now come to a decision point:  **Do we now coalesce around one language? If so, which one?  Or do we make available both languages so that we can meet people's skillsets/interests wherever they lie?** \n\nAt the beginning of the pilot I assumed we would go all in on R (most people come in now with some R experience due to MA programs, etc.) but now that we have Python scripts available and see how great that is - I am torn!  Although, having both would obviously make knowledge management, version control, onboarding and training more complicated.. e.g., if one team uses R and innovates on a methodology for one of our standard analyses then we would have to make sure that innovation get's reflected in the Python version as well. \n\nI am the person helping to lead the pilots and am responsible for making a formal recommendation to the company so thought it would be really great and helpful to hear how other companies and you experts do it! Do you have all multiple languages available and employees can choose which one they use? and why?\n\nSome more context:\n\n* We are smallish consulting company (&lt;100 people) whose projects are 80% analytical, so data savvy people but NOT software engineers or computer programmers and we don't necessarily higer for coding skills (but would plan to train and onboard like we do with excel)\n* The primary purpose of these languages is for the heavy data manipulation/cleaning/processing and analysis on tabular data (NOT to build tools or applications) - I know this screams \"Use R\" but still want to validate with the experts :) \n\nThanks in advance for any insight or help you can provide!", "author_fullname": "t2_4m9ye", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What data processing language(s) do(es) your company use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z7zt7e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669741941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Hopefully this is the right sub - but please feel free to direct me elsewhere if not!)&lt;/p&gt;\n\n&lt;p&gt;Exciting times are happening at my company -leadership has finally recognized that it&amp;#39;s time to move away from excel and use tools that are way better for processing large datasets - enough so they are putting company resources towards it (time, staffing, $$, etc.). Yay!&lt;/p&gt;\n\n&lt;p&gt;We have spent the last few months piloting R &amp;amp; Python - which essentially means that teams have been re-writing  standard business rules and data processes that use to live in excel using both of these languages.  (The reason we used these languages was bc of the skill sets and experiences that different people had on each team).&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been going great so far (albeit some obvious ways we need to improve and scale up).  However, we have now come to a decision point:  &lt;strong&gt;Do we now coalesce around one language? If so, which one?  Or do we make available both languages so that we can meet people&amp;#39;s skillsets/interests wherever they lie?&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;At the beginning of the pilot I assumed we would go all in on R (most people come in now with some R experience due to MA programs, etc.) but now that we have Python scripts available and see how great that is - I am torn!  Although, having both would obviously make knowledge management, version control, onboarding and training more complicated.. e.g., if one team uses R and innovates on a methodology for one of our standard analyses then we would have to make sure that innovation get&amp;#39;s reflected in the Python version as well. &lt;/p&gt;\n\n&lt;p&gt;I am the person helping to lead the pilots and am responsible for making a formal recommendation to the company so thought it would be really great and helpful to hear how other companies and you experts do it! Do you have all multiple languages available and employees can choose which one they use? and why?&lt;/p&gt;\n\n&lt;p&gt;Some more context:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We are smallish consulting company (&amp;lt;100 people) whose projects are 80% analytical, so data savvy people but NOT software engineers or computer programmers and we don&amp;#39;t necessarily higer for coding skills (but would plan to train and onboard like we do with excel)&lt;/li&gt;\n&lt;li&gt;The primary purpose of these languages is for the heavy data manipulation/cleaning/processing and analysis on tabular data (NOT to build tools or applications) - I know this screams &amp;quot;Use R&amp;quot; but still want to validate with the experts :) &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks in advance for any insight or help you can provide!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z7zt7e", "is_robot_indexable": true, "report_reasons": null, "author": "davidowj", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z7zt7e/what_data_processing_languages_does_your_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z7zt7e/what_data_processing_languages_does_your_company/", "subreddit_subscribers": 81449, "created_utc": 1669741941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, looking for advice on a lakehouse architecture pattern in databricks with aws. \n\nWe are looking to stream raw json data (mongoDb) stored in S3 to tables in Unity Catalog. We currently have a pipeline that does this using DLT, however, not being able to write to UC or set persistent views to UC is a deal breaker for us. However, this pipeline's ability to create a raw table (json is read into a single column with a timestamp) and then use a defined schema for a subsequent table has been very nice for us. \n\nCurrently experimenting with autoloader/spark streaming to S3, however, a limitation there is that you can't use autoloader in continuous mode with UC. \n\nWe are looking for an agile, scalable solution that is going to be able to handle at least close to continuous streaming with minimal latency (very very denormalized data doesn't help here...), any advice appreciated", "author_fullname": "t2_2nfde5yl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Lakehouse architecture advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z89hwi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669763614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, looking for advice on a lakehouse architecture pattern in databricks with aws. &lt;/p&gt;\n\n&lt;p&gt;We are looking to stream raw json data (mongoDb) stored in S3 to tables in Unity Catalog. We currently have a pipeline that does this using DLT, however, not being able to write to UC or set persistent views to UC is a deal breaker for us. However, this pipeline&amp;#39;s ability to create a raw table (json is read into a single column with a timestamp) and then use a defined schema for a subsequent table has been very nice for us. &lt;/p&gt;\n\n&lt;p&gt;Currently experimenting with autoloader/spark streaming to S3, however, a limitation there is that you can&amp;#39;t use autoloader in continuous mode with UC. &lt;/p&gt;\n\n&lt;p&gt;We are looking for an agile, scalable solution that is going to be able to handle at least close to continuous streaming with minimal latency (very very denormalized data doesn&amp;#39;t help here...), any advice appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z89hwi", "is_robot_indexable": true, "report_reasons": null, "author": "pmarct", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z89hwi/databricks_lakehouse_architecture_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z89hwi/databricks_lakehouse_architecture_advice/", "subreddit_subscribers": 81449, "created_utc": 1669763614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys! Many of us in the data science and ML world work with **multi-label data**, where the image or text is tagged with multiple labels. Often these datasets contain **frequent label errors** and/or **missing tags** (check what we found below in the CelebA dataset) that make it hard to train highly accurate ML models. Support for multi-label data was one of the top features requested \u2014 so we [added it](https://docs.cleanlab.ai/stable/tutorials/multilabel_classification.html), [benchmarked it](https://cleanlab.ai/blog/multilabel/), and published all of the [research](https://cleanlab.ai/blog/multilabel/).\n\n[Find errors and missing labels in multi-label datasets.](https://preview.redd.it/duwshue6lx2a1.png?width=1250&amp;format=png&amp;auto=webp&amp;s=2bf66e0e822cebd4c00eb5f4177265cb99275132)\n\nWe are excited to share this newest research on algorithms to automatically find label errors in multi-label classification datasets. Image/document tagging represents important instances of\u00a0**multi-label classification**\u00a0tasks, where each example can belong to multiple (or none) of K possible classes. Because annotating such data requires many decisions for each example, often multi-label classification datasets contain tons of label errors, which harm the performance of ML models.\n\nWe\u2019ve open-sourced our algorithms in the [recent release of cleanlab v2.2](https://github.com/cleanlab/cleanlab/releases/tag/v2.2.0). All you need to do to use them is write one line of open-source code via [cleanlab.filter.find\\_label\\_issues](https://docs.cleanlab.ai/stable/tutorials/multilabel_classification.html).\n\n    from cleanlab.filter import find_label_issues\n    \n    ranked_label_issues = find_label_issues(\n        labels=labels,\n        pred_probs=pred_probs,\n        multi_label=True,\n        return_indices_ranked_by=\"self_confidence\",\n    )\n    # labels: list of lists of (multiple) labels of each example\n    # pred_probs: predicted class probabilities from any trained classifier\n\nRunning the new `find_label_issues()`function on the [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) image tagging dataset reveals around **30,000 mislabeled images**! Check out a few of them in the blog post!\n\nResources:\n\n* Blog post: [https://cleanlab.ai/blog/multilabel/](https://cleanlab.ai/blog/multilabel/)\n* Paper: [https://arxiv.org/abs/2211.13895](https://arxiv.org/abs/2211.13895)\n* Tutorial: [https://docs.cleanlab.ai/stable/tutorials/multilabel\\_classification.html](https://docs.cleanlab.ai/stable/tutorials/multilabel_classification.html)\n* Benchmarks: [https://github.com/cleanlab/multilabel-error-detection-benchmarks](https://github.com/cleanlab/multilabel-error-detection-benchmarks)\n* Code: [https://github.com/cleanlab/cleanlab](https://github.com/cleanlab/cleanlab)\n\nHope you find these practical tools useful in your real-world data engineering applications!", "author_fullname": "t2_s0qucgfk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automatically Detect Annotation Errors in Image/Text Tagging Datasets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"duwshue6lx2a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/duwshue6lx2a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ba8a264f8bdb2135573e5a1b84f22b3fecf7ebc6"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/duwshue6lx2a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=51ec80c0ce557f464119dc0180e72c88b089b473"}, {"y": 179, "x": 320, "u": "https://preview.redd.it/duwshue6lx2a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=29b3f6381583d1ae4005459669fd81cfce26e790"}, {"y": 358, "x": 640, "u": "https://preview.redd.it/duwshue6lx2a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6ade7e3f4d8b550130e7a8638df0cced2a9ffc4"}, {"y": 537, "x": 960, "u": "https://preview.redd.it/duwshue6lx2a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=58929985ffea33237c589dfe9655b3da4f05a50a"}, {"y": 604, "x": 1080, "u": "https://preview.redd.it/duwshue6lx2a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b6344e5ba5da963e9601a058dd722b3b7127f0d"}], "s": {"y": 700, "x": 1250, "u": "https://preview.redd.it/duwshue6lx2a1.png?width=1250&amp;format=png&amp;auto=webp&amp;s=2bf66e0e822cebd4c00eb5f4177265cb99275132"}, "id": "duwshue6lx2a1"}}, "name": "t3_z81htm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3oJoyyViZm03wH0sXWrXqAZskqdjX-2zMd13hFjRM0M.jpg", "edited": 1669746230.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669745709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys! Many of us in the data science and ML world work with &lt;strong&gt;multi-label data&lt;/strong&gt;, where the image or text is tagged with multiple labels. Often these datasets contain &lt;strong&gt;frequent label errors&lt;/strong&gt; and/or &lt;strong&gt;missing tags&lt;/strong&gt; (check what we found below in the CelebA dataset) that make it hard to train highly accurate ML models. Support for multi-label data was one of the top features requested \u2014 so we &lt;a href=\"https://docs.cleanlab.ai/stable/tutorials/multilabel_classification.html\"&gt;added it&lt;/a&gt;, &lt;a href=\"https://cleanlab.ai/blog/multilabel/\"&gt;benchmarked it&lt;/a&gt;, and published all of the &lt;a href=\"https://cleanlab.ai/blog/multilabel/\"&gt;research&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/duwshue6lx2a1.png?width=1250&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2bf66e0e822cebd4c00eb5f4177265cb99275132\"&gt;Find errors and missing labels in multi-label datasets.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We are excited to share this newest research on algorithms to automatically find label errors in multi-label classification datasets. Image/document tagging represents important instances of\u00a0&lt;strong&gt;multi-label classification&lt;/strong&gt;\u00a0tasks, where each example can belong to multiple (or none) of K possible classes. Because annotating such data requires many decisions for each example, often multi-label classification datasets contain tons of label errors, which harm the performance of ML models.&lt;/p&gt;\n\n&lt;p&gt;We\u2019ve open-sourced our algorithms in the &lt;a href=\"https://github.com/cleanlab/cleanlab/releases/tag/v2.2.0\"&gt;recent release of cleanlab v2.2&lt;/a&gt;. All you need to do to use them is write one line of open-source code via &lt;a href=\"https://docs.cleanlab.ai/stable/tutorials/multilabel_classification.html\"&gt;cleanlab.filter.find_label_issues&lt;/a&gt;.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from cleanlab.filter import find_label_issues\n\nranked_label_issues = find_label_issues(\n    labels=labels,\n    pred_probs=pred_probs,\n    multi_label=True,\n    return_indices_ranked_by=&amp;quot;self_confidence&amp;quot;,\n)\n# labels: list of lists of (multiple) labels of each example\n# pred_probs: predicted class probabilities from any trained classifier\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Running the new &lt;code&gt;find_label_issues()&lt;/code&gt;function on the &lt;a href=\"https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\"&gt;CelebA&lt;/a&gt; image tagging dataset reveals around &lt;strong&gt;30,000 mislabeled images&lt;/strong&gt;! Check out a few of them in the blog post!&lt;/p&gt;\n\n&lt;p&gt;Resources:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Blog post: &lt;a href=\"https://cleanlab.ai/blog/multilabel/\"&gt;https://cleanlab.ai/blog/multilabel/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Paper: &lt;a href=\"https://arxiv.org/abs/2211.13895\"&gt;https://arxiv.org/abs/2211.13895&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Tutorial: &lt;a href=\"https://docs.cleanlab.ai/stable/tutorials/multilabel_classification.html\"&gt;https://docs.cleanlab.ai/stable/tutorials/multilabel_classification.html&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Benchmarks: &lt;a href=\"https://github.com/cleanlab/multilabel-error-detection-benchmarks\"&gt;https://github.com/cleanlab/multilabel-error-detection-benchmarks&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Code: &lt;a href=\"https://github.com/cleanlab/cleanlab\"&gt;https://github.com/cleanlab/cleanlab&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Hope you find these practical tools useful in your real-world data engineering applications!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "z81htm", "is_robot_indexable": true, "report_reasons": null, "author": "cmauck10", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z81htm/automatically_detect_annotation_errors_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z81htm/automatically_detect_annotation_errors_in/", "subreddit_subscribers": 81449, "created_utc": 1669745709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w2y7e99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS launches DataZone, a new ML-based data management service", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "name": "t3_z8mbdx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/arpopWQWXBFWXfbbTiull0iFD1W1YWg5kcrUYoCgztg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669797791.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "techcrunch.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://techcrunch.com/2022/11/29/aws-launches-datazone-a-new-ml-based-data-management-service", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RvYPdwRn9vW7cmJhuPDjk780OOIqZQFtWNLM5Av0x2Y.jpg?auto=webp&amp;s=eb83740613d544d61ff1f0a7344288a2b3acc600", "width": 938, "height": 521}, "resolutions": [{"url": "https://external-preview.redd.it/RvYPdwRn9vW7cmJhuPDjk780OOIqZQFtWNLM5Av0x2Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=41532793caaf485876e5860b463259f254de5567", "width": 108, "height": 59}, {"url": "https://external-preview.redd.it/RvYPdwRn9vW7cmJhuPDjk780OOIqZQFtWNLM5Av0x2Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fe32b9a3c0aec50750665906ccee97c5cb733d5", "width": 216, "height": 119}, {"url": "https://external-preview.redd.it/RvYPdwRn9vW7cmJhuPDjk780OOIqZQFtWNLM5Av0x2Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d64536f475f283da9e6c8975048651428730216", "width": 320, "height": 177}, {"url": "https://external-preview.redd.it/RvYPdwRn9vW7cmJhuPDjk780OOIqZQFtWNLM5Av0x2Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=04da1b0d1fccb4bbdab4813ddd0cd5c54ae3d96f", "width": 640, "height": 355}], "variants": {}, "id": "ZAHfycHEYb9lt1L70WkjhUo69dH2uwcQ9FnfhSuLGJE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8mbdx", "is_robot_indexable": true, "report_reasons": null, "author": "whichalps", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8mbdx/aws_launches_datazone_a_new_mlbased_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://techcrunch.com/2022/11/29/aws-launches-datazone-a-new-ml-based-data-management-service", "subreddit_subscribers": 81449, "created_utc": 1669797791.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Quick question: if you\u2019ve published articles related to Data Engineering on Medium or LinkedIn, would you include it in your CV?", "author_fullname": "t2_90pyvsip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Published Articles on CV", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8awzr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669766847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Quick question: if you\u2019ve published articles related to Data Engineering on Medium or LinkedIn, would you include it in your CV?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8awzr", "is_robot_indexable": true, "report_reasons": null, "author": "Gagan_Ku2905", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8awzr/published_articles_on_cv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8awzr/published_articles_on_cv/", "subreddit_subscribers": 81449, "created_utc": 1669766847.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "prefacing i'm a jr de,\n\nmy last org (platform) we weren't allowed to look at customers prod mysql DBs which would have helped resolve issues much faster instead of relying on calls and debugging together..\n\nhow do you design systems which ensure fast speed of debugging data issues for customers while also future proofing against compliance mandates you know you'll be subjected to as you grow from a small startup?", "author_fullname": "t2_piwlmz4s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SOC2 compliance vs data observability while debugging", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z822rb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669746994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;prefacing i&amp;#39;m a jr de,&lt;/p&gt;\n\n&lt;p&gt;my last org (platform) we weren&amp;#39;t allowed to look at customers prod mysql DBs which would have helped resolve issues much faster instead of relying on calls and debugging together..&lt;/p&gt;\n\n&lt;p&gt;how do you design systems which ensure fast speed of debugging data issues for customers while also future proofing against compliance mandates you know you&amp;#39;ll be subjected to as you grow from a small startup?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z822rb", "is_robot_indexable": true, "report_reasons": null, "author": "Fun_Story2003", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z822rb/soc2_compliance_vs_data_observability_while/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z822rb/soc2_compliance_vs_data_observability_while/", "subreddit_subscribers": 81449, "created_utc": 1669746994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "To ensure privacy compliance, businesses that deal with sensitive customer data must host on-premise data centers to fully control data storage, access, and auditing, making it harder to integrate their services.\n\nAt DoorDash I recently finished integration with a vendor that hosted services in on-premise data centers, and could only serve requests in its private network. I wanted to share how my team was able to set up private connections with these types of vendors by leveraging AWS Direct Connect. Check out my article to get the technical details and let me know what you think of my approach.\n\n[https://doordash.engineering/2022/11/29/how-doordash-secures-data-transfer-between-cloud-and-on-premise-data-centers/](https://doordash.engineering/2022/11/29/how-doordash-secures-data-transfer-between-cloud-and-on-premise-data-centers/)", "author_fullname": "t2_15hzyk6o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DoorDash uses AWS Direct Connect to bridge cloud resources with on-premise data centers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z7yrcd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669739522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To ensure privacy compliance, businesses that deal with sensitive customer data must host on-premise data centers to fully control data storage, access, and auditing, making it harder to integrate their services.&lt;/p&gt;\n\n&lt;p&gt;At DoorDash I recently finished integration with a vendor that hosted services in on-premise data centers, and could only serve requests in its private network. I wanted to share how my team was able to set up private connections with these types of vendors by leveraging AWS Direct Connect. Check out my article to get the technical details and let me know what you think of my approach.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://doordash.engineering/2022/11/29/how-doordash-secures-data-transfer-between-cloud-and-on-premise-data-centers/\"&gt;https://doordash.engineering/2022/11/29/how-doordash-secures-data-transfer-between-cloud-and-on-premise-data-centers/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z7yrcd", "is_robot_indexable": true, "report_reasons": null, "author": "rogerxman2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z7yrcd/doordash_uses_aws_direct_connect_to_bridge_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z7yrcd/doordash_uses_aws_direct_connect_to_bridge_cloud/", "subreddit_subscribers": 81449, "created_utc": 1669739522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\n\nI have a big dataframe (500GB approx.) which I want to write to an external hive table. The table is partitioned on week_start_date and the dataframe has 52 weeks of data. \n\nHow can I make sure that every Hive partition has equal size ORC files?\n\nWill it help if I write:\n\nmyDF.repartition(\"week_start_date\")\n  .format(\"orc\")\n  .mode(SaveModr.Overwrite)\n  .insertInto(hiveTable)\n\nOr should I use . reparation(4, \"week_start_date\"), to make sure that every Hive partition has 4 files. How it will affect parallelism of write?", "author_fullname": "t2_gzyg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Writing data to Externa Hive partitioned table using Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8psba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669809875.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I have a big dataframe (500GB approx.) which I want to write to an external hive table. The table is partitioned on week_start_date and the dataframe has 52 weeks of data. &lt;/p&gt;\n\n&lt;p&gt;How can I make sure that every Hive partition has equal size ORC files?&lt;/p&gt;\n\n&lt;p&gt;Will it help if I write:&lt;/p&gt;\n\n&lt;p&gt;myDF.repartition(&amp;quot;week_start_date&amp;quot;)\n  .format(&amp;quot;orc&amp;quot;)\n  .mode(SaveModr.Overwrite)\n  .insertInto(hiveTable)&lt;/p&gt;\n\n&lt;p&gt;Or should I use . reparation(4, &amp;quot;week_start_date&amp;quot;), to make sure that every Hive partition has 4 files. How it will affect parallelism of write?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8psba", "is_robot_indexable": true, "report_reasons": null, "author": "ps2931", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8psba/writing_data_to_externa_hive_partitioned_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8psba/writing_data_to_externa_hive_partitioned_table/", "subreddit_subscribers": 81449, "created_utc": 1669809875.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How many of you are using airflow for data ingestion framework? Why did you chose to use this tool compared to other tool like fivetran etc. Also, How are you using airflow for data ingestion? Curious!\n\n&amp;#x200B;\n\nEdit: I understand Airflow is an orchestration engine. I wanted to know how did you build an ingestion framework around airflow. The beautify of Airflow is running a python code, DAGs, notification, operator etc. I know fivetran is meant for data ingestion but I am not sure how customizable it is like event driven data ingestion, polling based data ingestion etc. ", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow for Data Ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8cbfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669790642.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669770232.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How many of you are using airflow for data ingestion framework? Why did you chose to use this tool compared to other tool like fivetran etc. Also, How are you using airflow for data ingestion? Curious!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: I understand Airflow is an orchestration engine. I wanted to know how did you build an ingestion framework around airflow. The beautify of Airflow is running a python code, DAGs, notification, operator etc. I know fivetran is meant for data ingestion but I am not sure how customizable it is like event driven data ingestion, polling based data ingestion etc. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8cbfs", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8cbfs/airflow_for_data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8cbfs/airflow_for_data_ingestion/", "subreddit_subscribers": 81449, "created_utc": 1669770232.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,   \n\n\nI have a requirement where the final destination to have the data in the Cloudera data warehouse and then connect to the Tableau dashboard.  \nMy data sources are PostgreSQL and MongoDB .\n\n1. What would be the best way to ingest data from MongoDB, the document structure is very dynamic- with nested loops and different objects .\n2. Do I convert the MongoDB documents to relational and then import the data or Have the documents imported as is from Mongo DB directly ?\n3. Would I be able to query the unstructured data if imported as is ? \n\nAny help would be highly appreciated.", "author_fullname": "t2_jr694bj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingest data from PostgreSQL and MongoDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z85svx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669755176.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,   &lt;/p&gt;\n\n&lt;p&gt;I have a requirement where the final destination to have the data in the Cloudera data warehouse and then connect to the Tableau dashboard.&lt;br/&gt;\nMy data sources are PostgreSQL and MongoDB .&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What would be the best way to ingest data from MongoDB, the document structure is very dynamic- with nested loops and different objects .&lt;/li&gt;\n&lt;li&gt;Do I convert the MongoDB documents to relational and then import the data or Have the documents imported as is from Mongo DB directly ?&lt;/li&gt;\n&lt;li&gt;Would I be able to query the unstructured data if imported as is ? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any help would be highly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z85svx", "is_robot_indexable": true, "report_reasons": null, "author": "FitPay8052", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z85svx/ingest_data_from_postgresql_and_mongodb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z85svx/ingest_data_from_postgresql_and_mongodb/", "subreddit_subscribers": 81449, "created_utc": 1669755176.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.youtube.com/watch?v=9-sc9I5BUNU](https://www.youtube.com/watch?v=9-sc9I5BUNU)", "author_fullname": "t2_17cxf7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Added a beginner video on \"HowToVideo: Create Azure Databricks MountPoints (Access Keys Method)\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z85rod", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669755100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=9-sc9I5BUNU\"&gt;https://www.youtube.com/watch?v=9-sc9I5BUNU&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FqhmajptEX5ZuZaica_lmUfsZ4wLLhT9-JVHKr40UXk.jpg?auto=webp&amp;s=e813384941b7a8b720cc49930b69a24be439e5ce", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/FqhmajptEX5ZuZaica_lmUfsZ4wLLhT9-JVHKr40UXk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c69afa075a30a260591250062d17e469dab323e3", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/FqhmajptEX5ZuZaica_lmUfsZ4wLLhT9-JVHKr40UXk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=421da3ac8b622de17535b6a5ccdbe7d799c90d09", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/FqhmajptEX5ZuZaica_lmUfsZ4wLLhT9-JVHKr40UXk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f68e9d85e11f3af93355f9c09d85fe08dfd34b0e", "width": 320, "height": 240}], "variants": {}, "id": "OjB4ev3VnfGA5ZvQXou2Lj7yFjoJeOBP20SMELK4rpc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z85rod", "is_robot_indexable": true, "report_reasons": null, "author": "AdityaSinghRathi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z85rod/added_a_beginner_video_on_howtovideo_create_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z85rod/added_a_beginner_video_on_howtovideo_create_azure/", "subreddit_subscribers": 81449, "created_utc": 1669755100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, looking for some advice on what to do next at my company regarding a \u201cwarehouse\u201d we made in Athena.\n\nI was 1 of 2 data engineers. The other guy was the lead and started before me so he set up the infrastructure. He was recently let go and now it\u2019s just me and our PM who is semi-technical.\n\nThe infrastructure is moving JSON data from an S3 source to another S3 source as parquet and some extra data which is then queried via Athena.\n\nPerformance is getting a little tough. Some queries around running like 8 mins for not THAT much data (10-20 million at most, typically less than 10 million rows). From what I\u2019m reading online is that Athena sucks when it comes to joining and we do ALOT of joins. We\u2019re also using ALOT of views that join other views so it gets even more time consuming. \n\nWhat would you do next? \n\nThe PM and I were thinking about extracting a CSV straight from the last X months and loading this all into an actual database instance. Like Postgres.\n\nIs that an okay approach? \n\nThe other option is to do these joins outside of Athena and load up some more Athena tables. This would at least eliminate a lot of our views.\n\nWould love to hear thoughts. My PM really wants to use Airbyte and I think that could maybe help?", "author_fullname": "t2_17a3rc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does it make sense to have a DB instance that is fed by Athena extracts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z85nt3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669754863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, looking for some advice on what to do next at my company regarding a \u201cwarehouse\u201d we made in Athena.&lt;/p&gt;\n\n&lt;p&gt;I was 1 of 2 data engineers. The other guy was the lead and started before me so he set up the infrastructure. He was recently let go and now it\u2019s just me and our PM who is semi-technical.&lt;/p&gt;\n\n&lt;p&gt;The infrastructure is moving JSON data from an S3 source to another S3 source as parquet and some extra data which is then queried via Athena.&lt;/p&gt;\n\n&lt;p&gt;Performance is getting a little tough. Some queries around running like 8 mins for not THAT much data (10-20 million at most, typically less than 10 million rows). From what I\u2019m reading online is that Athena sucks when it comes to joining and we do ALOT of joins. We\u2019re also using ALOT of views that join other views so it gets even more time consuming. &lt;/p&gt;\n\n&lt;p&gt;What would you do next? &lt;/p&gt;\n\n&lt;p&gt;The PM and I were thinking about extracting a CSV straight from the last X months and loading this all into an actual database instance. Like Postgres.&lt;/p&gt;\n\n&lt;p&gt;Is that an okay approach? &lt;/p&gt;\n\n&lt;p&gt;The other option is to do these joins outside of Athena and load up some more Athena tables. This would at least eliminate a lot of our views.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear thoughts. My PM really wants to use Airbyte and I think that could maybe help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z85nt3", "is_robot_indexable": true, "report_reasons": null, "author": "MsCardeno", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z85nt3/does_it_make_sense_to_have_a_db_instance_that_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z85nt3/does_it_make_sense_to_have_a_db_instance_that_is/", "subreddit_subscribers": 81449, "created_utc": 1669754863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the big bets for 2023 when it comes to data integration?\n\n[If you are curious, be part of the live conversation of ](https://www.linkedin.com/feed/update/urn:li:activity:7000822483570372608/)[Makesh Renganathan](https://www.linkedin.com/in/ACoAAAXU48ABzlBbpg1UUv6LhyyD1qS6Y_5BYhY) Director of Product Management, Informatica, and [John O'Brien](https://www.linkedin.com/in/ACoAAAB32mQBFdbChSMLAG7Dpn2Y_TUraWc5I0o), Principal Advisor and CEO, Radiant Advisors on 1st December. Join the webinar as they share their views and run a critical analysis of data integration trends!  \nPour in your questions and we will discuss the most voted ones during the webinar.\n\n[https://gateway.on24.com/wcc/eh/3857765/lp/4017059/data-integration-trends-for-2023?partnerref=Social](https://gateway.on24.com/wcc/eh/3857765/lp/4017059/data-integration-trends-for-2023?partnerref=Social)", "author_fullname": "t2_rlj24je0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Listen in and participate through polls and Q&amp;A, as an analyst and a product manager share their POV on data integration trends for 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z83fnr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669750023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the big bets for 2023 when it comes to data integration?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7000822483570372608/\"&gt;If you are curious, be part of the live conversation of &lt;/a&gt;&lt;a href=\"https://www.linkedin.com/in/ACoAAAXU48ABzlBbpg1UUv6LhyyD1qS6Y_5BYhY\"&gt;Makesh Renganathan&lt;/a&gt; Director of Product Management, Informatica, and &lt;a href=\"https://www.linkedin.com/in/ACoAAAB32mQBFdbChSMLAG7Dpn2Y_TUraWc5I0o\"&gt;John O&amp;#39;Brien&lt;/a&gt;, Principal Advisor and CEO, Radiant Advisors on 1st December. Join the webinar as they share their views and run a critical analysis of data integration trends!&lt;br/&gt;\nPour in your questions and we will discuss the most voted ones during the webinar.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://gateway.on24.com/wcc/eh/3857765/lp/4017059/data-integration-trends-for-2023?partnerref=Social\"&gt;https://gateway.on24.com/wcc/eh/3857765/lp/4017059/data-integration-trends-for-2023?partnerref=Social&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/txQNAvRAb-upCwgRNbtUSg-l72m3gpX22tKZ3qgczhQ.jpg?auto=webp&amp;s=9006df142a0cd676a385c1bfb60220b98e721937", "width": 576, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/txQNAvRAb-upCwgRNbtUSg-l72m3gpX22tKZ3qgczhQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aafe78a3dfafa10302bac729340687da69281872", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/txQNAvRAb-upCwgRNbtUSg-l72m3gpX22tKZ3qgczhQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8366a8cc0c8710cb83546a8641a8a3196838ea24", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/txQNAvRAb-upCwgRNbtUSg-l72m3gpX22tKZ3qgczhQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=117326313483fc31312ea6bae3244a7f2ec68cc7", "width": 320, "height": 166}], "variants": {}, "id": "Qa4JthtyCcYoTIQ8aXDKsER6-WjcsYi6-Zm3rKbz81A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z83fnr", "is_robot_indexable": true, "report_reasons": null, "author": "sudiptadatta", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z83fnr/listen_in_and_participate_through_polls_and_qa_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z83fnr/listen_in_and_participate_through_polls_and_qa_as/", "subreddit_subscribers": 81449, "created_utc": 1669750023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One of the initiatives I have been tasked with in my Data Engineering Leadership role is identifying an external partner to collaborate with that is:\n\n\u00b7 Well established\n\n\u00b7 Can help our scalability with staff augmentation\n\n\u00b7 Experienced enough to provide insight as to what is happening in the industry &amp; how that may apply to us\n\n\u00b7 Can be a sounding board for our own ideas of what we would like to do\n\n\u00b7 Help us to evolve \u2013 architecture, best practice, deployment, tools, UX\n\n\u00b7 Has SAP ECC &amp; S4 HANA experience\n\n\u00b7 Geographical preference of Europe or North America location\n\nCurrently we are a dedicated Microsoft shop, with SAP ECC ERP, looking to migrate to S4/Hana in the next few years.\n\nIf anyone has had good experiences with consultants that they would like to recommend partnering with us it would be greatly appreciated.\n\nThank you", "author_fullname": "t2_szazgnro", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Consultation Recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z82wkc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669749464.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669748826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of the initiatives I have been tasked with in my Data Engineering Leadership role is identifying an external partner to collaborate with that is:&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Well established&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Can help our scalability with staff augmentation&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Experienced enough to provide insight as to what is happening in the industry &amp;amp; how that may apply to us&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Can be a sounding board for our own ideas of what we would like to do&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Help us to evolve \u2013 architecture, best practice, deployment, tools, UX&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Has SAP ECC &amp;amp; S4 HANA experience&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Geographical preference of Europe or North America location&lt;/p&gt;\n\n&lt;p&gt;Currently we are a dedicated Microsoft shop, with SAP ECC ERP, looking to migrate to S4/Hana in the next few years.&lt;/p&gt;\n\n&lt;p&gt;If anyone has had good experiences with consultants that they would like to recommend partnering with us it would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z82wkc", "is_robot_indexable": true, "report_reasons": null, "author": "930mbsmith", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z82wkc/de_consultation_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z82wkc/de_consultation_recommendations/", "subreddit_subscribers": 81449, "created_utc": 1669748826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recently, I asked about how well a **Data Science** (DS) Junior role would be suited, as someone who learned a lot about that. The responses from this sub's members were mixed: some claimed it doesn't matter much, while others suggested DS is rather unfavorable, and a Software Engineering entry role (SWE) would be a much better fit.\n\nNow I found out about an open position in a data-heavy (mid-sized) company. The role is a **Junior SWE with DevOps focus**. Tbf, I am fairly neutral about DevOps. But I can see that it would be useful about DevOps practices as an aspiring DE.\n\nNormally such a position would just get a 'meh ok' from me. But I like reading into job requirements. Among the requirements here are solid **SQL** skills and cloud experience, preferably with **AWS**, which I also enjoy learning (currently am preparing for my 2nd certificate with it). On top of that, the company's business model mostly relies on creating value from data.\n\nHow appropriate would you deem such a role for someone who wants to break into the DE field later on?", "author_fullname": "t2_136crg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Normal SWE entry role with DevOps focus suitable for a future DE career path?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8poe9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669809561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently, I asked about how well a &lt;strong&gt;Data Science&lt;/strong&gt; (DS) Junior role would be suited, as someone who learned a lot about that. The responses from this sub&amp;#39;s members were mixed: some claimed it doesn&amp;#39;t matter much, while others suggested DS is rather unfavorable, and a Software Engineering entry role (SWE) would be a much better fit.&lt;/p&gt;\n\n&lt;p&gt;Now I found out about an open position in a data-heavy (mid-sized) company. The role is a &lt;strong&gt;Junior SWE with DevOps focus&lt;/strong&gt;. Tbf, I am fairly neutral about DevOps. But I can see that it would be useful about DevOps practices as an aspiring DE.&lt;/p&gt;\n\n&lt;p&gt;Normally such a position would just get a &amp;#39;meh ok&amp;#39; from me. But I like reading into job requirements. Among the requirements here are solid &lt;strong&gt;SQL&lt;/strong&gt; skills and cloud experience, preferably with &lt;strong&gt;AWS&lt;/strong&gt;, which I also enjoy learning (currently am preparing for my 2nd certificate with it). On top of that, the company&amp;#39;s business model mostly relies on creating value from data.&lt;/p&gt;\n\n&lt;p&gt;How appropriate would you deem such a role for someone who wants to break into the DE field later on?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z8poe9", "is_robot_indexable": true, "report_reasons": null, "author": "Boruroku", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8poe9/normal_swe_entry_role_with_devops_focus_suitable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8poe9/normal_swe_entry_role_with_devops_focus_suitable/", "subreddit_subscribers": 81449, "created_utc": 1669809561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve read a good bit about Atlan, Collibra, Alation and the like, and there are many good choices for SQL/structured data catalogs. \n\nBut is anyone involved with cataloging unstructured file-based data in a similar manner?\n\nFor example: connecting to an S3 bucket, ingesting image files, parsing metadata (i.e. capture date, GPS location), doing data quality checks, running ML models to classify or do object detection, and then store all that in some searchable catalog? (Use case: for analyzing volumes of aerial imagery)\n\nAre folks having to build these kinds of pipelines/workflows themselves, via OSS and code, or does something exist as a paid service?", "author_fullname": "t2_15wnt5aa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data catalog, but for unstructured data (images, video, docs, etc)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8muum", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669799796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve read a good bit about Atlan, Collibra, Alation and the like, and there are many good choices for SQL/structured data catalogs. &lt;/p&gt;\n\n&lt;p&gt;But is anyone involved with cataloging unstructured file-based data in a similar manner?&lt;/p&gt;\n\n&lt;p&gt;For example: connecting to an S3 bucket, ingesting image files, parsing metadata (i.e. capture date, GPS location), doing data quality checks, running ML models to classify or do object detection, and then store all that in some searchable catalog? (Use case: for analyzing volumes of aerial imagery)&lt;/p&gt;\n\n&lt;p&gt;Are folks having to build these kinds of pipelines/workflows themselves, via OSS and code, or does something exist as a paid service?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8muum", "is_robot_indexable": true, "report_reasons": null, "author": "DeadPukka", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8muum/data_catalog_but_for_unstructured_data_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8muum/data_catalog_but_for_unstructured_data_images/", "subreddit_subscribers": 81449, "created_utc": 1669799796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have the following df:  \n|Name|ID |MatrixRow                                                                                                                                                                                                                                                                                                                                                        |\n\n\\+----+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n|row1|   |\\[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0\\]                                                                                                                                                                                                                                                               |\n\n|row2|   |\\[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\\]                                                                                                                                                                                                                                                               |\n\n|row3|   |\\[0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0\\]  \n\n&amp;#x200B;\n\nI am only interested in the non zero values and their coordinates. So I'd like to have three columns: \"row\", \"column\", and \"value\" instead of MatrixRow. Value should store the value and the other two the coordinates.   \n\n\nDoes anyone know how to do this transformation?", "author_fullname": "t2_6cgfmz1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[PySpark] Does anyone know an efficient way to reduce a sparse matrix to COO format?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z8uv35", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669823349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have the following df:&lt;br/&gt;\n|Name|ID |MatrixRow                                                                                                                                                                                                                                                                                                                                                        |&lt;/p&gt;\n\n&lt;p&gt;+----+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+&lt;/p&gt;\n\n&lt;p&gt;|row1|   |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]                                                                                                                                                                                                                                                               |&lt;/p&gt;\n\n&lt;p&gt;|row2|   |[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                                                                                                                                                                                                                                                               |&lt;/p&gt;\n\n&lt;p&gt;|row3|   |[0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am only interested in the non zero values and their coordinates. So I&amp;#39;d like to have three columns: &amp;quot;row&amp;quot;, &amp;quot;column&amp;quot;, and &amp;quot;value&amp;quot; instead of MatrixRow. Value should store the value and the other two the coordinates.   &lt;/p&gt;\n\n&lt;p&gt;Does anyone know how to do this transformation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z8uv35", "is_robot_indexable": true, "report_reasons": null, "author": "BewitchedHare", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8uv35/pyspark_does_anyone_know_an_efficient_way_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8uv35/pyspark_does_anyone_know_an_efficient_way_to/", "subreddit_subscribers": 81449, "created_utc": 1669823349.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}