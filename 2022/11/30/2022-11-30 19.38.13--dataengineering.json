{"kind": "Listing", "data": {"after": "t3_z8x0lr", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Don't get me wrong, I know how hard things are for H1bs employees, and how hard they work. My spouse went through the whole F1, H1b situation. I've worked with a ton of people on H1bs, and a lot of my family friends have gone through/are in the situation. \n\nI'm just a bit frustrated because it feels like I've encountered some hiring managers who immigrated to the US whose entire teams are H1b employees. I've done a ton of interviews recently, and the data is heavily skewed towards rejection when this is the case, and I do fine/get offers in other cases. \n\nHas anyone had a similar experience? \n\nBy the way - this is in no way speaking negatively about anyone, and I know that being a US citizen gives me a ton of unfair advantageous.", "author_fullname": "t2_2djqd7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you noticed that some tech companies have a disproportionate amount of H1bs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8cvtv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 69, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 69, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669771610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Don&amp;#39;t get me wrong, I know how hard things are for H1bs employees, and how hard they work. My spouse went through the whole F1, H1b situation. I&amp;#39;ve worked with a ton of people on H1bs, and a lot of my family friends have gone through/are in the situation. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just a bit frustrated because it feels like I&amp;#39;ve encountered some hiring managers who immigrated to the US whose entire teams are H1b employees. I&amp;#39;ve done a ton of interviews recently, and the data is heavily skewed towards rejection when this is the case, and I do fine/get offers in other cases. &lt;/p&gt;\n\n&lt;p&gt;Has anyone had a similar experience? &lt;/p&gt;\n\n&lt;p&gt;By the way - this is in no way speaking negatively about anyone, and I know that being a US citizen gives me a ton of unfair advantageous.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8cvtv", "is_robot_indexable": true, "report_reasons": null, "author": "aznpersuazion", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8cvtv/have_you_noticed_that_some_tech_companies_have_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8cvtv/have_you_noticed_that_some_tech_companies_have_a/", "subreddit_subscribers": 81469, "created_utc": 1669771610.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_f7rx2o8s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "7 SQL Concepts You Should Know For Data Science - KDnuggets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_z8cen5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/paYLEadHXsFPRII12Dv5XsdYxjnm1uAcrcHMPf10V_g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669770449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kdnuggets.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.kdnuggets.com/2022/11/7-sql-concepts-needed-data-science.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QH9u-rzCZwrPb6832HFNTR0s99cCDKbXOufENA1myBc.jpg?auto=webp&amp;s=54b7d22b64d8464da18f8d7b8935411661d0d102", "width": 1000, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/QH9u-rzCZwrPb6832HFNTR0s99cCDKbXOufENA1myBc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e9ec91e12b022e3204de0923d8e5477e58775905", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/QH9u-rzCZwrPb6832HFNTR0s99cCDKbXOufENA1myBc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2fba71f65b78de007621731146aa18015724407b", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/QH9u-rzCZwrPb6832HFNTR0s99cCDKbXOufENA1myBc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ea700a89015dce36b15eb8aabec7eadbc485d6ab", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/QH9u-rzCZwrPb6832HFNTR0s99cCDKbXOufENA1myBc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=191ff9f3fa21e2f22f1b1666cc1b7ddbe4c4d0a7", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/QH9u-rzCZwrPb6832HFNTR0s99cCDKbXOufENA1myBc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e9d05210c64f9ddf9f9273532fe2bd85fc7879ce", "width": 960, "height": 576}], "variants": {}, "id": "xfS9I3SxennKVAgTfIDGPR-4hTpErjjKeDvjMrw9jTE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z8cen5", "is_robot_indexable": true, "report_reasons": null, "author": "No_Molasses_3859", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8cen5/7_sql_concepts_you_should_know_for_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.kdnuggets.com/2022/11/7-sql-concepts-needed-data-science.html", "subreddit_subscribers": 81469, "created_utc": 1669770449.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r6aazfpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Memgraph vs. Neo4j: A Performance Comparison", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_z8t2yh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pGCuKmdGp672BJqSaE0OQtFYrRLBUYQyLpy0DvrLlZY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669819012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memgraph.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memgraph.com/blog/memgraph-vs-neo4j-performance-benchmark-comparison", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?auto=webp&amp;s=c95f5fb4d03e8ee611fece6ca16ee53e9319cd47", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=34dd1c0af0ca1a777fe9c0b7a2eea3b8b7ca7ebe", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d4e40aa2cf0bbdfd747722ca56e6594c404b2b5", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d1b55a81f746a581c5337c562f40a4d95b3dd7b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=32bd87722c79868fc64f921f5df724e38061f45e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c019ea13d6df50a75187792bac5d77ef50aaf7ca", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/nheZysJJbVLoo7i6VXARSkZpPeRn-5vN0qfX06kXsMc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2edd5a8066f88e8dc8843d382e27c80d6d4e85b3", "width": 1080, "height": 540}], "variants": {}, "id": "xy6fojG3rvgz9t12Plo1ohQk81wf46np_f7PqRZPdAw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z8t2yh", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Cap6526", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8t2yh/memgraph_vs_neo4j_a_performance_comparison/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memgraph.com/blog/memgraph-vs-neo4j-performance-benchmark-comparison", "subreddit_subscribers": 81469, "created_utc": 1669819012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "  \n\nDear Data Engineers, \n\nI am a sociologist doing my doctorate in sociology at the University of Potsdam. In the context of my doctoral thesis, I am investigating the personal understanding of work and the work practice of Data Engineers.\n\nFor my study I am looking for people who are professionally active as Data Engineers, whom I can interview about their daily work routine. I am particularly interested in your personal work practices, i.e. \"HOW\" you do it in your professional work. I am particularly interested in your approach to problem solving and negotiation processes for finding solutions. I would like to conduct an interview with you, which should take about one hour. The interview can be conducted in presence or digitally, as desired. In both cases, an audio recording will be made for empirical analysis. All personal data will be anonymized.\n\nThe increasing number of users and companies using AI-based solutions makes your field particularly interesting for a sociological analysis. Therefore, I would be very pleased if you would be interested and have the time. \n\nWith kind regards", "author_fullname": "t2_jpt9q8kf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Phd Interview Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8qbd6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669811512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear Data Engineers, &lt;/p&gt;\n\n&lt;p&gt;I am a sociologist doing my doctorate in sociology at the University of Potsdam. In the context of my doctoral thesis, I am investigating the personal understanding of work and the work practice of Data Engineers.&lt;/p&gt;\n\n&lt;p&gt;For my study I am looking for people who are professionally active as Data Engineers, whom I can interview about their daily work routine. I am particularly interested in your personal work practices, i.e. &amp;quot;HOW&amp;quot; you do it in your professional work. I am particularly interested in your approach to problem solving and negotiation processes for finding solutions. I would like to conduct an interview with you, which should take about one hour. The interview can be conducted in presence or digitally, as desired. In both cases, an audio recording will be made for empirical analysis. All personal data will be anonymized.&lt;/p&gt;\n\n&lt;p&gt;The increasing number of users and companies using AI-based solutions makes your field particularly interesting for a sociological analysis. Therefore, I would be very pleased if you would be interested and have the time. &lt;/p&gt;\n\n&lt;p&gt;With kind regards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "z8qbd6", "is_robot_indexable": true, "report_reasons": null, "author": "SozUngl", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8qbd6/phd_interview_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8qbd6/phd_interview_data_engineers/", "subreddit_subscribers": 81469, "created_utc": 1669811512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3kxbd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hey Snowflake, send me an email", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_z84nmu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": "#46d160", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/i2IC1rRzLqVNpz5-_a6c3r1j9BqPL1kxEQ7ctKYDN2Y.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669752700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hoffa.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://hoffa.medium.com/hey-snowflake-send-me-an-email-243741a0fe3", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BBYGNlChqSwqoZAHtti_IFEUrXWnZkAfW1i0i0isDuU.jpg?auto=webp&amp;s=fc17dbb04093fd5b00325689b39b0537bde1fceb", "width": 576, "height": 384}, "resolutions": [{"url": "https://external-preview.redd.it/BBYGNlChqSwqoZAHtti_IFEUrXWnZkAfW1i0i0isDuU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4228863116462f40dd4356310fd6457f108b9f8", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/BBYGNlChqSwqoZAHtti_IFEUrXWnZkAfW1i0i0isDuU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7d5f9d28990c34834bb782b9019afc45a8e9014", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/BBYGNlChqSwqoZAHtti_IFEUrXWnZkAfW1i0i0isDuU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b58692b030be9f064606805e9fcc75791ba9a8b6", "width": 320, "height": 213}], "variants": {}, "id": "Hp2eKZv_as4Ck3asWrPQz-vgEMJ1brMwQh6sN_piWo4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "honorary mod | Snowflake", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z84nmu", "is_robot_indexable": true, "report_reasons": null, "author": "fhoffa", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/z84nmu/hey_snowflake_send_me_an_email/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://hoffa.medium.com/hey-snowflake-send-me-an-email-243741a0fe3", "subreddit_subscribers": 81469, "created_utc": 1669752700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Forrester Wave just released a report on \"Translytical\" databases. You could also probably refer to this category as [HTAP](https://en.wikipedia.org/wiki/Hybrid_transactional/analytical_processing) with possible exceptions. \n\nIMO, only a few in this report are actually designed for this category (and Oracle scoring perfect for \"product strategy\" is questionable)...but that's neither here nor there.\n\nI'm curious what you all think about this database type. \n\nWith ETL being most often used for transforming OLTP data into OLAP data into a separate data serving layer altogether, what would it take for a hybrid database to circumvent the need for a lot of that ETL? Granted, it's unlikely ETL would ever truly be replaced but there are certainly some use cases that lend themselves to this.\n\nThoughts?", "author_fullname": "t2_3wpf9ifb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Translytical/HTAP databases: What does DE need from these?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z850vy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669753493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Forrester Wave just released a report on &amp;quot;Translytical&amp;quot; databases. You could also probably refer to this category as &lt;a href=\"https://en.wikipedia.org/wiki/Hybrid_transactional/analytical_processing\"&gt;HTAP&lt;/a&gt; with possible exceptions. &lt;/p&gt;\n\n&lt;p&gt;IMO, only a few in this report are actually designed for this category (and Oracle scoring perfect for &amp;quot;product strategy&amp;quot; is questionable)...but that&amp;#39;s neither here nor there.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious what you all think about this database type. &lt;/p&gt;\n\n&lt;p&gt;With ETL being most often used for transforming OLTP data into OLAP data into a separate data serving layer altogether, what would it take for a hybrid database to circumvent the need for a lot of that ETL? Granted, it&amp;#39;s unlikely ETL would ever truly be replaced but there are certainly some use cases that lend themselves to this.&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z850vy", "is_robot_indexable": true, "report_reasons": null, "author": "samhld", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z850vy/translyticalhtap_databases_what_does_de_need_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z850vy/translyticalhtap_databases_what_does_de_need_from/", "subreddit_subscribers": 81469, "created_utc": 1669753493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w2y7e99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS launches DataZone, a new ML-based data management service", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "name": "t3_z8mbdx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/arpopWQWXBFWXfbbTiull0iFD1W1YWg5kcrUYoCgztg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669797791.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "techcrunch.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://techcrunch.com/2022/11/29/aws-launches-datazone-a-new-ml-based-data-management-service", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RvYPdwRn9vW7cmJhuPDjk780OOIqZQFtWNLM5Av0x2Y.jpg?auto=webp&amp;s=eb83740613d544d61ff1f0a7344288a2b3acc600", "width": 938, "height": 521}, "resolutions": [{"url": "https://external-preview.redd.it/RvYPdwRn9vW7cmJhuPDjk780OOIqZQFtWNLM5Av0x2Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=41532793caaf485876e5860b463259f254de5567", "width": 108, "height": 59}, {"url": "https://external-preview.redd.it/RvYPdwRn9vW7cmJhuPDjk780OOIqZQFtWNLM5Av0x2Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fe32b9a3c0aec50750665906ccee97c5cb733d5", "width": 216, "height": 119}, {"url": "https://external-preview.redd.it/RvYPdwRn9vW7cmJhuPDjk780OOIqZQFtWNLM5Av0x2Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d64536f475f283da9e6c8975048651428730216", "width": 320, "height": 177}, {"url": "https://external-preview.redd.it/RvYPdwRn9vW7cmJhuPDjk780OOIqZQFtWNLM5Av0x2Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=04da1b0d1fccb4bbdab4813ddd0cd5c54ae3d96f", "width": 640, "height": 355}], "variants": {}, "id": "ZAHfycHEYb9lt1L70WkjhUo69dH2uwcQ9FnfhSuLGJE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8mbdx", "is_robot_indexable": true, "report_reasons": null, "author": "whichalps", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8mbdx/aws_launches_datazone_a_new_mlbased_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://techcrunch.com/2022/11/29/aws-launches-datazone-a-new-ml-based-data-management-service", "subreddit_subscribers": 81469, "created_utc": 1669797791.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys, \n\nlet's say I have around 1 TO of json files , ezxh of these files has from 1 to n objects, and each of these objects has mostly the same properties.\n\nI want to query datas from the objects stored in those files.\n\nWhat is the best way to do that ? Store everything in a postgres database ? Can't i be able to query directly in these json files ? \n\nSoryy i'm a bit lost but not used to data eng.", "author_fullname": "t2_15hg5e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reading throught huge amont of json files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8un5k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669822809.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, &lt;/p&gt;\n\n&lt;p&gt;let&amp;#39;s say I have around 1 TO of json files , ezxh of these files has from 1 to n objects, and each of these objects has mostly the same properties.&lt;/p&gt;\n\n&lt;p&gt;I want to query datas from the objects stored in those files.&lt;/p&gt;\n\n&lt;p&gt;What is the best way to do that ? Store everything in a postgres database ? Can&amp;#39;t i be able to query directly in these json files ? &lt;/p&gt;\n\n&lt;p&gt;Soryy i&amp;#39;m a bit lost but not used to data eng.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z8un5k", "is_robot_indexable": true, "report_reasons": null, "author": "gBusato", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8un5k/reading_throught_huge_amont_of_json_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8un5k/reading_throught_huge_amont_of_json_files/", "subreddit_subscribers": 81469, "created_utc": 1669822809.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\n\nI have a big dataframe (500GB approx.) which I want to write to an external hive table. The table is partitioned on week_start_date and the dataframe has 52 weeks of data. \n\nHow can I make sure that every Hive partition has equal size ORC files?\n\nWill it help if I write:\n\nmyDF.repartition(\"week_start_date\")\n  .format(\"orc\")\n  .mode(SaveModr.Overwrite)\n  .insertInto(hiveTable)\n\nOr should I use . reparation(4, \"week_start_date\"), to make sure that every Hive partition has 4 files. How it will affect parallelism of write?", "author_fullname": "t2_gzyg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Writing data to Externa Hive partitioned table using Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8psba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669809875.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I have a big dataframe (500GB approx.) which I want to write to an external hive table. The table is partitioned on week_start_date and the dataframe has 52 weeks of data. &lt;/p&gt;\n\n&lt;p&gt;How can I make sure that every Hive partition has equal size ORC files?&lt;/p&gt;\n\n&lt;p&gt;Will it help if I write:&lt;/p&gt;\n\n&lt;p&gt;myDF.repartition(&amp;quot;week_start_date&amp;quot;)\n  .format(&amp;quot;orc&amp;quot;)\n  .mode(SaveModr.Overwrite)\n  .insertInto(hiveTable)&lt;/p&gt;\n\n&lt;p&gt;Or should I use . reparation(4, &amp;quot;week_start_date&amp;quot;), to make sure that every Hive partition has 4 files. How it will affect parallelism of write?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8psba", "is_robot_indexable": true, "report_reasons": null, "author": "ps2931", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8psba/writing_data_to_externa_hive_partitioned_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8psba/writing_data_to_externa_hive_partitioned_table/", "subreddit_subscribers": 81469, "created_utc": 1669809875.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, looking for advice on a lakehouse architecture pattern in databricks with aws. \n\nWe are looking to stream raw json data (mongoDb) stored in S3 to tables in Unity Catalog. We currently have a pipeline that does this using DLT, however, not being able to write to UC or set persistent views to UC is a deal breaker for us. However, this pipeline's ability to create a raw table (json is read into a single column with a timestamp) and then use a defined schema for a subsequent table has been very nice for us. \n\nCurrently experimenting with autoloader/spark streaming to S3, however, a limitation there is that you can't use autoloader in continuous mode with UC. \n\nWe are looking for an agile, scalable solution that is going to be able to handle at least close to continuous streaming with minimal latency (very very denormalized data doesn't help here...), any advice appreciated", "author_fullname": "t2_2nfde5yl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Lakehouse architecture advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z89hwi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669763614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, looking for advice on a lakehouse architecture pattern in databricks with aws. &lt;/p&gt;\n\n&lt;p&gt;We are looking to stream raw json data (mongoDb) stored in S3 to tables in Unity Catalog. We currently have a pipeline that does this using DLT, however, not being able to write to UC or set persistent views to UC is a deal breaker for us. However, this pipeline&amp;#39;s ability to create a raw table (json is read into a single column with a timestamp) and then use a defined schema for a subsequent table has been very nice for us. &lt;/p&gt;\n\n&lt;p&gt;Currently experimenting with autoloader/spark streaming to S3, however, a limitation there is that you can&amp;#39;t use autoloader in continuous mode with UC. &lt;/p&gt;\n\n&lt;p&gt;We are looking for an agile, scalable solution that is going to be able to handle at least close to continuous streaming with minimal latency (very very denormalized data doesn&amp;#39;t help here...), any advice appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z89hwi", "is_robot_indexable": true, "report_reasons": null, "author": "pmarct", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z89hwi/databricks_lakehouse_architecture_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z89hwi/databricks_lakehouse_architecture_advice/", "subreddit_subscribers": 81469, "created_utc": 1669763614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have any of you guys moved abroad and kept your US salary, Im getting ready to do this for a Major Investment Bank with many offices across the world in about a year or two.\n\nCancun Mexico, Perth Australia or Portugal seem nice and Im at about 160k USD with 3 years exp.\n\nI know some folks did this during the pandemic and they liked it and saved a ton of cash and bought properties just curious to see if any of you did a similar route and how did you convert your USD in a local currency.", "author_fullname": "t2_225yi2s2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving out of the US but keeping US salary", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z88tl8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669762087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have any of you guys moved abroad and kept your US salary, Im getting ready to do this for a Major Investment Bank with many offices across the world in about a year or two.&lt;/p&gt;\n\n&lt;p&gt;Cancun Mexico, Perth Australia or Portugal seem nice and Im at about 160k USD with 3 years exp.&lt;/p&gt;\n\n&lt;p&gt;I know some folks did this during the pandemic and they liked it and saved a ton of cash and bought properties just curious to see if any of you did a similar route and how did you convert your USD in a local currency.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z88tl8", "is_robot_indexable": true, "report_reasons": null, "author": "Analyst214", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z88tl8/moving_out_of_the_us_but_keeping_us_salary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z88tl8/moving_out_of_the_us_but_keeping_us_salary/", "subreddit_subscribers": 81469, "created_utc": 1669762087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How many of you are using airflow for data ingestion framework? Why did you chose to use this tool compared to other tool like fivetran etc. Also, How are you using airflow for data ingestion? Curious!\n\n&amp;#x200B;\n\nEdit: I understand Airflow is an orchestration engine. I wanted to know how did you build an ingestion framework around airflow. The beautify of Airflow is running a python code, DAGs, notification, operator etc. I know fivetran is meant for data ingestion but I am not sure how customizable it is like event driven data ingestion, polling based data ingestion etc. ", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow for Data Ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8cbfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669790642.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669770232.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How many of you are using airflow for data ingestion framework? Why did you chose to use this tool compared to other tool like fivetran etc. Also, How are you using airflow for data ingestion? Curious!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: I understand Airflow is an orchestration engine. I wanted to know how did you build an ingestion framework around airflow. The beautify of Airflow is running a python code, DAGs, notification, operator etc. I know fivetran is meant for data ingestion but I am not sure how customizable it is like event driven data ingestion, polling based data ingestion etc. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8cbfs", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8cbfs/airflow_for_data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8cbfs/airflow_for_data_ingestion/", "subreddit_subscribers": 81469, "created_utc": 1669770232.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Quick question: if you\u2019ve published articles related to Data Engineering on Medium or LinkedIn, would you include it in your CV?", "author_fullname": "t2_90pyvsip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Published Articles on CV", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8awzr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669766847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Quick question: if you\u2019ve published articles related to Data Engineering on Medium or LinkedIn, would you include it in your CV?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8awzr", "is_robot_indexable": true, "report_reasons": null, "author": "Gagan_Ku2905", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8awzr/published_articles_on_cv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8awzr/published_articles_on_cv/", "subreddit_subscribers": 81469, "created_utc": 1669766847.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've just joined a development team, I'm a little worried about how they develop their web applications, they don't use any known architecture patterns instead they just write plsql packages and procedures for reading and writing data,  the logic layer, data processing, and everything are developed in PLSQL procedures. Then the front-end team developers just call the procedures.\nWhat do you think about this method of designing applications? \n Do I need to worry about it?", "author_fullname": "t2_moy98sbf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this Architecture pattern standard", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z8xpe1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669830038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve just joined a development team, I&amp;#39;m a little worried about how they develop their web applications, they don&amp;#39;t use any known architecture patterns instead they just write plsql packages and procedures for reading and writing data,  the logic layer, data processing, and everything are developed in PLSQL procedures. Then the front-end team developers just call the procedures.\nWhat do you think about this method of designing applications? \n Do I need to worry about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8xpe1", "is_robot_indexable": true, "report_reasons": null, "author": "honnico", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8xpe1/is_this_architecture_pattern_standard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8xpe1/is_this_architecture_pattern_standard/", "subreddit_subscribers": 81469, "created_utc": 1669830038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recently, I asked about how well a **Data Science** (DS) Junior role would be suited, as someone who learned a lot about that. The responses from this sub's members were mixed: some claimed it doesn't matter much, while others suggested DS is rather unfavorable, and a Software Engineering entry role (SWE) would be a much better fit.\n\nNow I found out about an open position in a data-heavy (mid-sized) company. The role is a **Junior SWE with DevOps focus**. Tbf, I am fairly neutral about DevOps. But I can see that it would be useful about DevOps practices as an aspiring DE.\n\nNormally such a position would just get a 'meh ok' from me. But I like reading into job requirements. Among the requirements here are solid **SQL** skills and cloud experience, preferably with **AWS**, which I also enjoy learning (currently am preparing for my 2nd certificate with it). On top of that, the company's business model mostly relies on creating value from data.\n\nHow appropriate would you deem such a role for someone who wants to break into the DE field later on?", "author_fullname": "t2_136crg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Normal SWE entry role with DevOps focus suitable for a future DE career path?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8poe9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669809561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently, I asked about how well a &lt;strong&gt;Data Science&lt;/strong&gt; (DS) Junior role would be suited, as someone who learned a lot about that. The responses from this sub&amp;#39;s members were mixed: some claimed it doesn&amp;#39;t matter much, while others suggested DS is rather unfavorable, and a Software Engineering entry role (SWE) would be a much better fit.&lt;/p&gt;\n\n&lt;p&gt;Now I found out about an open position in a data-heavy (mid-sized) company. The role is a &lt;strong&gt;Junior SWE with DevOps focus&lt;/strong&gt;. Tbf, I am fairly neutral about DevOps. But I can see that it would be useful about DevOps practices as an aspiring DE.&lt;/p&gt;\n\n&lt;p&gt;Normally such a position would just get a &amp;#39;meh ok&amp;#39; from me. But I like reading into job requirements. Among the requirements here are solid &lt;strong&gt;SQL&lt;/strong&gt; skills and cloud experience, preferably with &lt;strong&gt;AWS&lt;/strong&gt;, which I also enjoy learning (currently am preparing for my 2nd certificate with it). On top of that, the company&amp;#39;s business model mostly relies on creating value from data.&lt;/p&gt;\n\n&lt;p&gt;How appropriate would you deem such a role for someone who wants to break into the DE field later on?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z8poe9", "is_robot_indexable": true, "report_reasons": null, "author": "Boruroku", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8poe9/normal_swe_entry_role_with_devops_focus_suitable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8poe9/normal_swe_entry_role_with_devops_focus_suitable/", "subreddit_subscribers": 81469, "created_utc": 1669809561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,   \n\n\nI have a requirement where the final destination to have the data in the Cloudera data warehouse and then connect to the Tableau dashboard.  \nMy data sources are PostgreSQL and MongoDB .\n\n1. What would be the best way to ingest data from MongoDB, the document structure is very dynamic- with nested loops and different objects .\n2. Do I convert the MongoDB documents to relational and then import the data or Have the documents imported as is from Mongo DB directly ?\n3. Would I be able to query the unstructured data if imported as is ? \n\nAny help would be highly appreciated.", "author_fullname": "t2_jr694bj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingest data from PostgreSQL and MongoDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z85svx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669755176.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,   &lt;/p&gt;\n\n&lt;p&gt;I have a requirement where the final destination to have the data in the Cloudera data warehouse and then connect to the Tableau dashboard.&lt;br/&gt;\nMy data sources are PostgreSQL and MongoDB .&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What would be the best way to ingest data from MongoDB, the document structure is very dynamic- with nested loops and different objects .&lt;/li&gt;\n&lt;li&gt;Do I convert the MongoDB documents to relational and then import the data or Have the documents imported as is from Mongo DB directly ?&lt;/li&gt;\n&lt;li&gt;Would I be able to query the unstructured data if imported as is ? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any help would be highly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z85svx", "is_robot_indexable": true, "report_reasons": null, "author": "FitPay8052", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z85svx/ingest_data_from_postgresql_and_mongodb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z85svx/ingest_data_from_postgresql_and_mongodb/", "subreddit_subscribers": 81469, "created_utc": 1669755176.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.youtube.com/watch?v=9-sc9I5BUNU](https://www.youtube.com/watch?v=9-sc9I5BUNU)", "author_fullname": "t2_17cxf7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Added a beginner video on \"HowToVideo: Create Azure Databricks MountPoints (Access Keys Method)\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z85rod", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669755100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=9-sc9I5BUNU\"&gt;https://www.youtube.com/watch?v=9-sc9I5BUNU&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FqhmajptEX5ZuZaica_lmUfsZ4wLLhT9-JVHKr40UXk.jpg?auto=webp&amp;s=e813384941b7a8b720cc49930b69a24be439e5ce", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/FqhmajptEX5ZuZaica_lmUfsZ4wLLhT9-JVHKr40UXk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c69afa075a30a260591250062d17e469dab323e3", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/FqhmajptEX5ZuZaica_lmUfsZ4wLLhT9-JVHKr40UXk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=421da3ac8b622de17535b6a5ccdbe7d799c90d09", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/FqhmajptEX5ZuZaica_lmUfsZ4wLLhT9-JVHKr40UXk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f68e9d85e11f3af93355f9c09d85fe08dfd34b0e", "width": 320, "height": 240}], "variants": {}, "id": "OjB4ev3VnfGA5ZvQXou2Lj7yFjoJeOBP20SMELK4rpc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z85rod", "is_robot_indexable": true, "report_reasons": null, "author": "AdityaSinghRathi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z85rod/added_a_beginner_video_on_howtovideo_create_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z85rod/added_a_beginner_video_on_howtovideo_create_azure/", "subreddit_subscribers": 81469, "created_utc": 1669755100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, looking for some advice on what to do next at my company regarding a \u201cwarehouse\u201d we made in Athena.\n\nI was 1 of 2 data engineers. The other guy was the lead and started before me so he set up the infrastructure. He was recently let go and now it\u2019s just me and our PM who is semi-technical.\n\nThe infrastructure is moving JSON data from an S3 source to another S3 source as parquet and some extra data which is then queried via Athena.\n\nPerformance is getting a little tough. Some queries around running like 8 mins for not THAT much data (10-20 million at most, typically less than 10 million rows). From what I\u2019m reading online is that Athena sucks when it comes to joining and we do ALOT of joins. We\u2019re also using ALOT of views that join other views so it gets even more time consuming. \n\nWhat would you do next? \n\nThe PM and I were thinking about extracting a CSV straight from the last X months and loading this all into an actual database instance. Like Postgres.\n\nIs that an okay approach? \n\nThe other option is to do these joins outside of Athena and load up some more Athena tables. This would at least eliminate a lot of our views.\n\nWould love to hear thoughts. My PM really wants to use Airbyte and I think that could maybe help?", "author_fullname": "t2_17a3rc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does it make sense to have a DB instance that is fed by Athena extracts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z85nt3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669754863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, looking for some advice on what to do next at my company regarding a \u201cwarehouse\u201d we made in Athena.&lt;/p&gt;\n\n&lt;p&gt;I was 1 of 2 data engineers. The other guy was the lead and started before me so he set up the infrastructure. He was recently let go and now it\u2019s just me and our PM who is semi-technical.&lt;/p&gt;\n\n&lt;p&gt;The infrastructure is moving JSON data from an S3 source to another S3 source as parquet and some extra data which is then queried via Athena.&lt;/p&gt;\n\n&lt;p&gt;Performance is getting a little tough. Some queries around running like 8 mins for not THAT much data (10-20 million at most, typically less than 10 million rows). From what I\u2019m reading online is that Athena sucks when it comes to joining and we do ALOT of joins. We\u2019re also using ALOT of views that join other views so it gets even more time consuming. &lt;/p&gt;\n\n&lt;p&gt;What would you do next? &lt;/p&gt;\n\n&lt;p&gt;The PM and I were thinking about extracting a CSV straight from the last X months and loading this all into an actual database instance. Like Postgres.&lt;/p&gt;\n\n&lt;p&gt;Is that an okay approach? &lt;/p&gt;\n\n&lt;p&gt;The other option is to do these joins outside of Athena and load up some more Athena tables. This would at least eliminate a lot of our views.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear thoughts. My PM really wants to use Airbyte and I think that could maybe help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z85nt3", "is_robot_indexable": true, "report_reasons": null, "author": "MsCardeno", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z85nt3/does_it_make_sense_to_have_a_db_instance_that_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z85nt3/does_it_make_sense_to_have_a_db_instance_that_is/", "subreddit_subscribers": 81469, "created_utc": 1669754863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the big bets for 2023 when it comes to data integration?\n\n[If you are curious, be part of the live conversation of ](https://www.linkedin.com/feed/update/urn:li:activity:7000822483570372608/)[Makesh Renganathan](https://www.linkedin.com/in/ACoAAAXU48ABzlBbpg1UUv6LhyyD1qS6Y_5BYhY) Director of Product Management, Informatica, and [John O'Brien](https://www.linkedin.com/in/ACoAAAB32mQBFdbChSMLAG7Dpn2Y_TUraWc5I0o), Principal Advisor and CEO, Radiant Advisors on 1st December. Join the webinar as they share their views and run a critical analysis of data integration trends!  \nPour in your questions and we will discuss the most voted ones during the webinar.\n\n[https://gateway.on24.com/wcc/eh/3857765/lp/4017059/data-integration-trends-for-2023?partnerref=Social](https://gateway.on24.com/wcc/eh/3857765/lp/4017059/data-integration-trends-for-2023?partnerref=Social)", "author_fullname": "t2_rlj24je0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Listen in and participate through polls and Q&amp;A, as an analyst and a product manager share their POV on data integration trends for 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z83fnr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669750023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the big bets for 2023 when it comes to data integration?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7000822483570372608/\"&gt;If you are curious, be part of the live conversation of &lt;/a&gt;&lt;a href=\"https://www.linkedin.com/in/ACoAAAXU48ABzlBbpg1UUv6LhyyD1qS6Y_5BYhY\"&gt;Makesh Renganathan&lt;/a&gt; Director of Product Management, Informatica, and &lt;a href=\"https://www.linkedin.com/in/ACoAAAB32mQBFdbChSMLAG7Dpn2Y_TUraWc5I0o\"&gt;John O&amp;#39;Brien&lt;/a&gt;, Principal Advisor and CEO, Radiant Advisors on 1st December. Join the webinar as they share their views and run a critical analysis of data integration trends!&lt;br/&gt;\nPour in your questions and we will discuss the most voted ones during the webinar.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://gateway.on24.com/wcc/eh/3857765/lp/4017059/data-integration-trends-for-2023?partnerref=Social\"&gt;https://gateway.on24.com/wcc/eh/3857765/lp/4017059/data-integration-trends-for-2023?partnerref=Social&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/txQNAvRAb-upCwgRNbtUSg-l72m3gpX22tKZ3qgczhQ.jpg?auto=webp&amp;s=9006df142a0cd676a385c1bfb60220b98e721937", "width": 576, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/txQNAvRAb-upCwgRNbtUSg-l72m3gpX22tKZ3qgczhQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aafe78a3dfafa10302bac729340687da69281872", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/txQNAvRAb-upCwgRNbtUSg-l72m3gpX22tKZ3qgczhQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8366a8cc0c8710cb83546a8641a8a3196838ea24", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/txQNAvRAb-upCwgRNbtUSg-l72m3gpX22tKZ3qgczhQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=117326313483fc31312ea6bae3244a7f2ec68cc7", "width": 320, "height": 166}], "variants": {}, "id": "Qa4JthtyCcYoTIQ8aXDKsER6-WjcsYi6-Zm3rKbz81A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z83fnr", "is_robot_indexable": true, "report_reasons": null, "author": "sudiptadatta", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z83fnr/listen_in_and_participate_through_polls_and_qa_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z83fnr/listen_in_and_participate_through_polls_and_qa_as/", "subreddit_subscribers": 81469, "created_utc": 1669750023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One of the initiatives I have been tasked with in my Data Engineering Leadership role is identifying an external partner to collaborate with that is:\n\n\u00b7 Well established\n\n\u00b7 Can help our scalability with staff augmentation\n\n\u00b7 Experienced enough to provide insight as to what is happening in the industry &amp; how that may apply to us\n\n\u00b7 Can be a sounding board for our own ideas of what we would like to do\n\n\u00b7 Help us to evolve \u2013 architecture, best practice, deployment, tools, UX\n\n\u00b7 Has SAP ECC &amp; S4 HANA experience\n\n\u00b7 Geographical preference of Europe or North America location\n\nCurrently we are a dedicated Microsoft shop, with SAP ECC ERP, looking to migrate to S4/Hana in the next few years.\n\nIf anyone has had good experiences with consultants that they would like to recommend partnering with us it would be greatly appreciated.\n\nThank you", "author_fullname": "t2_szazgnro", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Consultation Recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z82wkc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669749464.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669748826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of the initiatives I have been tasked with in my Data Engineering Leadership role is identifying an external partner to collaborate with that is:&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Well established&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Can help our scalability with staff augmentation&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Experienced enough to provide insight as to what is happening in the industry &amp;amp; how that may apply to us&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Can be a sounding board for our own ideas of what we would like to do&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Help us to evolve \u2013 architecture, best practice, deployment, tools, UX&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Has SAP ECC &amp;amp; S4 HANA experience&lt;/p&gt;\n\n&lt;p&gt;\u00b7 Geographical preference of Europe or North America location&lt;/p&gt;\n\n&lt;p&gt;Currently we are a dedicated Microsoft shop, with SAP ECC ERP, looking to migrate to S4/Hana in the next few years.&lt;/p&gt;\n\n&lt;p&gt;If anyone has had good experiences with consultants that they would like to recommend partnering with us it would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z82wkc", "is_robot_indexable": true, "report_reasons": null, "author": "930mbsmith", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z82wkc/de_consultation_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z82wkc/de_consultation_recommendations/", "subreddit_subscribers": 81469, "created_utc": 1669748826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I get this is vague, but what type of projects can I do to sharpen data engineering skills?\nI have access to plenty of databases but besides honing my SQL skills with queries and joins, I\u2019m looking to expand what I do to play around with all this.\n\nLook into creating my own pipeline from databases/tables?\n\nAny books y\u2019all would recommend that would be helpful to capitalize on the enterprise data I have at my fingertips?\n\nAny help to point me in a direction I can start running is much appreciated.", "author_fullname": "t2_ezoxvddi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer Practice at Work", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z8y65x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669831156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I get this is vague, but what type of projects can I do to sharpen data engineering skills?\nI have access to plenty of databases but besides honing my SQL skills with queries and joins, I\u2019m looking to expand what I do to play around with all this.&lt;/p&gt;\n\n&lt;p&gt;Look into creating my own pipeline from databases/tables?&lt;/p&gt;\n\n&lt;p&gt;Any books y\u2019all would recommend that would be helpful to capitalize on the enterprise data I have at my fingertips?&lt;/p&gt;\n\n&lt;p&gt;Any help to point me in a direction I can start running is much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8y65x", "is_robot_indexable": true, "report_reasons": null, "author": "Financial-Jicama6619", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8y65x/data_engineer_practice_at_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8y65x/data_engineer_practice_at_work/", "subreddit_subscribers": 81469, "created_utc": 1669831156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A few months ago, my team hired a new data engineer. My team is a data science team in a medium-sized startup, and because we deal with a complicated domain of data, our hiring process focuses more on domain knowledge than technical experience (I'm not a fan of this but I don't have much say here). We only had two data engineers, including myself, so I was excited to have another teammate. But since I was pretty swamped with my own workload, I didn't get to interact much with the new DE or see their work until recently.\n\nTwo weeks ago, I saw their first pull request to our DBT GitHub repository, and their code was a nightmare. The two SQL files are 600-700 lines each, but since the lines were so long (entire select statements and complex case-when statements on single lines), they'd each be at least twice as long if properly formatted. The code has almost zero comments, the queries have many nested subqueries (without any extra indentation), and all the CTE/alias names are either confusing initialisms (stuff like od1, fdf, ittt) or generic names like \"sourcedata1\". I've spent a few hours trying to read through the code, and it looks like there's plenty of copy-pasted code within the same models.\n\nUnfortunately, less than a week after I saw this nightmare code, my team lead deployed it to production. And even worse, our customers want updates to this data model very soon. I've had multiple conversations with my team lead, and he knows that the code quality is poor and a huge risk for our team, but he's had so much pressure to release the feature based on that data that he had to deploy it anyway. Both of us have had conversations with the new DE, and the new DE understands what the problem is and wants to improve their code, but since they've never written code like this before, they feel lost on how to start writing comments or fixing up the code.\n\n*\\[I want to clarify that I have a great team and I love my job. But our industry is super busy around Black Friday and the holidays, and my team is in crunch mode right now. And the downside of working on a data science team is that there isn't much software engineering background, especially in leadership.\\]*\n\nWhat can I do to help this new data engineer improve their code? They seem willing to learn and improve, but genuinely lost and confused, and I want to help them succeed in this new role.", "author_fullname": "t2_mz5uepkn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My team has a new data engineer with very little programming experience. How can I help them understand code style, comments, and other good code practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8x9n6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669829019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A few months ago, my team hired a new data engineer. My team is a data science team in a medium-sized startup, and because we deal with a complicated domain of data, our hiring process focuses more on domain knowledge than technical experience (I&amp;#39;m not a fan of this but I don&amp;#39;t have much say here). We only had two data engineers, including myself, so I was excited to have another teammate. But since I was pretty swamped with my own workload, I didn&amp;#39;t get to interact much with the new DE or see their work until recently.&lt;/p&gt;\n\n&lt;p&gt;Two weeks ago, I saw their first pull request to our DBT GitHub repository, and their code was a nightmare. The two SQL files are 600-700 lines each, but since the lines were so long (entire select statements and complex case-when statements on single lines), they&amp;#39;d each be at least twice as long if properly formatted. The code has almost zero comments, the queries have many nested subqueries (without any extra indentation), and all the CTE/alias names are either confusing initialisms (stuff like od1, fdf, ittt) or generic names like &amp;quot;sourcedata1&amp;quot;. I&amp;#39;ve spent a few hours trying to read through the code, and it looks like there&amp;#39;s plenty of copy-pasted code within the same models.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, less than a week after I saw this nightmare code, my team lead deployed it to production. And even worse, our customers want updates to this data model very soon. I&amp;#39;ve had multiple conversations with my team lead, and he knows that the code quality is poor and a huge risk for our team, but he&amp;#39;s had so much pressure to release the feature based on that data that he had to deploy it anyway. Both of us have had conversations with the new DE, and the new DE understands what the problem is and wants to improve their code, but since they&amp;#39;ve never written code like this before, they feel lost on how to start writing comments or fixing up the code.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;[I want to clarify that I have a great team and I love my job. But our industry is super busy around Black Friday and the holidays, and my team is in crunch mode right now. And the downside of working on a data science team is that there isn&amp;#39;t much software engineering background, especially in leadership.]&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;What can I do to help this new data engineer improve their code? They seem willing to learn and improve, but genuinely lost and confused, and I want to help them succeed in this new role.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z8x9n6", "is_robot_indexable": true, "report_reasons": null, "author": "bigdatabro", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8x9n6/my_team_has_a_new_data_engineer_with_very_little/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8x9n6/my_team_has_a_new_data_engineer_with_very_little/", "subreddit_subscribers": 81469, "created_utc": 1669829019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve read a good bit about Atlan, Collibra, Alation and the like, and there are many good choices for SQL/structured data catalogs. \n\nBut is anyone involved with cataloging unstructured file-based data in a similar manner?\n\nFor example: connecting to an S3 bucket, ingesting image files, parsing metadata (i.e. capture date, GPS location), doing data quality checks, running ML models to classify or do object detection, and then store all that in some searchable catalog? (Use case: for analyzing volumes of aerial imagery)\n\nAre folks having to build these kinds of pipelines/workflows themselves, via OSS and code, or does something exist as a paid service?", "author_fullname": "t2_15wnt5aa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data catalog, but for unstructured data (images, video, docs, etc)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8muum", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669799796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve read a good bit about Atlan, Collibra, Alation and the like, and there are many good choices for SQL/structured data catalogs. &lt;/p&gt;\n\n&lt;p&gt;But is anyone involved with cataloging unstructured file-based data in a similar manner?&lt;/p&gt;\n\n&lt;p&gt;For example: connecting to an S3 bucket, ingesting image files, parsing metadata (i.e. capture date, GPS location), doing data quality checks, running ML models to classify or do object detection, and then store all that in some searchable catalog? (Use case: for analyzing volumes of aerial imagery)&lt;/p&gt;\n\n&lt;p&gt;Are folks having to build these kinds of pipelines/workflows themselves, via OSS and code, or does something exist as a paid service?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z8muum", "is_robot_indexable": true, "report_reasons": null, "author": "DeadPukka", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8muum/data_catalog_but_for_unstructured_data_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8muum/data_catalog_but_for_unstructured_data_images/", "subreddit_subscribers": 81469, "created_utc": 1669799796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am reading from kafka this schema  \n`|-- package_sender`\n\n`|-- package_recipient`\n\n  \nand would like to get to this schema  \n`|-- names: string` \n\n`|-- count: long` \n\n&amp;#x200B;\n\nmy line of reasoning was to group by the package sender and recipient and join these 2 dataframes on their names but \"Multiple streaming aggregations are not supported with streaming\" error.  \nseems like a trivial problem at first glance. what are your ways around such error?", "author_fullname": "t2_9ossfypw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "spark structured streaming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z8ylph", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669832131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am reading from kafka this schema&lt;br/&gt;\n&lt;code&gt;|-- package_sender&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;|-- package_recipient&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;and would like to get to this schema&lt;br/&gt;\n&lt;code&gt;|-- names: string&lt;/code&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;|-- count: long&lt;/code&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;my line of reasoning was to group by the package sender and recipient and join these 2 dataframes on their names but &amp;quot;Multiple streaming aggregations are not supported with streaming&amp;quot; error.&lt;br/&gt;\nseems like a trivial problem at first glance. what are your ways around such error?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z8ylph", "is_robot_indexable": true, "report_reasons": null, "author": "External-Peach8286", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8ylph/spark_structured_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8ylph/spark_structured_streaming/", "subreddit_subscribers": 81469, "created_utc": 1669832131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI'm a UK-based computer science graduate with 6 months of training in big data engineering. I'm looking for a junior-level position in data engineering so that I can continue to expand my skills. I was contacted by a recruiter this afternoon asking if I was interested in the **trainee CRM Systems Administrator** role they're advertising. Obviously, from the name, it isn't exactly the same as data engineering but I would like to know if I can gain some transferable skills from this job which I can use in a future data engineering role. \n\nI want to ask people with experience in this field whether they believe this role would be appropriate for someone that wants to break into the DE field.", "author_fullname": "t2_6ipsq393", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Entry-level Job Hunt - Advice Needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8x0lr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669828444.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a UK-based computer science graduate with 6 months of training in big data engineering. I&amp;#39;m looking for a junior-level position in data engineering so that I can continue to expand my skills. I was contacted by a recruiter this afternoon asking if I was interested in the &lt;strong&gt;trainee CRM Systems Administrator&lt;/strong&gt; role they&amp;#39;re advertising. Obviously, from the name, it isn&amp;#39;t exactly the same as data engineering but I would like to know if I can gain some transferable skills from this job which I can use in a future data engineering role. &lt;/p&gt;\n\n&lt;p&gt;I want to ask people with experience in this field whether they believe this role would be appropriate for someone that wants to break into the DE field.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z8x0lr", "is_robot_indexable": true, "report_reasons": null, "author": "AbundantKaizen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z8x0lr/entrylevel_job_hunt_advice_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z8x0lr/entrylevel_job_hunt_advice_needed/", "subreddit_subscribers": 81469, "created_utc": 1669828444.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}