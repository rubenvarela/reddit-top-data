{"kind": "Listing", "data": {"after": "t3_z8ti34", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_hbwoyw04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "20tb Sata/SAS $269.99-$304.99 -or- 18tb Sata $189.99 Seagate recertified with 2 year warranty, wish I had the money, thought I'd share here!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_z8irvw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 100, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 100, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/O4bB82ABNHVdXXdfMBZQnBvA5Mk6ksZ9Ns88tZbB3rw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669786924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "serverpartdeals.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://serverpartdeals.com/collections/recertified-seagate-exos", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?auto=webp&amp;s=4c1db49fea90382fef14d5213d071fac818998ff", "width": 2000, "height": 2000}, "resolutions": [{"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e7ff784b044c5a132792a51783e12546aa096933", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ebb5efa12a9e165a566f9c8d169e9861369acd2f", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=763ac82ff67bfc932ed93a299e8d40c5271a9258", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=487c4ebc47f1af49ca7b4ed1e032a4d96672a443", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=32fed97e301ba788e5b9661d6052a5e41a4a23c1", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f4c9097a5b110531b40d5950876ae2530c195a25", "width": 1080, "height": 1080}], "variants": {}, "id": "_D0H66BxLDdRjM-Vxtc776wegerPH5Vt6qcZASPwe_c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "z8irvw", "is_robot_indexable": true, "report_reasons": null, "author": "igmyeongui", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8irvw/20tb_satasas_2699930499_or_18tb_sata_18999/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://serverpartdeals.com/collections/recertified-seagate-exos", "subreddit_subscribers": 656848, "created_utc": 1669786924.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought a new, faster NVMe drive and cloned my Windows system drive onto it using Macrium\n\nHowever, there's a small discrepancy in the total files/file sizes.\n\nIs there a tool to analyze both drives and show me which files it finds that differ between the two?\n\nEverything seems to be (mostly) in order, but there's a 6GB difference in used space on the two drives.\n\n(Also worth noting that my taskbar flashes for a few seconds while the desktop loads on the cloned drive, which doesn't happen if I boot from the old drive. I am using an app called TaskbarX, but it isn't working properly since cloning and I don't think that's the issue as the flashing was happening prior to re-configuring that program. In any case, this isn't the proper subreddit for that particular issue)", "author_fullname": "t2_7d7au", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tools to show the difference in files between two cloned drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8l12y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669793579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a new, faster NVMe drive and cloned my Windows system drive onto it using Macrium&lt;/p&gt;\n\n&lt;p&gt;However, there&amp;#39;s a small discrepancy in the total files/file sizes.&lt;/p&gt;\n\n&lt;p&gt;Is there a tool to analyze both drives and show me which files it finds that differ between the two?&lt;/p&gt;\n\n&lt;p&gt;Everything seems to be (mostly) in order, but there&amp;#39;s a 6GB difference in used space on the two drives.&lt;/p&gt;\n\n&lt;p&gt;(Also worth noting that my taskbar flashes for a few seconds while the desktop loads on the cloned drive, which doesn&amp;#39;t happen if I boot from the old drive. I am using an app called TaskbarX, but it isn&amp;#39;t working properly since cloning and I don&amp;#39;t think that&amp;#39;s the issue as the flashing was happening prior to re-configuring that program. In any case, this isn&amp;#39;t the proper subreddit for that particular issue)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z8l12y", "is_robot_indexable": true, "report_reasons": null, "author": "MiguelLancaster", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8l12y/best_tools_to_show_the_difference_in_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8l12y/best_tools_to_show_the_difference_in_files/", "subreddit_subscribers": 656848, "created_utc": 1669793579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI want Parity Based server/nas/external HDD for ONE PC 10Gb direct connection and each Pc is also connected to router with slower link\n\n&amp;#x200B;\n\nUnraid 120USD\n\nWindows 2022 Server 9$ \\[found online shop, works for me\\]\n\nHow parity works in windows spaces? I have HDDs of many sizes 16,14, 12 \\[all my HDDs totaling near 370Tb, but I dont need so much and ill be using all 16 and 14tb, no 12 or 10, maybe 250ish Tb\\]\n\n&amp;#x200B;\n\nI know on Unraid you chose Parity drives yourself so you can just set the biggest HDD's manually.\n\nBut from the videos i seen about windows, you create a pool of all drives and it creates parity across all of them there isnt  \"parity HDD\" by itself.\n\nSo how its going to work with different sized drives?\n\n&amp;#x200B;\n\nI prefer to use windows for ease of use, and so i can use that PC as backup PC for just you know PC stuff\n\nNo need for virtualization, maybe torrenting but i torrent to SSD, HDD wont work for that with my 2.5Gb fiber connection\n\nI do have bunch of different SSDs, that i want to utilize for cache thou, 2Tb Samsung EVO and bunch of 500Gb Samsung pros \\[which better suited to be cache drives\\]\n\nAlso im a windows guy, afraid Unraid will be confusing for me", "author_fullname": "t2_onbly768", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unraid or Server 2022? [NAS with Parity]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8hcj1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669782832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I want Parity Based server/nas/external HDD for ONE PC 10Gb direct connection and each Pc is also connected to router with slower link&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Unraid 120USD&lt;/p&gt;\n\n&lt;p&gt;Windows 2022 Server 9$ [found online shop, works for me]&lt;/p&gt;\n\n&lt;p&gt;How parity works in windows spaces? I have HDDs of many sizes 16,14, 12 [all my HDDs totaling near 370Tb, but I dont need so much and ill be using all 16 and 14tb, no 12 or 10, maybe 250ish Tb]&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I know on Unraid you chose Parity drives yourself so you can just set the biggest HDD&amp;#39;s manually.&lt;/p&gt;\n\n&lt;p&gt;But from the videos i seen about windows, you create a pool of all drives and it creates parity across all of them there isnt  &amp;quot;parity HDD&amp;quot; by itself.&lt;/p&gt;\n\n&lt;p&gt;So how its going to work with different sized drives?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I prefer to use windows for ease of use, and so i can use that PC as backup PC for just you know PC stuff&lt;/p&gt;\n\n&lt;p&gt;No need for virtualization, maybe torrenting but i torrent to SSD, HDD wont work for that with my 2.5Gb fiber connection&lt;/p&gt;\n\n&lt;p&gt;I do have bunch of different SSDs, that i want to utilize for cache thou, 2Tb Samsung EVO and bunch of 500Gb Samsung pros [which better suited to be cache drives]&lt;/p&gt;\n\n&lt;p&gt;Also im a windows guy, afraid Unraid will be confusing for me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8hcj1", "is_robot_indexable": true, "report_reasons": null, "author": "-Hexenhammer-", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8hcj1/unraid_or_server_2022_nas_with_parity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8hcj1/unraid_or_server_2022_nas_with_parity/", "subreddit_subscribers": 656848, "created_utc": 1669782832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nI'm trying to download some videos from this site - [https://www.skatehype.com](https://www.skatehype.com/) \\- most of the browser extensions (in Edge) I've tried haven't been able to detect the video, the one that can is unable to download because it continutally pauses after getting too many errors.\n\nIf I inspect, network, media etc it looks like the video file is [https://www.skatehype.com/s/v/12/3/12383.mp4](https://www.skatehype.com/s/v/12/3/12383.mp4) \\- but I can find a way of using this to download the video.\n\nAny pointers in the right directions would be greatly appreciated, thanks.", "author_fullname": "t2_liqtl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help hoarding video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z825og", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669747171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to download some videos from this site - &lt;a href=\"https://www.skatehype.com/\"&gt;https://www.skatehype.com&lt;/a&gt; - most of the browser extensions (in Edge) I&amp;#39;ve tried haven&amp;#39;t been able to detect the video, the one that can is unable to download because it continutally pauses after getting too many errors.&lt;/p&gt;\n\n&lt;p&gt;If I inspect, network, media etc it looks like the video file is &lt;a href=\"https://www.skatehype.com/s/v/12/3/12383.mp4\"&gt;https://www.skatehype.com/s/v/12/3/12383.mp4&lt;/a&gt; - but I can find a way of using this to download the video.&lt;/p&gt;\n\n&lt;p&gt;Any pointers in the right directions would be greatly appreciated, thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z825og", "is_robot_indexable": true, "report_reasons": null, "author": "gorillabankrolls", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z825og/help_hoarding_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z825og/help_hoarding_video/", "subreddit_subscribers": 656848, "created_utc": 1669747171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm new to this and debating getting a NAS for general backup.\n\nI understand that if a drive fails you can just swap it out and the NAS will repair/repopulate data from the other drive, but I don't know what would happen if the NAS itself failed? Do you just connect the drives to PC and copy/paste the files inside, or do you have to get another NAS of the same brand and hook the drives up to the new NAS to access those files?\n\nI'm planning on getting a 2 bay Synology and set up RAID 1.", "author_fullname": "t2_5cwbkrur", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recovering data when NAS fails (the actual NAS not the disks)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z86qo5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669757242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to this and debating getting a NAS for general backup.&lt;/p&gt;\n\n&lt;p&gt;I understand that if a drive fails you can just swap it out and the NAS will repair/repopulate data from the other drive, but I don&amp;#39;t know what would happen if the NAS itself failed? Do you just connect the drives to PC and copy/paste the files inside, or do you have to get another NAS of the same brand and hook the drives up to the new NAS to access those files?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning on getting a 2 bay Synology and set up RAID 1.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z86qo5", "is_robot_indexable": true, "report_reasons": null, "author": "khoa-gritson", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z86qo5/recovering_data_when_nas_fails_the_actual_nas_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z86qo5/recovering_data_when_nas_fails_the_actual_nas_not/", "subreddit_subscribers": 656848, "created_utc": 1669757242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a complete noob when it comes to technology, and I desperately need your help.\n\nI want a storage solution for family photos and videos kept on over 20 separate CDs and 2-3 USB drives.\n\nI found this HDD in a local store and was wondering whether it could be used for storage (connected to a docking station) from time to time.\n\nI read that this is a surveillance HDD that can be used 24/7, so my question is if it can be used occasionally, say once every two months, and then stored somewhere else without power?", "author_fullname": "t2_kh69bbbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can SEAGATE SkyHawk be used for storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8qsqv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669813269.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669812953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a complete noob when it comes to technology, and I desperately need your help.&lt;/p&gt;\n\n&lt;p&gt;I want a storage solution for family photos and videos kept on over 20 separate CDs and 2-3 USB drives.&lt;/p&gt;\n\n&lt;p&gt;I found this HDD in a local store and was wondering whether it could be used for storage (connected to a docking station) from time to time.&lt;/p&gt;\n\n&lt;p&gt;I read that this is a surveillance HDD that can be used 24/7, so my question is if it can be used occasionally, say once every two months, and then stored somewhere else without power?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8qsqv", "is_robot_indexable": true, "report_reasons": null, "author": "the-emotional-emu", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8qsqv/can_seagate_skyhawk_be_used_for_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8qsqv/can_seagate_skyhawk_be_used_for_storage/", "subreddit_subscribers": 656848, "created_utc": 1669812953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Well it finally happened to me, one of my 14TB's in my Synology failed. It's one of 2 14TB's that I shucked and I kept everything (box, plastic shells, cables, etc) and opened it without breaking any clips; HOWEVER it doesn't look like I marked which internal drive came from which enclosure.\n\nI read through some threads here and it seems very hit or miss either way (sending bare shucked drive or reassembling). I'm guessing there's no way for us to tell which internal drive belongs in which shell?", "author_fullname": "t2_p5ds8v6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD 14TB HDD - RMA bare shucked drive or in enclosure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8rsqx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669816861.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669815685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Well it finally happened to me, one of my 14TB&amp;#39;s in my Synology failed. It&amp;#39;s one of 2 14TB&amp;#39;s that I shucked and I kept everything (box, plastic shells, cables, etc) and opened it without breaking any clips; HOWEVER it doesn&amp;#39;t look like I marked which internal drive came from which enclosure.&lt;/p&gt;\n\n&lt;p&gt;I read through some threads here and it seems very hit or miss either way (sending bare shucked drive or reassembling). I&amp;#39;m guessing there&amp;#39;s no way for us to tell which internal drive belongs in which shell?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8rsqx", "is_robot_indexable": true, "report_reasons": null, "author": "CiViCKiDD", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8rsqx/wd_14tb_hdd_rma_bare_shucked_drive_or_in_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8rsqx/wd_14tb_hdd_rma_bare_shucked_drive_or_in_enclosure/", "subreddit_subscribers": 656848, "created_utc": 1669815685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I planning to use 30 hdds in hba mode no software raid just direct attached drives. Are Adaptec HBA Ultra 1200p-32i is a good choice.  \nhttps://www.microchip.com/en-us/product/HBA-1200up-32i", "author_fullname": "t2_x5b73", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adaptec HBA Ultra 1200p-32i", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8mhqq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669798477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I planning to use 30 hdds in hba mode no software raid just direct attached drives. Are Adaptec HBA Ultra 1200p-32i is a good choice.&lt;br/&gt;\n&lt;a href=\"https://www.microchip.com/en-us/product/HBA-1200up-32i\"&gt;https://www.microchip.com/en-us/product/HBA-1200up-32i&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8mhqq", "is_robot_indexable": true, "report_reasons": null, "author": "BlastSD", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8mhqq/adaptec_hba_ultra_1200p32i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8mhqq/adaptec_hba_ultra_1200p32i/", "subreddit_subscribers": 656848, "created_utc": 1669798477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, \n\nI recently bought some Seagate Exos 18tb (ST1800NM000J) and the SMART data does not look right. \n\nThe fields such as Read Error Rate, Seek Error Rate, Airflow Temperature, Emergency retract Count, Load Unload Count, Temperature, Head Flight Hours have really high values.\nI've ran a write test using Victoria for Windows and it did not find any bad blocks. I'm currently running a long test using SeaTools to further check. \n\nPictures: https://imgur.com/a/FcNpy6d\n\nDoes anyone have Exos drives with SMART data like this?", "author_fullname": "t2_11y9ym", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Exos 18tb SMART Data Inaccuracies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8lghx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669794964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I recently bought some Seagate Exos 18tb (ST1800NM000J) and the SMART data does not look right. &lt;/p&gt;\n\n&lt;p&gt;The fields such as Read Error Rate, Seek Error Rate, Airflow Temperature, Emergency retract Count, Load Unload Count, Temperature, Head Flight Hours have really high values.\nI&amp;#39;ve ran a write test using Victoria for Windows and it did not find any bad blocks. I&amp;#39;m currently running a long test using SeaTools to further check. &lt;/p&gt;\n\n&lt;p&gt;Pictures: &lt;a href=\"https://imgur.com/a/FcNpy6d\"&gt;https://imgur.com/a/FcNpy6d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Does anyone have Exos drives with SMART data like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hOOlOUay_6gntSHHDouA23YINz30PCHM5AY68Wf-6rg.jpg?auto=webp&amp;s=5334c689263701bf021521d17d5e40ff7f7b1a36", "width": 1336, "height": 1456}, "resolutions": [{"url": "https://external-preview.redd.it/hOOlOUay_6gntSHHDouA23YINz30PCHM5AY68Wf-6rg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ec7caf26771c2d35518dac93ac6edb9b26eb863", "width": 108, "height": 117}, {"url": "https://external-preview.redd.it/hOOlOUay_6gntSHHDouA23YINz30PCHM5AY68Wf-6rg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=93aabc907da0ae43b0e6846cc6ef2a719ef31560", "width": 216, "height": 235}, {"url": "https://external-preview.redd.it/hOOlOUay_6gntSHHDouA23YINz30PCHM5AY68Wf-6rg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f695fbed309c9b1844c046c00521c0d7b6a174d", "width": 320, "height": 348}, {"url": "https://external-preview.redd.it/hOOlOUay_6gntSHHDouA23YINz30PCHM5AY68Wf-6rg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c41625114266fb836b56b12bda171280eef176da", "width": 640, "height": 697}, {"url": "https://external-preview.redd.it/hOOlOUay_6gntSHHDouA23YINz30PCHM5AY68Wf-6rg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=52109e6508414ddc036c50067f6cb89e03677fc1", "width": 960, "height": 1046}, {"url": "https://external-preview.redd.it/hOOlOUay_6gntSHHDouA23YINz30PCHM5AY68Wf-6rg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=78972489befec29d6e92bab2e965054ab5387f4c", "width": 1080, "height": 1177}], "variants": {}, "id": "Kra4M3aMO7lqfPFnuufUfG6ZgmboKM3pTXzYSAFBHNA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8lghx", "is_robot_indexable": true, "report_reasons": null, "author": "thetoshman1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8lghx/seagate_exos_18tb_smart_data_inaccuracies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8lghx/seagate_exos_18tb_smart_data_inaccuracies/", "subreddit_subscribers": 656848, "created_utc": 1669794964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys, \n\nI'm having a hard time finding a quality 6+ port USB hub to hook temporarily (4-5 months) six MyBooks (all 14TB) to a mini-PC on a Type-C USB 3.1 Gen 2 port. Typical usage would be copy/move stuff between the internal SSD drive to any one of the six MyBooks (one operation at a time). The important thing is the presence of individual on/off switches, so the drives would only run on demand (I need each one only 2-3 times a week and don't want them spinning up randomly or at boot/shutdown).\n\nThe only semi-legit device I can find is [this hub by StarTech](https://www.startech.com/en-eu/cards-adapters/5g7aibs-usb-hub-eu), however, most people in the sub have a pretty bad opinion about that brand and the price seems a bit steep, to be honest. That being said, the last thing I want is for my data to become corrupted because of some alphabet soup brand's bad hardware... (speaking of which, any recommendations for a solid USB-B to Type-C data cable are much appreciated!)\n\nWhat I like about StarTech's device is the use of actual switches, and not buttons, which usually don't remember the last state after power-cycling...\n\nThanks for reading and have a good one!", "author_fullname": "t2_r8d6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which USB hub with individual on/off switches?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z85s5u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669755129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having a hard time finding a quality 6+ port USB hub to hook temporarily (4-5 months) six MyBooks (all 14TB) to a mini-PC on a Type-C USB 3.1 Gen 2 port. Typical usage would be copy/move stuff between the internal SSD drive to any one of the six MyBooks (one operation at a time). The important thing is the presence of individual on/off switches, so the drives would only run on demand (I need each one only 2-3 times a week and don&amp;#39;t want them spinning up randomly or at boot/shutdown).&lt;/p&gt;\n\n&lt;p&gt;The only semi-legit device I can find is &lt;a href=\"https://www.startech.com/en-eu/cards-adapters/5g7aibs-usb-hub-eu\"&gt;this hub by StarTech&lt;/a&gt;, however, most people in the sub have a pretty bad opinion about that brand and the price seems a bit steep, to be honest. That being said, the last thing I want is for my data to become corrupted because of some alphabet soup brand&amp;#39;s bad hardware... (speaking of which, any recommendations for a solid USB-B to Type-C data cable are much appreciated!)&lt;/p&gt;\n\n&lt;p&gt;What I like about StarTech&amp;#39;s device is the use of actual switches, and not buttons, which usually don&amp;#39;t remember the last state after power-cycling...&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading and have a good one!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WWUhFm1UH1d76VeFn5Jq1cUqG3TT3n3wMI2Dar76WPY.jpg?auto=webp&amp;s=0d14aa9fae0a561c95e7ee557dceb55abc4fafa9", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/WWUhFm1UH1d76VeFn5Jq1cUqG3TT3n3wMI2Dar76WPY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=102bc651816f4608d640676c682a5cc3eac686b1", "width": 108, "height": 108}], "variants": {}, "id": "Yoe5OAAoT-eQrXAUU7Zn4hlPTdiNYSyG2maWRB4c7Ts"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z85s5u", "is_robot_indexable": true, "report_reasons": null, "author": "kraddock", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z85s5u/which_usb_hub_with_individual_onoff_switches/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z85s5u/which_usb_hub_with_individual_onoff_switches/", "subreddit_subscribers": 656848, "created_utc": 1669755129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Long story short I'm pretty sure this isn't possible but y'all would know if there's some weird way to do it so I figured I would ask. Basically I have ultra critical stuff on M-disks, but I know there are those 100gb mdisks. Is it possible to burn a few gigs to an M-disk, and then come back later, do a file compare and add more files to the same disk? Keeping everything that's already on the disk just adding more data. Thank you!", "author_fullname": "t2_tfp74", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding files to already burnt M-Disk?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z82bni", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669747538.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I&amp;#39;m pretty sure this isn&amp;#39;t possible but y&amp;#39;all would know if there&amp;#39;s some weird way to do it so I figured I would ask. Basically I have ultra critical stuff on M-disks, but I know there are those 100gb mdisks. Is it possible to burn a few gigs to an M-disk, and then come back later, do a file compare and add more files to the same disk? Keeping everything that&amp;#39;s already on the disk just adding more data. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z82bni", "is_robot_indexable": true, "report_reasons": null, "author": "rickyh7", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z82bni/adding_files_to_already_burnt_mdisk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z82bni/adding_files_to_already_burnt_mdisk/", "subreddit_subscribers": 656848, "created_utc": 1669747538.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I managed to save a huge treasure trove of rare audio content that could be deleted by the site at any time. To be able to rip the site, I injected simple javascript that replaced the audio player with chronologically listing the mp3-files from the player as &lt;a&gt; tags.\n\nUnfortunately, the audio files are quite randomly named, and often not ID3-tagged at all. \n\nHere are the two relevant parts of a podcast HTML page:\n\n**Metadata part:**\n\n    &lt;script type=\"application/ld+json\"&gt;\n        {\n            \"@context\": \"https://schema.org/\",\n            \"@type\": \"Podcast\",\n            \"@id\": \"https://podcast.com/podcast/sidsel-dalen-21-netter/#player\",\n            \"abridged\": \"false\",\n            \"requiresSubscription\": \"false\",\n            \"author\": \"Author Name\",\n            \"name\": \"21 netter\",\n            \"description\": \"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\",\n            \"datePublished\": \"2010\",\n            \"character\": \"Mia Piasen\",\n            \"keywords\": \"Mia Piasen\",\n            \"duration\": \"PT10H46M00S\",\n            \"genre\": \"Krimpodcast\",\n            \"readBy\": \"Hanna Jakobsen\"\n            }\n        }\n    &lt;/script&gt;\n\n**File link part:**\n\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6771.mp3\" &gt;track_6771.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6772.mp3\" &gt;track_6772.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6773.mp3\" &gt;track_6773.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6774.mp3\" &gt;track_6774.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6775.mp3\" &gt;track_6775.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6776.mp3\" &gt;track_6776.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6777.mp3\" &gt;track_6777.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6778.mp3\" &gt;track_6778.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6779.mp3\" &gt;track_6779.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6780.mp3\" &gt;track_6780.mp3&lt;/a&gt;&lt;br&gt;\n\nHow do I extract that metadata info from the HTML page and use it to rename, move and ID3-tag the linked MP3 files in the right order?", "author_fullname": "t2_3dqo0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to rename, move and ID3-tag MP3 files with info and order from HTML pages that link to them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8p2md", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669807530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I managed to save a huge treasure trove of rare audio content that could be deleted by the site at any time. To be able to rip the site, I injected simple javascript that replaced the audio player with chronologically listing the mp3-files from the player as &amp;lt;a&amp;gt; tags.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, the audio files are quite randomly named, and often not ID3-tagged at all. &lt;/p&gt;\n\n&lt;p&gt;Here are the two relevant parts of a podcast HTML page:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Metadata part:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;lt;script type=&amp;quot;application/ld+json&amp;quot;&amp;gt;\n    {\n        &amp;quot;@context&amp;quot;: &amp;quot;https://schema.org/&amp;quot;,\n        &amp;quot;@type&amp;quot;: &amp;quot;Podcast&amp;quot;,\n        &amp;quot;@id&amp;quot;: &amp;quot;https://podcast.com/podcast/sidsel-dalen-21-netter/#player&amp;quot;,\n        &amp;quot;abridged&amp;quot;: &amp;quot;false&amp;quot;,\n        &amp;quot;requiresSubscription&amp;quot;: &amp;quot;false&amp;quot;,\n        &amp;quot;author&amp;quot;: &amp;quot;Author Name&amp;quot;,\n        &amp;quot;name&amp;quot;: &amp;quot;21 netter&amp;quot;,\n        &amp;quot;description&amp;quot;: &amp;quot;&amp;quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&amp;quot;&amp;quot;,\n        &amp;quot;datePublished&amp;quot;: &amp;quot;2010&amp;quot;,\n        &amp;quot;character&amp;quot;: &amp;quot;Mia Piasen&amp;quot;,\n        &amp;quot;keywords&amp;quot;: &amp;quot;Mia Piasen&amp;quot;,\n        &amp;quot;duration&amp;quot;: &amp;quot;PT10H46M00S&amp;quot;,\n        &amp;quot;genre&amp;quot;: &amp;quot;Krimpodcast&amp;quot;,\n        &amp;quot;readBy&amp;quot;: &amp;quot;Hanna Jakobsen&amp;quot;\n        }\n    }\n&amp;lt;/script&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;File link part:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6771.mp3&amp;quot; &amp;gt;track_6771.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6772.mp3&amp;quot; &amp;gt;track_6772.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6773.mp3&amp;quot; &amp;gt;track_6773.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6774.mp3&amp;quot; &amp;gt;track_6774.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6775.mp3&amp;quot; &amp;gt;track_6775.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6776.mp3&amp;quot; &amp;gt;track_6776.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6777.mp3&amp;quot; &amp;gt;track_6777.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6778.mp3&amp;quot; &amp;gt;track_6778.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6779.mp3&amp;quot; &amp;gt;track_6779.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6780.mp3&amp;quot; &amp;gt;track_6780.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;How do I extract that metadata info from the HTML page and use it to rename, move and ID3-tag the linked MP3 files in the right order?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8p2md", "is_robot_indexable": true, "report_reasons": null, "author": "kris33", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8p2md/how_to_rename_move_and_id3tag_mp3_files_with_info/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8p2md/how_to_rename_move_and_id3tag_mp3_files_with_info/", "subreddit_subscribers": 656848, "created_utc": 1669807530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\n I take a lot of photos in RAW formats and they get difficult to store. I typically transfer the files to external drives, but they fill up and I have to label them and then keep them around in storage in case I need to access them again. It's a pain in the butt and I've gotten tired of having a ton of small external drives around. I would like to have one giant storage device that I don't need to physically connect to my computer. \n\nTo that effect, I'm trying to set up a moderately sized local network storage device to dump files into so I can have more local drive space on my laptop. I'm planning 20-30TB to start, but I'll probably expand it as time goes on. I've got an AXE7500 router, so bandwidth for file dump and retreival isn't a huge bottleneck for my purposes. The convenience of having the drive available wirelessly is worth the potentially slower transfer speeds. Looking around, I think that my main options are NAS devices, RAID arrays, and \"dumb storage\" multi-bay enclosures. I'm trying to understand my options within those choices.\n\nAs i understand it, a NAS setup works similarly to a fileserver in that an operating system within the device handles requests within a local network. As I understand it, they would also allow for file access over the internet as well. Depending on how complicated they are, they can run programs off device. The main issue for me here is that they seem crazy expensive. \n\nA RAID array would essentially be multiple drives linked via hardware and/or software to act as a large drive with backup redundancy built in. The main use for these seems to be backup and protecting data against equipment failure?\n\nThe dumb storage is essentially just a big external drive plugged into my router. Likely be the shittiest in terms of speed, but they agree with my budget. \n\nDo I understand these 3 systems correctly? If I do, what kind of flexibility would I have in each choice? I don't want to be forced into learning linux or learning to be a network admin, I just want a storage drive that I can access via my home network and over the internet in a pinch. If I can set up something that let's me access files while away from the home network, that would also be great.", "author_fullname": "t2_i67bb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "looking to understand options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8nz0r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669803682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I take a lot of photos in RAW formats and they get difficult to store. I typically transfer the files to external drives, but they fill up and I have to label them and then keep them around in storage in case I need to access them again. It&amp;#39;s a pain in the butt and I&amp;#39;ve gotten tired of having a ton of small external drives around. I would like to have one giant storage device that I don&amp;#39;t need to physically connect to my computer. &lt;/p&gt;\n\n&lt;p&gt;To that effect, I&amp;#39;m trying to set up a moderately sized local network storage device to dump files into so I can have more local drive space on my laptop. I&amp;#39;m planning 20-30TB to start, but I&amp;#39;ll probably expand it as time goes on. I&amp;#39;ve got an AXE7500 router, so bandwidth for file dump and retreival isn&amp;#39;t a huge bottleneck for my purposes. The convenience of having the drive available wirelessly is worth the potentially slower transfer speeds. Looking around, I think that my main options are NAS devices, RAID arrays, and &amp;quot;dumb storage&amp;quot; multi-bay enclosures. I&amp;#39;m trying to understand my options within those choices.&lt;/p&gt;\n\n&lt;p&gt;As i understand it, a NAS setup works similarly to a fileserver in that an operating system within the device handles requests within a local network. As I understand it, they would also allow for file access over the internet as well. Depending on how complicated they are, they can run programs off device. The main issue for me here is that they seem crazy expensive. &lt;/p&gt;\n\n&lt;p&gt;A RAID array would essentially be multiple drives linked via hardware and/or software to act as a large drive with backup redundancy built in. The main use for these seems to be backup and protecting data against equipment failure?&lt;/p&gt;\n\n&lt;p&gt;The dumb storage is essentially just a big external drive plugged into my router. Likely be the shittiest in terms of speed, but they agree with my budget. &lt;/p&gt;\n\n&lt;p&gt;Do I understand these 3 systems correctly? If I do, what kind of flexibility would I have in each choice? I don&amp;#39;t want to be forced into learning linux or learning to be a network admin, I just want a storage drive that I can access via my home network and over the internet in a pinch. If I can set up something that let&amp;#39;s me access files while away from the home network, that would also be great.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8nz0r", "is_robot_indexable": true, "report_reasons": null, "author": "Lotaxi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8nz0r/looking_to_understand_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8nz0r/looking_to_understand_options/", "subreddit_subscribers": 656848, "created_utc": 1669803682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently struggling to come up with a good data integrity check for my backups. I am using \\[restic\\]([https://restic.net/](https://restic.net/)) (which has a lot of the same features as borg) as a backup engine. It already includes \\[integrity and consistency checks\\]([https://restic.readthedocs.io/en/stable/045\\_working\\_with\\_repos.html#checking-integrity-and-consistency](https://restic.readthedocs.io/en/stable/045_working_with_repos.html#checking-integrity-and-consistency)) but I feel like the described \\`check\\` command still leaves room for silently corrupted data, even with its \\`--read-data\\` flag : the integrity seems to be checked against hashes of already-packed data, but what if something happened during the packing process ?\n\nI'm looking for a integrity check scheme that checks restored data against checksums computed from the originally backed up data, such as described in \\[this issue I opened on the restic repo\\]([https://github.com/restic/restic/issues/4057](https://github.com/restic/restic/issues/4057)). I'm aware this kind of scheme must take into account the possibility of the data mutating on the source disk while being backed up.\n\nIs the Restic integrity check enough ? Am I being paranoid ? Are you aware of any way to achieve my goal ?\n\nThank you to anyone who takes the time to answer, even with an incomplete answer. I'm usually decent at googling, but I'm not having luck with this one.", "author_fullname": "t2_2lq2eg3f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What makes a good backup integrity check ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8ncss", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669801625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently struggling to come up with a good data integrity check for my backups. I am using [restic](&lt;a href=\"https://restic.net/\"&gt;https://restic.net/&lt;/a&gt;) (which has a lot of the same features as borg) as a backup engine. It already includes [integrity and consistency checks](&lt;a href=\"https://restic.readthedocs.io/en/stable/045_working_with_repos.html#checking-integrity-and-consistency\"&gt;https://restic.readthedocs.io/en/stable/045_working_with_repos.html#checking-integrity-and-consistency&lt;/a&gt;) but I feel like the described `check` command still leaves room for silently corrupted data, even with its `--read-data` flag : the integrity seems to be checked against hashes of already-packed data, but what if something happened during the packing process ?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a integrity check scheme that checks restored data against checksums computed from the originally backed up data, such as described in [this issue I opened on the restic repo](&lt;a href=\"https://github.com/restic/restic/issues/4057\"&gt;https://github.com/restic/restic/issues/4057&lt;/a&gt;). I&amp;#39;m aware this kind of scheme must take into account the possibility of the data mutating on the source disk while being backed up.&lt;/p&gt;\n\n&lt;p&gt;Is the Restic integrity check enough ? Am I being paranoid ? Are you aware of any way to achieve my goal ?&lt;/p&gt;\n\n&lt;p&gt;Thank you to anyone who takes the time to answer, even with an incomplete answer. I&amp;#39;m usually decent at googling, but I&amp;#39;m not having luck with this one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8ncss", "is_robot_indexable": true, "report_reasons": null, "author": "pcouy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8ncss/what_makes_a_good_backup_integrity_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8ncss/what_makes_a_good_backup_integrity_check/", "subreddit_subscribers": 656848, "created_utc": 1669801625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI've been working on a project that requires to convert a website to static HTML.\n\nSo far I've been working with Heritrix and I love it.\nAfter that, I use Warcat to concatenate every .warc.gz files into one,\u00a0and then I use\u00a0warc2html to convert that warc.gz into static HTML files.\n\nHowever, the warc2html project doesn't always give the best results.\nDoes anyone have any feedback/tools that might be useful for me?\n\nThanks!", "author_fullname": "t2_1naagq16", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Warc to Static HTML | Heritrix -&gt; Warcat -&gt; warc2html", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8n0rg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669800393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working on a project that requires to convert a website to static HTML.&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve been working with Heritrix and I love it.\nAfter that, I use Warcat to concatenate every .warc.gz files into one,\u00a0and then I use\u00a0warc2html to convert that warc.gz into static HTML files.&lt;/p&gt;\n\n&lt;p&gt;However, the warc2html project doesn&amp;#39;t always give the best results.\nDoes anyone have any feedback/tools that might be useful for me?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8n0rg", "is_robot_indexable": true, "report_reasons": null, "author": "xhicoBala", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8n0rg/warc_to_static_html_heritrix_warcat_warc2html/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8n0rg/warc_to_static_html_heritrix_warcat_warc2html/", "subreddit_subscribers": 656848, "created_utc": 1669800393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had an old server with an 8-drive RAIDZ-2 configuration for my data storage. I have created a new server and my intention was to plug in my PCI to 6 SATA port card and do an import on the ZFS pool. When I booted everything up I discovered the PCI card I was using was not recognized by my motherboard. I did some digging and it looks like the common recommendation is to get an LSI card flashed in \"IT mode\" to see the disks recognized. The card I purchased was this one:\n\n[https://www.newegg.com/p/14G-0006-001G5?Item=9SIAWFGJP29168](https://www.newegg.com/p/14G-0006-001G5?Item=9SIAWFGJP29168)\n\nI received the card today and plugged everything in. After booting into Ubuntu and running lsblk I discovered the drives are not being recognized. Running lspci reveals the LSI card is being recognized. I rebooted the computer and jumped into the WebBIOS for the card to see what was going on. The card is recognizing all 8 drives labeled as:\n\n    Slot:0,SATA,HDD,7.277TB, JBOD\n    Slot:1,SATA,HDD,7.277TB, JBOD\n    Slot:2,SATA,HDD,7.277TB, JBOD\n    Slot:3,SATA,HDD,7.277TB, JBOD\n    Slot:4,SATA,HDD,7.277TB, JBOD\n    Slot:5,SATA,HDD,7.277TB, JBOD\n    Slot:6,SATA,HDD,7.277TB, JBOD\n    Slot:7,SATA,HDD,7.277TB, JBOD\n\nThis looks correct to me. Why can't I see these drives in Ubuntu? Do I need to do something in WebBIOS in order to pass-through the information? Is there a way for me to check if this was actually flashed in IT mode?\n\nThe big thing I am scared of is losing the data on the drives. I don't have the ability to make a backup before messing with these drives and I'm terrified the card is going to try and initialize something.\n\nAny help would be greatly appreciated.", "author_fullname": "t2_68ots9ni", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating RAIDZ to new computer - How to make sure I don't lose data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8fmhw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669778313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had an old server with an 8-drive RAIDZ-2 configuration for my data storage. I have created a new server and my intention was to plug in my PCI to 6 SATA port card and do an import on the ZFS pool. When I booted everything up I discovered the PCI card I was using was not recognized by my motherboard. I did some digging and it looks like the common recommendation is to get an LSI card flashed in &amp;quot;IT mode&amp;quot; to see the disks recognized. The card I purchased was this one:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.newegg.com/p/14G-0006-001G5?Item=9SIAWFGJP29168\"&gt;https://www.newegg.com/p/14G-0006-001G5?Item=9SIAWFGJP29168&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I received the card today and plugged everything in. After booting into Ubuntu and running lsblk I discovered the drives are not being recognized. Running lspci reveals the LSI card is being recognized. I rebooted the computer and jumped into the WebBIOS for the card to see what was going on. The card is recognizing all 8 drives labeled as:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Slot:0,SATA,HDD,7.277TB, JBOD\nSlot:1,SATA,HDD,7.277TB, JBOD\nSlot:2,SATA,HDD,7.277TB, JBOD\nSlot:3,SATA,HDD,7.277TB, JBOD\nSlot:4,SATA,HDD,7.277TB, JBOD\nSlot:5,SATA,HDD,7.277TB, JBOD\nSlot:6,SATA,HDD,7.277TB, JBOD\nSlot:7,SATA,HDD,7.277TB, JBOD\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This looks correct to me. Why can&amp;#39;t I see these drives in Ubuntu? Do I need to do something in WebBIOS in order to pass-through the information? Is there a way for me to check if this was actually flashed in IT mode?&lt;/p&gt;\n\n&lt;p&gt;The big thing I am scared of is losing the data on the drives. I don&amp;#39;t have the ability to make a backup before messing with these drives and I&amp;#39;m terrified the card is going to try and initialize something.&lt;/p&gt;\n\n&lt;p&gt;Any help would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8fmhw", "is_robot_indexable": true, "report_reasons": null, "author": "tomsrobots", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8fmhw/migrating_raidz_to_new_computer_how_to_make_sure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8fmhw/migrating_raidz_to_new_computer_how_to_make_sure/", "subreddit_subscribers": 656848, "created_utc": 1669778313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've decided to go blu ray as a backup. \n\nI have some questions please.\n\n1. What drive to buy (what speed and quality/reliability ) \n2. What software (I want to point a large 500gb folder and the software split the size to each disk)\n3. What disks are recommended.", "author_fullname": "t2_iz37lez6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blu ray backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z84wlv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669753233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve decided to go blu ray as a backup. &lt;/p&gt;\n\n&lt;p&gt;I have some questions please.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What drive to buy (what speed and quality/reliability ) &lt;/li&gt;\n&lt;li&gt;What software (I want to point a large 500gb folder and the software split the size to each disk)&lt;/li&gt;\n&lt;li&gt;What disks are recommended.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z84wlv", "is_robot_indexable": true, "report_reasons": null, "author": "green_handl3", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z84wlv/blu_ray_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z84wlv/blu_ray_backup/", "subreddit_subscribers": 656848, "created_utc": 1669753233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there !\n\nI just received 4 ironwolf 4TB drives (ST4000VN006) and started checking them for potential damage from shipping.So i did a smart check on all of them ([https://file.io/0e2x4hA6sV8m](https://file.io/0e2x4hA6sV8m))\n\n    smartctl /dev/sdX -x\n\nand discovered that one of them (Disk 2) is having \"Pending Defects log (GP Log 0x0c)\" unlike the 3 others.\n\nIs that something i should be worried about ? Should i send it back for replacement ?I can't seem to find what those infos means.\n\nI also started a badblocks and a long smart test on all 4 drives, but it's gonna take time.\n\n    badblocks -v /dev/sdX\n    smartctl /dev/sdX -t long\n\nThank you in advance.", "author_fullname": "t2_lcwx2c6y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting \"Pending defects log\" on brand new drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z83kas", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669750310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there !&lt;/p&gt;\n\n&lt;p&gt;I just received 4 ironwolf 4TB drives (ST4000VN006) and started checking them for potential damage from shipping.So i did a smart check on all of them (&lt;a href=\"https://file.io/0e2x4hA6sV8m\"&gt;https://file.io/0e2x4hA6sV8m&lt;/a&gt;)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;smartctl /dev/sdX -x\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;and discovered that one of them (Disk 2) is having &amp;quot;Pending Defects log (GP Log 0x0c)&amp;quot; unlike the 3 others.&lt;/p&gt;\n\n&lt;p&gt;Is that something i should be worried about ? Should i send it back for replacement ?I can&amp;#39;t seem to find what those infos means.&lt;/p&gt;\n\n&lt;p&gt;I also started a badblocks and a long smart test on all 4 drives, but it&amp;#39;s gonna take time.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;badblocks -v /dev/sdX\nsmartctl /dev/sdX -t long\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z83kas", "is_robot_indexable": true, "report_reasons": null, "author": "UsrnameBetween3and20", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z83kas/getting_pending_defects_log_on_brand_new_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z83kas/getting_pending_defects_log_on_brand_new_drives/", "subreddit_subscribers": 656848, "created_utc": 1669750310.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Henlo.\n\nI've finished ripping the Blu-Rays for Avatar: The Last Airbender. And while I'm very happy with the result, each episode clocks in at around 5.5GB which isn't exactly ideal for streaming.\n\nI will of course be keeping these original files, but I've recently built a new PC with a 5700x and a 6700xt so I can once again re-encode things (hooray).\n\nThe BD of ATLA is an upscaled version of the original 480p broadcasts (might've been 480i, unsure). I'm looking for \"visually lossless\", which should be easy as many scenes already look not the best. I'd have to apply lots sharpening filters and such to get a better look, something which I don't have experience with.\n\nAnyhow, using ffmpeg, how do you guys usually re-encode media for smaller sizes? Doom9 mentions a few x265 options specifically, but ofc many clients don't support direct h265 streams.\n\nBonus question: blu ray subs are not fun, what's the best way to turn them into actual subtitles?", "author_fullname": "t2_1hutcmww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Media Server] Best practices for video compression?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8317z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669749112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Henlo.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve finished ripping the Blu-Rays for Avatar: The Last Airbender. And while I&amp;#39;m very happy with the result, each episode clocks in at around 5.5GB which isn&amp;#39;t exactly ideal for streaming.&lt;/p&gt;\n\n&lt;p&gt;I will of course be keeping these original files, but I&amp;#39;ve recently built a new PC with a 5700x and a 6700xt so I can once again re-encode things (hooray).&lt;/p&gt;\n\n&lt;p&gt;The BD of ATLA is an upscaled version of the original 480p broadcasts (might&amp;#39;ve been 480i, unsure). I&amp;#39;m looking for &amp;quot;visually lossless&amp;quot;, which should be easy as many scenes already look not the best. I&amp;#39;d have to apply lots sharpening filters and such to get a better look, something which I don&amp;#39;t have experience with.&lt;/p&gt;\n\n&lt;p&gt;Anyhow, using ffmpeg, how do you guys usually re-encode media for smaller sizes? Doom9 mentions a few x265 options specifically, but ofc many clients don&amp;#39;t support direct h265 streams.&lt;/p&gt;\n\n&lt;p&gt;Bonus question: blu ray subs are not fun, what&amp;#39;s the best way to turn them into actual subtitles?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Spent $330 for a single show", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8317z", "is_robot_indexable": true, "report_reasons": null, "author": "General-Stryker", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z8317z/media_server_best_practices_for_video_compression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8317z/media_server_best_practices_for_video_compression/", "subreddit_subscribers": 656848, "created_utc": 1669749112.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi\n\nI'm buying a new computer, but I have a problem, the case I want, have only 2 hdd drive bays, while I have about 8, so I am going to need some external drive cases\n\nTime ago, I had 2 cheap ones from amazon, one the typical 2 3.5 with a backup button and another for a single drive\n\nI ended stopping using both of them, because no matter what I did, and how I configured the power management of the usbs, the cases turned off my hdd every 10 minutes if I didn't use it. Meaning that when I wanted to use, I needed to wait 10 seconds, or started to hear distracting turn on noises on the hdds working\n\nSo, is out there any decent case that won't destroy the hdd with vibrations and that won't force my drives to stop and boot 50 times per day?\n\nAlso, if the case can work with usb 3 without an external source of power, that would be great.", "author_fullname": "t2_59mxp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a good external 3.5 case that doesn't turn off my hdd every 5 minutes of inactivity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z82tl1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669748639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m buying a new computer, but I have a problem, the case I want, have only 2 hdd drive bays, while I have about 8, so I am going to need some external drive cases&lt;/p&gt;\n\n&lt;p&gt;Time ago, I had 2 cheap ones from amazon, one the typical 2 3.5 with a backup button and another for a single drive&lt;/p&gt;\n\n&lt;p&gt;I ended stopping using both of them, because no matter what I did, and how I configured the power management of the usbs, the cases turned off my hdd every 10 minutes if I didn&amp;#39;t use it. Meaning that when I wanted to use, I needed to wait 10 seconds, or started to hear distracting turn on noises on the hdds working&lt;/p&gt;\n\n&lt;p&gt;So, is out there any decent case that won&amp;#39;t destroy the hdd with vibrations and that won&amp;#39;t force my drives to stop and boot 50 times per day?&lt;/p&gt;\n\n&lt;p&gt;Also, if the case can work with usb 3 without an external source of power, that would be great.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z82tl1", "is_robot_indexable": true, "report_reasons": null, "author": "DrKersh", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z82tl1/looking_for_a_good_external_35_case_that_doesnt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z82tl1/looking_for_a_good_external_35_case_that_doesnt/", "subreddit_subscribers": 656848, "created_utc": 1669748639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Thank you for your input.  My current SSD just failed on me.", "author_fullname": "t2_11dj0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a good price for SAMSUNG 870 EVO 4TB 2.5 that is brand new?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z81uny", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669746516.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thank you for your input.  My current SSD just failed on me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "30TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z81uny", "is_robot_indexable": true, "report_reasons": null, "author": "nando1969", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z81uny/what_is_a_good_price_for_samsung_870_evo_4tb_25/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z81uny/what_is_a_good_price_for_samsung_870_evo_4tb_25/", "subreddit_subscribers": 656848, "created_utc": 1669746516.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nWhen I copy files from a source to iDrive e2 using rclone, I am getting a lot of errors from e2:\n\n\"upload corrupted: Etag differ:\"\n\nrclone then retries and succeeds.\n\nIs that normal?", "author_fullname": "t2_lwumyho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "iDrive e2 (S3-compatible storage): lots of errors \"upload corrupted: Etag differ:\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z81oge", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669746121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;When I copy files from a source to iDrive e2 using rclone, I am getting a lot of errors from e2:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;upload corrupted: Etag differ:&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;rclone then retries and succeeds.&lt;/p&gt;\n\n&lt;p&gt;Is that normal?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z81oge", "is_robot_indexable": true, "report_reasons": null, "author": "TedBob99", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z81oge/idrive_e2_s3compatible_storage_lots_of_errors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z81oge/idrive_e2_s3compatible_storage_lots_of_errors/", "subreddit_subscribers": 656848, "created_utc": 1669746121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for a solution for cloning images of my drives that works for Linux systems and supports incremental backups. My current solution is Clonezilla. Works really well but requires me to clone the entire drive every time.\n\nI've combed through the community posts. Many seem to recommend Macrium Reflect. The posts are a bit on the older side, though. Is Macrium still a good option? \n\nAlso, not strictly necessary; but is there an option that allows me to keep using my system as it does the backup? I don't think even Macrium allows that on Linux systems.\n\nI'd really appreciate any guidance.", "author_fullname": "t2_40b70wjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental drive imaging solutions for Linux.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z80szt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669744183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a solution for cloning images of my drives that works for Linux systems and supports incremental backups. My current solution is Clonezilla. Works really well but requires me to clone the entire drive every time.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve combed through the community posts. Many seem to recommend Macrium Reflect. The posts are a bit on the older side, though. Is Macrium still a good option? &lt;/p&gt;\n\n&lt;p&gt;Also, not strictly necessary; but is there an option that allows me to keep using my system as it does the backup? I don&amp;#39;t think even Macrium allows that on Linux systems.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d really appreciate any guidance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z80szt", "is_robot_indexable": true, "report_reasons": null, "author": "Ushahin_Ceann", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z80szt/incremental_drive_imaging_solutions_for_linux/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z80szt/incremental_drive_imaging_solutions_for_linux/", "subreddit_subscribers": 656848, "created_utc": 1669744183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all. I\u2019m looking to expand my media server and add extra hdd. Question about Sata power. I have a 650w modular psu. I\u2019m out of Sata connections, but have 2 molex on the psu. Would molex to Sata be a complete NO? Anyone have experience using Sata splitters? Those are my current only two options without buying a new psu with more Sata ports. Any feedback is greatly appreciated.", "author_fullname": "t2_q4jng28", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD power support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z7ziyi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669741286.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. I\u2019m looking to expand my media server and add extra hdd. Question about Sata power. I have a 650w modular psu. I\u2019m out of Sata connections, but have 2 molex on the psu. Would molex to Sata be a complete NO? Anyone have experience using Sata splitters? Those are my current only two options without buying a new psu with more Sata ports. Any feedback is greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z7ziyi", "is_robot_indexable": true, "report_reasons": null, "author": "3rdmangreen", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z7ziyi/hdd_power_support/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z7ziyi/hdd_power_support/", "subreddit_subscribers": 656848, "created_utc": 1669741286.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I run Linux Mint. I'm currently doing sync with help of my custom Python script developed to get rid of duplicates and manual operations. As of now I mostly add data to say couple of backup disks (add to one or the other, sync one to other with checksum control), connected via USB (local disks). Two-way sync is important for me, I tend to modify not one disk.\n\nI want to perform large re-shuffle (move/rename folders/files) of my data (and be able to do so in the future efficiently and easily). I want to be able to do it in GUI for one disk/location and be able to quickly propagate changes to other disk, both ways. Is there any software that can do that?\n\nWhat I've looked at so far:\n\n`rsync` - that classic does not identify moves/renames, I recall reading developers talk about it but it did not get into mainstream as of now AFAIK.\n\n`Syncthing` does not do local per FAQ.\n\n`Unison` docs say \"Unison sees the rename as a delete and a separate create\", I've tried it and indeed looks so.\n\n`btrfs` snapshots. Not sure yet, but in https://www.reddit.com/r/btrfs/comments/odpybp/btrfs_to_share_data_between_machines/ : \"Two-way syncing is not possible using btrfs snapshots and btrfs send/receive\"; \"I would not recommend them for backup purposes however; relying on filesystem-specific structures is not sustainable.\". \n\nIf there none is found, how difficult it is to use `inotify()` to get list of moves of inodes? Does `inotify()` provide information same as in `mv path1 path2` command?\n\nBack to `Syncthing`. If somebody has extensive experience, sees it can do what I want and can explain how to set up one PC so that it runs two `Syncthing` instances that will communicate and sync - you are welcome.\n\nTIA", "author_fullname": "t2_o727buez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any free Linux local file syncing software that efficiently handles moves and do checksum?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z8ti34", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669821088.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669820074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run Linux Mint. I&amp;#39;m currently doing sync with help of my custom Python script developed to get rid of duplicates and manual operations. As of now I mostly add data to say couple of backup disks (add to one or the other, sync one to other with checksum control), connected via USB (local disks). Two-way sync is important for me, I tend to modify not one disk.&lt;/p&gt;\n\n&lt;p&gt;I want to perform large re-shuffle (move/rename folders/files) of my data (and be able to do so in the future efficiently and easily). I want to be able to do it in GUI for one disk/location and be able to quickly propagate changes to other disk, both ways. Is there any software that can do that?&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;ve looked at so far:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;rsync&lt;/code&gt; - that classic does not identify moves/renames, I recall reading developers talk about it but it did not get into mainstream as of now AFAIK.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Syncthing&lt;/code&gt; does not do local per FAQ.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Unison&lt;/code&gt; docs say &amp;quot;Unison sees the rename as a delete and a separate create&amp;quot;, I&amp;#39;ve tried it and indeed looks so.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;btrfs&lt;/code&gt; snapshots. Not sure yet, but in &lt;a href=\"https://www.reddit.com/r/btrfs/comments/odpybp/btrfs_to_share_data_between_machines/\"&gt;https://www.reddit.com/r/btrfs/comments/odpybp/btrfs_to_share_data_between_machines/&lt;/a&gt; : &amp;quot;Two-way syncing is not possible using btrfs snapshots and btrfs send/receive&amp;quot;; &amp;quot;I would not recommend them for backup purposes however; relying on filesystem-specific structures is not sustainable.&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;If there none is found, how difficult it is to use &lt;code&gt;inotify()&lt;/code&gt; to get list of moves of inodes? Does &lt;code&gt;inotify()&lt;/code&gt; provide information same as in &lt;code&gt;mv path1 path2&lt;/code&gt; command?&lt;/p&gt;\n\n&lt;p&gt;Back to &lt;code&gt;Syncthing&lt;/code&gt;. If somebody has extensive experience, sees it can do what I want and can explain how to set up one PC so that it runs two &lt;code&gt;Syncthing&lt;/code&gt; instances that will communicate and sync - you are welcome.&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8ti34", "is_robot_indexable": true, "report_reasons": null, "author": "UncertainAboutIt", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8ti34/any_free_linux_local_file_syncing_software_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8ti34/any_free_linux_local_file_syncing_software_that/", "subreddit_subscribers": 656848, "created_utc": 1669820074.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}