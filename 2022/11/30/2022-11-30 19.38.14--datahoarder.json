{"kind": "Listing", "data": {"after": "t3_z8ncss", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_hbwoyw04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "20tb Sata/SAS $269.99-$304.99 -or- 18tb Sata $189.99 Seagate recertified with 2 year warranty, wish I had the money, thought I'd share here!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_z8irvw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 144, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 144, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/O4bB82ABNHVdXXdfMBZQnBvA5Mk6ksZ9Ns88tZbB3rw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669786924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "serverpartdeals.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://serverpartdeals.com/collections/recertified-seagate-exos", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?auto=webp&amp;s=4c1db49fea90382fef14d5213d071fac818998ff", "width": 2000, "height": 2000}, "resolutions": [{"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e7ff784b044c5a132792a51783e12546aa096933", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ebb5efa12a9e165a566f9c8d169e9861369acd2f", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=763ac82ff67bfc932ed93a299e8d40c5271a9258", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=487c4ebc47f1af49ca7b4ed1e032a4d96672a443", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=32fed97e301ba788e5b9661d6052a5e41a4a23c1", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/Fs6C9svF_Yci9-7roJ3v71orbPpfFyPn329BHhNSQU8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f4c9097a5b110531b40d5950876ae2530c195a25", "width": 1080, "height": 1080}], "variants": {}, "id": "_D0H66BxLDdRjM-Vxtc776wegerPH5Vt6qcZASPwe_c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "z8irvw", "is_robot_indexable": true, "report_reasons": null, "author": "igmyeongui", "discussion_type": null, "num_comments": 58, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8irvw/20tb_satasas_2699930499_or_18tb_sata_18999/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://serverpartdeals.com/collections/recertified-seagate-exos", "subreddit_subscribers": 656875, "created_utc": 1669786924.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I want to ask the community for some emotional help. I've lurked for a long time but made an account today just for this post(and hope to stop by occasionally).\n\nI'm not exactly a major datahoarder but I'm definitely a sentimentalist. Recently thanks to some dumb decisions on my part and not paying attention while working on hard drives, and some circumstances that led to me losing my backups, I lost 2TB of super important sentimental data spanning 18 years almost. I feel like an idiot for screwing this up so badly. It's been three weeks and I'm horribly depressed to say the least. I feel like I lost a loved one.\n\nThose who can share experiences and give advice, please do, I want to stop feeling so much intense grief. Or at least reduce it.\n\n(Before anyone brings it up, no, data recovery is impossible)", "author_fullname": "t2_uo1fs1kg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coping with dataloss?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8u78p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669821733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I want to ask the community for some emotional help. I&amp;#39;ve lurked for a long time but made an account today just for this post(and hope to stop by occasionally).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not exactly a major datahoarder but I&amp;#39;m definitely a sentimentalist. Recently thanks to some dumb decisions on my part and not paying attention while working on hard drives, and some circumstances that led to me losing my backups, I lost 2TB of super important sentimental data spanning 18 years almost. I feel like an idiot for screwing this up so badly. It&amp;#39;s been three weeks and I&amp;#39;m horribly depressed to say the least. I feel like I lost a loved one.&lt;/p&gt;\n\n&lt;p&gt;Those who can share experiences and give advice, please do, I want to stop feeling so much intense grief. Or at least reduce it.&lt;/p&gt;\n\n&lt;p&gt;(Before anyone brings it up, no, data recovery is impossible)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z8u78p", "is_robot_indexable": true, "report_reasons": null, "author": "Scramatic", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8u78p/coping_with_dataloss/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8u78p/coping_with_dataloss/", "subreddit_subscribers": 656875, "created_utc": 1669821733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought a new, faster NVMe drive and cloned my Windows system drive onto it using Macrium\n\nHowever, there's a small discrepancy in the total files/file sizes.\n\nIs there a tool to analyze both drives and show me which files it finds that differ between the two?\n\nEverything seems to be (mostly) in order, but there's a 6GB difference in used space on the two drives.\n\n(Also worth noting that my taskbar flashes for a few seconds while the desktop loads on the cloned drive, which doesn't happen if I boot from the old drive. I am using an app called TaskbarX, but it isn't working properly since cloning and I don't think that's the issue as the flashing was happening prior to re-configuring that program. In any case, this isn't the proper subreddit for that particular issue)", "author_fullname": "t2_7d7au", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tools to show the difference in files between two cloned drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8l12y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669793579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a new, faster NVMe drive and cloned my Windows system drive onto it using Macrium&lt;/p&gt;\n\n&lt;p&gt;However, there&amp;#39;s a small discrepancy in the total files/file sizes.&lt;/p&gt;\n\n&lt;p&gt;Is there a tool to analyze both drives and show me which files it finds that differ between the two?&lt;/p&gt;\n\n&lt;p&gt;Everything seems to be (mostly) in order, but there&amp;#39;s a 6GB difference in used space on the two drives.&lt;/p&gt;\n\n&lt;p&gt;(Also worth noting that my taskbar flashes for a few seconds while the desktop loads on the cloned drive, which doesn&amp;#39;t happen if I boot from the old drive. I am using an app called TaskbarX, but it isn&amp;#39;t working properly since cloning and I don&amp;#39;t think that&amp;#39;s the issue as the flashing was happening prior to re-configuring that program. In any case, this isn&amp;#39;t the proper subreddit for that particular issue)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z8l12y", "is_robot_indexable": true, "report_reasons": null, "author": "MiguelLancaster", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8l12y/best_tools_to_show_the_difference_in_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8l12y/best_tools_to_show_the_difference_in_files/", "subreddit_subscribers": 656875, "created_utc": 1669793579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A while ago I made a huge mistake and I lost all my data on my exteenal hard drive, over 2TB of stuff just gone. The expert I talked to said that there was no chances of recovery. However during my Chrisrmas cleaning I found a backup of an older hard-drive that I completely forgot about and I managed to recover 80% of my data. Through my Oblivion and Sims mods are gone as they were only on the destroyed drive, I got most of my audiobooks, ebooks and comics back, as well as personal pictures and data. I feel so relieved. I was fully prepared to have to start from 0 and was grieving my loss and then this. I could dance. From now on I will make regular backups and not put all my eggs in one basket.\n\nEdit: Story for any mistakes english is not my first language.", "author_fullname": "t2_iw79c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I found the backup and got 80% of my data back", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8xd9w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669829250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A while ago I made a huge mistake and I lost all my data on my exteenal hard drive, over 2TB of stuff just gone. The expert I talked to said that there was no chances of recovery. However during my Chrisrmas cleaning I found a backup of an older hard-drive that I completely forgot about and I managed to recover 80% of my data. Through my Oblivion and Sims mods are gone as they were only on the destroyed drive, I got most of my audiobooks, ebooks and comics back, as well as personal pictures and data. I feel so relieved. I was fully prepared to have to start from 0 and was grieving my loss and then this. I could dance. From now on I will make regular backups and not put all my eggs in one basket.&lt;/p&gt;\n\n&lt;p&gt;Edit: Story for any mistakes english is not my first language.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z8xd9w", "is_robot_indexable": true, "report_reasons": null, "author": "RedRiverValley", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8xd9w/i_found_the_backup_and_got_80_of_my_data_back/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8xd9w/i_found_the_backup_and_got_80_of_my_data_back/", "subreddit_subscribers": 656875, "created_utc": 1669829250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI want Parity Based server/nas/external HDD for ONE PC 10Gb direct connection and each Pc is also connected to router with slower link\n\n&amp;#x200B;\n\nUnraid 120USD\n\nWindows 2022 Server 9$ \\[found online shop, works for me\\]\n\nHow parity works in windows spaces? I have HDDs of many sizes 16,14, 12 \\[all my HDDs totaling near 370Tb, but I dont need so much and ill be using all 16 and 14tb, no 12 or 10, maybe 250ish Tb\\]\n\n&amp;#x200B;\n\nI know on Unraid you chose Parity drives yourself so you can just set the biggest HDD's manually.\n\nBut from the videos i seen about windows, you create a pool of all drives and it creates parity across all of them there isnt  \"parity HDD\" by itself.\n\nSo how its going to work with different sized drives?\n\n&amp;#x200B;\n\nI prefer to use windows for ease of use, and so i can use that PC as backup PC for just you know PC stuff\n\nNo need for virtualization, maybe torrenting but i torrent to SSD, HDD wont work for that with my 2.5Gb fiber connection\n\nI do have bunch of different SSDs, that i want to utilize for cache thou, 2Tb Samsung EVO and bunch of 500Gb Samsung pros \\[which better suited to be cache drives\\]\n\nAlso im a windows guy, afraid Unraid will be confusing for me", "author_fullname": "t2_onbly768", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unraid or Server 2022? [NAS with Parity]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8hcj1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669782832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I want Parity Based server/nas/external HDD for ONE PC 10Gb direct connection and each Pc is also connected to router with slower link&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Unraid 120USD&lt;/p&gt;\n\n&lt;p&gt;Windows 2022 Server 9$ [found online shop, works for me]&lt;/p&gt;\n\n&lt;p&gt;How parity works in windows spaces? I have HDDs of many sizes 16,14, 12 [all my HDDs totaling near 370Tb, but I dont need so much and ill be using all 16 and 14tb, no 12 or 10, maybe 250ish Tb]&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I know on Unraid you chose Parity drives yourself so you can just set the biggest HDD&amp;#39;s manually.&lt;/p&gt;\n\n&lt;p&gt;But from the videos i seen about windows, you create a pool of all drives and it creates parity across all of them there isnt  &amp;quot;parity HDD&amp;quot; by itself.&lt;/p&gt;\n\n&lt;p&gt;So how its going to work with different sized drives?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I prefer to use windows for ease of use, and so i can use that PC as backup PC for just you know PC stuff&lt;/p&gt;\n\n&lt;p&gt;No need for virtualization, maybe torrenting but i torrent to SSD, HDD wont work for that with my 2.5Gb fiber connection&lt;/p&gt;\n\n&lt;p&gt;I do have bunch of different SSDs, that i want to utilize for cache thou, 2Tb Samsung EVO and bunch of 500Gb Samsung pros [which better suited to be cache drives]&lt;/p&gt;\n\n&lt;p&gt;Also im a windows guy, afraid Unraid will be confusing for me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8hcj1", "is_robot_indexable": true, "report_reasons": null, "author": "-Hexenhammer-", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8hcj1/unraid_or_server_2022_nas_with_parity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8hcj1/unraid_or_server_2022_nas_with_parity/", "subreddit_subscribers": 656875, "created_utc": 1669782832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Well it finally happened to me, one of my 14TB's in my Synology failed. It's one of 2 14TB's that I shucked and I kept everything (box, plastic shells, cables, etc) and opened it without breaking any clips; HOWEVER it doesn't look like I marked which internal drive came from which enclosure.\n\nI read through some threads here and it seems very hit or miss either way (sending bare shucked drive or reassembling). I'm guessing there's no way for us to tell which internal drive belongs in which shell?", "author_fullname": "t2_p5ds8v6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD 14TB HDD - RMA bare shucked drive or in enclosure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8rsqx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669816861.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669815685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Well it finally happened to me, one of my 14TB&amp;#39;s in my Synology failed. It&amp;#39;s one of 2 14TB&amp;#39;s that I shucked and I kept everything (box, plastic shells, cables, etc) and opened it without breaking any clips; HOWEVER it doesn&amp;#39;t look like I marked which internal drive came from which enclosure.&lt;/p&gt;\n\n&lt;p&gt;I read through some threads here and it seems very hit or miss either way (sending bare shucked drive or reassembling). I&amp;#39;m guessing there&amp;#39;s no way for us to tell which internal drive belongs in which shell?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8rsqx", "is_robot_indexable": true, "report_reasons": null, "author": "CiViCKiDD", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8rsqx/wd_14tb_hdd_rma_bare_shucked_drive_or_in_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8rsqx/wd_14tb_hdd_rma_bare_shucked_drive_or_in_enclosure/", "subreddit_subscribers": 656875, "created_utc": 1669815685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm new to this and debating getting a NAS for general backup.\n\nI understand that if a drive fails you can just swap it out and the NAS will repair/repopulate data from the other drive, but I don't know what would happen if the NAS itself failed? Do you just connect the drives to PC and copy/paste the files inside, or do you have to get another NAS of the same brand and hook the drives up to the new NAS to access those files?\n\nI'm planning on getting a 2 bay Synology and set up RAID 1.", "author_fullname": "t2_5cwbkrur", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recovering data when NAS fails (the actual NAS not the disks)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z86qo5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669757242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to this and debating getting a NAS for general backup.&lt;/p&gt;\n\n&lt;p&gt;I understand that if a drive fails you can just swap it out and the NAS will repair/repopulate data from the other drive, but I don&amp;#39;t know what would happen if the NAS itself failed? Do you just connect the drives to PC and copy/paste the files inside, or do you have to get another NAS of the same brand and hook the drives up to the new NAS to access those files?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning on getting a 2 bay Synology and set up RAID 1.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z86qo5", "is_robot_indexable": true, "report_reasons": null, "author": "khoa-gritson", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z86qo5/recovering_data_when_nas_fails_the_actual_nas_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z86qo5/recovering_data_when_nas_fails_the_actual_nas_not/", "subreddit_subscribers": 656875, "created_utc": 1669757242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a complete noob when it comes to technology, and I desperately need your help.\n\nI want a storage solution for family photos and videos kept on over 20 separate CDs and 2-3 USB drives.\n\nI found this HDD in a local store and was wondering whether it could be used for storage (connected to a docking station) from time to time.\n\nI read that this is a surveillance HDD that can be used 24/7, so my question is if it can be used occasionally, say once every two months, and then stored somewhere else without power?", "author_fullname": "t2_kh69bbbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can SEAGATE SkyHawk be used for storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8qsqv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669813269.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669812953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a complete noob when it comes to technology, and I desperately need your help.&lt;/p&gt;\n\n&lt;p&gt;I want a storage solution for family photos and videos kept on over 20 separate CDs and 2-3 USB drives.&lt;/p&gt;\n\n&lt;p&gt;I found this HDD in a local store and was wondering whether it could be used for storage (connected to a docking station) from time to time.&lt;/p&gt;\n\n&lt;p&gt;I read that this is a surveillance HDD that can be used 24/7, so my question is if it can be used occasionally, say once every two months, and then stored somewhere else without power?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8qsqv", "is_robot_indexable": true, "report_reasons": null, "author": "the-emotional-emu", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8qsqv/can_seagate_skyhawk_be_used_for_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8qsqv/can_seagate_skyhawk_be_used_for_storage/", "subreddit_subscribers": 656875, "created_utc": 1669812953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I planning to use 30 hdds in hba mode no software raid just direct attached drives. Are Adaptec HBA Ultra 1200p-32i is a good choice.  \nhttps://www.microchip.com/en-us/product/HBA-1200up-32i", "author_fullname": "t2_x5b73", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adaptec HBA Ultra 1200p-32i", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8mhqq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669798477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I planning to use 30 hdds in hba mode no software raid just direct attached drives. Are Adaptec HBA Ultra 1200p-32i is a good choice.&lt;br/&gt;\n&lt;a href=\"https://www.microchip.com/en-us/product/HBA-1200up-32i\"&gt;https://www.microchip.com/en-us/product/HBA-1200up-32i&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8mhqq", "is_robot_indexable": true, "report_reasons": null, "author": "BlastSD", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8mhqq/adaptec_hba_ultra_1200p32i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8mhqq/adaptec_hba_ultra_1200p32i/", "subreddit_subscribers": 656875, "created_utc": 1669798477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a doctoral student, and while I've been storing copies of my research files (camera trap photos, R code files, some GIS files, etc) on an external drive, I've been thinking about adding a cloud storage service for redundancy. My research files at the moment total just under 100 GB (my system in total takes up under 350 GB, in case a full system backup is what is recommended). I don't expect to access these files often (ideally never) but in case something happens on my machine, or I just do something stupid and delete a file/folder I need, I want to be able to quickly access/retrieve the needed files/folders (so not necessarily retrieving the whole system). It would be nice to encrypt my research files, but I'm not working on anything where I am unduly worried about data theft in any case. I'd say my priorities are: \n\n1. Reliability \n2. Cost (of storage primarily, then upload/download)\n3. Download Speed/accessibility \n4. Security/Privacy \n5. Upload speed\n\nI use a Windows machine from Lenovo if that makes any difference, but could conceivably move to a Macbook down the line. I use File History with my external backup drive.\n\nI'm posting cus I see so many different recommendations from iDrive to Backblaze to IceDrive to Mega, etc, including in this subreddit, and I figure at this point I'm overthinking it and should seek recs for my specific use case scenario (or maybe for what I want, it doesn't really matter all that much what I choose)", "author_fullname": "t2_16h7mr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best data storage solution for a researcher", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8xcf6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669829197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a doctoral student, and while I&amp;#39;ve been storing copies of my research files (camera trap photos, R code files, some GIS files, etc) on an external drive, I&amp;#39;ve been thinking about adding a cloud storage service for redundancy. My research files at the moment total just under 100 GB (my system in total takes up under 350 GB, in case a full system backup is what is recommended). I don&amp;#39;t expect to access these files often (ideally never) but in case something happens on my machine, or I just do something stupid and delete a file/folder I need, I want to be able to quickly access/retrieve the needed files/folders (so not necessarily retrieving the whole system). It would be nice to encrypt my research files, but I&amp;#39;m not working on anything where I am unduly worried about data theft in any case. I&amp;#39;d say my priorities are: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Reliability &lt;/li&gt;\n&lt;li&gt;Cost (of storage primarily, then upload/download)&lt;/li&gt;\n&lt;li&gt;Download Speed/accessibility &lt;/li&gt;\n&lt;li&gt;Security/Privacy &lt;/li&gt;\n&lt;li&gt;Upload speed&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I use a Windows machine from Lenovo if that makes any difference, but could conceivably move to a Macbook down the line. I use File History with my external backup drive.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m posting cus I see so many different recommendations from iDrive to Backblaze to IceDrive to Mega, etc, including in this subreddit, and I figure at this point I&amp;#39;m overthinking it and should seek recs for my specific use case scenario (or maybe for what I want, it doesn&amp;#39;t really matter all that much what I choose)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8xcf6", "is_robot_indexable": true, "report_reasons": null, "author": "TigerLeader", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8xcf6/best_data_storage_solution_for_a_researcher/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8xcf6/best_data_storage_solution_for_a_researcher/", "subreddit_subscribers": 656875, "created_utc": 1669829197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m not sure if I\u2019m in the right place\u2026 but I just gained access to my old MySpace Account and was trying to gather the photos from it, but they are not loading. \n\nI\u2019m aware that typically means MySpace no longer has them. But is there any way to recover them? \n\nI have tried:\nWayback machine\nImage downloading extensions\nAnd more. \n\nThank you!", "author_fullname": "t2_42i85vo6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MySpace Photo Recovery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8vqea", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669825415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m not sure if I\u2019m in the right place\u2026 but I just gained access to my old MySpace Account and was trying to gather the photos from it, but they are not loading. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m aware that typically means MySpace no longer has them. But is there any way to recover them? &lt;/p&gt;\n\n&lt;p&gt;I have tried:\nWayback machine\nImage downloading extensions\nAnd more. &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8vqea", "is_robot_indexable": true, "report_reasons": null, "author": "lacefacehardin", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8vqea/myspace_photo_recovery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8vqea/myspace_photo_recovery/", "subreddit_subscribers": 656875, "created_utc": 1669825415.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm not sure if this is allowed here. Let me also preface this by saying this is for personal use. I also tried to search the sub but mainly came across Bluray and DVD ripping.\n\nOn some sites like Mubi, there are very interesting films which can be hard to come across in other places. I have discovered many rare films this way and when they remove them from the catalog they are very hard to find. I work in film stuff and would love to be able to revisit many of these later on.\n\nCan anyone point me to how I could do this? Any sites, videos, posts, etc, which explain this would be greatly appreciated.\n\nThank you", "author_fullname": "t2_90me57qk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ripping from streaming sites?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8ug9c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669822347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not sure if this is allowed here. Let me also preface this by saying this is for personal use. I also tried to search the sub but mainly came across Bluray and DVD ripping.&lt;/p&gt;\n\n&lt;p&gt;On some sites like Mubi, there are very interesting films which can be hard to come across in other places. I have discovered many rare films this way and when they remove them from the catalog they are very hard to find. I work in film stuff and would love to be able to revisit many of these later on.&lt;/p&gt;\n\n&lt;p&gt;Can anyone point me to how I could do this? Any sites, videos, posts, etc, which explain this would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8ug9c", "is_robot_indexable": true, "report_reasons": null, "author": "Powerful-Employer-20", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8ug9c/ripping_from_streaming_sites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8ug9c/ripping_from_streaming_sites/", "subreddit_subscribers": 656875, "created_utc": 1669822347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to archive all of my personal data onto M-Disks (around 500gb). I want to get 5 100GB Blu-Ray M-Disks for a long-term cold storage, but I've heard in passing that DVD's are more reliable, but they hold a lot less. Is the difference negligible or for long-term storage is one better than the other?", "author_fullname": "t2_r17l20lj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DVD or Blu Ray M-Disks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8t4cb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669819113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to archive all of my personal data onto M-Disks (around 500gb). I want to get 5 100GB Blu-Ray M-Disks for a long-term cold storage, but I&amp;#39;ve heard in passing that DVD&amp;#39;s are more reliable, but they hold a lot less. Is the difference negligible or for long-term storage is one better than the other?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8t4cb", "is_robot_indexable": true, "report_reasons": null, "author": "PassportNerd", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8t4cb/dvd_or_blu_ray_mdisks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8t4cb/dvd_or_blu_ray_mdisks/", "subreddit_subscribers": 656875, "created_utc": 1669819113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had an old server with an 8-drive RAIDZ-2 configuration for my data storage. I have created a new server and my intention was to plug in my PCI to 6 SATA port card and do an import on the ZFS pool. When I booted everything up I discovered the PCI card I was using was not recognized by my motherboard. I did some digging and it looks like the common recommendation is to get an LSI card flashed in \"IT mode\" to see the disks recognized. The card I purchased was this one:\n\n[https://www.newegg.com/p/14G-0006-001G5?Item=9SIAWFGJP29168](https://www.newegg.com/p/14G-0006-001G5?Item=9SIAWFGJP29168)\n\nI received the card today and plugged everything in. After booting into Ubuntu and running lsblk I discovered the drives are not being recognized. Running lspci reveals the LSI card is being recognized. I rebooted the computer and jumped into the WebBIOS for the card to see what was going on. The card is recognizing all 8 drives labeled as:\n\n    Slot:0,SATA,HDD,7.277TB, JBOD\n    Slot:1,SATA,HDD,7.277TB, JBOD\n    Slot:2,SATA,HDD,7.277TB, JBOD\n    Slot:3,SATA,HDD,7.277TB, JBOD\n    Slot:4,SATA,HDD,7.277TB, JBOD\n    Slot:5,SATA,HDD,7.277TB, JBOD\n    Slot:6,SATA,HDD,7.277TB, JBOD\n    Slot:7,SATA,HDD,7.277TB, JBOD\n\nThis looks correct to me. Why can't I see these drives in Ubuntu? Do I need to do something in WebBIOS in order to pass-through the information? Is there a way for me to check if this was actually flashed in IT mode?\n\nThe big thing I am scared of is losing the data on the drives. I don't have the ability to make a backup before messing with these drives and I'm terrified the card is going to try and initialize something.\n\nAny help would be greatly appreciated.", "author_fullname": "t2_68ots9ni", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating RAIDZ to new computer - How to make sure I don't lose data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8fmhw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669778313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had an old server with an 8-drive RAIDZ-2 configuration for my data storage. I have created a new server and my intention was to plug in my PCI to 6 SATA port card and do an import on the ZFS pool. When I booted everything up I discovered the PCI card I was using was not recognized by my motherboard. I did some digging and it looks like the common recommendation is to get an LSI card flashed in &amp;quot;IT mode&amp;quot; to see the disks recognized. The card I purchased was this one:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.newegg.com/p/14G-0006-001G5?Item=9SIAWFGJP29168\"&gt;https://www.newegg.com/p/14G-0006-001G5?Item=9SIAWFGJP29168&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I received the card today and plugged everything in. After booting into Ubuntu and running lsblk I discovered the drives are not being recognized. Running lspci reveals the LSI card is being recognized. I rebooted the computer and jumped into the WebBIOS for the card to see what was going on. The card is recognizing all 8 drives labeled as:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Slot:0,SATA,HDD,7.277TB, JBOD\nSlot:1,SATA,HDD,7.277TB, JBOD\nSlot:2,SATA,HDD,7.277TB, JBOD\nSlot:3,SATA,HDD,7.277TB, JBOD\nSlot:4,SATA,HDD,7.277TB, JBOD\nSlot:5,SATA,HDD,7.277TB, JBOD\nSlot:6,SATA,HDD,7.277TB, JBOD\nSlot:7,SATA,HDD,7.277TB, JBOD\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This looks correct to me. Why can&amp;#39;t I see these drives in Ubuntu? Do I need to do something in WebBIOS in order to pass-through the information? Is there a way for me to check if this was actually flashed in IT mode?&lt;/p&gt;\n\n&lt;p&gt;The big thing I am scared of is losing the data on the drives. I don&amp;#39;t have the ability to make a backup before messing with these drives and I&amp;#39;m terrified the card is going to try and initialize something.&lt;/p&gt;\n\n&lt;p&gt;Any help would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8fmhw", "is_robot_indexable": true, "report_reasons": null, "author": "tomsrobots", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8fmhw/migrating_raidz_to_new_computer_how_to_make_sure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8fmhw/migrating_raidz_to_new_computer_how_to_make_sure/", "subreddit_subscribers": 656875, "created_utc": 1669778313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys, \n\nI'm having a hard time finding a quality 6+ port USB hub to hook temporarily (4-5 months) six MyBooks (all 14TB) to a mini-PC on a Type-C USB 3.1 Gen 2 port. Typical usage would be copy/move stuff between the internal SSD drive to any one of the six MyBooks (one operation at a time). The important thing is the presence of individual on/off switches, so the drives would only run on demand (I need each one only 2-3 times a week and don't want them spinning up randomly or at boot/shutdown).\n\nThe only semi-legit device I can find is [this hub by StarTech](https://www.startech.com/en-eu/cards-adapters/5g7aibs-usb-hub-eu), however, most people in the sub have a pretty bad opinion about that brand and the price seems a bit steep, to be honest. That being said, the last thing I want is for my data to become corrupted because of some alphabet soup brand's bad hardware... (speaking of which, any recommendations for a solid USB-B to Type-C data cable are much appreciated!)\n\nWhat I like about StarTech's device is the use of actual switches, and not buttons, which usually don't remember the last state after power-cycling...\n\nThanks for reading and have a good one!", "author_fullname": "t2_r8d6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which USB hub with individual on/off switches?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z85s5u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669755129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having a hard time finding a quality 6+ port USB hub to hook temporarily (4-5 months) six MyBooks (all 14TB) to a mini-PC on a Type-C USB 3.1 Gen 2 port. Typical usage would be copy/move stuff between the internal SSD drive to any one of the six MyBooks (one operation at a time). The important thing is the presence of individual on/off switches, so the drives would only run on demand (I need each one only 2-3 times a week and don&amp;#39;t want them spinning up randomly or at boot/shutdown).&lt;/p&gt;\n\n&lt;p&gt;The only semi-legit device I can find is &lt;a href=\"https://www.startech.com/en-eu/cards-adapters/5g7aibs-usb-hub-eu\"&gt;this hub by StarTech&lt;/a&gt;, however, most people in the sub have a pretty bad opinion about that brand and the price seems a bit steep, to be honest. That being said, the last thing I want is for my data to become corrupted because of some alphabet soup brand&amp;#39;s bad hardware... (speaking of which, any recommendations for a solid USB-B to Type-C data cable are much appreciated!)&lt;/p&gt;\n\n&lt;p&gt;What I like about StarTech&amp;#39;s device is the use of actual switches, and not buttons, which usually don&amp;#39;t remember the last state after power-cycling...&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading and have a good one!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WWUhFm1UH1d76VeFn5Jq1cUqG3TT3n3wMI2Dar76WPY.jpg?auto=webp&amp;s=0d14aa9fae0a561c95e7ee557dceb55abc4fafa9", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/WWUhFm1UH1d76VeFn5Jq1cUqG3TT3n3wMI2Dar76WPY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=102bc651816f4608d640676c682a5cc3eac686b1", "width": 108, "height": 108}], "variants": {}, "id": "Yoe5OAAoT-eQrXAUU7Zn4hlPTdiNYSyG2maWRB4c7Ts"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z85s5u", "is_robot_indexable": true, "report_reasons": null, "author": "kraddock", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z85s5u/which_usb_hub_with_individual_onoff_switches/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z85s5u/which_usb_hub_with_individual_onoff_switches/", "subreddit_subscribers": 656875, "created_utc": 1669755129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there !\n\nI just received 4 ironwolf 4TB drives (ST4000VN006) and started checking them for potential damage from shipping.So i did a smart check on all of them ([https://file.io/0e2x4hA6sV8m](https://file.io/0e2x4hA6sV8m))\n\n    smartctl /dev/sdX -x\n\nand discovered that one of them (Disk 2) is having \"Pending Defects log (GP Log 0x0c)\" unlike the 3 others.\n\nIs that something i should be worried about ? Should i send it back for replacement ?I can't seem to find what those infos means.\n\nI also started a badblocks and a long smart test on all 4 drives, but it's gonna take time.\n\n    badblocks -v /dev/sdX\n    smartctl /dev/sdX -t long\n\nThank you in advance.", "author_fullname": "t2_lcwx2c6y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting \"Pending defects log\" on brand new drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z83kas", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669750310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there !&lt;/p&gt;\n\n&lt;p&gt;I just received 4 ironwolf 4TB drives (ST4000VN006) and started checking them for potential damage from shipping.So i did a smart check on all of them (&lt;a href=\"https://file.io/0e2x4hA6sV8m\"&gt;https://file.io/0e2x4hA6sV8m&lt;/a&gt;)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;smartctl /dev/sdX -x\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;and discovered that one of them (Disk 2) is having &amp;quot;Pending Defects log (GP Log 0x0c)&amp;quot; unlike the 3 others.&lt;/p&gt;\n\n&lt;p&gt;Is that something i should be worried about ? Should i send it back for replacement ?I can&amp;#39;t seem to find what those infos means.&lt;/p&gt;\n\n&lt;p&gt;I also started a badblocks and a long smart test on all 4 drives, but it&amp;#39;s gonna take time.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;badblocks -v /dev/sdX\nsmartctl /dev/sdX -t long\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z83kas", "is_robot_indexable": true, "report_reasons": null, "author": "UsrnameBetween3and20", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z83kas/getting_pending_defects_log_on_brand_new_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z83kas/getting_pending_defects_log_on_brand_new_drives/", "subreddit_subscribers": 656875, "created_utc": 1669750310.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Henlo.\n\nI've finished ripping the Blu-Rays for Avatar: The Last Airbender. And while I'm very happy with the result, each episode clocks in at around 5.5GB which isn't exactly ideal for streaming.\n\nI will of course be keeping these original files, but I've recently built a new PC with a 5700x and a 6700xt so I can once again re-encode things (hooray).\n\nThe BD of ATLA is an upscaled version of the original 480p broadcasts (might've been 480i, unsure). I'm looking for \"visually lossless\", which should be easy as many scenes already look not the best. I'd have to apply lots sharpening filters and such to get a better look, something which I don't have experience with.\n\nAnyhow, using ffmpeg, how do you guys usually re-encode media for smaller sizes? Doom9 mentions a few x265 options specifically, but ofc many clients don't support direct h265 streams.\n\nBonus question: blu ray subs are not fun, what's the best way to turn them into actual subtitles?", "author_fullname": "t2_1hutcmww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Media Server] Best practices for video compression?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8317z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669749112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Henlo.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve finished ripping the Blu-Rays for Avatar: The Last Airbender. And while I&amp;#39;m very happy with the result, each episode clocks in at around 5.5GB which isn&amp;#39;t exactly ideal for streaming.&lt;/p&gt;\n\n&lt;p&gt;I will of course be keeping these original files, but I&amp;#39;ve recently built a new PC with a 5700x and a 6700xt so I can once again re-encode things (hooray).&lt;/p&gt;\n\n&lt;p&gt;The BD of ATLA is an upscaled version of the original 480p broadcasts (might&amp;#39;ve been 480i, unsure). I&amp;#39;m looking for &amp;quot;visually lossless&amp;quot;, which should be easy as many scenes already look not the best. I&amp;#39;d have to apply lots sharpening filters and such to get a better look, something which I don&amp;#39;t have experience with.&lt;/p&gt;\n\n&lt;p&gt;Anyhow, using ffmpeg, how do you guys usually re-encode media for smaller sizes? Doom9 mentions a few x265 options specifically, but ofc many clients don&amp;#39;t support direct h265 streams.&lt;/p&gt;\n\n&lt;p&gt;Bonus question: blu ray subs are not fun, what&amp;#39;s the best way to turn them into actual subtitles?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Spent $330 for a single show", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8317z", "is_robot_indexable": true, "report_reasons": null, "author": "General-Stryker", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z8317z/media_server_best_practices_for_video_compression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8317z/media_server_best_practices_for_video_compression/", "subreddit_subscribers": 656875, "created_utc": 1669749112.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was using RaiDrive with my Google Drive enterprise and when I wanted to install it on another computer, now when I try to add my drive, you cannot choose the account on the \"personal\" page, it wil complain \"accoubnt is for business purpose\" and on the page for business, the \"writable\" is locked behind PRO.\n\nFirst they removed the \"Shared drive\" writable access. And now this too.\n\nVery strange, since it is even the same version I was running, so seems they have made the changes on the server side and that is why it had kept working normally.\n\nWell anyway, I just switched the main Google Drive desktop app and that works fine, it feels faster too.\n\nShame, RaiDrive is really gone then. But at least I can still use that for Dropbox to have it on more than 3 machines like the official Dropbox app is restricted to.", "author_fullname": "t2_10fiz2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FYI: RaiDrive seems to have locked writable Google Drive business finally behind the paid plan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z8zh4h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669834002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was using RaiDrive with my Google Drive enterprise and when I wanted to install it on another computer, now when I try to add my drive, you cannot choose the account on the &amp;quot;personal&amp;quot; page, it wil complain &amp;quot;accoubnt is for business purpose&amp;quot; and on the page for business, the &amp;quot;writable&amp;quot; is locked behind PRO.&lt;/p&gt;\n\n&lt;p&gt;First they removed the &amp;quot;Shared drive&amp;quot; writable access. And now this too.&lt;/p&gt;\n\n&lt;p&gt;Very strange, since it is even the same version I was running, so seems they have made the changes on the server side and that is why it had kept working normally.&lt;/p&gt;\n\n&lt;p&gt;Well anyway, I just switched the main Google Drive desktop app and that works fine, it feels faster too.&lt;/p&gt;\n\n&lt;p&gt;Shame, RaiDrive is really gone then. But at least I can still use that for Dropbox to have it on more than 3 machines like the official Dropbox app is restricted to.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z8zh4h", "is_robot_indexable": true, "report_reasons": null, "author": "Boogertwilliams", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8zh4h/fyi_raidrive_seems_to_have_locked_writable_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8zh4h/fyi_raidrive_seems_to_have_locked_writable_google/", "subreddit_subscribers": 656875, "created_utc": 1669834002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, looking to buy an 1TB HDD for longterm storage. Can you guys tell me which brand/model is the best you can get? 5400 or 7200? Seagate, Western Digital, LaCie or Sandisk? If possible with USB-C since i have an Macbook", "author_fullname": "t2_mhetsykv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which 1TB HDD for longterm storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8xbsc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669829155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, looking to buy an 1TB HDD for longterm storage. Can you guys tell me which brand/model is the best you can get? 5400 or 7200? Seagate, Western Digital, LaCie or Sandisk? If possible with USB-C since i have an Macbook&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8xbsc", "is_robot_indexable": true, "report_reasons": null, "author": "AnarkyGotham", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8xbsc/which_1tb_hdd_for_longterm_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8xbsc/which_1tb_hdd_for_longterm_storage/", "subreddit_subscribers": 656875, "created_utc": 1669829155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to add data to one device in one location and have it sync nightly or hourly with another NAS in a different location on another private network without creating a VPN tunnel.", "author_fullname": "t2_9vbi3ot", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I mirror sync two Synology DS220 across two physical locations without creating a WAN tunnel?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8uvjc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669823380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to add data to one device in one location and have it sync nightly or hourly with another NAS in a different location on another private network without creating a VPN tunnel.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8uvjc", "is_robot_indexable": true, "report_reasons": null, "author": "ambient_whooshing", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8uvjc/can_i_mirror_sync_two_synology_ds220_across_two/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8uvjc/can_i_mirror_sync_two_synology_ds220_across_two/", "subreddit_subscribers": 656875, "created_utc": 1669823380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I run Linux Mint. I'm currently doing sync with help of my custom Python script developed to get rid of duplicates and manual operations. As of now I mostly add data to say couple of backup disks (add to one or the other, sync one to other with checksum control), connected via USB (local disks). Two-way sync is important for me, I tend to modify not one disk.\n\nI want to perform large re-shuffle (move/rename folders/files) of my data (and be able to do so in the future efficiently and easily). I want to be able to do it in GUI for one disk/location and be able to quickly propagate changes to other disk, both ways. Is there any software that can do that?\n\nWhat I've looked at so far:\n\n`rsync` - that classic does not identify moves/renames, I recall reading developers talk about it but it did not get into mainstream as of now AFAIK.\n\n`Syncthing` does not do local per FAQ.\n\n`Unison` docs say \"Unison sees the rename as a delete and a separate create\", I've tried it and indeed looks so.\n\n`btrfs` snapshots. Not sure yet, but in https://www.reddit.com/r/btrfs/comments/odpybp/btrfs_to_share_data_between_machines/ : \"Two-way syncing is not possible using btrfs snapshots and btrfs send/receive\"; \"I would not recommend them for backup purposes however; relying on filesystem-specific structures is not sustainable.\". \n\nIf there none is found, how difficult it is to use `inotify()` to get list of moves of inodes? Does `inotify()` provide information same as in `mv path1 path2` command?\n\nBack to `Syncthing`. If somebody has extensive experience, sees it can do what I want and can explain how to set up one PC so that it runs two `Syncthing` instances that will communicate and sync - you are welcome.\n\nTIA", "author_fullname": "t2_o727buez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any free Linux local file syncing software that efficiently handles moves and do checksum?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8ti34", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669821088.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669820074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run Linux Mint. I&amp;#39;m currently doing sync with help of my custom Python script developed to get rid of duplicates and manual operations. As of now I mostly add data to say couple of backup disks (add to one or the other, sync one to other with checksum control), connected via USB (local disks). Two-way sync is important for me, I tend to modify not one disk.&lt;/p&gt;\n\n&lt;p&gt;I want to perform large re-shuffle (move/rename folders/files) of my data (and be able to do so in the future efficiently and easily). I want to be able to do it in GUI for one disk/location and be able to quickly propagate changes to other disk, both ways. Is there any software that can do that?&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;ve looked at so far:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;rsync&lt;/code&gt; - that classic does not identify moves/renames, I recall reading developers talk about it but it did not get into mainstream as of now AFAIK.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Syncthing&lt;/code&gt; does not do local per FAQ.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Unison&lt;/code&gt; docs say &amp;quot;Unison sees the rename as a delete and a separate create&amp;quot;, I&amp;#39;ve tried it and indeed looks so.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;btrfs&lt;/code&gt; snapshots. Not sure yet, but in &lt;a href=\"https://www.reddit.com/r/btrfs/comments/odpybp/btrfs_to_share_data_between_machines/\"&gt;https://www.reddit.com/r/btrfs/comments/odpybp/btrfs_to_share_data_between_machines/&lt;/a&gt; : &amp;quot;Two-way syncing is not possible using btrfs snapshots and btrfs send/receive&amp;quot;; &amp;quot;I would not recommend them for backup purposes however; relying on filesystem-specific structures is not sustainable.&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;If there none is found, how difficult it is to use &lt;code&gt;inotify()&lt;/code&gt; to get list of moves of inodes? Does &lt;code&gt;inotify()&lt;/code&gt; provide information same as in &lt;code&gt;mv path1 path2&lt;/code&gt; command?&lt;/p&gt;\n\n&lt;p&gt;Back to &lt;code&gt;Syncthing&lt;/code&gt;. If somebody has extensive experience, sees it can do what I want and can explain how to set up one PC so that it runs two &lt;code&gt;Syncthing&lt;/code&gt; instances that will communicate and sync - you are welcome.&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8ti34", "is_robot_indexable": true, "report_reasons": null, "author": "UncertainAboutIt", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8ti34/any_free_linux_local_file_syncing_software_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8ti34/any_free_linux_local_file_syncing_software_that/", "subreddit_subscribers": 656875, "created_utc": 1669820074.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&amp;#x200B;\n\n I am looking to update the firmware of  one MSL2024 and one MSL4048 to be able to install newer LTO drives, but  can't download it since I don't have any kind of contract with HPE. The  links for the download are the follow:  \n \n\n[HPE Support Center](https://support.hpe.com/connect/s/softwaredetails?softwareId=MTX_f5a56567a3f2412aafaa46d6f4)  \n [HPE Support Center](https://support.hpe.com/connect/s/softwaredetails?softwareId=MTX_fa3e6da7b4f24625a6870350f7)  \n \n\nI know it is a lot to ask... but if anyone could download the firmware  files for both the libraries and the drives... and upload them to mega  or something like that, I would apreciate it a lot.", "author_fullname": "t2_13m4j3m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HPE StoreEver firmwares help requested", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8peky", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669808660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am looking to update the firmware of  one MSL2024 and one MSL4048 to be able to install newer LTO drives, but  can&amp;#39;t download it since I don&amp;#39;t have any kind of contract with HPE. The  links for the download are the follow:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://support.hpe.com/connect/s/softwaredetails?softwareId=MTX_f5a56567a3f2412aafaa46d6f4\"&gt;HPE Support Center&lt;/a&gt;&lt;br/&gt;\n &lt;a href=\"https://support.hpe.com/connect/s/softwaredetails?softwareId=MTX_fa3e6da7b4f24625a6870350f7\"&gt;HPE Support Center&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;I know it is a lot to ask... but if anyone could download the firmware  files for both the libraries and the drives... and upload them to mega  or something like that, I would apreciate it a lot.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8peky", "is_robot_indexable": true, "report_reasons": null, "author": "quarkpower", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8peky/hpe_storeever_firmwares_help_requested/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8peky/hpe_storeever_firmwares_help_requested/", "subreddit_subscribers": 656875, "created_utc": 1669808660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I managed to save a huge treasure trove of rare audio content that could be deleted by the site at any time. To be able to rip the site, I injected simple javascript that replaced the audio player with chronologically listing the mp3-files from the player as &lt;a&gt; tags.\n\nUnfortunately, the audio files are quite randomly named, and often not ID3-tagged at all. \n\nHere are the two relevant parts of a podcast HTML page:\n\n**Metadata part:**\n\n    &lt;script type=\"application/ld+json\"&gt;\n        {\n            \"@context\": \"https://schema.org/\",\n            \"@type\": \"Podcast\",\n            \"@id\": \"https://podcast.com/podcast/sidsel-dalen-21-netter/#player\",\n            \"abridged\": \"false\",\n            \"requiresSubscription\": \"false\",\n            \"author\": \"Author Name\",\n            \"name\": \"21 netter\",\n            \"description\": \"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\",\n            \"datePublished\": \"2010\",\n            \"character\": \"Mia Piasen\",\n            \"keywords\": \"Mia Piasen\",\n            \"duration\": \"PT10H46M00S\",\n            \"genre\": \"Krimpodcast\",\n            \"readBy\": \"Hanna Jakobsen\"\n            }\n        }\n    &lt;/script&gt;\n\n**File link part:**\n\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6771.mp3\" &gt;track_6771.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6772.mp3\" &gt;track_6772.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6773.mp3\" &gt;track_6773.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6774.mp3\" &gt;track_6774.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6775.mp3\" &gt;track_6775.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6776.mp3\" &gt;track_6776.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6777.mp3\" &gt;track_6777.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6778.mp3\" &gt;track_6778.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6779.mp3\" &gt;track_6779.mp3&lt;/a&gt;&lt;br&gt;\n    &lt;a class=\"filelinks\" href=\"../../file_storage/m2031/track_6780.mp3\" &gt;track_6780.mp3&lt;/a&gt;&lt;br&gt;\n\nHow do I extract that metadata info from the HTML page and use it to rename, move and ID3-tag the linked MP3 files in the right order?", "author_fullname": "t2_3dqo0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to rename, move and ID3-tag MP3 files with info and order from HTML pages that link to them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8p2md", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669807530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I managed to save a huge treasure trove of rare audio content that could be deleted by the site at any time. To be able to rip the site, I injected simple javascript that replaced the audio player with chronologically listing the mp3-files from the player as &amp;lt;a&amp;gt; tags.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, the audio files are quite randomly named, and often not ID3-tagged at all. &lt;/p&gt;\n\n&lt;p&gt;Here are the two relevant parts of a podcast HTML page:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Metadata part:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;lt;script type=&amp;quot;application/ld+json&amp;quot;&amp;gt;\n    {\n        &amp;quot;@context&amp;quot;: &amp;quot;https://schema.org/&amp;quot;,\n        &amp;quot;@type&amp;quot;: &amp;quot;Podcast&amp;quot;,\n        &amp;quot;@id&amp;quot;: &amp;quot;https://podcast.com/podcast/sidsel-dalen-21-netter/#player&amp;quot;,\n        &amp;quot;abridged&amp;quot;: &amp;quot;false&amp;quot;,\n        &amp;quot;requiresSubscription&amp;quot;: &amp;quot;false&amp;quot;,\n        &amp;quot;author&amp;quot;: &amp;quot;Author Name&amp;quot;,\n        &amp;quot;name&amp;quot;: &amp;quot;21 netter&amp;quot;,\n        &amp;quot;description&amp;quot;: &amp;quot;&amp;quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&amp;quot;&amp;quot;,\n        &amp;quot;datePublished&amp;quot;: &amp;quot;2010&amp;quot;,\n        &amp;quot;character&amp;quot;: &amp;quot;Mia Piasen&amp;quot;,\n        &amp;quot;keywords&amp;quot;: &amp;quot;Mia Piasen&amp;quot;,\n        &amp;quot;duration&amp;quot;: &amp;quot;PT10H46M00S&amp;quot;,\n        &amp;quot;genre&amp;quot;: &amp;quot;Krimpodcast&amp;quot;,\n        &amp;quot;readBy&amp;quot;: &amp;quot;Hanna Jakobsen&amp;quot;\n        }\n    }\n&amp;lt;/script&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;File link part:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6771.mp3&amp;quot; &amp;gt;track_6771.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6772.mp3&amp;quot; &amp;gt;track_6772.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6773.mp3&amp;quot; &amp;gt;track_6773.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6774.mp3&amp;quot; &amp;gt;track_6774.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6775.mp3&amp;quot; &amp;gt;track_6775.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6776.mp3&amp;quot; &amp;gt;track_6776.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6777.mp3&amp;quot; &amp;gt;track_6777.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6778.mp3&amp;quot; &amp;gt;track_6778.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6779.mp3&amp;quot; &amp;gt;track_6779.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;a class=&amp;quot;filelinks&amp;quot; href=&amp;quot;../../file_storage/m2031/track_6780.mp3&amp;quot; &amp;gt;track_6780.mp3&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;How do I extract that metadata info from the HTML page and use it to rename, move and ID3-tag the linked MP3 files in the right order?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8p2md", "is_robot_indexable": true, "report_reasons": null, "author": "kris33", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8p2md/how_to_rename_move_and_id3tag_mp3_files_with_info/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8p2md/how_to_rename_move_and_id3tag_mp3_files_with_info/", "subreddit_subscribers": 656875, "created_utc": 1669807530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I\u2019m trying to sort a portable RAID for my ancient. I\u2019ve bought a pair of 2.5\u201d 5GB Seagate drives and a Rugged Raid Pro to swap them into, only to find out, the damn thing won\u2019t fit 15mm disks - massive research error on my end.\n\nDoes anyone know a similarly small factor external RAID drive or RAID enclosure I could fit with these bad boys? I\u2019ve looked at the Akitio side-by-side enclosure (SK-2520 U3.1) but I can\u2019t get hold of it in the UK. Just asked a retailer about specs for a Raidon unit too, waiting for their reply. If anyone has any suggestions, I\u2019d be ever so grateful. Ty!", "author_fullname": "t2_4klbdouw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for portable external raid enclosure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8og7a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669805418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I\u2019m trying to sort a portable RAID for my ancient. I\u2019ve bought a pair of 2.5\u201d 5GB Seagate drives and a Rugged Raid Pro to swap them into, only to find out, the damn thing won\u2019t fit 15mm disks - massive research error on my end.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know a similarly small factor external RAID drive or RAID enclosure I could fit with these bad boys? I\u2019ve looked at the Akitio side-by-side enclosure (SK-2520 U3.1) but I can\u2019t get hold of it in the UK. Just asked a retailer about specs for a Raidon unit too, waiting for their reply. If anyone has any suggestions, I\u2019d be ever so grateful. Ty!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8og7a", "is_robot_indexable": true, "report_reasons": null, "author": "zolbear", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8og7a/looking_for_portable_external_raid_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8og7a/looking_for_portable_external_raid_enclosure/", "subreddit_subscribers": 656875, "created_utc": 1669805418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently struggling to come up with a good data integrity check for my backups. I am using \\[restic\\]([https://restic.net/](https://restic.net/)) (which has a lot of the same features as borg) as a backup engine. It already includes \\[integrity and consistency checks\\]([https://restic.readthedocs.io/en/stable/045\\_working\\_with\\_repos.html#checking-integrity-and-consistency](https://restic.readthedocs.io/en/stable/045_working_with_repos.html#checking-integrity-and-consistency)) but I feel like the described \\`check\\` command still leaves room for silently corrupted data, even with its \\`--read-data\\` flag : the integrity seems to be checked against hashes of already-packed data, but what if something happened during the packing process ?\n\nI'm looking for a integrity check scheme that checks restored data against checksums computed from the originally backed up data, such as described in \\[this issue I opened on the restic repo\\]([https://github.com/restic/restic/issues/4057](https://github.com/restic/restic/issues/4057)). I'm aware this kind of scheme must take into account the possibility of the data mutating on the source disk while being backed up.\n\nIs the Restic integrity check enough ? Am I being paranoid ? Are you aware of any way to achieve my goal ?\n\nThank you to anyone who takes the time to answer, even with an incomplete answer. I'm usually decent at googling, but I'm not having luck with this one.", "author_fullname": "t2_2lq2eg3f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What makes a good backup integrity check ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z8ncss", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669801625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently struggling to come up with a good data integrity check for my backups. I am using [restic](&lt;a href=\"https://restic.net/\"&gt;https://restic.net/&lt;/a&gt;) (which has a lot of the same features as borg) as a backup engine. It already includes [integrity and consistency checks](&lt;a href=\"https://restic.readthedocs.io/en/stable/045_working_with_repos.html#checking-integrity-and-consistency\"&gt;https://restic.readthedocs.io/en/stable/045_working_with_repos.html#checking-integrity-and-consistency&lt;/a&gt;) but I feel like the described `check` command still leaves room for silently corrupted data, even with its `--read-data` flag : the integrity seems to be checked against hashes of already-packed data, but what if something happened during the packing process ?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a integrity check scheme that checks restored data against checksums computed from the originally backed up data, such as described in [this issue I opened on the restic repo](&lt;a href=\"https://github.com/restic/restic/issues/4057\"&gt;https://github.com/restic/restic/issues/4057&lt;/a&gt;). I&amp;#39;m aware this kind of scheme must take into account the possibility of the data mutating on the source disk while being backed up.&lt;/p&gt;\n\n&lt;p&gt;Is the Restic integrity check enough ? Am I being paranoid ? Are you aware of any way to achieve my goal ?&lt;/p&gt;\n\n&lt;p&gt;Thank you to anyone who takes the time to answer, even with an incomplete answer. I&amp;#39;m usually decent at googling, but I&amp;#39;m not having luck with this one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z8ncss", "is_robot_indexable": true, "report_reasons": null, "author": "pcouy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z8ncss/what_makes_a_good_backup_integrity_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z8ncss/what_makes_a_good_backup_integrity_check/", "subreddit_subscribers": 656875, "created_utc": 1669801625.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}