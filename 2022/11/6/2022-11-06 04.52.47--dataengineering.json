{"kind": "Listing", "data": {"after": null, "dist": 20, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dfcot0zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cool ML Engineering diagram.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_ymjubx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 163, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 163, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/R46WL4O-SxXLqPv9csUuZtH1s9udW0QVYbrV6q8gMhc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667621721.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/77lc2zkhn3y91.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/77lc2zkhn3y91.png?auto=webp&amp;s=9943349f8ce2aad65d17f819a243066e2f513b55", "width": 1080, "height": 1159}, "resolutions": [{"url": "https://preview.redd.it/77lc2zkhn3y91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e24c6ac3aa7c823d63db2ef76ade029f97f21ae", "width": 108, "height": 115}, {"url": "https://preview.redd.it/77lc2zkhn3y91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c5217070b9f2b5c7dc1b3df3a97ccbfdbb354dd4", "width": 216, "height": 231}, {"url": "https://preview.redd.it/77lc2zkhn3y91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1207b9de26362e1d48e3c2c50a3979c2620861b0", "width": 320, "height": 343}, {"url": "https://preview.redd.it/77lc2zkhn3y91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5e5db11d6d76591018cd5d69dfc52dd2859a6453", "width": 640, "height": 686}, {"url": "https://preview.redd.it/77lc2zkhn3y91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b0c2bebc2b766813f1d7091108ce144265a7816a", "width": 960, "height": 1030}, {"url": "https://preview.redd.it/77lc2zkhn3y91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=197021d3c92c1308e15bf84071723e39ecbc39a8", "width": 1080, "height": 1159}], "variants": {}, "id": "UOxP6CxgiIHRO8bYMq6DqG90WFS48e8kQf7bfpU67LU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ymjubx", "is_robot_indexable": true, "report_reasons": null, "author": "DrRedmondNYC", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ymjubx/cool_ml_engineering_diagram/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/77lc2zkhn3y91.png", "subreddit_subscribers": 79024, "created_utc": 1667621721.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The ones you feel are critical, but are missing, especially if the pipeline was developed by beginners.", "author_fullname": "t2_g4v8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you share some of the best software engineering practices you can apply to data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymz0oh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 70, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 70, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667666190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The ones you feel are critical, but are missing, especially if the pipeline was developed by beginners.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ymz0oh", "is_robot_indexable": true, "report_reasons": null, "author": "swapripper", "discussion_type": null, "num_comments": 64, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ymz0oh/can_you_share_some_of_the_best_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ymz0oh/can_you_share_some_of_the_best_software/", "subreddit_subscribers": 79024, "created_utc": 1667666190.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I\u2019m looking for learning resources to create CI/CD processes for our Azure Databricks and GitHub repo. Is GitHub Actions the answer? Any recommendations or help is appreciated.", "author_fullname": "t2_a0qsnkph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CI/CD with Databricks and GitHub", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymr04r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667646735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I\u2019m looking for learning resources to create CI/CD processes for our Azure Databricks and GitHub repo. Is GitHub Actions the answer? Any recommendations or help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ymr04r", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Membership-8", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ymr04r/cicd_with_databricks_and_github/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ymr04r/cicd_with_databricks_and_github/", "subreddit_subscribers": 79024, "created_utc": 1667646735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "According to Statista, nearly half of the emails sent worldwide are spam. In 2021, it was estimated that nearly 319.6 billion emails were sent and received daily.\n\nThough Gmail marks most of the emails as spam, still we receive bunch of marketing and promotional emails. I have tried to develope the datapipeline to see from what all domains, I receive emails daily. I have created dashboard where I can see all these stats and I can go and block the particular domains which makes my task lil easier instead of going through each and every email and blocking.\n\nTech Stack :\n\nPython\n\nAirflow\n\nGrafana\n\n&amp;#x200B;\n\nDashboard Link : [https://snapshots.raintank.io/dashboard/snapshot/E3bVrLkkPYU0XzpfjPRbwZXsLCMlwg7t](https://snapshots.raintank.io/dashboard/snapshot/E3bVrLkkPYU0XzpfjPRbwZXsLCMlwg7t)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Dash Board](https://preview.redd.it/d7oeprsn09y91.png?width=2742&amp;format=png&amp;auto=webp&amp;s=91692867b9920c391d5eeaa62085576f1ee89950)\n\nGitHub :\n\n[https://github.com/amrgb50/MANAGE-GMAIL](https://github.com/amrgb50/MANAGE-GMAIL)\n\n&amp;#x200B;\n\n[App Flow](https://preview.redd.it/dck7scg8l8y91.png?width=1804&amp;format=png&amp;auto=webp&amp;s=0b4400dfd0678a46ea01f00d295f8b7f7f7b3580)\n\n&amp;#x200B;\n\nImprovements and next plan :\n\n1. I am learning docker and kubernetes. So next step will be containerizing this app and run in cloud.\n2. Implementing DQ checks.\n\nAny and all feedback is absolutely welcome! This is my first project and trying to hone my skills for DE profession. Please feel free to provide any feedback!", "author_fullname": "t2_r1secbn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Project - Gmail Manager", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "media_metadata": {"dck7scg8l8y91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 37, "x": 108, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0dffff623ebbfe409250ebbd80ff2bd8b5476d0a"}, {"y": 75, "x": 216, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9338350505ff672e7c568fe6ec6b54704a9f2b9"}, {"y": 112, "x": 320, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed630de193a5457c786ad9150370411478fe37c9"}, {"y": 224, "x": 640, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=71b32d4b65f2edc862a1aeb21e883d2b67690cd5"}, {"y": 337, "x": 960, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=af6ea7275d0952d7482e1084aca994d3ebb40c28"}, {"y": 379, "x": 1080, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f5c385593044b6e139d5e6f3e91a21a043642d92"}], "s": {"y": 634, "x": 1804, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=1804&amp;format=png&amp;auto=webp&amp;s=0b4400dfd0678a46ea01f00d295f8b7f7f7b3580"}, "id": "dck7scg8l8y91"}, "d7oeprsn09y91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a62fbdcecabd64c9eb2c5ff12db1bfc7f3be889"}, {"y": 109, "x": 216, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d1fc6d9815ee21d54a95ffca3c2bfa9c0571b3a"}, {"y": 162, "x": 320, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=537fb6930ff4972f62c4a567d5dce5ba5c4bb13d"}, {"y": 325, "x": 640, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf753896251acf68d97b36c6a6d9090b7563f1d7"}, {"y": 488, "x": 960, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2582ab372832bde0cb2d494bb57d2a45dd27d53a"}, {"y": 549, "x": 1080, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=31300bad910245f3a11c3033f61ae2cf767d0ff7"}], "s": {"y": 1396, "x": 2742, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=2742&amp;format=png&amp;auto=webp&amp;s=91692867b9920c391d5eeaa62085576f1ee89950"}, "id": "d7oeprsn09y91"}}, "name": "t3_yndu7k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gVA8P5cPQOCY8cnD5k6J0X-TP9LodqhJdW4tDlBKY0s.jpg", "edited": 1667704672.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667700565.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;According to Statista, nearly half of the emails sent worldwide are spam. In 2021, it was estimated that nearly 319.6 billion emails were sent and received daily.&lt;/p&gt;\n\n&lt;p&gt;Though Gmail marks most of the emails as spam, still we receive bunch of marketing and promotional emails. I have tried to develope the datapipeline to see from what all domains, I receive emails daily. I have created dashboard where I can see all these stats and I can go and block the particular domains which makes my task lil easier instead of going through each and every email and blocking.&lt;/p&gt;\n\n&lt;p&gt;Tech Stack :&lt;/p&gt;\n\n&lt;p&gt;Python&lt;/p&gt;\n\n&lt;p&gt;Airflow&lt;/p&gt;\n\n&lt;p&gt;Grafana&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Dashboard Link : &lt;a href=\"https://snapshots.raintank.io/dashboard/snapshot/E3bVrLkkPYU0XzpfjPRbwZXsLCMlwg7t\"&gt;https://snapshots.raintank.io/dashboard/snapshot/E3bVrLkkPYU0XzpfjPRbwZXsLCMlwg7t&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/d7oeprsn09y91.png?width=2742&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91692867b9920c391d5eeaa62085576f1ee89950\"&gt;Dash Board&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GitHub :&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/amrgb50/MANAGE-GMAIL\"&gt;https://github.com/amrgb50/MANAGE-GMAIL&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dck7scg8l8y91.png?width=1804&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b4400dfd0678a46ea01f00d295f8b7f7f7b3580\"&gt;App Flow&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Improvements and next plan :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I am learning docker and kubernetes. So next step will be containerizing this app and run in cloud.&lt;/li&gt;\n&lt;li&gt;Implementing DQ checks.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any and all feedback is absolutely welcome! This is my first project and trying to hone my skills for DE profession. Please feel free to provide any feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "yndu7k", "is_robot_indexable": true, "report_reasons": null, "author": "Educational-Log-2723", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yndu7k/data_engineering_project_gmail_manager/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yndu7k/data_engineering_project_gmail_manager/", "subreddit_subscribers": 79024, "created_utc": 1667700565.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a working professional and want to get the certification in 7 days \nHoping that's reasonable.\n\nI have no cloud experience but have completed the virtual 5 day training", "author_fullname": "t2_f836ym4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what's the best roadmap for DP-900 after completing Microsoft training? Thinking of taking runes YouTube course or what else ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymt58v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667652892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a working professional and want to get the certification in 7 days \nHoping that&amp;#39;s reasonable.&lt;/p&gt;\n\n&lt;p&gt;I have no cloud experience but have completed the virtual 5 day training&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "ymt58v", "is_robot_indexable": true, "report_reasons": null, "author": "Aggravating_Wind8365", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ymt58v/whats_the_best_roadmap_for_dp900_after_completing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ymt58v/whats_the_best_roadmap_for_dp900_after_completing/", "subreddit_subscribers": 79024, "created_utc": 1667652892.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a python script that pulls data using an API and loads a table onto an amazon rds database and i\u2019m using mysql to query the database and then creating a tableau dashboard using the information. Would i be able to use airflow to trigger the python script to update the database and then run the mysql queries and update the dashboard?", "author_fullname": "t2_sfg62", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should i use airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yn7t0y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667685968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a python script that pulls data using an API and loads a table onto an amazon rds database and i\u2019m using mysql to query the database and then creating a tableau dashboard using the information. Would i be able to use airflow to trigger the python script to update the database and then run the mysql queries and update the dashboard?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yn7t0y", "is_robot_indexable": true, "report_reasons": null, "author": "2teknical", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yn7t0y/should_i_use_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yn7t0y/should_i_use_airflow/", "subreddit_subscribers": 79024, "created_utc": 1667685968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I specially want to hear from those who have used other tool like airflow, fivetran etc. Why do you like and dislike about adf?", "author_fullname": "t2_gpjiv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you guys think of azure data factory?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymlx7q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667628671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I specially want to hear from those who have used other tool like airflow, fivetran etc. Why do you like and dislike about adf?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ymlx7q", "is_robot_indexable": true, "report_reasons": null, "author": "HBoogi", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ymlx7q/what_do_you_guys_think_of_azure_data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ymlx7q/what_do_you_guys_think_of_azure_data_factory/", "subreddit_subscribers": 79024, "created_utc": 1667628671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have never worked with data larger than 50GB and am wondering how dashboards like Tableau, which have a 15gb extract limit, visualize 2TB+ data? \n\nThe common steps I've taken are to roll-up dates from days to months (maybe need to do months to quarters), reduce the number of columns to minimum needed, and filter further into a collection of themed fragment tables (by one nun-null dimension). \n\nWondering how others handle the fragmentation of TB+ data size for dashboard output tools like Tableau that have limited extract sizes.", "author_fullname": "t2_7y3cr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle TB data aggregations for dashboard extracts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yndujd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667700592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have never worked with data larger than 50GB and am wondering how dashboards like Tableau, which have a 15gb extract limit, visualize 2TB+ data? &lt;/p&gt;\n\n&lt;p&gt;The common steps I&amp;#39;ve taken are to roll-up dates from days to months (maybe need to do months to quarters), reduce the number of columns to minimum needed, and filter further into a collection of themed fragment tables (by one nun-null dimension). &lt;/p&gt;\n\n&lt;p&gt;Wondering how others handle the fragmentation of TB+ data size for dashboard output tools like Tableau that have limited extract sizes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yndujd", "is_robot_indexable": true, "report_reasons": null, "author": "SevenEyes", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yndujd/how_do_you_handle_tb_data_aggregations_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yndujd/how_do_you_handle_tb_data_aggregations_for/", "subreddit_subscribers": 79024, "created_utc": 1667700592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I want to know how to run a Python script that is hosted outside the Airflow env.\nI have airflow installed on WSL and my script is in the local system. So how can I achieve this? \nI want in the future to run Airflow on a server and that every user can schedule it's tasks using the server but running on their own computer. \nIdk if I'm explaining well..\n\nThanks in advance!", "author_fullname": "t2_5xg75zcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to run scripts outside wsl using airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yml7qn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667626264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I want to know how to run a Python script that is hosted outside the Airflow env.\nI have airflow installed on WSL and my script is in the local system. So how can I achieve this? \nI want in the future to run Airflow on a server and that every user can schedule it&amp;#39;s tasks using the server but running on their own computer. \nIdk if I&amp;#39;m explaining well..&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yml7qn", "is_robot_indexable": true, "report_reasons": null, "author": "aisakee", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yml7qn/how_to_run_scripts_outside_wsl_using_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yml7qn/how_to_run_scripts_outside_wsl_using_airflow/", "subreddit_subscribers": 79024, "created_utc": 1667626264.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking for books/blogs/courses that helped you to advance further from an intermediate level. Secret resources that almost felt like cheat-codes after learning it. The ones that you don't see being talked about much.", "author_fullname": "t2_g4v8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the most underrated/unknown resources that helped you upskill?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymyw76", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667665911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for books/blogs/courses that helped you to advance further from an intermediate level. Secret resources that almost felt like cheat-codes after learning it. The ones that you don&amp;#39;t see being talked about much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ymyw76", "is_robot_indexable": true, "report_reasons": null, "author": "swapripper", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ymyw76/what_are_the_most_underratedunknown_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ymyw76/what_are_the_most_underratedunknown_resources/", "subreddit_subscribers": 79024, "created_utc": 1667665911.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have requirement to build SCD TYPE 2 table in delta lake, It's monthly load and columns do change in each refresh. We cannot have static columns. Any one have idea how can we implement without unpivoting the columns which would cause redundant data?", "author_fullname": "t2_g06clmjl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to implement SCD Type 2 table with ever changing columns in each load?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymmf09", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667630440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have requirement to build SCD TYPE 2 table in delta lake, It&amp;#39;s monthly load and columns do change in each refresh. We cannot have static columns. Any one have idea how can we implement without unpivoting the columns which would cause redundant data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ymmf09", "is_robot_indexable": true, "report_reasons": null, "author": "Junior_Abies_2213", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ymmf09/how_to_implement_scd_type_2_table_with_ever/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ymmf09/how_to_implement_scd_type_2_table_with_ever/", "subreddit_subscribers": 79024, "created_utc": 1667630440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\n\nI'm trying to design some table with standardized data from multiple sources, in the most effective way.\n\nI'm working with Postgres and the current design is as following:\n\n1. The \\`main\\` schema has dimension tables (for example \\`units\\`, \\`currencies\\` and \\`countries\\`)\n2. Every source has it's own schema with dimension tables for the same columns (units, currencies and countries)\n3. In every schema, the dimension tables have an additional column \\`main\\_id\\` that is null at first, and requires an analyst to map the source's value to the corresponding one in our main schema. (For example, \\`US\\` in in source #1 should be mapped to \\`United States\\` in the main schema).\n4. There are materialized views in the \\`main\\` schema, unioning the data from every source, taking the \\`main\\_id\\` value of every dimension in every source. BUT, if some row in a source has a value who's \\`main\\_id\\` is still null (meaning, it wasn't mapped), then this row is filtered out.\n\n&amp;#x200B;\n\n[Architecture](https://preview.redd.it/cass3z81y7y91.png?width=1496&amp;format=png&amp;auto=webp&amp;s=8fa7821595d5209d88c91a3ca31f5bf8ba1852b8)\n\nSo currently, if new data is introduced into a source or if a new value is mapped, I just need to refresh the materialized views and the new data will be accessible in the mviews.\n\nNow I need those mviews to be actual tables and I'm wondering what the best way is to keep them up to date.\n\n1. Should I have a stored procedure per schema, inserting new data into the main tables? I could call it manually after a mapping is done or it could be triggered every few minutes\n2. Should I have a Python script implement the insertion logic?\n3. Should I have a trigger on every insert (data tables) / update (the mapping tables) to insert the data to the main tables? (I don't love triggers because they add hidden complexity)\n\n&amp;#x200B;\n\nAny other ideas will be very appreciated!\n\n&amp;#x200B;\n\nThanks :)", "author_fullname": "t2_74d9mi1r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Standardization after Dimension Mapping", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 106, "top_awarded_type": null, "hide_score": false, "media_metadata": {"cass3z81y7y91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/cass3z81y7y91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=92f10bde1d25385db89dc4af94c3256f562a5191"}, {"y": 163, "x": 216, "u": "https://preview.redd.it/cass3z81y7y91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a7ad4f4ef294944932c452ffd1485bcbf66fae14"}, {"y": 242, "x": 320, "u": "https://preview.redd.it/cass3z81y7y91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=05470d433a30d114dc8c6cb516daeefc2193c1bd"}, {"y": 485, "x": 640, "u": "https://preview.redd.it/cass3z81y7y91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=870d7161fdd07754e1e02847d902ffb080676d8b"}, {"y": 727, "x": 960, "u": "https://preview.redd.it/cass3z81y7y91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ac16404696df978a5b4ec337bb21363a5c7afb2"}, {"y": 818, "x": 1080, "u": "https://preview.redd.it/cass3z81y7y91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=328e2bf9fabb349af65e96c3c88ed31fa42d4b2b"}], "s": {"y": 1134, "x": 1496, "u": "https://preview.redd.it/cass3z81y7y91.png?width=1496&amp;format=png&amp;auto=webp&amp;s=8fa7821595d5209d88c91a3ca31f5bf8ba1852b8"}, "id": "cass3z81y7y91"}}, "name": "t3_ynajp4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/G5rUXvTKrIZLmKTvZAZ5UBVEIBMAZBcQonqapI6C4Wg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667691991.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to design some table with standardized data from multiple sources, in the most effective way.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working with Postgres and the current design is as following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The `main` schema has dimension tables (for example `units`, `currencies` and `countries`)&lt;/li&gt;\n&lt;li&gt;Every source has it&amp;#39;s own schema with dimension tables for the same columns (units, currencies and countries)&lt;/li&gt;\n&lt;li&gt;In every schema, the dimension tables have an additional column `main_id` that is null at first, and requires an analyst to map the source&amp;#39;s value to the corresponding one in our main schema. (For example, `US` in in source #1 should be mapped to `United States` in the main schema).&lt;/li&gt;\n&lt;li&gt;There are materialized views in the `main` schema, unioning the data from every source, taking the `main_id` value of every dimension in every source. BUT, if some row in a source has a value who&amp;#39;s `main_id` is still null (meaning, it wasn&amp;#39;t mapped), then this row is filtered out.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cass3z81y7y91.png?width=1496&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8fa7821595d5209d88c91a3ca31f5bf8ba1852b8\"&gt;Architecture&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So currently, if new data is introduced into a source or if a new value is mapped, I just need to refresh the materialized views and the new data will be accessible in the mviews.&lt;/p&gt;\n\n&lt;p&gt;Now I need those mviews to be actual tables and I&amp;#39;m wondering what the best way is to keep them up to date.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Should I have a stored procedure per schema, inserting new data into the main tables? I could call it manually after a mapping is done or it could be triggered every few minutes&lt;/li&gt;\n&lt;li&gt;Should I have a Python script implement the insertion logic?&lt;/li&gt;\n&lt;li&gt;Should I have a trigger on every insert (data tables) / update (the mapping tables) to insert the data to the main tables? (I don&amp;#39;t love triggers because they add hidden complexity)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any other ideas will be very appreciated!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ynajp4", "is_robot_indexable": true, "report_reasons": null, "author": "Helpful_Artist1439", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynajp4/data_standardization_after_dimension_mapping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynajp4/data_standardization_after_dimension_mapping/", "subreddit_subscribers": 79024, "created_utc": 1667691991.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8c13n8vj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "10 Pandas Functions That Help You Understand a Dataset Completely", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_yn570b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7o0tOYgJlf16b_a8KzPtwXhYSe6EgJF5XOo5k36TiDQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667680027.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/techtofreedom/10-pandas-functions-that-help-you-understand-a-dataset-completely-b7de7e7e14ab", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pURynvTum1PfB1n80TuI1gA5AZdgZEsoBN7Om_56MDw.jpg?auto=webp&amp;s=a7cb64a1167a1a4517d6c0646e0d42d86d6c9002", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/pURynvTum1PfB1n80TuI1gA5AZdgZEsoBN7Om_56MDw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0bab686777db435ff29d6b80b47f7e49f2eb63b0", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/pURynvTum1PfB1n80TuI1gA5AZdgZEsoBN7Om_56MDw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=98b03ee58005d02b067bf6f2dbf2e13ea231fed0", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/pURynvTum1PfB1n80TuI1gA5AZdgZEsoBN7Om_56MDw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b65026aa876f83b83419f635bff0ae7e64ba241", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/pURynvTum1PfB1n80TuI1gA5AZdgZEsoBN7Om_56MDw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9f7dab2817ff400f9fcbe7efcaba57f45f0b4fcd", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/pURynvTum1PfB1n80TuI1gA5AZdgZEsoBN7Om_56MDw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc1fc64115c345320d8a157bad9d55ca7b985f4f", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/pURynvTum1PfB1n80TuI1gA5AZdgZEsoBN7Om_56MDw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2d84077ad712c6dfe401df4cd3d8f9bb38bda57e", "width": 1080, "height": 720}], "variants": {}, "id": "7wBi4n5WKWMEkMStw6hzcfzdIhpwvwBWmosCaE4JK94"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yn570b", "is_robot_indexable": true, "report_reasons": null, "author": "yangzhou1993", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yn570b/10_pandas_functions_that_help_you_understand_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/techtofreedom/10-pandas-functions-that-help-you-understand-a-dataset-completely-b7de7e7e14ab", "subreddit_subscribers": 79024, "created_utc": 1667680027.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nWould you recommend me to apply for google STEP or some real intern program at google?\n\nMy CV: [https://ibb.co/R9vMNPf](https://ibb.co/R9vMNPf)", "author_fullname": "t2_t5tmlvv2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apply to google STEP or 'real' internship at google?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yn55oi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667679931.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;Would you recommend me to apply for google STEP or some real intern program at google?&lt;/p&gt;\n\n&lt;p&gt;My CV: &lt;a href=\"https://ibb.co/R9vMNPf\"&gt;https://ibb.co/R9vMNPf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8lpUvTVfQiPnccMXPZMlvhgoj2Ev98Y946j8Wzy39go.jpg?auto=webp&amp;s=9eceb005322fac3cc5491e4ed7a9595b674dc91c", "width": 765, "height": 1019}, "resolutions": [{"url": "https://external-preview.redd.it/8lpUvTVfQiPnccMXPZMlvhgoj2Ev98Y946j8Wzy39go.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c4c35e1d727a58e6fa484261d8a842a12bf0e0e9", "width": 108, "height": 143}, {"url": "https://external-preview.redd.it/8lpUvTVfQiPnccMXPZMlvhgoj2Ev98Y946j8Wzy39go.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6676fe5254e76ff797dccdd79aa258ab64950e42", "width": 216, "height": 287}, {"url": "https://external-preview.redd.it/8lpUvTVfQiPnccMXPZMlvhgoj2Ev98Y946j8Wzy39go.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d2ee0a0d0bcb4d2d75de59d0124eeab5c4c5774", "width": 320, "height": 426}, {"url": "https://external-preview.redd.it/8lpUvTVfQiPnccMXPZMlvhgoj2Ev98Y946j8Wzy39go.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8863f65182a7ed706c4e3100c682c482937441f", "width": 640, "height": 852}], "variants": {}, "id": "KdPcVejIeM09FUZ8-3u4srYV1GigHnD3bFF-2Ga9d28"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "yn55oi", "is_robot_indexable": true, "report_reasons": null, "author": "cs_phil", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yn55oi/apply_to_google_step_or_real_internship_at_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yn55oi/apply_to_google_step_or_real_internship_at_google/", "subreddit_subscribers": 79024, "created_utc": 1667679931.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Passed AZ, DP and SC. Just got an entry level role as a data engineer for our synapse platform. It\u2019s just been handed over to us and no one entirely knows how to use it. I\u2019m basically going to be maintaining and adding to it. \n\nCan anyone give any tips/advice for taking on this role. This is purely an Azure Synapse role. \n\nI\u2019ve got a pretty basic understanding, branch creation, moving the code to prod. But anything else I don\u2019t know. Best way to learn?", "author_fullname": "t2_173udy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some tips for moving into a DE entry level role - Synapse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yn47dq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667677729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Passed AZ, DP and SC. Just got an entry level role as a data engineer for our synapse platform. It\u2019s just been handed over to us and no one entirely knows how to use it. I\u2019m basically going to be maintaining and adding to it. &lt;/p&gt;\n\n&lt;p&gt;Can anyone give any tips/advice for taking on this role. This is purely an Azure Synapse role. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve got a pretty basic understanding, branch creation, moving the code to prod. But anything else I don\u2019t know. Best way to learn?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yn47dq", "is_robot_indexable": true, "report_reasons": null, "author": "prodigypro", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yn47dq/some_tips_for_moving_into_a_de_entry_level_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yn47dq/some_tips_for_moving_into_a_de_entry_level_role/", "subreddit_subscribers": 79024, "created_utc": 1667677729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Requirements:\n- All datasets to have a primary key named \u201cid\u201d before SQL MERGE \n- Some datasets already contain pk named \u201cid\u201d\n- Some datasets contain pk named \u201cId\u201d\n- Some datasets contain pk named \u201cotherId\u201d\n- Some datasets do not contain a pk but contain other \u201cids\u201d and a JSON record which can be hashed to form a unique surrogate key\n\nShould I define the PK in the datasets and enhance each row during extraction to conform?\n\nThis is an ELT pipeline so I want to do as little in Python as necessary to enable faster deployment of new connectors to new APIs down the road.", "author_fullname": "t2_ltzfd6pc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you like to define primary keys during ETL / ELT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yn2sw2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667674506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Requirements:\n- All datasets to have a primary key named \u201cid\u201d before SQL MERGE \n- Some datasets already contain pk named \u201cid\u201d\n- Some datasets contain pk named \u201cId\u201d\n- Some datasets contain pk named \u201cotherId\u201d\n- Some datasets do not contain a pk but contain other \u201cids\u201d and a JSON record which can be hashed to form a unique surrogate key&lt;/p&gt;\n\n&lt;p&gt;Should I define the PK in the datasets and enhance each row during extraction to conform?&lt;/p&gt;\n\n&lt;p&gt;This is an ELT pipeline so I want to do as little in Python as necessary to enable faster deployment of new connectors to new APIs down the road.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yn2sw2", "is_robot_indexable": true, "report_reasons": null, "author": "FactMuncher", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yn2sw2/how_do_you_like_to_define_primary_keys_during_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yn2sw2/how_do_you_like_to_define_primary_keys_during_etl/", "subreddit_subscribers": 79024, "created_utc": 1667674506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working as a data engineer in a non-tech company. My day to day work involves fetching data from various internal sources like API, Data warehouse, Data lake, operational tables to name a few. After fetching the data, I transform it according to the business requirements and load it to whatever target the end users want it in. Finally, the data scientists create the dashboard as per end user requirements using the data from my target destination. \n\nJust to be clear, I have not been criticized, warned or questioned about my work. However, I just want to start demonstrating the value of my work to business users. It seems like the folks who create the final dashboard on my data find it relatively easier to show the value of their work to end business users.", "author_fullname": "t2_t6oufyru", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to increase my visibility to business users as a data engineer? It seems like the final dashboard creators have a higher visibility and find it relatively easier to show the value of their work to end buisness users", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymuxae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667657182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working as a data engineer in a non-tech company. My day to day work involves fetching data from various internal sources like API, Data warehouse, Data lake, operational tables to name a few. After fetching the data, I transform it according to the business requirements and load it to whatever target the end users want it in. Finally, the data scientists create the dashboard as per end user requirements using the data from my target destination. &lt;/p&gt;\n\n&lt;p&gt;Just to be clear, I have not been criticized, warned or questioned about my work. However, I just want to start demonstrating the value of my work to business users. It seems like the folks who create the final dashboard on my data find it relatively easier to show the value of their work to end business users.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "ymuxae", "is_robot_indexable": true, "report_reasons": null, "author": "MatchCaseFirst", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ymuxae/how_to_increase_my_visibility_to_business_users/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ymuxae/how_to_increase_my_visibility_to_business_users/", "subreddit_subscribers": 79024, "created_utc": 1667657182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am currently implementing an ingestion pipeline where the full data flow is:\n\n* Data is extracted from MySQL using Debezium and stored to Kafka (1 topic per database schema, we have \\~3000 schemas divided in \\~100 servers, different table schemas are stored in the confluent schema registry)\n* Data is fetched from kafka using a Spark Structured streaming application running N concurrent streaming queries where N is the number of database schemas inside the mysql server.\n* Data is both stored AS-IS - append to delta table - and upserted to a delta table to create a SCD2 table (custom logic inside `forEachBatch` is used, the batch `DataFrame` is cached before running all the operations)\n\nI have two questions:\n\n* Is there any open source project attempting to do the same that can be used as a reference implementation?\n* I am witnessing a strange phenomenon: In the Spark UI, it is shown that the time to process a batch is \\~35-40 seconds, but in reality if I look at the log messages I'm seeing that 3 minutes pass between the processing of two batches - during this time executors are completely idle. What could be causing this information inconsistency? I'll also add that there is a trigger set to ProcessingTime 60 seconds, so lower than those 3 minutes wait time. ", "author_fullname": "t2_3dhhvh7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kafka to Spark Streaming + Delta ingestion pipeline, weird slowdowns and reference implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymn4ok", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667633491.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667633039.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am currently implementing an ingestion pipeline where the full data flow is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data is extracted from MySQL using Debezium and stored to Kafka (1 topic per database schema, we have ~3000 schemas divided in ~100 servers, different table schemas are stored in the confluent schema registry)&lt;/li&gt;\n&lt;li&gt;Data is fetched from kafka using a Spark Structured streaming application running N concurrent streaming queries where N is the number of database schemas inside the mysql server.&lt;/li&gt;\n&lt;li&gt;Data is both stored AS-IS - append to delta table - and upserted to a delta table to create a SCD2 table (custom logic inside &lt;code&gt;forEachBatch&lt;/code&gt; is used, the batch &lt;code&gt;DataFrame&lt;/code&gt; is cached before running all the operations)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have two questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is there any open source project attempting to do the same that can be used as a reference implementation?&lt;/li&gt;\n&lt;li&gt;I am witnessing a strange phenomenon: In the Spark UI, it is shown that the time to process a batch is ~35-40 seconds, but in reality if I look at the log messages I&amp;#39;m seeing that 3 minutes pass between the processing of two batches - during this time executors are completely idle. What could be causing this information inconsistency? I&amp;#39;ll also add that there is a trigger set to ProcessingTime 60 seconds, so lower than those 3 minutes wait time. &lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "6 YoE | Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ymn4ok", "is_robot_indexable": true, "report_reasons": null, "author": "vektor888", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/ymn4ok/kafka_to_spark_streaming_delta_ingestion_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ymn4ok/kafka_to_spark_streaming_delta_ingestion_pipeline/", "subreddit_subscribers": 79024, "created_utc": 1667633039.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have hosted Apache superset on Azure web app for PoC purpose \n\nIf you are using it in your company , where you are hosting it for production\n\nVirtual Machine ? Kubernetes ? Preset ?", "author_fullname": "t2_dgq3lsfa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Superset hosting platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ymtt2v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667654581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have hosted Apache superset on Azure web app for PoC purpose &lt;/p&gt;\n\n&lt;p&gt;If you are using it in your company , where you are hosting it for production&lt;/p&gt;\n\n&lt;p&gt;Virtual Machine ? Kubernetes ? Preset ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "ymtt2v", "is_robot_indexable": true, "report_reasons": null, "author": "authentichooman", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ymtt2v/apache_superset_hosting_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ymtt2v/apache_superset_hosting_platform/", "subreddit_subscribers": 79024, "created_utc": 1667654581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there opportunities for career progression?", "author_fullname": "t2_ecofamsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is de a dead end field?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yneoqh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.27, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667703052.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there opportunities for career progression?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yneoqh", "is_robot_indexable": true, "report_reasons": null, "author": "Flat_Selection1105", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yneoqh/is_de_a_dead_end_field/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yneoqh/is_de_a_dead_end_field/", "subreddit_subscribers": 79024, "created_utc": 1667703052.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}