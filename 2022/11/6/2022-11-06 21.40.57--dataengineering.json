{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently a data warehouse engineer making $140k/year.  What skills should I master next to increase my salary to $200k and eventually to $300k?  What are some positions with higher salaries that I could transition to?  Any advice would be greatly appreciated!\n\nBelow are a few skills that I have already mastered:\n\n* SQL Server\n* SSIS\n* SSRS\n* Tableau\n* Excel\n* VBA\n* Visual Basic.Net\n* C#\n\nI'm interested in data engineering, GCP/Azure/AWS, AI/machine learning, automation, big data, etc. and have a strong programming background.  I love learning new technologies, and there's so much that I want to learn, but want to focus on the most useful skills first that a $200k - $300k position would most likely require.", "author_fullname": "t2_ipzgsi9u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Skills to Learn for $200k - $300k Position?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yntyev", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 70, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 70, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667750407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently a data warehouse engineer making $140k/year.  What skills should I master next to increase my salary to $200k and eventually to $300k?  What are some positions with higher salaries that I could transition to?  Any advice would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Below are a few skills that I have already mastered:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SQL Server&lt;/li&gt;\n&lt;li&gt;SSIS&lt;/li&gt;\n&lt;li&gt;SSRS&lt;/li&gt;\n&lt;li&gt;Tableau&lt;/li&gt;\n&lt;li&gt;Excel&lt;/li&gt;\n&lt;li&gt;VBA&lt;/li&gt;\n&lt;li&gt;Visual Basic.Net&lt;/li&gt;\n&lt;li&gt;C#&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m interested in data engineering, GCP/Azure/AWS, AI/machine learning, automation, big data, etc. and have a strong programming background.  I love learning new technologies, and there&amp;#39;s so much that I want to learn, but want to focus on the most useful skills first that a $200k - $300k position would most likely require.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yntyev", "is_robot_indexable": true, "report_reasons": null, "author": "DesignedIt", "discussion_type": null, "num_comments": 61, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yntyev/skills_to_learn_for_200k_300k_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yntyev/skills_to_learn_for_200k_300k_position/", "subreddit_subscribers": 79083, "created_utc": 1667750407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "According to Statista, nearly half of the emails sent worldwide are spam. In 2021, it was estimated that nearly 319.6 billion emails were sent and received daily.\n\nThough Gmail marks most of the emails as spam, still we receive bunch of marketing and promotional emails. I have tried to develope the datapipeline to see from what all domains, I receive emails daily. I have created dashboard where I can see all these stats and I can go and block the particular domains which makes my task lil easier instead of going through each and every email and blocking.\n\nTech Stack :\n\nPython\n\nAirflow\n\nGrafana\n\n&amp;#x200B;\n\nDashboard Link : [https://snapshots.raintank.io/dashboard/snapshot/E3bVrLkkPYU0XzpfjPRbwZXsLCMlwg7t](https://snapshots.raintank.io/dashboard/snapshot/E3bVrLkkPYU0XzpfjPRbwZXsLCMlwg7t)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Dash Board](https://preview.redd.it/d7oeprsn09y91.png?width=2742&amp;format=png&amp;auto=webp&amp;s=91692867b9920c391d5eeaa62085576f1ee89950)\n\nGitHub :\n\n[https://github.com/amrgb50/MANAGE-GMAIL](https://github.com/amrgb50/MANAGE-GMAIL)\n\n&amp;#x200B;\n\n[App Flow](https://preview.redd.it/dck7scg8l8y91.png?width=1804&amp;format=png&amp;auto=webp&amp;s=0b4400dfd0678a46ea01f00d295f8b7f7f7b3580)\n\n&amp;#x200B;\n\nImprovements and next plan :\n\n1. I am learning docker and kubernetes. So next step will be containerizing this app and run in cloud.\n2. Implementing DQ checks.\n\nAny and all feedback is absolutely welcome! This is my first project and trying to hone my skills for DE profession. Please feel free to provide any feedback!", "author_fullname": "t2_r1secbn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Project - Gmail Manager", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 71, "top_awarded_type": null, "hide_score": false, "media_metadata": {"dck7scg8l8y91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 37, "x": 108, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0dffff623ebbfe409250ebbd80ff2bd8b5476d0a"}, {"y": 75, "x": 216, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9338350505ff672e7c568fe6ec6b54704a9f2b9"}, {"y": 112, "x": 320, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed630de193a5457c786ad9150370411478fe37c9"}, {"y": 224, "x": 640, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=71b32d4b65f2edc862a1aeb21e883d2b67690cd5"}, {"y": 337, "x": 960, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=af6ea7275d0952d7482e1084aca994d3ebb40c28"}, {"y": 379, "x": 1080, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f5c385593044b6e139d5e6f3e91a21a043642d92"}], "s": {"y": 634, "x": 1804, "u": "https://preview.redd.it/dck7scg8l8y91.png?width=1804&amp;format=png&amp;auto=webp&amp;s=0b4400dfd0678a46ea01f00d295f8b7f7f7b3580"}, "id": "dck7scg8l8y91"}, "d7oeprsn09y91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a62fbdcecabd64c9eb2c5ff12db1bfc7f3be889"}, {"y": 109, "x": 216, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d1fc6d9815ee21d54a95ffca3c2bfa9c0571b3a"}, {"y": 162, "x": 320, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=537fb6930ff4972f62c4a567d5dce5ba5c4bb13d"}, {"y": 325, "x": 640, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf753896251acf68d97b36c6a6d9090b7563f1d7"}, {"y": 488, "x": 960, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2582ab372832bde0cb2d494bb57d2a45dd27d53a"}, {"y": 549, "x": 1080, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=31300bad910245f3a11c3033f61ae2cf767d0ff7"}], "s": {"y": 1396, "x": 2742, "u": "https://preview.redd.it/d7oeprsn09y91.png?width=2742&amp;format=png&amp;auto=webp&amp;s=91692867b9920c391d5eeaa62085576f1ee89950"}, "id": "d7oeprsn09y91"}}, "name": "t3_yndu7k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 66, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 66, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gVA8P5cPQOCY8cnD5k6J0X-TP9LodqhJdW4tDlBKY0s.jpg", "edited": 1667704672.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667700565.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;According to Statista, nearly half of the emails sent worldwide are spam. In 2021, it was estimated that nearly 319.6 billion emails were sent and received daily.&lt;/p&gt;\n\n&lt;p&gt;Though Gmail marks most of the emails as spam, still we receive bunch of marketing and promotional emails. I have tried to develope the datapipeline to see from what all domains, I receive emails daily. I have created dashboard where I can see all these stats and I can go and block the particular domains which makes my task lil easier instead of going through each and every email and blocking.&lt;/p&gt;\n\n&lt;p&gt;Tech Stack :&lt;/p&gt;\n\n&lt;p&gt;Python&lt;/p&gt;\n\n&lt;p&gt;Airflow&lt;/p&gt;\n\n&lt;p&gt;Grafana&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Dashboard Link : &lt;a href=\"https://snapshots.raintank.io/dashboard/snapshot/E3bVrLkkPYU0XzpfjPRbwZXsLCMlwg7t\"&gt;https://snapshots.raintank.io/dashboard/snapshot/E3bVrLkkPYU0XzpfjPRbwZXsLCMlwg7t&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/d7oeprsn09y91.png?width=2742&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91692867b9920c391d5eeaa62085576f1ee89950\"&gt;Dash Board&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GitHub :&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/amrgb50/MANAGE-GMAIL\"&gt;https://github.com/amrgb50/MANAGE-GMAIL&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dck7scg8l8y91.png?width=1804&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b4400dfd0678a46ea01f00d295f8b7f7f7b3580\"&gt;App Flow&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Improvements and next plan :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I am learning docker and kubernetes. So next step will be containerizing this app and run in cloud.&lt;/li&gt;\n&lt;li&gt;Implementing DQ checks.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any and all feedback is absolutely welcome! This is my first project and trying to hone my skills for DE profession. Please feel free to provide any feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "yndu7k", "is_robot_indexable": true, "report_reasons": null, "author": "Educational-Log-2723", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yndu7k/data_engineering_project_gmail_manager/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yndu7k/data_engineering_project_gmail_manager/", "subreddit_subscribers": 79083, "created_utc": 1667700565.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a python script that pulls data using an API and loads a table onto an amazon rds database and i\u2019m using mysql to query the database and then creating a tableau dashboard using the information. Would i be able to use airflow to trigger the python script to update the database and then run the mysql queries and update the dashboard?", "author_fullname": "t2_sfg62", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should i use airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yn7t0y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667685968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a python script that pulls data using an API and loads a table onto an amazon rds database and i\u2019m using mysql to query the database and then creating a tableau dashboard using the information. Would i be able to use airflow to trigger the python script to update the database and then run the mysql queries and update the dashboard?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yn7t0y", "is_robot_indexable": true, "report_reasons": null, "author": "2teknical", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yn7t0y/should_i_use_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yn7t0y/should_i_use_airflow/", "subreddit_subscribers": 79083, "created_utc": 1667685968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was wondering how senior data engineers prepare for interviews?\n\n\n\n\n\nI switched into a mid-level data engineering role from my new grad data science job.  My current job had one easy python question and one moderately difficult SQL question in the coding round.  I also had a system design question that wasn't too difficult.\n\n\n\n\n\nHow should I expect to interview prep for more senior data engineering roles compared to being a mid-level engineer?", "author_fullname": "t2_3v6ob2bf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview prep for senior data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynqjwo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667743361.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667742994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering how senior data engineers prepare for interviews?&lt;/p&gt;\n\n&lt;p&gt;I switched into a mid-level data engineering role from my new grad data science job.  My current job had one easy python question and one moderately difficult SQL question in the coding round.  I also had a system design question that wasn&amp;#39;t too difficult.&lt;/p&gt;\n\n&lt;p&gt;How should I expect to interview prep for more senior data engineering roles compared to being a mid-level engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ynqjwo", "is_robot_indexable": true, "report_reasons": null, "author": "data_preprocessing", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynqjwo/interview_prep_for_senior_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynqjwo/interview_prep_for_senior_data_engineers/", "subreddit_subscribers": 79083, "created_utc": 1667742994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have never worked with data larger than 50GB and am wondering how dashboards like Tableau, which have a 15gb extract limit, visualize 2TB+ data? \n\nThe common steps I've taken are to roll-up dates from days to months (maybe need to do months to quarters), reduce the number of columns to minimum needed, and filter further into a collection of themed fragment tables (by one nun-null dimension). \n\nWondering how others handle the fragmentation of TB+ data size for dashboard output tools like Tableau that have limited extract sizes.", "author_fullname": "t2_7y3cr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle TB data aggregations for dashboard extracts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yndujd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667700592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have never worked with data larger than 50GB and am wondering how dashboards like Tableau, which have a 15gb extract limit, visualize 2TB+ data? &lt;/p&gt;\n\n&lt;p&gt;The common steps I&amp;#39;ve taken are to roll-up dates from days to months (maybe need to do months to quarters), reduce the number of columns to minimum needed, and filter further into a collection of themed fragment tables (by one nun-null dimension). &lt;/p&gt;\n\n&lt;p&gt;Wondering how others handle the fragmentation of TB+ data size for dashboard output tools like Tableau that have limited extract sizes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yndujd", "is_robot_indexable": true, "report_reasons": null, "author": "SevenEyes", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yndujd/how_do_you_handle_tb_data_aggregations_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yndujd/how_do_you_handle_tb_data_aggregations_for/", "subreddit_subscribers": 79083, "created_utc": 1667700592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recently I automated our deployments, we have an in-house built data pipeline for snowflake, the price are  triggered from Azure data factory. We have a different stages in data pipeline. \n\nI used flyway as database change management tool,  since I was an application developer in the past instead of using standalone flyway, I went down maven path as it allowed me to represent each stage similar to our daily run, I also configured stages and tasks in devops corresponding to our stages. During our deployment we update one stage and trigger adf run wait for it complete and deploy the next stage views etc. Our deployment jobs runs in a self hosted agent which connects to snowflake using Oauth. Maven also for its dependency management. \n\n\nI have written some custom Java class to check status of jobs in snowflake. Jdbc calls and not api. Log4j logs for printing what jobs are running and what has failed, for easy debugging. \n\nSo far the deployments are smooth and we had three releases and we could see lots of hour being saved. The deployments are smooth by the time the scripts reaches UAT. \n\nWe also mapped git branches to different environments, deployments are automated in dev, sit, uat and preprod. \n\nI also provide user to deploy a certain stage and run its corresponding adf. \n\nIs there anything that I can improve? Did I over do it?  Thanks", "author_fullname": "t2_lnc7n53y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my implementation of continuous deployment an overkill?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynqhox", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667742830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently I automated our deployments, we have an in-house built data pipeline for snowflake, the price are  triggered from Azure data factory. We have a different stages in data pipeline. &lt;/p&gt;\n\n&lt;p&gt;I used flyway as database change management tool,  since I was an application developer in the past instead of using standalone flyway, I went down maven path as it allowed me to represent each stage similar to our daily run, I also configured stages and tasks in devops corresponding to our stages. During our deployment we update one stage and trigger adf run wait for it complete and deploy the next stage views etc. Our deployment jobs runs in a self hosted agent which connects to snowflake using Oauth. Maven also for its dependency management. &lt;/p&gt;\n\n&lt;p&gt;I have written some custom Java class to check status of jobs in snowflake. Jdbc calls and not api. Log4j logs for printing what jobs are running and what has failed, for easy debugging. &lt;/p&gt;\n\n&lt;p&gt;So far the deployments are smooth and we had three releases and we could see lots of hour being saved. The deployments are smooth by the time the scripts reaches UAT. &lt;/p&gt;\n\n&lt;p&gt;We also mapped git branches to different environments, deployments are automated in dev, sit, uat and preprod. &lt;/p&gt;\n\n&lt;p&gt;I also provide user to deploy a certain stage and run its corresponding adf. &lt;/p&gt;\n\n&lt;p&gt;Is there anything that I can improve? Did I over do it?  Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "ynqhox", "is_robot_indexable": true, "report_reasons": null, "author": "a_widower", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynqhox/is_my_implementation_of_continuous_deployment_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynqhox/is_my_implementation_of_continuous_deployment_an/", "subreddit_subscribers": 79083, "created_utc": 1667742830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am seeing a new trend online with universities offering  \"Micromasters\" and Professional Certificates that are only 4-6 months. Technically my Data Science degree from Syracuse is a Micro Masters since it's only 5 graduate level courses.\n\nI'd love to finish it but the program is rather expensive Bout $3500 a course and I'd need 6 more classes to get it.\n\nIve been browsing edX.org because I took their Introduction to Computer Science course taught by MIT professors and really enjoyed it. Learned lots of computer science  concepts as well as learning Python.\n\nI'm seeing quite a few Data Engineering programs on there but browsing though them some seem a bit to basic.\n\nIf you look at this program it's only $1000 or so and is 14  courses and says you can finish it in a year doing 2-3 hours a week but obviously you can finish it much faster since the program is your own pace.\n\nThe link I here  https://www.edx.org/professional-certificate/ibm-data-engineering\n\nHowever this is the description for their SQL for Data Engineering Courses \n\n```\nSQL Concepts for Data Engineers\n\nIn this short course you will learn additional\nSQL concepts such as views, stored\nprocedures, transactions and joins.\n\n```\nNot only is this not what I would consider SQL for Data Engineering but it Also seems rather simple. Joins, views thats\nThe stuff you learn in Intro to SQL courses. If I do this program I'd prolly just blast through it in one afternoon.\n\nBut then there are classes like this that seem very useful \n```\nBuilding ETL and Data Pipelines with Bash, Airflow and Kafka\n\n2\u20134 hours per week, for 5 weeks\n\nThis course provides you with practical skills to build and manage data pipelines and Extract, Transform, Load (ETL) processes using shell scripts, Airflow and Kafka.\n\nView the course\n\n```\nOr all of these \n\n```\n\nLinux Commands &amp; Shell Scripting\n\n3\u20134 hours per week, for 1 weeks\n\nThis\u00a0mini-course describes shell commands and how to use the advanced features of the Bash shell to automate complicated database tasks. For those not familiar with shell scripting, this course provides an overview of common Linux Shell Commands and shell scripting basics.\n\nView the course\n\nRelational Database Administration (DBA)\n\n2\u20133 hours per week, for 8 weeks\n\nThis course helps you develop the foundational skills required to perform the role of a Database Administrator (DBA) including designing, implementing, securing, maintaining, troubleshooting and automating databases such as MySQL, PostgreSQL and Db2.\n\nView the course\n\nBuilding ETL and Data Pipelines with Bash, Airflow and Kafka\n\n2\u20134 hours per week, for 5 weeks\n\nThis course provides you with practical skills to build and manage data pipelines and Extract, Transform, Load (ETL) processes using shell scripts, Airflow and Kafka.\n\nView the course\n\nData Warehousing and BI Analytics\n\n2\u20133 hours per week, for 6 weeks\n\nThis course introduces you to designing, implementing and populating a data warehouse and analyzing its data using SQL &amp; Business Intelligence (BI) tools.\n\nView the course\n\nNoSQL Database Basics\n\n2\u20133 hours per week, for 5 weeks\n\nThis course introduces you to the fundamentals of NoSQL, including the four key non-relational database categories. By the end of the course you will have hands-on skills for working with MongoDB, Cassandra and IBM Cloudant NoSQL databases.\n\nView the course\n\nBig Data, Hadoop, and Spark Basics\n\n2\u20133 hours per week, for 6 weeks\n\nThis course provides foundational big data practitioner knowledge and analytical skills using popular big data tools, including Hadoop and Spark. Learn and practice your big data skills hands-on.\n\nView the course\n\nApache Spark for Data Engineering and Machine Learning\n\n2\u20133 hours per week, for 3 weeks\n\nThis short course introduces you to the fundamentals of Data Engineering and Machine Learning with Apache Spark, including Spark Structured Streaming,\u00a0ETL for Machine Learning (ML) Pipelines,\u00a0and Spark ML. By the end of the course, you will have hands-on experience applying Spark skills to ETL and ML workflows.\n\nView the course\n\n ```\nAnd this is just one program \n\nMIT has one too that's a bit more expensive but still super reasonable. https://executive-ed.xpro.mit.edu/professional-certificate-data-engineering\n\n\n\n\nEither way I think I'll be taking one of these courses  or another similar one somewhere else to expand my skill set just out of SQL Server, Basic Python, SSIS and SSAS,  Azure and good ol Excel the world's greatest database.\n\nIf anyone else here would like to look into these programs and read through the curriculums and try to determine which program is the best and even have a group of us take it together that would be really cool . We can make a Git Hub for it and everything and work on some of the more challenging projects together.", "author_fullname": "t2_dfcot0zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Degree/Certificate programs for Data Engineering.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yngvld", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667710168.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667709845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am seeing a new trend online with universities offering  &amp;quot;Micromasters&amp;quot; and Professional Certificates that are only 4-6 months. Technically my Data Science degree from Syracuse is a Micro Masters since it&amp;#39;s only 5 graduate level courses.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to finish it but the program is rather expensive Bout $3500 a course and I&amp;#39;d need 6 more classes to get it.&lt;/p&gt;\n\n&lt;p&gt;Ive been browsing edX.org because I took their Introduction to Computer Science course taught by MIT professors and really enjoyed it. Learned lots of computer science  concepts as well as learning Python.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m seeing quite a few Data Engineering programs on there but browsing though them some seem a bit to basic.&lt;/p&gt;\n\n&lt;p&gt;If you look at this program it&amp;#39;s only $1000 or so and is 14  courses and says you can finish it in a year doing 2-3 hours a week but obviously you can finish it much faster since the program is your own pace.&lt;/p&gt;\n\n&lt;p&gt;The link I here  &lt;a href=\"https://www.edx.org/professional-certificate/ibm-data-engineering\"&gt;https://www.edx.org/professional-certificate/ibm-data-engineering&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;However this is the description for their SQL for Data Engineering Courses &lt;/p&gt;\n\n&lt;p&gt;```\nSQL Concepts for Data Engineers&lt;/p&gt;\n\n&lt;p&gt;In this short course you will learn additional\nSQL concepts such as views, stored\nprocedures, transactions and joins.&lt;/p&gt;\n\n&lt;p&gt;```\nNot only is this not what I would consider SQL for Data Engineering but it Also seems rather simple. Joins, views thats\nThe stuff you learn in Intro to SQL courses. If I do this program I&amp;#39;d prolly just blast through it in one afternoon.&lt;/p&gt;\n\n&lt;p&gt;But then there are classes like this that seem very useful \n```\nBuilding ETL and Data Pipelines with Bash, Airflow and Kafka&lt;/p&gt;\n\n&lt;p&gt;2\u20134 hours per week, for 5 weeks&lt;/p&gt;\n\n&lt;p&gt;This course provides you with practical skills to build and manage data pipelines and Extract, Transform, Load (ETL) processes using shell scripts, Airflow and Kafka.&lt;/p&gt;\n\n&lt;p&gt;View the course&lt;/p&gt;\n\n&lt;p&gt;```\nOr all of these &lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;Linux Commands &amp;amp; Shell Scripting&lt;/p&gt;\n\n&lt;p&gt;3\u20134 hours per week, for 1 weeks&lt;/p&gt;\n\n&lt;p&gt;This\u00a0mini-course describes shell commands and how to use the advanced features of the Bash shell to automate complicated database tasks. For those not familiar with shell scripting, this course provides an overview of common Linux Shell Commands and shell scripting basics.&lt;/p&gt;\n\n&lt;p&gt;View the course&lt;/p&gt;\n\n&lt;p&gt;Relational Database Administration (DBA)&lt;/p&gt;\n\n&lt;p&gt;2\u20133 hours per week, for 8 weeks&lt;/p&gt;\n\n&lt;p&gt;This course helps you develop the foundational skills required to perform the role of a Database Administrator (DBA) including designing, implementing, securing, maintaining, troubleshooting and automating databases such as MySQL, PostgreSQL and Db2.&lt;/p&gt;\n\n&lt;p&gt;View the course&lt;/p&gt;\n\n&lt;p&gt;Building ETL and Data Pipelines with Bash, Airflow and Kafka&lt;/p&gt;\n\n&lt;p&gt;2\u20134 hours per week, for 5 weeks&lt;/p&gt;\n\n&lt;p&gt;This course provides you with practical skills to build and manage data pipelines and Extract, Transform, Load (ETL) processes using shell scripts, Airflow and Kafka.&lt;/p&gt;\n\n&lt;p&gt;View the course&lt;/p&gt;\n\n&lt;p&gt;Data Warehousing and BI Analytics&lt;/p&gt;\n\n&lt;p&gt;2\u20133 hours per week, for 6 weeks&lt;/p&gt;\n\n&lt;p&gt;This course introduces you to designing, implementing and populating a data warehouse and analyzing its data using SQL &amp;amp; Business Intelligence (BI) tools.&lt;/p&gt;\n\n&lt;p&gt;View the course&lt;/p&gt;\n\n&lt;p&gt;NoSQL Database Basics&lt;/p&gt;\n\n&lt;p&gt;2\u20133 hours per week, for 5 weeks&lt;/p&gt;\n\n&lt;p&gt;This course introduces you to the fundamentals of NoSQL, including the four key non-relational database categories. By the end of the course you will have hands-on skills for working with MongoDB, Cassandra and IBM Cloudant NoSQL databases.&lt;/p&gt;\n\n&lt;p&gt;View the course&lt;/p&gt;\n\n&lt;p&gt;Big Data, Hadoop, and Spark Basics&lt;/p&gt;\n\n&lt;p&gt;2\u20133 hours per week, for 6 weeks&lt;/p&gt;\n\n&lt;p&gt;This course provides foundational big data practitioner knowledge and analytical skills using popular big data tools, including Hadoop and Spark. Learn and practice your big data skills hands-on.&lt;/p&gt;\n\n&lt;p&gt;View the course&lt;/p&gt;\n\n&lt;p&gt;Apache Spark for Data Engineering and Machine Learning&lt;/p&gt;\n\n&lt;p&gt;2\u20133 hours per week, for 3 weeks&lt;/p&gt;\n\n&lt;p&gt;This short course introduces you to the fundamentals of Data Engineering and Machine Learning with Apache Spark, including Spark Structured Streaming,\u00a0ETL for Machine Learning (ML) Pipelines,\u00a0and Spark ML. By the end of the course, you will have hands-on experience applying Spark skills to ETL and ML workflows.&lt;/p&gt;\n\n&lt;p&gt;View the course&lt;/p&gt;\n\n&lt;p&gt;```\nAnd this is just one program &lt;/p&gt;\n\n&lt;p&gt;MIT has one too that&amp;#39;s a bit more expensive but still super reasonable. &lt;a href=\"https://executive-ed.xpro.mit.edu/professional-certificate-data-engineering\"&gt;https://executive-ed.xpro.mit.edu/professional-certificate-data-engineering&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Either way I think I&amp;#39;ll be taking one of these courses  or another similar one somewhere else to expand my skill set just out of SQL Server, Basic Python, SSIS and SSAS,  Azure and good ol Excel the world&amp;#39;s greatest database.&lt;/p&gt;\n\n&lt;p&gt;If anyone else here would like to look into these programs and read through the curriculums and try to determine which program is the best and even have a group of us take it together that would be really cool . We can make a Git Hub for it and everything and work on some of the more challenging projects together.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4EJYmWa3ftloTyhO2bqOr8kGnBIW5VOLoBZdVhsJTZ0.jpg?auto=webp&amp;s=97515d509296f883a009858015c0ac346717d04c", "width": 1134, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/4EJYmWa3ftloTyhO2bqOr8kGnBIW5VOLoBZdVhsJTZ0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f02e1392c5a0328ceb56526065a4e93579b2b5d", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/4EJYmWa3ftloTyhO2bqOr8kGnBIW5VOLoBZdVhsJTZ0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cdbc3c367cdd8fa57faadd371159537a3dbe0a80", "width": 216, "height": 128}, {"url": "https://external-preview.redd.it/4EJYmWa3ftloTyhO2bqOr8kGnBIW5VOLoBZdVhsJTZ0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=92fffb3c6fe9342496cad163d3d6cc68a976eda5", "width": 320, "height": 190}, {"url": "https://external-preview.redd.it/4EJYmWa3ftloTyhO2bqOr8kGnBIW5VOLoBZdVhsJTZ0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7967c13350a6b98d7c0291c5c31ebef04a6afaca", "width": 640, "height": 380}, {"url": "https://external-preview.redd.it/4EJYmWa3ftloTyhO2bqOr8kGnBIW5VOLoBZdVhsJTZ0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=948b3eefeda76aa731968b75ce65948cb290b648", "width": 960, "height": 571}, {"url": "https://external-preview.redd.it/4EJYmWa3ftloTyhO2bqOr8kGnBIW5VOLoBZdVhsJTZ0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=efb47c31a31727ed5b8c3fc103ad2c306a3fc746", "width": 1080, "height": 642}], "variants": {}, "id": "I6YIO-70iBGhizTbOgRrkR_gC460yPGOSFOIvNw-6Yg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yngvld", "is_robot_indexable": true, "report_reasons": null, "author": "DrRedmondNYC", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yngvld/degreecertificate_programs_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yngvld/degreecertificate_programs_for_data_engineering/", "subreddit_subscribers": 79083, "created_utc": 1667709845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a3ns5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Requesting C&amp;C on DE diagram of a basic data platform setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_ynlt5c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fs8I1FnHI6qNjQo_IvZkb_RkM2xRBdmvKEXQnKxb9uE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667727772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/rpn8b0dwecy91.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/rpn8b0dwecy91.png?auto=webp&amp;s=46546085a2bad7f56c746d1610a2e0adfc07c813", "width": 1767, "height": 1172}, "resolutions": [{"url": "https://preview.redd.it/rpn8b0dwecy91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=73d69a851401188543a70875abf6ce2fa93d379a", "width": 108, "height": 71}, {"url": "https://preview.redd.it/rpn8b0dwecy91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f505f7c1a5ec9b8e886989ea119335f8e3fc4c19", "width": 216, "height": 143}, {"url": "https://preview.redd.it/rpn8b0dwecy91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7247754881d4ce807da9551f68921ca3dd848585", "width": 320, "height": 212}, {"url": "https://preview.redd.it/rpn8b0dwecy91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=accea3bc97449a68c1c2300bc9cc90422d7ccbea", "width": 640, "height": 424}, {"url": "https://preview.redd.it/rpn8b0dwecy91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7f11004bece30826b951fc935ed1762acad18ed", "width": 960, "height": 636}, {"url": "https://preview.redd.it/rpn8b0dwecy91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96cf6b11462ae5520230effed5336400bfcb28ff", "width": 1080, "height": 716}], "variants": {}, "id": "jFdDtZUCIPdJ8cRx-lMI32tmEfC_EmQ8s2lRwWH25yA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ynlt5c", "is_robot_indexable": true, "report_reasons": null, "author": "fico86", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynlt5c/requesting_cc_on_de_diagram_of_a_basic_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/rpn8b0dwecy91.png", "subreddit_subscribers": 79083, "created_utc": 1667727772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are tons of templates for all software fields.\nI'm looking for a template for a data engineering project that helps all the team get concise and relevant information about the design/requirements of the project. \n\nWhich template has worked for you?", "author_fullname": "t2_6ol23fw0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you share a good design document template when starting a data engineering project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynys0m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667768812.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667760393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are tons of templates for all software fields.\nI&amp;#39;m looking for a template for a data engineering project that helps all the team get concise and relevant information about the design/requirements of the project. &lt;/p&gt;\n\n&lt;p&gt;Which template has worked for you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ynys0m", "is_robot_indexable": true, "report_reasons": null, "author": "ElBartouk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynys0m/can_you_share_a_good_design_document_template/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynys0m/can_you_share_a_good_design_document_template/", "subreddit_subscribers": 79083, "created_utc": 1667760393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm about to start as a data engineer at a new role. Their primary cloud platform is AWS. They also mentioned Snowflake is important to know and is also used a lot, like I need to know how to move data from s3 to Snowflake. While I don't know the major details, but Snowflake does seem a bit costly, what are some reasons why a company would want to use Snowflake instead of one of AWS's services like Redshift? Is there a big difference?", "author_fullname": "t2_slggv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about AWS and Snowflake integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynpz6e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667741431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m about to start as a data engineer at a new role. Their primary cloud platform is AWS. They also mentioned Snowflake is important to know and is also used a lot, like I need to know how to move data from s3 to Snowflake. While I don&amp;#39;t know the major details, but Snowflake does seem a bit costly, what are some reasons why a company would want to use Snowflake instead of one of AWS&amp;#39;s services like Redshift? Is there a big difference?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ynpz6e", "is_robot_indexable": true, "report_reasons": null, "author": "RHowardL", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynpz6e/question_about_aws_and_snowflake_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynpz6e/question_about_aws_and_snowflake_integration/", "subreddit_subscribers": 79083, "created_utc": 1667741431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\n\nI'm trying to design some table with standardized data from multiple sources, in the most effective way.\n\nI'm working with Postgres and the current design is as following:\n\n1. The \\`main\\` schema has dimension tables (for example \\`units\\`, \\`currencies\\` and \\`countries\\`)\n2. Every source has it's own schema with dimension tables for the same columns (units, currencies and countries)\n3. In every schema, the dimension tables have an additional column \\`main\\_id\\` that is null at first, and requires an analyst to map the source's value to the corresponding one in our main schema. (For example, \\`US\\` in in source #1 should be mapped to \\`United States\\` in the main schema).\n4. There are materialized views in the \\`main\\` schema, unioning the data from every source, taking the \\`main\\_id\\` value of every dimension in every source. BUT, if some row in a source has a value who's \\`main\\_id\\` is still null (meaning, it wasn't mapped), then this row is filtered out.\n\n&amp;#x200B;\n\n[Architecture](https://preview.redd.it/cass3z81y7y91.png?width=1496&amp;format=png&amp;auto=webp&amp;s=8fa7821595d5209d88c91a3ca31f5bf8ba1852b8)\n\nSo currently, if new data is introduced into a source or if a new value is mapped, I just need to refresh the materialized views and the new data will be accessible in the mviews.\n\nNow I need those mviews to be actual tables and I'm wondering what the best way is to keep them up to date.\n\n1. Should I have a stored procedure per schema, inserting new data into the main tables? I could call it manually after a mapping is done or it could be triggered every few minutes\n2. Should I have a Python script implement the insertion logic?\n3. Should I have a trigger on every insert (data tables) / update (the mapping tables) to insert the data to the main tables? (I don't love triggers because they add hidden complexity)\n\n&amp;#x200B;\n\nAny other ideas will be very appreciated!\n\n&amp;#x200B;\n\nThanks :)", "author_fullname": "t2_74d9mi1r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Standardization after Dimension Mapping", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 106, "top_awarded_type": null, "hide_score": false, "media_metadata": {"cass3z81y7y91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/cass3z81y7y91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=92f10bde1d25385db89dc4af94c3256f562a5191"}, {"y": 163, "x": 216, "u": "https://preview.redd.it/cass3z81y7y91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a7ad4f4ef294944932c452ffd1485bcbf66fae14"}, {"y": 242, "x": 320, "u": "https://preview.redd.it/cass3z81y7y91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=05470d433a30d114dc8c6cb516daeefc2193c1bd"}, {"y": 485, "x": 640, "u": "https://preview.redd.it/cass3z81y7y91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=870d7161fdd07754e1e02847d902ffb080676d8b"}, {"y": 727, "x": 960, "u": "https://preview.redd.it/cass3z81y7y91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ac16404696df978a5b4ec337bb21363a5c7afb2"}, {"y": 818, "x": 1080, "u": "https://preview.redd.it/cass3z81y7y91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=328e2bf9fabb349af65e96c3c88ed31fa42d4b2b"}], "s": {"y": 1134, "x": 1496, "u": "https://preview.redd.it/cass3z81y7y91.png?width=1496&amp;format=png&amp;auto=webp&amp;s=8fa7821595d5209d88c91a3ca31f5bf8ba1852b8"}, "id": "cass3z81y7y91"}}, "name": "t3_ynajp4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/G5rUXvTKrIZLmKTvZAZ5UBVEIBMAZBcQonqapI6C4Wg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667691991.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to design some table with standardized data from multiple sources, in the most effective way.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working with Postgres and the current design is as following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The `main` schema has dimension tables (for example `units`, `currencies` and `countries`)&lt;/li&gt;\n&lt;li&gt;Every source has it&amp;#39;s own schema with dimension tables for the same columns (units, currencies and countries)&lt;/li&gt;\n&lt;li&gt;In every schema, the dimension tables have an additional column `main_id` that is null at first, and requires an analyst to map the source&amp;#39;s value to the corresponding one in our main schema. (For example, `US` in in source #1 should be mapped to `United States` in the main schema).&lt;/li&gt;\n&lt;li&gt;There are materialized views in the `main` schema, unioning the data from every source, taking the `main_id` value of every dimension in every source. BUT, if some row in a source has a value who&amp;#39;s `main_id` is still null (meaning, it wasn&amp;#39;t mapped), then this row is filtered out.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cass3z81y7y91.png?width=1496&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8fa7821595d5209d88c91a3ca31f5bf8ba1852b8\"&gt;Architecture&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So currently, if new data is introduced into a source or if a new value is mapped, I just need to refresh the materialized views and the new data will be accessible in the mviews.&lt;/p&gt;\n\n&lt;p&gt;Now I need those mviews to be actual tables and I&amp;#39;m wondering what the best way is to keep them up to date.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Should I have a stored procedure per schema, inserting new data into the main tables? I could call it manually after a mapping is done or it could be triggered every few minutes&lt;/li&gt;\n&lt;li&gt;Should I have a Python script implement the insertion logic?&lt;/li&gt;\n&lt;li&gt;Should I have a trigger on every insert (data tables) / update (the mapping tables) to insert the data to the main tables? (I don&amp;#39;t love triggers because they add hidden complexity)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any other ideas will be very appreciated!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ynajp4", "is_robot_indexable": true, "report_reasons": null, "author": "Helpful_Artist1439", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynajp4/data_standardization_after_dimension_mapping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynajp4/data_standardization_after_dimension_mapping/", "subreddit_subscribers": 79083, "created_utc": 1667691991.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Everyone \n\nI am trying to break into DE role, but having hard time with studying at the moment,  need to work 2 jobs to support the Family, is there an Entry-level position in Data where company would be willing to train, and eventually can be a good stepping stone into a DE role, can't really afford to quit one of the jobs so I can study but figure if  I can find a Job that is willing to train,  it would be better than just giving up on the dream at the moment", "author_fullname": "t2_gtqs4z53", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "carrer path help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynxpbu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667758183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everyone &lt;/p&gt;\n\n&lt;p&gt;I am trying to break into DE role, but having hard time with studying at the moment,  need to work 2 jobs to support the Family, is there an Entry-level position in Data where company would be willing to train, and eventually can be a good stepping stone into a DE role, can&amp;#39;t really afford to quit one of the jobs so I can study but figure if  I can find a Job that is willing to train,  it would be better than just giving up on the dream at the moment&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "ynxpbu", "is_robot_indexable": true, "report_reasons": null, "author": "10xbek", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynxpbu/carrer_path_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynxpbu/carrer_path_help/", "subreddit_subscribers": 79083, "created_utc": 1667758183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone used Snowflake with external delta tables extensively? We have Snowflake as our primary data warehouse, and we use Databricks as a general purpose data science/data engineering workbench. There are some cases in which having the Snowflake data as a registered table would make things a bit easier. Curious about your experiences. I know of course the Snowflake performance would suffer but are there any other major functionality limitations?", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External Delta tables in Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynsx0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667748318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone used Snowflake with external delta tables extensively? We have Snowflake as our primary data warehouse, and we use Databricks as a general purpose data science/data engineering workbench. There are some cases in which having the Snowflake data as a registered table would make things a bit easier. Curious about your experiences. I know of course the Snowflake performance would suffer but are there any other major functionality limitations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ynsx0t", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynsx0t/external_delta_tables_in_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynsx0t/external_delta_tables_in_snowflake/", "subreddit_subscribers": 79083, "created_utc": 1667748318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m building a framework for getting data into or out of Snowflake using Python. It\u2019s just bare bones and will be extended over time.\n\nI\u2019m looking for name suggestions. Pipewire has already been used. The current \u2018Python Ingress/Egress Framework\u2019 is just a bit, well, sh!t.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Name suggestions: Pipeline Framework", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynl2hg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667724969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m building a framework for getting data into or out of Snowflake using Python. It\u2019s just bare bones and will be extended over time.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking for name suggestions. Pipewire has already been used. The current \u2018Python Ingress/Egress Framework\u2019 is just a bit, well, sh!t.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ynl2hg", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynl2hg/name_suggestions_pipeline_framework/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynl2hg/name_suggestions_pipeline_framework/", "subreddit_subscribers": 79083, "created_utc": 1667724969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently came across articles where it was stated external tokenisation does not work with large amount of data and can slow down the overall data load time in the pipeline. If anyone can link anything as to why or explain why tokenisation will not work at scale?", "author_fullname": "t2_62ee3cao", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can Snowflake\u2019s external tokenisation work at scale (huge amount on data)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynkvk7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667724184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently came across articles where it was stated external tokenisation does not work with large amount of data and can slow down the overall data load time in the pipeline. If anyone can link anything as to why or explain why tokenisation will not work at scale?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ynkvk7", "is_robot_indexable": true, "report_reasons": null, "author": "dotslash06", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynkvk7/can_snowflakes_external_tokenisation_work_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynkvk7/can_snowflakes_external_tokenisation_work_at/", "subreddit_subscribers": 79083, "created_utc": 1667724184.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\n\nso I am a PM for a ML product and we are running into scaling trouble. The product uses K8s to scale horizontally and the Pods needs to run initially some OLAP queries one currently a PostGre instance. Aggregation is used.\n\nIt will have ~ 100 tables and each table will have ~ 400 Million rows growing 210k rows each day. There are just a few columns ~ 15 with either integer or strings.\n\nOptions I see is:\n- Use Connection Pooling and Read Replicas -&gt; could this be sufficient already?\n- Use Snowflake and it\u2019s auto scale feature and let the scheduler do the coordination work -&gt; heard snowflake is however bad for many concurrent short lived connections?\n- Use Citus Hyperscale -&gt; Engineers told me this is super expensive on Azure hence no option probably?\n- Put stuff low level on the Storage with suitable partitioning and then directly read files from there. -&gt; High development effort and one creates a solution for a problem that should normally be already solved better by some technology I assume?\n- Spark is for whatever reason no option for us according to the engineers.\n\nWhat is your take? How would you go about this scaling bottleneck and rearchitecture?\n\nThanks a lot for reading!", "author_fullname": "t2_cg3vl4i3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What DB/Storage to choose with K8s, OLAP queries and a lot of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynkrjv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667723754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;so I am a PM for a ML product and we are running into scaling trouble. The product uses K8s to scale horizontally and the Pods needs to run initially some OLAP queries one currently a PostGre instance. Aggregation is used.&lt;/p&gt;\n\n&lt;p&gt;It will have ~ 100 tables and each table will have ~ 400 Million rows growing 210k rows each day. There are just a few columns ~ 15 with either integer or strings.&lt;/p&gt;\n\n&lt;p&gt;Options I see is:\n- Use Connection Pooling and Read Replicas -&amp;gt; could this be sufficient already?\n- Use Snowflake and it\u2019s auto scale feature and let the scheduler do the coordination work -&amp;gt; heard snowflake is however bad for many concurrent short lived connections?\n- Use Citus Hyperscale -&amp;gt; Engineers told me this is super expensive on Azure hence no option probably?\n- Put stuff low level on the Storage with suitable partitioning and then directly read files from there. -&amp;gt; High development effort and one creates a solution for a problem that should normally be already solved better by some technology I assume?\n- Spark is for whatever reason no option for us according to the engineers.&lt;/p&gt;\n\n&lt;p&gt;What is your take? How would you go about this scaling bottleneck and rearchitecture?&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ynkrjv", "is_robot_indexable": true, "report_reasons": null, "author": "DataDemystifier", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynkrjv/what_dbstorage_to_choose_with_k8s_olap_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynkrjv/what_dbstorage_to_choose_with_k8s_olap_queries/", "subreddit_subscribers": 79083, "created_utc": 1667723754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm stuck in a scenario and looking forward to possible scenarios to tackle it.\n \nWe have an account number and the columns next to it contain the description. Till now we have unique account nos in table.\n\nAccount.  | Descp. | 2nd Descp. | 3rd Descp\n123.          | Abc.     | Cdf.              | Ghi\n345.          | Xyz.      | Cde.             | Ghi\n345.          | Xyz.      | Cde.             | Jkl\n\nSomething like the above table. The thing is that in the 3rd Description I have two values ( possibly more in future). The above table is a Dimension table and it's also joining the fact table and both fact &amp; Dim are used in the dashboard as well. So in order to avoid the many-to-many join what will be the best method? \n\nSolutions till I consider:\n1- Create another dimension table to store the records there and join both dimensions.\n2- Create separate columns for each of the 3rd description values.\n3- Create a 4th description column if the 3rd description has a second value then insert it in the 4th description else null.", "author_fullname": "t2_mqex15ng", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiple values against single account no", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yntap2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667749087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m stuck in a scenario and looking forward to possible scenarios to tackle it.&lt;/p&gt;\n\n&lt;p&gt;We have an account number and the columns next to it contain the description. Till now we have unique account nos in table.&lt;/p&gt;\n\n&lt;p&gt;Account.  | Descp. | 2nd Descp. | 3rd Descp\n123.          | Abc.     | Cdf.              | Ghi\n345.          | Xyz.      | Cde.             | Ghi\n345.          | Xyz.      | Cde.             | Jkl&lt;/p&gt;\n\n&lt;p&gt;Something like the above table. The thing is that in the 3rd Description I have two values ( possibly more in future). The above table is a Dimension table and it&amp;#39;s also joining the fact table and both fact &amp;amp; Dim are used in the dashboard as well. So in order to avoid the many-to-many join what will be the best method? &lt;/p&gt;\n\n&lt;p&gt;Solutions till I consider:\n1- Create another dimension table to store the records there and join both dimensions.\n2- Create separate columns for each of the 3rd description values.\n3- Create a 4th description column if the 3rd description has a second value then insert it in the 4th description else null.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yntap2", "is_robot_indexable": true, "report_reasons": null, "author": "adnankhan16", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yntap2/multiple_values_against_single_account_no/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yntap2/multiple_values_against_single_account_no/", "subreddit_subscribers": 79083, "created_utc": 1667749087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1bnhotlu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Responsible Cloud Security certifications for engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_ynrif9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zJX9m1gDfrd21Yglpyms9GUFB1_qOOimHpF6AoS8eMY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667745331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kanger.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://kanger.dev/cloud-security-certifications/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OvW1pHKAfuP6TcwXlm8C1iRsE1zEFl9dJPdY7RBjYpk.jpg?auto=webp&amp;s=9ece3dd62d1536c4478eb5304d6be24d606cdb64", "width": 800, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/OvW1pHKAfuP6TcwXlm8C1iRsE1zEFl9dJPdY7RBjYpk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=24bff59e710d43fb8a5179adddb3eb040881bda0", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/OvW1pHKAfuP6TcwXlm8C1iRsE1zEFl9dJPdY7RBjYpk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd0918b470ed4f9fe4b4ccd5aab9ee794908ff04", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/OvW1pHKAfuP6TcwXlm8C1iRsE1zEFl9dJPdY7RBjYpk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d888b49cda77cd2842c0231b6d9d8ca7543bf89b", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/OvW1pHKAfuP6TcwXlm8C1iRsE1zEFl9dJPdY7RBjYpk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=30bdbd391f3c058e8810d5e0e7c14dd76eb66255", "width": 640, "height": 480}], "variants": {}, "id": "bDLn3nN1l6R3EMyywqG_BDgS11J9ZgCdNPji_AwagXI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "ynrif9", "is_robot_indexable": true, "report_reasons": null, "author": "skj8", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynrif9/responsible_cloud_security_certifications_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://kanger.dev/cloud-security-certifications/", "subreddit_subscribers": 79083, "created_utc": 1667745331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title.\n\nIm in the German job market and see way more job offers with a Cloud XYZ title than data engineering. Looking into the specs, the tasks and responsibilities seem to be very similar.\n\nWhat do you think?", "author_fullname": "t2_x33zo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Data Engineer and Cloud Architect / X the same role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynn81r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667732999.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title.&lt;/p&gt;\n\n&lt;p&gt;Im in the German job market and see way more job offers with a Cloud XYZ title than data engineering. Looking into the specs, the tasks and responsibilities seem to be very similar.&lt;/p&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "ynn81r", "is_robot_indexable": true, "report_reasons": null, "author": "fitnessfragen", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynn81r/is_data_engineer_and_cloud_architect_x_the_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynn81r/is_data_engineer_and_cloud_architect_x_the_same/", "subreddit_subscribers": 79083, "created_utc": 1667732999.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been learning DE for a few weeks now hoping to eventually land a job but I still have some concerns.\n\nMy impression is that DE is similar to DevOps where you're only given attention when something breaks and your superior needs you to fix it ASAP. When everything is working as expected, you are rarely recognized for your good work.\n\nWhen something goes wrong, you're the person that everyone points their finger at and blames. You're expected to be a jack of all trades developer while at the same time being a master DE developer. Basically, you need to know a lot of shit\n\nWhile doing my research about the field, I came across another reddit post where people were mentioning that working as a DE was stressful AF.", "author_fullname": "t2_yex15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is DE one of those thankless jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynkjno", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667722915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been learning DE for a few weeks now hoping to eventually land a job but I still have some concerns.&lt;/p&gt;\n\n&lt;p&gt;My impression is that DE is similar to DevOps where you&amp;#39;re only given attention when something breaks and your superior needs you to fix it ASAP. When everything is working as expected, you are rarely recognized for your good work.&lt;/p&gt;\n\n&lt;p&gt;When something goes wrong, you&amp;#39;re the person that everyone points their finger at and blames. You&amp;#39;re expected to be a jack of all trades developer while at the same time being a master DE developer. Basically, you need to know a lot of shit&lt;/p&gt;\n\n&lt;p&gt;While doing my research about the field, I came across another reddit post where people were mentioning that working as a DE was stressful AF.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "ynkjno", "is_robot_indexable": true, "report_reasons": null, "author": "desperate-1", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynkjno/is_de_one_of_those_thankless_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynkjno/is_de_one_of_those_thankless_jobs/", "subreddit_subscribers": 79083, "created_utc": 1667722915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Thas has web interface?", "author_fullname": "t2_b6wvl1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source etl tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynzvhw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667762624.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thas has web interface?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ynzvhw", "is_robot_indexable": true, "report_reasons": null, "author": "v2da", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynzvhw/open_source_etl_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynzvhw/open_source_etl_tool/", "subreddit_subscribers": 79083, "created_utc": 1667762624.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have wondered (since I am studying) if it is worth investing more years of my life in something that can be very competitive and difficult and with poor pay, existential doubt", "author_fullname": "t2_rzm8vs2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The future for data engineering is promising? or is it perhaps excessive work with little pay", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ynhgwu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667711791.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have wondered (since I am studying) if it is worth investing more years of my life in something that can be very competitive and difficult and with poor pay, existential doubt&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ynhgwu", "is_robot_indexable": true, "report_reasons": null, "author": "IlustriousCap", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ynhgwu/the_future_for_data_engineering_is_promising_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ynhgwu/the_future_for_data_engineering_is_promising_or/", "subreddit_subscribers": 79083, "created_utc": 1667711791.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there opportunities for career progression?", "author_fullname": "t2_ecofamsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is de a dead end field?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yneoqh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.11, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667703052.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there opportunities for career progression?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yneoqh", "is_robot_indexable": true, "report_reasons": null, "author": "Flat_Selection1105", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yneoqh/is_de_a_dead_end_field/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yneoqh/is_de_a_dead_end_field/", "subreddit_subscribers": 79083, "created_utc": 1667703052.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}