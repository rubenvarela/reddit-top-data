{"kind": "Listing", "data": {"after": "t3_z7dwwz", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_32nu9tpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Elements 12TB: $175 after $40 off promo code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 53, "top_awarded_type": null, "hide_score": false, "name": "t3_z6z4xe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 248, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 248, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EecN7D0NOmxars9JZSQaWp7TArdt0eQiuhqdKq7dSgU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669648651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/qet2t0j2lp2a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?auto=webp&amp;s=c40c3b7a7ad094899748d2ea343232f8cad7833e", "width": 1828, "height": 696}, "resolutions": [{"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=716870203a3f2c715dd5a406f2ae998b612aeae6", "width": 108, "height": 41}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=83bcf71c8b11fdf7eb628a1e7371846e6e7edb76", "width": 216, "height": 82}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d78414140ee2eaee7f515e0dbb9a54a95f3959b", "width": 320, "height": 121}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bcd4582e1b12b84c9816b614091553bd2e8bf24", "width": 640, "height": 243}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=25ac76353e7c9e33dd5b0cb3b21c87b6d227a98a", "width": 960, "height": 365}, {"url": "https://preview.redd.it/qet2t0j2lp2a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=412d2b11079b810832e50aaa9433344ebe0feaaf", "width": 1080, "height": 411}], "variants": {}, "id": "vLd_u51BuwpxsUA6Owsmqe0DJSQ32OjxAvsPEhxGye0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "z6z4xe", "is_robot_indexable": true, "report_reasons": null, "author": "Kosofkors", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6z4xe/wd_elements_12tb_175_after_40_off_promo_code/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/qet2t0j2lp2a1.png", "subreddit_subscribers": 656672, "created_utc": 1669648651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ud20v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you all monitor ambient temps for your drives? Cooking drives is no fun... I think I found a decent solution with these $12 Govee bluetooth thermometers and Home Assistant.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_z6yt5j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 249, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 249, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/x3Mb9g8NNWNcr8NdTGW4r5ePEQ7g6Gzfd7G18_NNB8U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669647819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "austinsnerdythings.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://austinsnerdythings.com/2021/12/27/using-the-govee-bluetooth-thermometer-with-home-assistant-python-and-mqtt/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?auto=webp&amp;s=b48a34bccb2954db8eb1e4c65492d55fd53a6469", "width": 797, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a4e406978acf38f4e1bc40cc05bcf511d8a97ea0", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38d7e5b765a309c12b1f74510985e49efeaaad5d", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=52144e94d62313773ce70d6ed7f890458c3e7d29", "width": 320, "height": 321}, {"url": "https://external-preview.redd.it/esDCCiBgxKOkGPVZ0hQfcX_7iiqy7pV0c3iDwBPCZTA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da6e2c281454349bd4a62444a47de41ac3cb6365", "width": 640, "height": 642}], "variants": {}, "id": "-Z7KWEbgh-aaJdZinEtRKgixRmRb1ywIvERkB5QovFs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6yt5j", "is_robot_indexable": true, "report_reasons": null, "author": "MzCWzL", "discussion_type": null, "num_comments": 97, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6yt5j/how_do_you_all_monitor_ambient_temps_for_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://austinsnerdythings.com/2021/12/27/using-the-govee-bluetooth-thermometer-with-home-assistant-python-and-mqtt/", "subreddit_subscribers": 656672, "created_utc": 1669647819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good morning everyone. Like most people studying, I got fucked by the pandemic and became part of the Zoom Academy, and most of the materials we used are digital only. I wanted to know if there's a way to download 3 years' worth of documents, assignments, and such on Google Classroom to my computer. My access to these documents is coming close, so I wanted to store them for future use. Is there anything I can do, or should I download everything manually?\n\nA random guy on Discord said I should come to this sub to search for answers, so sorry if this is not the correct place", "author_fullname": "t2_2g2sxfio", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download of Google Classroom Materials", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6vrk8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669639855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good morning everyone. Like most people studying, I got fucked by the pandemic and became part of the Zoom Academy, and most of the materials we used are digital only. I wanted to know if there&amp;#39;s a way to download 3 years&amp;#39; worth of documents, assignments, and such on Google Classroom to my computer. My access to these documents is coming close, so I wanted to store them for future use. Is there anything I can do, or should I download everything manually?&lt;/p&gt;\n\n&lt;p&gt;A random guy on Discord said I should come to this sub to search for answers, so sorry if this is not the correct place&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6vrk8", "is_robot_indexable": true, "report_reasons": null, "author": "Sanslution", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6vrk8/download_of_google_classroom_materials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6vrk8/download_of_google_classroom_materials/", "subreddit_subscribers": 656672, "created_utc": 1669639855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have seen a website similar to Wayback Machine / archive.org that used to crawl and index HTML code of  every webpage / website. I forgot the website. Does anyone know anything similar?\n\nFor example, we could search for some term like \"&lt;h1&gt;twitter&lt;/h1&gt;\" and it used to return all webpages that has a twitter inside head tag in their html code.\n\nThanks in advance", "author_fullname": "t2_9s1kb9su", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web Archive - HTML Code dump of websites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6w06z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669640499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen a website similar to Wayback Machine / archive.org that used to crawl and index HTML code of  every webpage / website. I forgot the website. Does anyone know anything similar?&lt;/p&gt;\n\n&lt;p&gt;For example, we could search for some term like &amp;quot;&amp;lt;h1&amp;gt;twitter&amp;lt;/h1&amp;gt;&amp;quot; and it used to return all webpages that has a twitter inside head tag in their html code.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6w06z", "is_robot_indexable": true, "report_reasons": null, "author": "karthiksudhan-wild", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6w06z/web_archive_html_code_dump_of_websites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6w06z/web_archive_html_code_dump_of_websites/", "subreddit_subscribers": 656672, "created_utc": 1669640499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_gh87r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "18TB Seagate Exos Enterprise HDD 7200 RPM - $269.99 ($15/TB)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z7dxp9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1669681685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "newegg.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.newegg.com/seagate-exos-x18-st18000nm000j-18tb/p/1B4-00VK-00616", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z7dxp9", "is_robot_indexable": true, "report_reasons": null, "author": "Viknee", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z7dxp9/18tb_seagate_exos_enterprise_hdd_7200_rpm_26999/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.newegg.com/seagate-exos-x18-st18000nm000j-18tb/p/1B4-00VK-00616", "subreddit_subscribers": 656672, "created_utc": 1669681685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "$13.62/TB\n\nAmazon: https://www.amazon.com/dp/B09KMGQG5Y\n\nWestern Digital: https://www.westerndigital.com/products/internal-drives/wd-blue-desktop-sata-hdd#WD80EAZZ\n\nSeems Amazon ships pretty much immediately, WD Store indicates available 3-4 weeks.", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Blue 3.5\" 8TB CMR 5640 RPM HDD $109.99 (USA) Cyber Monday Deal (Amazon and WD Store)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6z5zl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669648728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;$13.62/TB&lt;/p&gt;\n\n&lt;p&gt;Amazon: &lt;a href=\"https://www.amazon.com/dp/B09KMGQG5Y\"&gt;https://www.amazon.com/dp/B09KMGQG5Y&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Western Digital: &lt;a href=\"https://www.westerndigital.com/products/internal-drives/wd-blue-desktop-sata-hdd#WD80EAZZ\"&gt;https://www.westerndigital.com/products/internal-drives/wd-blue-desktop-sata-hdd#WD80EAZZ&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Seems Amazon ships pretty much immediately, WD Store indicates available 3-4 weeks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z6z5zl", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6z5zl/wd_blue_35_8tb_cmr_5640_rpm_hdd_10999_usa_cyber/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6z5zl/wd_blue_35_8tb_cmr_5640_rpm_hdd_10999_usa_cyber/", "subreddit_subscribers": 656672, "created_utc": 1669648728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_h3zc4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD 16TB Elements External @ $230, 20tb @$320", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_z6uljs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XQeO52zNtio13p2nbLbQSnEKOTDkng2PtQV7Ij4piiA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669636453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bhphotovideo.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.bhphotovideo.com/c/product/1604996-REG/wd_wdbwlg0160hbk_nesn_wd_elements_desktop_16tb.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?auto=webp&amp;s=d533c09e4e1c69de0bd50bc25ae8da9e7dbafee7", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6754091ac5eb6a439260facfe4e2f85cfc9f9260", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c8cd696a54908ce6e149eb7f17ba706715b5992", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee30d5223ed5c1434c89d0fd0000d85515ced8e0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=08f43d5bc65c353767346d1847f8bd9e62526bf3", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8cec47c342d745aa908a41fe83669752d0c890b0", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/sl-jKLXWfamwYrVvLlNJl8A98qFRYd21LJx1pv0WSeU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=638ff173faccb4ac7c5e3898d30b6c71d30e31d0", "width": 1080, "height": 567}], "variants": {}, "id": "MLLs0tCS3MSYCsXnYaXuEEuXfYT7OImhc-S9BT9FlNI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "z6uljs", "is_robot_indexable": true, "report_reasons": null, "author": "Anzial", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6uljs/wd_16tb_elements_external_230_20tb_320/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.bhphotovideo.com/c/product/1604996-REG/wd_wdbwlg0160hbk_nesn_wd_elements_desktop_16tb.html", "subreddit_subscribers": 656672, "created_utc": 1669636453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If I understand it correctly these contain multiple bdxls, so if I take them out can I write and read them in a regular bdxl compatible drive instead of using the whole cartridge in $10000 readers? It seems cheaper than buying 100 or 128gb bdxls by themselves.", "author_fullname": "t2_50fpwuu0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can discs be removed from from optical disc archival cartridges?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6wm22", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669642158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I understand it correctly these contain multiple bdxls, so if I take them out can I write and read them in a regular bdxl compatible drive instead of using the whole cartridge in $10000 readers? It seems cheaper than buying 100 or 128gb bdxls by themselves.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6wm22", "is_robot_indexable": true, "report_reasons": null, "author": "Voldy256", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6wm22/can_discs_be_removed_from_from_optical_disc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6wm22/can_discs_be_removed_from_from_optical_disc/", "subreddit_subscribers": 656672, "created_utc": 1669642158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been backing up my PC games from places like Steam, GOG, etc. on 100 GB Blu-ray discs. However, I have a lot of games to back up, and modern games can very easily go over 100 GB per game. I've been throwing around the idea of giving up optical media for my backups, and going with something like a 4TB SSD, especially today on Cyber Monday; putting games onto an SSD would be a lot faster than burning them to a Blu-ray disc, and obviously any SSD that I choose is going to hold more than a single Blu-ray disc, even a triple-layer one. If all I'm using the SSD for is long-term storage, can I expect reliability? I don't even plan on putting it into my PC, I would only plug it in externally whenever I want to add something to it for safekeeping, or if I want to restore/recover something later.\n\nOr, if I'm looking to save some money, how are mechanical HDDs for the long-term? I assume they're not as good as something with no moving parts.\n\nThanks in advance for any insight, I really love the idea of long-term archiving, but I admit that I'm pretty new at this.", "author_fullname": "t2_cfqqjb1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is an SSD a reliable form of long-term storage for backed up files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z7756k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669667501.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669666593.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been backing up my PC games from places like Steam, GOG, etc. on 100 GB Blu-ray discs. However, I have a lot of games to back up, and modern games can very easily go over 100 GB per game. I&amp;#39;ve been throwing around the idea of giving up optical media for my backups, and going with something like a 4TB SSD, especially today on Cyber Monday; putting games onto an SSD would be a lot faster than burning them to a Blu-ray disc, and obviously any SSD that I choose is going to hold more than a single Blu-ray disc, even a triple-layer one. If all I&amp;#39;m using the SSD for is long-term storage, can I expect reliability? I don&amp;#39;t even plan on putting it into my PC, I would only plug it in externally whenever I want to add something to it for safekeeping, or if I want to restore/recover something later.&lt;/p&gt;\n\n&lt;p&gt;Or, if I&amp;#39;m looking to save some money, how are mechanical HDDs for the long-term? I assume they&amp;#39;re not as good as something with no moving parts.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any insight, I really love the idea of long-term archiving, but I admit that I&amp;#39;m pretty new at this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z7756k", "is_robot_indexable": true, "report_reasons": null, "author": "FireCrow1013", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z7756k/is_an_ssd_a_reliable_form_of_longterm_storage_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z7756k/is_an_ssd_a_reliable_form_of_longterm_storage_for/", "subreddit_subscribers": 656672, "created_utc": 1669666593.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi reddit,\n\n&amp;#x200B;\n\nI need to download a specific file type from my dropbox account. In this case only all .mp4-files. Any suggestion how I can do it? \n\n&amp;#x200B;\n\nIt's not possible to use the \"Multi file organize/Custom filters\" to move all the files to a new directory. All directories needs to be kept as they are.", "author_fullname": "t2_jakf6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download specific file types from Dropbox", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6vqqz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669639795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi reddit,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I need to download a specific file type from my dropbox account. In this case only all .mp4-files. Any suggestion how I can do it? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not possible to use the &amp;quot;Multi file organize/Custom filters&amp;quot; to move all the files to a new directory. All directories needs to be kept as they are.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6vqqz", "is_robot_indexable": true, "report_reasons": null, "author": "blowmycool", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6vqqz/download_specific_file_types_from_dropbox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6vqqz/download_specific_file_types_from_dropbox/", "subreddit_subscribers": 656672, "created_utc": 1669639795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I was looking to buy a 2.5'' 5Tb external hard drive, however after some searching, it seems like the WD drives don't have an external SATA connector or something like that, so I was wondering if in the case of a possible disk failure, that should be something I should have in mind (like buying a Seagate disk that has the SATA connector). I am not interested in shucking the hard drive though.\n\nSeagate disks seemed like a good option too, but however from what I've searched, they seemed to have a higher failure rate compared to other brands...\n\nWith all this in mind, what would be your suggestions? Should I care about the SATA connector I'll probably never use (unless it helps recovering disk content, if it fails one day), or should I look into other brands?\n\nThanks in advance!", "author_fullname": "t2_ycqmv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2.5\" 5TB external HDD - WD vs Seagate vs others?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6p8jr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669617607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I was looking to buy a 2.5&amp;#39;&amp;#39; 5Tb external hard drive, however after some searching, it seems like the WD drives don&amp;#39;t have an external SATA connector or something like that, so I was wondering if in the case of a possible disk failure, that should be something I should have in mind (like buying a Seagate disk that has the SATA connector). I am not interested in shucking the hard drive though.&lt;/p&gt;\n\n&lt;p&gt;Seagate disks seemed like a good option too, but however from what I&amp;#39;ve searched, they seemed to have a higher failure rate compared to other brands...&lt;/p&gt;\n\n&lt;p&gt;With all this in mind, what would be your suggestions? Should I care about the SATA connector I&amp;#39;ll probably never use (unless it helps recovering disk content, if it fails one day), or should I look into other brands?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6p8jr", "is_robot_indexable": true, "report_reasons": null, "author": "supercar1x", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6p8jr/25_5tb_external_hdd_wd_vs_seagate_vs_others/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6p8jr/25_5tb_external_hdd_wd_vs_seagate_vs_others/", "subreddit_subscribers": 656672, "created_utc": 1669617607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am planning to build a NAS with unraid which will have 2x14, 2x8 TB,  for now. This NAS will be a standalone and act as NAS without Plex or  any other services. I want to have another system working as the main  server. I currently have few optoins at my disposal and was wondering  what would be the best use of current resources. Some are already built  systems and some need to be built.\n\nExpand-ability: slowly but not sure to what storage.\n\nPower consumption: Preferred low, as it might be idle 70% at least for the first year.\n\n&amp;#x200B;\n\nOption1 (Preferred option):\n\n[i5-2400](https://www.intel.com/content/www/us/en/products/sku/52207/intel-core-i52400-processor-6m-cache-up-to-3-40-ghz/specifications.html), 8GB (not expandable) ram as NAS and[ i5-8500T](https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html), 16GB as the main server\n\nOption2:\n\n[i3-10105F](https://www.intel.com/content/www/us/en/products/sku/203474/intel-core-i310105f-processor-6m-cache-up-to-4-40-ghz/specifications.html), xGB (need to be built) and [i5-8500T](https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html), 16GB - either can be used for either purpose\n\nOption3:\n\n[i7-6700](https://www.intel.com/content/www/us/en/products/sku/88195/intel-core-i76700k-processor-8m-cache-up-to-4-20-ghz/specifications.html), xGB (need to be built) and [i5-8500T](https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html), 16GB - either can be used for either purpose\n\nOption4:\n\nUse either of the above systems to do the whole server in one massive system, which has all the HDD's and the main server.\n\n&amp;#x200B;\n\nComparison of CPU's  \n[https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F](https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F)\n\n&amp;#x200B;\n\nAlso, have a [1060](https://www.evga.com/products/specs/gpu.aspx?pn=e466f076-3e8f-4825-b4b0-f1863e893972), 6GB nvidia if that can be leveraged.\n\nAs mentioned would prefer to use the existing builds but wondering if 2nd gen would be sufficient enough.", "author_fullname": "t2_2lkmjxkk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[BUILD HELP] Choosing right CPU and RAM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6ojsg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669615353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning to build a NAS with unraid which will have 2x14, 2x8 TB,  for now. This NAS will be a standalone and act as NAS without Plex or  any other services. I want to have another system working as the main  server. I currently have few optoins at my disposal and was wondering  what would be the best use of current resources. Some are already built  systems and some need to be built.&lt;/p&gt;\n\n&lt;p&gt;Expand-ability: slowly but not sure to what storage.&lt;/p&gt;\n\n&lt;p&gt;Power consumption: Preferred low, as it might be idle 70% at least for the first year.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Option1 (Preferred option):&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.intel.com/content/www/us/en/products/sku/52207/intel-core-i52400-processor-6m-cache-up-to-3-40-ghz/specifications.html\"&gt;i5-2400&lt;/a&gt;, 8GB (not expandable) ram as NAS and&lt;a href=\"https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html\"&gt; i5-8500T&lt;/a&gt;, 16GB as the main server&lt;/p&gt;\n\n&lt;p&gt;Option2:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.intel.com/content/www/us/en/products/sku/203474/intel-core-i310105f-processor-6m-cache-up-to-4-40-ghz/specifications.html\"&gt;i3-10105F&lt;/a&gt;, xGB (need to be built) and &lt;a href=\"https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html\"&gt;i5-8500T&lt;/a&gt;, 16GB - either can be used for either purpose&lt;/p&gt;\n\n&lt;p&gt;Option3:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.intel.com/content/www/us/en/products/sku/88195/intel-core-i76700k-processor-8m-cache-up-to-4-20-ghz/specifications.html\"&gt;i7-6700&lt;/a&gt;, xGB (need to be built) and &lt;a href=\"https://ark.intel.com/content/www/us/en/ark/products/129941/intel-core-i58500t-processor-9m-cache-up-to-3-50-ghz.html\"&gt;i5-8500T&lt;/a&gt;, 16GB - either can be used for either purpose&lt;/p&gt;\n\n&lt;p&gt;Option4:&lt;/p&gt;\n\n&lt;p&gt;Use either of the above systems to do the whole server in one massive system, which has all the HDD&amp;#39;s and the main server.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Comparison of CPU&amp;#39;s&lt;br/&gt;\n&lt;a href=\"https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F\"&gt;https://www.cpubenchmark.net/compare/3231vs2565vs4175/Intel-i5-8500T-vs-Intel-i7-6700K-vs-Intel-i3-10105F&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Also, have a &lt;a href=\"https://www.evga.com/products/specs/gpu.aspx?pn=e466f076-3e8f-4825-b4b0-f1863e893972\"&gt;1060&lt;/a&gt;, 6GB nvidia if that can be leveraged.&lt;/p&gt;\n\n&lt;p&gt;As mentioned would prefer to use the existing builds but wondering if 2nd gen would be sufficient enough.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?auto=webp&amp;s=0402e358a1b5b71d7e7e3840908c20761a156712", "width": 586, "height": 387}, "resolutions": [{"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a95ee4184562ea6e68359542a9ba4fac4f67356", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db96825d26a40d3ff36bd9249696f6589a6c12f4", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/Hlbf52-zMgk3lZiQPiNlVwXxuQqfSuHNMRFDsvyExVQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=09df17da18177e610ef5f2a653e8af6a754ba14d", "width": 320, "height": 211}], "variants": {}, "id": "7wlnlsb1p8kqETD6MDhVPbujRAXC14mk8BJ3S2O8fCg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6ojsg", "is_robot_indexable": true, "report_reasons": null, "author": "batmaniac77", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6ojsg/build_help_choosing_right_cpu_and_ram/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6ojsg/build_help_choosing_right_cpu_and_ram/", "subreddit_subscribers": 656672, "created_utc": 1669615353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have no idea if this is the right place to ask, but I\u2019m absolutely lost and I figured y\u2019all might be able help. I\u2019ve been working on a project that involves some image files from around 1997. The only issue is that (almost) all of the files I was given seem to be corrupted and unusable. They don\u2019t have any file extensions, and adding extensions doesn\u2019t work either. The most important images still seem to take up a decent bit of space, so is there any chance something like this could be salvaged? Figured I might as well shoot out this last cry for help, if you think you can help I\u2019d be glad to send over some of the problem files to look at. Thanks!", "author_fullname": "t2_8rjlxo5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Corrupted image files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6kfc5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669602979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have no idea if this is the right place to ask, but I\u2019m absolutely lost and I figured y\u2019all might be able help. I\u2019ve been working on a project that involves some image files from around 1997. The only issue is that (almost) all of the files I was given seem to be corrupted and unusable. They don\u2019t have any file extensions, and adding extensions doesn\u2019t work either. The most important images still seem to take up a decent bit of space, so is there any chance something like this could be salvaged? Figured I might as well shoot out this last cry for help, if you think you can help I\u2019d be glad to send over some of the problem files to look at. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6kfc5", "is_robot_indexable": true, "report_reasons": null, "author": "Pokeballersprime2", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6kfc5/corrupted_image_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6kfc5/corrupted_image_files/", "subreddit_subscribers": 656672, "created_utc": 1669602979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyway to scrape this page and DL all at once instead of 1 at a time?\n\nhttp://tehne.com/library/tehnicheskaya-estetika-byulleten-zhurnal-moskva-1964-1992", "author_fullname": "t2_akf0w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Howto?: DL archive of Technical Aesthetics, a monthly industrial design magazine published by the Soviet Technical Aesthetics Research Institute from 1964 - 1992.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z79f9f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669671505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyway to scrape this page and DL all at once instead of 1 at a time?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://tehne.com/library/tehnicheskaya-estetika-byulleten-zhurnal-moskva-1964-1992\"&gt;http://tehne.com/library/tehnicheskaya-estetika-byulleten-zhurnal-moskva-1964-1992&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z79f9f", "is_robot_indexable": true, "report_reasons": null, "author": "badatmathdave", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z79f9f/howto_dl_archive_of_technical_aesthetics_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z79f9f/howto_dl_archive_of_technical_aesthetics_a/", "subreddit_subscribers": 656672, "created_utc": 1669671505.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There is not really much to say - just looking for a way to download all photos/videos from a users instagram. The account i wanna download it all from has 1598 posts and i simply can't go through downloading all of them at once.", "author_fullname": "t2_ct8lqncf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do i download all posts/videos from someones instagram account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z7701m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669666284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There is not really much to say - just looking for a way to download all photos/videos from a users instagram. The account i wanna download it all from has 1598 posts and i simply can&amp;#39;t go through downloading all of them at once.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z7701m", "is_robot_indexable": true, "report_reasons": null, "author": "Hellboymeep", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z7701m/how_do_i_download_all_postsvideos_from_someones/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z7701m/how_do_i_download_all_postsvideos_from_someones/", "subreddit_subscribers": 656672, "created_utc": 1669666284.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been trying to find deleted YouTube videos. These videos were uploaded between 2007-2009 and were deleted early 2010. I was wondering if there are any obscure or non-mainstream web archiving services that may be able to help me in my hunt. I have some of the video URL's but I don't know if I can do anything with them. This is my first time posting here so I don't know if this is the right place to post this. Any help would be greatly appreciated.", "author_fullname": "t2_59far0un", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Obscure Web Archives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z7dins", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669680650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been trying to find deleted YouTube videos. These videos were uploaded between 2007-2009 and were deleted early 2010. I was wondering if there are any obscure or non-mainstream web archiving services that may be able to help me in my hunt. I have some of the video URL&amp;#39;s but I don&amp;#39;t know if I can do anything with them. This is my first time posting here so I don&amp;#39;t know if this is the right place to post this. Any help would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z7dins", "is_robot_indexable": true, "report_reasons": null, "author": "Wilsonc22", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z7dins/obscure_web_archives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z7dins/obscure_web_archives/", "subreddit_subscribers": 656672, "created_utc": 1669680650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nRecently got some [4TB Oracle-Branded HGST drives](https://www.ebay.com/itm/285006670691) with the hopes of using them in my storage array, but it appears they must have some kind of custom firmware that prevents them from being used outside of Oracle systems. The drives show up fine on my proxmox host, but cannot be accessed in any way it seems.\n\nHardware is a Dell R620 running PVE 7.4, with an LSI 9200-8E card connected to a KTN-STL3 with the  \n303-115-003D interposers (this setup has been working flawlessly with other HGST drives for months).\n\nOutput of `lsscsi -sig` shows the drives, but without size information:\n\n    sudo lsscsi -sig\n    ...\n    [1:0:5:0]    disk    HGST     H7240AS60SUN4.0T A3A0  /dev/sdh   35000cca07321dee0  /dev/sg7        -\n    [1:0:6:0]    disk    HGST     H7240AS60SUN4.0T A3A0  /dev/sdi   35000cca0734068c8  /dev/sg8        -\n    [1:0:7:0]    disk    HGST     H7240AS60SUN4.0T A3A0  /dev/sdj   35000cca03b529a88  /dev/sg9        -\n    ...\n\nAnd it appears that my controller (a 9200-8e flashed with P20 IT firmware) can't spin them up:\n\n    [  537.342364] sd 1:0:5:0: [sdh] Spinning up disk...\n    [  538.350836] ..................................................................................................not responding...\n    [  637.725088] sd 1:0:5:0: [sdh] Read Capacity(16) failed: Result: hostbyte=DID_OK driverbyte=DRIVER_OK\n    [  637.725099] sd 1:0:5:0: [sdh] Sense Key : Not Ready [current] [descriptor]\n    [  637.725104] sd 1:0:5:0: [sdh] Add. Sense: Logical unit not ready, initializing command required\n    [  637.725888] sd 1:0:5:0: [sdh] Read Capacity(10) failed: Result: hostbyte=DID_OK driverbyte=DRIVER_OK\n    [  637.725894] sd 1:0:5:0: [sdh] Sense Key : Not Ready [current] [descriptor]\n    [  637.725897] sd 1:0:5:0: [sdh] Add. Sense: Logical unit not ready, initializing command required\n    [  637.785055] sd 1:0:6:0: [sdi] Spinning up disk...\n    [  638.812532] ..................................................................................................not responding...\n    [  738.146715] sd 1:0:6:0: [sdi] Read Capacity(16) failed: Result: hostbyte=DID_OK driverbyte=DRIVER_OK\n    [  738.146723] sd 1:0:6:0: [sdi] Sense Key : Not Ready [current] [descriptor]\n    [  738.146726] sd 1:0:6:0: [sdi] Add. Sense: Logical unit not ready, initializing command required\n    [  738.147544] sd 1:0:6:0: [sdi] Read Capacity(10) failed: Result: hostbyte=DID_OK driverbyte=DRIVER_OK\n    [  738.147547] sd 1:0:6:0: [sdi] Sense Key : Not Ready [current] [descriptor]\n    [  738.147549] sd 1:0:6:0: [sdi] Add. Sense: Logical unit not ready, initializing command required\n    [  738.176189] sd 1:0:7:0: [sdj] Spinning up disk...\n    [  739.201768] ..................................................................................................not responding...\n    [  838.528163] sd 1:0:7:0: [sdj] Read Capacity(16) failed: Result: hostbyte=DID_OK driverbyte=DRIVER_OK\n    [  838.528175] sd 1:0:7:0: [sdj] Sense Key : Not Ready [current] [descriptor]\n    [  838.528180] sd 1:0:7:0: [sdj] Add. Sense: Logical unit not ready, initializing command required\n    [  838.528912] sd 1:0:7:0: [sdj] Read Capacity(10) failed: Result: hostbyte=DID_OK driverbyte=DRIVER_OK\n    [  838.528917] sd 1:0:7:0: [sdj] Sense Key : Not Ready [current] [descriptor]\n    [  838.528921] sd 1:0:7:0: [sdj] Add. Sense: Logical unit not ready, initializing command required\n\nAnyone have any experience in this realm? I know you can typically reflash drive firmware, but I was unable to find any information for this particular model online.", "author_fullname": "t2_9zr82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Oracle-branded HGST Drives Unusable in KTN-STL3?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z7chh9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669678265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Recently got some &lt;a href=\"https://www.ebay.com/itm/285006670691\"&gt;4TB Oracle-Branded HGST drives&lt;/a&gt; with the hopes of using them in my storage array, but it appears they must have some kind of custom firmware that prevents them from being used outside of Oracle systems. The drives show up fine on my proxmox host, but cannot be accessed in any way it seems.&lt;/p&gt;\n\n&lt;p&gt;Hardware is a Dell R620 running PVE 7.4, with an LSI 9200-8E card connected to a KTN-STL3 with the&lt;br/&gt;\n303-115-003D interposers (this setup has been working flawlessly with other HGST drives for months).&lt;/p&gt;\n\n&lt;p&gt;Output of &lt;code&gt;lsscsi -sig&lt;/code&gt; shows the drives, but without size information:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo lsscsi -sig\n...\n[1:0:5:0]    disk    HGST     H7240AS60SUN4.0T A3A0  /dev/sdh   35000cca07321dee0  /dev/sg7        -\n[1:0:6:0]    disk    HGST     H7240AS60SUN4.0T A3A0  /dev/sdi   35000cca0734068c8  /dev/sg8        -\n[1:0:7:0]    disk    HGST     H7240AS60SUN4.0T A3A0  /dev/sdj   35000cca03b529a88  /dev/sg9        -\n...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And it appears that my controller (a 9200-8e flashed with P20 IT firmware) can&amp;#39;t spin them up:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[  537.342364] sd 1:0:5:0: [sdh] Spinning up disk...\n[  538.350836] ..................................................................................................not responding...\n[  637.725088] sd 1:0:5:0: [sdh] Read Capacity(16) failed: Result: hostbyte=DID_OK driverbyte=DRIVER_OK\n[  637.725099] sd 1:0:5:0: [sdh] Sense Key : Not Ready [current] [descriptor]\n[  637.725104] sd 1:0:5:0: [sdh] Add. Sense: Logical unit not ready, initializing command required\n[  637.725888] sd 1:0:5:0: [sdh] Read Capacity(10) failed: Result: hostbyte=DID_OK driverbyte=DRIVER_OK\n[  637.725894] sd 1:0:5:0: [sdh] Sense Key : Not Ready [current] [descriptor]\n[  637.725897] sd 1:0:5:0: [sdh] Add. Sense: Logical unit not ready, initializing command required\n[  637.785055] sd 1:0:6:0: [sdi] Spinning up disk...\n[  638.812532] ..................................................................................................not responding...\n[  738.146715] sd 1:0:6:0: [sdi] Read Capacity(16) failed: Result: hostbyte=DID_OK driverbyte=DRIVER_OK\n[  738.146723] sd 1:0:6:0: [sdi] Sense Key : Not Ready [current] [descriptor]\n[  738.146726] sd 1:0:6:0: [sdi] Add. Sense: Logical unit not ready, initializing command required\n[  738.147544] sd 1:0:6:0: [sdi] Read Capacity(10) failed: Result: hostbyte=DID_OK driverbyte=DRIVER_OK\n[  738.147547] sd 1:0:6:0: [sdi] Sense Key : Not Ready [current] [descriptor]\n[  738.147549] sd 1:0:6:0: [sdi] Add. Sense: Logical unit not ready, initializing command required\n[  738.176189] sd 1:0:7:0: [sdj] Spinning up disk...\n[  739.201768] ..................................................................................................not responding...\n[  838.528163] sd 1:0:7:0: [sdj] Read Capacity(16) failed: Result: hostbyte=DID_OK driverbyte=DRIVER_OK\n[  838.528175] sd 1:0:7:0: [sdj] Sense Key : Not Ready [current] [descriptor]\n[  838.528180] sd 1:0:7:0: [sdj] Add. Sense: Logical unit not ready, initializing command required\n[  838.528912] sd 1:0:7:0: [sdj] Read Capacity(10) failed: Result: hostbyte=DID_OK driverbyte=DRIVER_OK\n[  838.528917] sd 1:0:7:0: [sdj] Sense Key : Not Ready [current] [descriptor]\n[  838.528921] sd 1:0:7:0: [sdj] Add. Sense: Logical unit not ready, initializing command required\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Anyone have any experience in this realm? I know you can typically reflash drive firmware, but I was unable to find any information for this particular model online.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CICHxXLGq68ilw8d2PYBwLJPx8Q7XK2fkdP0tQ3V4zk.jpg?auto=webp&amp;s=edeb780257355018112ac22467f829b77cf12790", "width": 275, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/CICHxXLGq68ilw8d2PYBwLJPx8Q7XK2fkdP0tQ3V4zk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1745bc6a57b08475bfa733ceebc3e4592525e71c", "width": 108, "height": 157}, {"url": "https://external-preview.redd.it/CICHxXLGq68ilw8d2PYBwLJPx8Q7XK2fkdP0tQ3V4zk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b53237f53b5bcee0791613e2abe56b48672c5235", "width": 216, "height": 314}], "variants": {}, "id": "__Djrwb6BmNB9rv2VyP-KlLf47KN5pGux85pxP_T4Ek"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z7chh9", "is_robot_indexable": true, "report_reasons": null, "author": "Papkee", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z7chh9/oraclebranded_hgst_drives_unusable_in_ktnstl3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z7chh9/oraclebranded_hgst_drives_unusable_in_ktnstl3/", "subreddit_subscribers": 656672, "created_utc": 1669678265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently running a r710 and MD1200 looking to mirror some s3 buckets. I was looking for the best way to do this. Current running idea is to ssh and run rclone. This is my first setup and any advice would be greatly appreciated!", "author_fullname": "t2_3q045jle", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to transfer from S3 buckets to TrueNAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z7bymw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669677314.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669677071.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently running a r710 and MD1200 looking to mirror some s3 buckets. I was looking for the best way to do this. Current running idea is to ssh and run rclone. This is my first setup and any advice would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z7bymw", "is_robot_indexable": true, "report_reasons": null, "author": "kylewizerd15", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z7bymw/best_way_to_transfer_from_s3_buckets_to_truenas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z7bymw/best_way_to_transfer_from_s3_buckets_to_truenas/", "subreddit_subscribers": 656672, "created_utc": 1669677071.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, due to the massive rocket fire from the russian federation and the lack of light for almost half a day, I am trying to find an additional opportunity to have a backup copy and use it\n One of the options is a smartphone, but it does not support ext4/btrfs, only fat32.\n Is there any way to solve this problem?", "author_fullname": "t2_lo8e4f55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am trying to find an additional option to have a backup and use it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6obui", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669614665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, due to the massive rocket fire from the russian federation and the lack of light for almost half a day, I am trying to find an additional opportunity to have a backup copy and use it\n One of the options is a smartphone, but it does not support ext4/btrfs, only fat32.\n Is there any way to solve this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "russian military ship, go to hell", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6obui", "is_robot_indexable": true, "report_reasons": null, "author": "kovach_ua", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6obui/i_am_trying_to_find_an_additional_option_to_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6obui/i_am_trying_to_find_an_additional_option_to_have/", "subreddit_subscribers": 656672, "created_utc": 1669614665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am using Syncovery and it has an option to compress each file it uploads. I am debating pros cons.\n\nMy initial (first time) upload will be 1.5 TB and about 160k files. I was thinking to do ultra compression since it is one time. That way, when I have to download/restore, it is less to download. But the initial backup will take 2-3 times longer with compression than if I do no compression.\n\nI'm hoping for some good insights from the community.\n\n[View Poll](https://www.reddit.com/poll/z6mw4w)", "author_fullname": "t2_5wpob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Poll: Do folks compress their file level backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6mw4w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669610277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using Syncovery and it has an option to compress each file it uploads. I am debating pros cons.&lt;/p&gt;\n\n&lt;p&gt;My initial (first time) upload will be 1.5 TB and about 160k files. I was thinking to do ultra compression since it is one time. That way, when I have to download/restore, it is less to download. But the initial backup will take 2-3 times longer with compression than if I do no compression.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping for some good insights from the community.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/z6mw4w\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6mw4w", "is_robot_indexable": true, "report_reasons": null, "author": "imthenachoman", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1669869477674, "options": [{"text": "Yes, I compress", "id": "20063642"}, {"text": "No, I do not compress", "id": "20063643"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 141, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6mw4w/poll_do_folks_compress_their_file_level_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/DataHoarder/comments/z6mw4w/poll_do_folks_compress_their_file_level_backups/", "subreddit_subscribers": 656672, "created_utc": 1669610277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2000+ .fbx mo-cap animation library for free.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6kppk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_2b9r7ngd", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free Data", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "gamedev", "selftext": "**TLDR**: Over 2,000 free 3d humanoid character animations that can be used commercially. The only thing you can't do with them, is sell them, although you can redistribute them if they're free. **Download link is below the discalimer**, which you should at least read the top part of if you plan to use these.\n\n[Sample](https://reddit.com/link/z6djuc/video/34um1xs99k2a1/player)\n\n# Disclaimer and boring stuff:\n\n**I did not create the animations/assets used in this**. The data used in this project was obtained from [mocap.cs.cmu.edu](https://mocap.cs.cmu.edu).  The database was created with funding from NSF EIA-0196217. I converted it to .fbx \\*~~and .glTF~~ from .bvh files I got from [cgspeed.com](https://cgspeed.com) with the use of Blender. I also re-targeted it to an example CC0 model with the use of a Blender add-on, [Auto-Rig Pro](https://blendermarket.com/products/auto-rig-pro)(unfortunetly not free, but you don't need it to use these). The model was made by [Quaternius](https://quaternius.itch.io/). The animations are free to use/modify/redistribute, so long as you don't just sell them as animations.\n\n**I have no affiliation with any of these mentioned parties and  just because I'm allowed to use these assets and distribute them,  doesn't mean any of them necessarily endorse this use. Having read the  terms of use though, I feel like this is something they were intended for.** \n\nThis library has been converted several times, by several people. You might have seen it around. If you wanted to, you could convert it and distribute a \"competing\" version(so long as you don't charge anything). I've downloaded both Unreal and Unity versions, but neither of which were able to be opened in Blender(.uasset files and the Unity .fbx files weren't compatible with Blender), which is a problem because they're not quite game ready as is, and editing animations in either engine is not ideal. I've found a bunch of dead links to other versions, like a different one that was supposed to be .fbx, but since it was a dead link, it wasn't very helpful. I actually started converting these with Godot in mind because it's still newer and there aren't all the assets available like there are for Unreal/Unity. I also initially tried to convert to both .fbx and .glTF because .glTF is a little better for Godot. The .glTF file was somehow including small pieces of data left over from previous conversions no matter how much I tried cleaning up Blender between them. Basically, each conversion would be slightly larger than the previous. It was pretty small, but that does add up when you're iterating over 2,000 files. I improved the conversion script a bunch over the process of converting the library, so if for some reason people needed .glTF versions, I can actually efficiently convert to it now. All 3 of those engines take .fbx and even if you're using other 3D software than Blender, .fbx originates from AutoDesk, so it's perfectly compatible with Maya/3DMax(Or should be, those programs are too rich for my blood so I didn't test it).\n\n# Link:\n\n[https://rancidmilk.itch.io/free-character-animations](https://rancidmilk.itch.io/free-character-animations)\n\nI wasn't sure of the best way to distribute these. I chose [itch.io](https://itch.io) because it's intended for games/game assets and let's you upload a gb before you have to start bugging them for more space. I completely turned off any payment methods. If you wanted to thank me in any way, you could help me improve the animations for everyone to use. It would be nice to cobble together a game ready pack for people to use that's just plug and play and free.\n\n# More info:\n\nThere's a lot more info on the itch page. If you have questions, I'm happy to answer them, please look there before asking though.\n\n# Bonus Content:\n\nI've included my conversion script and added a control rig(which I double checked I'm also allowed to distribute) for easy animation editing. \n\n# Summary:\n\nSo, while I have a lot of effort/time into converting this library, I literally only made the included Blender script and assembled everything.", "author_fullname": "t2_rylxysdt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I converted a massive library of mo-cap animations to .fbx which you can use freely with ALMOST no restrictions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/gamedev", "hidden": false, "pwls": 6, "link_flair_css_class": "assets cat-event", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"34um1xs99k2a1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/z6djuc/asset/34um1xs99k2a1/DASHPlaylist.mpd?a=1672279910%2CNmMxOTM4M2Y5MjJmMzNlM2E4ZTliYmE1ODU4OTQzOGUzNzJhMTc1NzlkZjUyYjIzODA0MDUyN2E1YWJlNTk0Yg%3D%3D&amp;v=1&amp;f=sd", "x": 1280, "y": 720, "hlsUrl": "https://v.redd.it/link/z6djuc/asset/34um1xs99k2a1/HLSPlaylist.m3u8?a=1672279910%2CZWE2MzMzODBjNTlmOWUxMjBlNGU1ZWYzZTc2MTI2ZmJiYjhjMWFjOTVmMGU1NjJkNzlhMWJhYzcwM2Q1MzU0OA%3D%3D&amp;v=1&amp;f=sd", "id": "34um1xs99k2a1", "isGif": false}}, "name": "t3_z6djuc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1477, "total_awards_received": 3, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Assets", "can_mod_post": false, "score": 1477, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669585404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.gamedev", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: Over 2,000 free 3d humanoid character animations that can be used commercially. The only thing you can&amp;#39;t do with them, is sell them, although you can redistribute them if they&amp;#39;re free. &lt;strong&gt;Download link is below the discalimer&lt;/strong&gt;, which you should at least read the top part of if you plan to use these.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/z6djuc/video/34um1xs99k2a1/player\"&gt;Sample&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Disclaimer and boring stuff:&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;I did not create the animations/assets used in this&lt;/strong&gt;. The data used in this project was obtained from &lt;a href=\"https://mocap.cs.cmu.edu\"&gt;mocap.cs.cmu.edu&lt;/a&gt;.  The database was created with funding from NSF EIA-0196217. I converted it to .fbx *&lt;del&gt;and .glTF&lt;/del&gt; from .bvh files I got from &lt;a href=\"https://cgspeed.com\"&gt;cgspeed.com&lt;/a&gt; with the use of Blender. I also re-targeted it to an example CC0 model with the use of a Blender add-on, &lt;a href=\"https://blendermarket.com/products/auto-rig-pro\"&gt;Auto-Rig Pro&lt;/a&gt;(unfortunetly not free, but you don&amp;#39;t need it to use these). The model was made by &lt;a href=\"https://quaternius.itch.io/\"&gt;Quaternius&lt;/a&gt;. The animations are free to use/modify/redistribute, so long as you don&amp;#39;t just sell them as animations.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I have no affiliation with any of these mentioned parties and  just because I&amp;#39;m allowed to use these assets and distribute them,  doesn&amp;#39;t mean any of them necessarily endorse this use. Having read the  terms of use though, I feel like this is something they were intended for.&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;This library has been converted several times, by several people. You might have seen it around. If you wanted to, you could convert it and distribute a &amp;quot;competing&amp;quot; version(so long as you don&amp;#39;t charge anything). I&amp;#39;ve downloaded both Unreal and Unity versions, but neither of which were able to be opened in Blender(.uasset files and the Unity .fbx files weren&amp;#39;t compatible with Blender), which is a problem because they&amp;#39;re not quite game ready as is, and editing animations in either engine is not ideal. I&amp;#39;ve found a bunch of dead links to other versions, like a different one that was supposed to be .fbx, but since it was a dead link, it wasn&amp;#39;t very helpful. I actually started converting these with Godot in mind because it&amp;#39;s still newer and there aren&amp;#39;t all the assets available like there are for Unreal/Unity. I also initially tried to convert to both .fbx and .glTF because .glTF is a little better for Godot. The .glTF file was somehow including small pieces of data left over from previous conversions no matter how much I tried cleaning up Blender between them. Basically, each conversion would be slightly larger than the previous. It was pretty small, but that does add up when you&amp;#39;re iterating over 2,000 files. I improved the conversion script a bunch over the process of converting the library, so if for some reason people needed .glTF versions, I can actually efficiently convert to it now. All 3 of those engines take .fbx and even if you&amp;#39;re using other 3D software than Blender, .fbx originates from AutoDesk, so it&amp;#39;s perfectly compatible with Maya/3DMax(Or should be, those programs are too rich for my blood so I didn&amp;#39;t test it).&lt;/p&gt;\n\n&lt;h1&gt;Link:&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://rancidmilk.itch.io/free-character-animations\"&gt;https://rancidmilk.itch.io/free-character-animations&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I wasn&amp;#39;t sure of the best way to distribute these. I chose &lt;a href=\"https://itch.io\"&gt;itch.io&lt;/a&gt; because it&amp;#39;s intended for games/game assets and let&amp;#39;s you upload a gb before you have to start bugging them for more space. I completely turned off any payment methods. If you wanted to thank me in any way, you could help me improve the animations for everyone to use. It would be nice to cobble together a game ready pack for people to use that&amp;#39;s just plug and play and free.&lt;/p&gt;\n\n&lt;h1&gt;More info:&lt;/h1&gt;\n\n&lt;p&gt;There&amp;#39;s a lot more info on the itch page. If you have questions, I&amp;#39;m happy to answer them, please look there before asking though.&lt;/p&gt;\n\n&lt;h1&gt;Bonus Content:&lt;/h1&gt;\n\n&lt;p&gt;I&amp;#39;ve included my conversion script and added a control rig(which I double checked I&amp;#39;m also allowed to distribute) for easy animation editing. &lt;/p&gt;\n\n&lt;h1&gt;Summary:&lt;/h1&gt;\n\n&lt;p&gt;So, while I have a lot of effort/time into converting this library, I literally only made the included Blender script and assembled everything.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 150, "id": "award_f44611f1-b89e-46dc-97fe-892280b13b82", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Thank you stranger. Shows the award.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 2, "static_icon_height": 2048, "name": "Helpful", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png"}, {"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 125, "id": "award_5f123e3d-4f48-42f4-9c11-e98b566d5897", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "When you come across a feel-good thing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Wholesome", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "7272f776-ba6a-11e5-a76d-0eb130e0b479", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qi0a", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z6djuc", "is_robot_indexable": true, "report_reasons": null, "author": "RancidMilkGames", "discussion_type": null, "num_comments": 70, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/gamedev/comments/z6djuc/i_converted_a_massive_library_of_mocap_animations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/gamedev/comments/z6djuc/i_converted_a_massive_library_of_mocap_animations/", "subreddit_subscribers": 927903, "created_utc": 1669585404.0, "num_crossposts": 3, "media": null, "is_video": false}], "created": 1669603832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.gamedev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/gamedev/comments/z6djuc/i_converted_a_massive_library_of_mocap_animations/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "z6kppk", "is_robot_indexable": true, "report_reasons": null, "author": "RustedBlade7", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_z6djuc", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z6kppk/2000_fbx_mocap_animation_library_for_free/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/gamedev/comments/z6djuc/i_converted_a_massive_library_of_mocap_animations/", "subreddit_subscribers": 656672, "created_utc": 1669603832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I intend to download the entirety of a popular fandom wiki (funkipedia) using WikiTeam's scripts. It's fandom so iirc it's based off MediaWiki should it should be compatible with the scripts. My question is what kind of size will the wiki be if I just do a full text dump + all images.\n\nI know this wiki also hosts music but I probably won't download that. Same goes for all revisions and comments to articles. I'm just going to get the pages and maybe the images. If it's just text I'm estimating it will be at least less than 5gb but with images I don't know. I ask because I'm going to be running this on VPS with limited space so I want to be sure I don't have to restart because I ran out of room. \n\nI'm also curious as to how the script displays the completed wiki download. Does it give you some kind of directory structure and make it easily navigatable like if I downloaded it with HTTrack or Wget (with --convert-links)? I see it downloads the site as XML files; do I need to make something to parse all the XML? \n\nAnyone have any experience using these tools that could help? I've looked through the wiki. It doesn't seem like these guys offer a full site download.", "author_fullname": "t2_d3o3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What kind of sizes should I expect downloading a fandom wiki? And has anyone have experience using WikiTeam's tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z6kgua", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669603106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I intend to download the entirety of a popular fandom wiki (funkipedia) using WikiTeam&amp;#39;s scripts. It&amp;#39;s fandom so iirc it&amp;#39;s based off MediaWiki should it should be compatible with the scripts. My question is what kind of size will the wiki be if I just do a full text dump + all images.&lt;/p&gt;\n\n&lt;p&gt;I know this wiki also hosts music but I probably won&amp;#39;t download that. Same goes for all revisions and comments to articles. I&amp;#39;m just going to get the pages and maybe the images. If it&amp;#39;s just text I&amp;#39;m estimating it will be at least less than 5gb but with images I don&amp;#39;t know. I ask because I&amp;#39;m going to be running this on VPS with limited space so I want to be sure I don&amp;#39;t have to restart because I ran out of room. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also curious as to how the script displays the completed wiki download. Does it give you some kind of directory structure and make it easily navigatable like if I downloaded it with HTTrack or Wget (with --convert-links)? I see it downloads the site as XML files; do I need to make something to parse all the XML? &lt;/p&gt;\n\n&lt;p&gt;Anyone have any experience using these tools that could help? I&amp;#39;ve looked through the wiki. It doesn&amp;#39;t seem like these guys offer a full site download.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "LP-Archive", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z6kgua", "is_robot_indexable": true, "report_reasons": null, "author": "StormGaza", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/z6kgua/what_kind_of_sizes_should_i_expect_downloading_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z6kgua/what_kind_of_sizes_should_i_expect_downloading_a/", "subreddit_subscribers": 656672, "created_utc": 1669603106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not entirely sure if this fits here, but I am making a collection of media (movies, games, books, etc) to put on a flash drive as a gift for someone else. For games I have been using Playnite as manager (especially helpful for Dosbox games) and for eBooks I am using Calibre. Both are self contained and act as a nice interface for the recipient so they do not have to worry about downloading anything to their computer or changing shortcuts to whatever drive letter Windows assigns it. \n\nI am trying to find something similar to these for movies/series but am having a hard time. This is for presentation and pizazz as the recipient can easily just open the file in the player of their choice. But I want a slick, easy interface for them to use. \n\nI have tried VLC and PotPlayer portable from PortableApps but neither really has the library presentation I am looking for. Something to scroll through with cover images. Any help would be much appreciated!", "author_fullname": "t2_a34mj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Portable Video Manager/Player for USB Movie Library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z7fe1v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669685674.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669685486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not entirely sure if this fits here, but I am making a collection of media (movies, games, books, etc) to put on a flash drive as a gift for someone else. For games I have been using Playnite as manager (especially helpful for Dosbox games) and for eBooks I am using Calibre. Both are self contained and act as a nice interface for the recipient so they do not have to worry about downloading anything to their computer or changing shortcuts to whatever drive letter Windows assigns it. &lt;/p&gt;\n\n&lt;p&gt;I am trying to find something similar to these for movies/series but am having a hard time. This is for presentation and pizazz as the recipient can easily just open the file in the player of their choice. But I want a slick, easy interface for them to use. &lt;/p&gt;\n\n&lt;p&gt;I have tried VLC and PotPlayer portable from PortableApps but neither really has the library presentation I am looking for. Something to scroll through with cover images. Any help would be much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z7fe1v", "is_robot_indexable": true, "report_reasons": null, "author": "xxDMMxx", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z7fe1v/portable_video_managerplayer_for_usb_movie_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z7fe1v/portable_video_managerplayer_for_usb_movie_library/", "subreddit_subscribers": 656672, "created_utc": 1669685486.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What is considered the go-to M-Disc device that is supported by current Macs? I am looking at burning images and videos to disk for long-term storage.", "author_fullname": "t2_ss08mswb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "File archival: images and video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z7f7kt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669685000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is considered the go-to M-Disc device that is supported by current Macs? I am looking at burning images and videos to disk for long-term storage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z7f7kt", "is_robot_indexable": true, "report_reasons": null, "author": "clorth0", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z7f7kt/file_archival_images_and_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z7f7kt/file_archival_images_and_video/", "subreddit_subscribers": 656672, "created_utc": 1669685000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been using anti twin today, which is an older program but works exactly like I need it to.  The one thing that is a bit challenging is that if I resolve some of the duplicates without all of them.  It \"concludes\" my session and requires me to essentially start over.  I was wondering if theres a similar program that allows you to \"recheck found duplicates\" or \"update\" without starting over...  Thanks", "author_fullname": "t2_3vqdp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there something like anti-twin that works pretty much exactly the same but allows you to update the existing \"found duplicates\" without starting a fresh scan?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z7dwwz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669681626.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using anti twin today, which is an older program but works exactly like I need it to.  The one thing that is a bit challenging is that if I resolve some of the duplicates without all of them.  It &amp;quot;concludes&amp;quot; my session and requires me to essentially start over.  I was wondering if theres a similar program that allows you to &amp;quot;recheck found duplicates&amp;quot; or &amp;quot;update&amp;quot; without starting over...  Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "z7dwwz", "is_robot_indexable": true, "report_reasons": null, "author": "deten", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/z7dwwz/is_there_something_like_antitwin_that_works/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/z7dwwz/is_there_something_like_antitwin_that_works/", "subreddit_subscribers": 656672, "created_utc": 1669681626.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}