{"kind": "Listing", "data": {"after": "t3_yyecpx", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&gt;Twitter has emailed staffers: \"Hi, Effective immediately, we are temporarily closing our office buildings and all badge access will be suspended. Offices will reopen on Monday, November 21st. .. We look forward to working with you on Twitter\u2019s exciting future.\"\n\n&amp;#x200B;\n\n&gt;Story to be updated soon with more: Am hearing that several \u201ccritical\u201d infra engineering teams at Twitter have completely resigned. \u201cYou cannot run Twitter without this team,\u201d one current engineer tells me of one such group. Also, Twitter has shut off badge access to its offices.\n\n&amp;#x200B;\n\n&gt;What I\u2019m hearing from Twitter employees; It looks like roughly 75% of the remaining 3,700ish Twitter employees have not opted to stay after the \u201chardcore\u201d email.  \n&gt;  \n&gt;Even though the deadline has passed, everyone still has access to their systems.\n\n&amp;#x200B;\n\n&gt;\u201cI know of six critical systems (like \u2018serving tweets\u2019 levels of critical) which no longer have any engineers,\" the former employee said. \"There is no longer even a skeleton crew manning the system. It will continue to coast until it runs into something, and then it will stop.\u201d  \n&gt;  \n&gt;Resignations and departures were already taking a toll on Twitter\u2019s service, employees said. \u201cBreakages are already happening slowly and accumulating,\u201d one said. \u201cIf you want to export your tweets, do it now.\u201d\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Link 1](https://twitter.com/oliverdarcy/status/1593394621627138048)\n\n[Link 2](https://twitter.com/alexeheath/status/1593399683086327808)\n\n[Link 3](https://twitter.com/kyliebytes/status/1593391167718113280)\n\n[Link 4](https://www.washingtonpost.com/technology/2022/11/17/twitter-musk-easing-rto-order/)\n\n&amp;#x200B;\n\nEdit:\n\n[twitter-scraper (github no api-key needed)](https://github.com/n0madic/twitter-scraper)\n\n[twitter-media-downloader (github no api-key needed)](https://github.com/mmpx12/twitter-media-downloader)\n\n&amp;#x200B;\n\nEdit2:\n\n[https://github.com/markowanga/stweet](https://github.com/markowanga/stweet)\n\n\n\nEdit3: \n\n[gallery-dl guide by /u/Scripter17](https://www.reddit.com/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/)", "author_fullname": "t2_ioi0j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup twitter now! Multiple critical infra teams have resigned", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy7tig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 420, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 420, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668751389.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668735892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Twitter has emailed staffers: &amp;quot;Hi, Effective immediately, we are temporarily closing our office buildings and all badge access will be suspended. Offices will reopen on Monday, November 21st. .. We look forward to working with you on Twitter\u2019s exciting future.&amp;quot;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Story to be updated soon with more: Am hearing that several \u201ccritical\u201d infra engineering teams at Twitter have completely resigned. \u201cYou cannot run Twitter without this team,\u201d one current engineer tells me of one such group. Also, Twitter has shut off badge access to its offices.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;What I\u2019m hearing from Twitter employees; It looks like roughly 75% of the remaining 3,700ish Twitter employees have not opted to stay after the \u201chardcore\u201d email.  &lt;/p&gt;\n\n&lt;p&gt;Even though the deadline has passed, everyone still has access to their systems.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;\u201cI know of six critical systems (like \u2018serving tweets\u2019 levels of critical) which no longer have any engineers,&amp;quot; the former employee said. &amp;quot;There is no longer even a skeleton crew manning the system. It will continue to coast until it runs into something, and then it will stop.\u201d  &lt;/p&gt;\n\n&lt;p&gt;Resignations and departures were already taking a toll on Twitter\u2019s service, employees said. \u201cBreakages are already happening slowly and accumulating,\u201d one said. \u201cIf you want to export your tweets, do it now.\u201d&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/oliverdarcy/status/1593394621627138048\"&gt;Link 1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/alexeheath/status/1593399683086327808\"&gt;Link 2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/kyliebytes/status/1593391167718113280\"&gt;Link 3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.washingtonpost.com/technology/2022/11/17/twitter-musk-easing-rto-order/\"&gt;Link 4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/n0madic/twitter-scraper\"&gt;twitter-scraper (github no api-key needed)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mmpx12/twitter-media-downloader\"&gt;twitter-media-downloader (github no api-key needed)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit2:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/markowanga/stweet\"&gt;https://github.com/markowanga/stweet&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Edit3: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/\"&gt;gallery-dl guide by /u/Scripter17&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yy7tig", "is_robot_indexable": true, "report_reasons": null, "author": "fourDnet", "discussion_type": null, "num_comments": 184, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy7tig/backup_twitter_now_multiple_critical_infra_teams/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/yy7tig/backup_twitter_now_multiple_critical_infra_teams/", "subreddit_subscribers": 654584, "created_utc": 1668735892.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5s2jw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RIP unlimited Google workspace for education so sad", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_yxuyof", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 303, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 303, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZJzgS5JeIqSiM-S4S_ZMfo9x_ok42HTD7BQN1dBqruA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668703414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.imgur.com/WIq41oo.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?auto=webp&amp;s=0735b911b2f5092d8e3a6f537ab26c58dfdc480b", "width": 1440, "height": 3120}, "resolutions": [{"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f556e8d409447f35d120f43223d24293101359cd", "width": 108, "height": 216}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7b195ecb97e447e8fb122db7364de94d876fd94a", "width": 216, "height": 432}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b630ff166942e4ba407be4f0d2a3262dbc86c259", "width": 320, "height": 640}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5d9d5f1acf434fafac965e8287ab8c8f05f14959", "width": 640, "height": 1280}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ea00e99830f8772720df410dbc757afae117f6e7", "width": 960, "height": 1920}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f3d8a3b27fc330b38d13e49555f63d6a1318a08", "width": 1080, "height": 2160}], "variants": {}, "id": "tXJO2nqpeXoWEfbeINadLiA5MRKOwAiwWes_Q_YJ3iM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxuyof", "is_robot_indexable": true, "report_reasons": null, "author": "UACEENGR", "discussion_type": null, "num_comments": 102, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxuyof/rip_unlimited_google_workspace_for_education_so/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.imgur.com/WIq41oo.jpg", "subreddit_subscribers": 654584, "created_utc": 1668703414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Rewritten for clarity because speedrunning a post like this tends to leave questions\n\nHow to get started:\n\n1. Install [Python](https://www.python.org/). There is a standalone .exe but this just makes it easier to upgrade and all that\n\n2. Run `pip install gallery-dl` in command prompt (windows) or Bash (Linux)\n\n3. From there running `gallery-dl &lt;url&gt;` in the same command line should download the url's contents\n\n## config.json\n\nThe config.json is located at `%APPDATA%\\gallery-dl\\config.json` (windows) and `/etc/gallery-dl.conf` (Linux)\n\nIf the folder/file doesn't exist, just making it yourself should work\n\nThe basic config I recommend is this. If this is your first time with gallery-dl it's safe to just replace the entire file with this. If it's not your first time you should know how to transplant this into your existing config\n\n    {\n        \"extractor\":{\n            \"cookies\": [\"&lt;your browser (firefox, chromium, etc)&gt;\"],\n            \"twitter\":{\n                \"users\": \"https://twitter.com/{legacy[screen_name]}\",\n                \"text-tweets\":true,\n                \"retweets\":true,\n                \"quoted\":true,\n                \"logout\":true,\n                \"replies\":true,\n                \"postprocessors\":[\n                    {\"name\": \"metadata\", \"event\": \"post\", \"filename\": \"{tweet_id}_main.json\"}\n                ]\n            }\n        }\n    }\n\nThe documentation for the config.json is [here](https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst) and the specific part about getting cookies from your browser is [here](https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst#extractorcookies)\n\nCurrently supplying your login as a username/password combo seems to be broken. Idk if this is an issue with twitter or gallery-dl but using browser cookies is just easier in the long run\n\n## URLs:\n\n[The twitter API limits getting a user's page to the latest ~3200 tweets](https://github.com/mikf/gallery-dl/issues/2226). To get the as much as possible I recommend getting the main tab, the media tab, *and* the URL when you search for `from:&lt;user&gt;`\n\nTo make downloading the media tab not immediately exit when it sees a duplicate image, you'll want to add `-o skip=true` to the command you put in the command line. This can also be specified in the config. I have mine set to 20 when I'm just updating an existing download. If it sees 20 known images in a row then it moves on to the next one.\n\nThe 3 URLs I recommend downloading are:\n\n- `https://www.twitter.com/&lt;user&gt;`\n- `https://www.twitter.com/&lt;user&gt;/media`\n- `https://twitter.com/search?q=from:&lt;user&gt;`\n\nTo get someone's likes the URL is `https://www.twitter.com/&lt;user&gt;/likes`\n\nTo get your bookmarks the URL is `https://twitter.com/i/bookmarks`\n\n**Note**: Because twitter honestly just sucks and has for quite a while, you should run each download a few times (again with `-o skip=true`) to make sure you get everything\n\n## Commands:\n\nAnd the commands you're running should look like `gallery-dl &lt;url&gt; --write-metadata -o skip=true`\n\n`--write-metadata` saves `.json` files with metadata about each image. the `\"postprocessors\"` part of the config already writes the metadata for the tweet itself but the per-image metadata has some extra stuff\n\nIf you run `gallery-dl -g https://twitter.com/&lt;your handle&gt;/following` you can get a list of everyone you follow.\n\n### Windows:\n\nIf you have a text editor that supports regex replacement (CTRL+H in Sublime Text. Enable the button that looks like a .*), you can paste the list gallery-dl gave you and replace `(.+\\/)([^/\\r\\n]+)` with `gallery-dl $1$2               --write-metadata -o skip=true\\ngallery-dl $1$2/media         --write-metadata -o skip=true\\ngallery-dl $1search?q=from:$2 --write-metadata -o skip=true -o \"directory=[\"\"twitter\"\",\"\"{$2}\"\"]\"`\n\nYou should see something along the lines of\n\n    gallery-dl https://twitter.com/test1               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test1/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test1 --write-metadata -o skip=true -o \"directory=[\"\"twitter\"\",\"\"{test1}\"\"]\"\n    gallery-dl https://twitter.com/test2               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test2/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test2 --write-metadata -o skip=true -o \"directory=[\"\"twitter\"\",\"\"{test2}\"\"]\"\n    gallery-dl https://twitter.com/test3               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test3/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test3 --write-metadata -o skip=true -o \"directory=[\"\"twitter\"\",\"\"{test3}\"\"]\"\n\nThen put an `@echo off` at the top of the file and save it as a `.bat`\n\n### Linux:\n\nIf you have a text editor that supports regex replacement, you can paste the list gallery-dl gave you and replace `(.+\\/)([^/\\r\\n]+)` with `gallery-dl $1$2               --write-metadata -o skip=true\\ngallery-dl $1$2/media         --write-metadata -o skip=true\\ngallery-dl $1search?q=from:$2 --write-metadata -o skip=true -o \"directory=[\\\"twitter\\\",\\\"{$2}\\\"]\"`\n\nYou should see something along the lines of\n\n    gallery-dl https://twitter.com/test1               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test1/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test1 --write-metadata -o skip=true -o \"directory=[\\\"twitter\\\",\\\"{test1}\\\"]\"\n    gallery-dl https://twitter.com/test2               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test2/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test2 --write-metadata -o skip=true -o \"directory=[\\\"twitter\\\",\\\"{test2}\\\"]\"\n    gallery-dl https://twitter.com/test3               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test3/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test3 --write-metadata -o skip=true -o \"directory=[\\\"twitter\\\",\\\"{test3}\\\"]\"\n\nThen save it as a `.sh` file\n\nIf, on either OS, the resulting commands has a bunch of `$1` and `$2` in it, replace the `$`s in the replacement string with `\\`s and do it again.\n\nAfter that, running the file should (assuming I got all the steps right) download everyone you follow\n\n.\n\nNow, if you excuse me, it's almost 6am and I need to sleep. I really hope I haven't made any catastrophic errors", "author_fullname": "t2_yj3jz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For everyone using gallery-dl to backup twitter: Make sure you do it right", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy8o9w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668768882.0, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668738416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Rewritten for clarity because speedrunning a post like this tends to leave questions&lt;/p&gt;\n\n&lt;p&gt;How to get started:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Install &lt;a href=\"https://www.python.org/\"&gt;Python&lt;/a&gt;. There is a standalone .exe but this just makes it easier to upgrade and all that&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run &lt;code&gt;pip install gallery-dl&lt;/code&gt; in command prompt (windows) or Bash (Linux)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;From there running &lt;code&gt;gallery-dl &amp;lt;url&amp;gt;&lt;/code&gt; in the same command line should download the url&amp;#39;s contents&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h2&gt;config.json&lt;/h2&gt;\n\n&lt;p&gt;The config.json is located at &lt;code&gt;%APPDATA%\\gallery-dl\\config.json&lt;/code&gt; (windows) and &lt;code&gt;/etc/gallery-dl.conf&lt;/code&gt; (Linux)&lt;/p&gt;\n\n&lt;p&gt;If the folder/file doesn&amp;#39;t exist, just making it yourself should work&lt;/p&gt;\n\n&lt;p&gt;The basic config I recommend is this. If this is your first time with gallery-dl it&amp;#39;s safe to just replace the entire file with this. If it&amp;#39;s not your first time you should know how to transplant this into your existing config&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n    &amp;quot;extractor&amp;quot;:{\n        &amp;quot;cookies&amp;quot;: [&amp;quot;&amp;lt;your browser (firefox, chromium, etc)&amp;gt;&amp;quot;],\n        &amp;quot;twitter&amp;quot;:{\n            &amp;quot;users&amp;quot;: &amp;quot;https://twitter.com/{legacy[screen_name]}&amp;quot;,\n            &amp;quot;text-tweets&amp;quot;:true,\n            &amp;quot;retweets&amp;quot;:true,\n            &amp;quot;quoted&amp;quot;:true,\n            &amp;quot;logout&amp;quot;:true,\n            &amp;quot;replies&amp;quot;:true,\n            &amp;quot;postprocessors&amp;quot;:[\n                {&amp;quot;name&amp;quot;: &amp;quot;metadata&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;post&amp;quot;, &amp;quot;filename&amp;quot;: &amp;quot;{tweet_id}_main.json&amp;quot;}\n            ]\n        }\n    }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The documentation for the config.json is &lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst\"&gt;here&lt;/a&gt; and the specific part about getting cookies from your browser is &lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst#extractorcookies\"&gt;here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Currently supplying your login as a username/password combo seems to be broken. Idk if this is an issue with twitter or gallery-dl but using browser cookies is just easier in the long run&lt;/p&gt;\n\n&lt;h2&gt;URLs:&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mikf/gallery-dl/issues/2226\"&gt;The twitter API limits getting a user&amp;#39;s page to the latest ~3200 tweets&lt;/a&gt;. To get the as much as possible I recommend getting the main tab, the media tab, &lt;em&gt;and&lt;/em&gt; the URL when you search for &lt;code&gt;from:&amp;lt;user&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;To make downloading the media tab not immediately exit when it sees a duplicate image, you&amp;#39;ll want to add &lt;code&gt;-o skip=true&lt;/code&gt; to the command you put in the command line. This can also be specified in the config. I have mine set to 20 when I&amp;#39;m just updating an existing download. If it sees 20 known images in a row then it moves on to the next one.&lt;/p&gt;\n\n&lt;p&gt;The 3 URLs I recommend downloading are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;https://www.twitter.com/&amp;lt;user&amp;gt;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;https://www.twitter.com/&amp;lt;user&amp;gt;/media&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;https://twitter.com/search?q=from:&amp;lt;user&amp;gt;&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To get someone&amp;#39;s likes the URL is &lt;code&gt;https://www.twitter.com/&amp;lt;user&amp;gt;/likes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;To get your bookmarks the URL is &lt;code&gt;https://twitter.com/i/bookmarks&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Because twitter honestly just sucks and has for quite a while, you should run each download a few times (again with &lt;code&gt;-o skip=true&lt;/code&gt;) to make sure you get everything&lt;/p&gt;\n\n&lt;h2&gt;Commands:&lt;/h2&gt;\n\n&lt;p&gt;And the commands you&amp;#39;re running should look like &lt;code&gt;gallery-dl &amp;lt;url&amp;gt; --write-metadata -o skip=true&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;--write-metadata&lt;/code&gt; saves &lt;code&gt;.json&lt;/code&gt; files with metadata about each image. the &lt;code&gt;&amp;quot;postprocessors&amp;quot;&lt;/code&gt; part of the config already writes the metadata for the tweet itself but the per-image metadata has some extra stuff&lt;/p&gt;\n\n&lt;p&gt;If you run &lt;code&gt;gallery-dl -g https://twitter.com/&amp;lt;your handle&amp;gt;/following&lt;/code&gt; you can get a list of everyone you follow.&lt;/p&gt;\n\n&lt;h3&gt;Windows:&lt;/h3&gt;\n\n&lt;p&gt;If you have a text editor that supports regex replacement (CTRL+H in Sublime Text. Enable the button that looks like a .*), you can paste the list gallery-dl gave you and replace &lt;code&gt;(.+\\/)([^/\\r\\n]+)&lt;/code&gt; with &lt;code&gt;gallery-dl $1$2               --write-metadata -o skip=true\\ngallery-dl $1$2/media         --write-metadata -o skip=true\\ngallery-dl $1search?q=from:$2 --write-metadata -o skip=true -o &amp;quot;directory=[&amp;quot;&amp;quot;twitter&amp;quot;&amp;quot;,&amp;quot;&amp;quot;{$2}&amp;quot;&amp;quot;]&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;You should see something along the lines of&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gallery-dl https://twitter.com/test1               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test1/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test1 --write-metadata -o skip=true -o &amp;quot;directory=[&amp;quot;&amp;quot;twitter&amp;quot;&amp;quot;,&amp;quot;&amp;quot;{test1}&amp;quot;&amp;quot;]&amp;quot;\ngallery-dl https://twitter.com/test2               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test2/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test2 --write-metadata -o skip=true -o &amp;quot;directory=[&amp;quot;&amp;quot;twitter&amp;quot;&amp;quot;,&amp;quot;&amp;quot;{test2}&amp;quot;&amp;quot;]&amp;quot;\ngallery-dl https://twitter.com/test3               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test3/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test3 --write-metadata -o skip=true -o &amp;quot;directory=[&amp;quot;&amp;quot;twitter&amp;quot;&amp;quot;,&amp;quot;&amp;quot;{test3}&amp;quot;&amp;quot;]&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then put an &lt;code&gt;@echo off&lt;/code&gt; at the top of the file and save it as a &lt;code&gt;.bat&lt;/code&gt;&lt;/p&gt;\n\n&lt;h3&gt;Linux:&lt;/h3&gt;\n\n&lt;p&gt;If you have a text editor that supports regex replacement, you can paste the list gallery-dl gave you and replace &lt;code&gt;(.+\\/)([^/\\r\\n]+)&lt;/code&gt; with &lt;code&gt;gallery-dl $1$2               --write-metadata -o skip=true\\ngallery-dl $1$2/media         --write-metadata -o skip=true\\ngallery-dl $1search?q=from:$2 --write-metadata -o skip=true -o &amp;quot;directory=[\\&amp;quot;twitter\\&amp;quot;,\\&amp;quot;{$2}\\&amp;quot;]&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;You should see something along the lines of&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gallery-dl https://twitter.com/test1               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test1/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test1 --write-metadata -o skip=true -o &amp;quot;directory=[\\&amp;quot;twitter\\&amp;quot;,\\&amp;quot;{test1}\\&amp;quot;]&amp;quot;\ngallery-dl https://twitter.com/test2               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test2/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test2 --write-metadata -o skip=true -o &amp;quot;directory=[\\&amp;quot;twitter\\&amp;quot;,\\&amp;quot;{test2}\\&amp;quot;]&amp;quot;\ngallery-dl https://twitter.com/test3               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test3/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test3 --write-metadata -o skip=true -o &amp;quot;directory=[\\&amp;quot;twitter\\&amp;quot;,\\&amp;quot;{test3}\\&amp;quot;]&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then save it as a &lt;code&gt;.sh&lt;/code&gt; file&lt;/p&gt;\n\n&lt;p&gt;If, on either OS, the resulting commands has a bunch of &lt;code&gt;$1&lt;/code&gt; and &lt;code&gt;$2&lt;/code&gt; in it, replace the &lt;code&gt;$&lt;/code&gt;s in the replacement string with &lt;code&gt;\\&lt;/code&gt;s and do it again.&lt;/p&gt;\n\n&lt;p&gt;After that, running the file should (assuming I got all the steps right) download everyone you follow&lt;/p&gt;\n\n&lt;p&gt;.&lt;/p&gt;\n\n&lt;p&gt;Now, if you excuse me, it&amp;#39;s almost 6am and I need to sleep. I really hope I haven&amp;#39;t made any catastrophic errors&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XACIlvLRgD0CrsIsxO7yWQMICHPINiGesu3WjxQxeXs.jpg?auto=webp&amp;s=c7f0d77306cb94adced1c514958fdf68f575791c", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/XACIlvLRgD0CrsIsxO7yWQMICHPINiGesu3WjxQxeXs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0a156ee41a904137a12d7727f03ba51aa2a31c7", "width": 108, "height": 108}], "variants": {}, "id": "_QTobzuJkr1Zm6t-xAciOuvRRUG3sFX1cl1tVTmHCMU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "The sexiest data storage medium", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy8o9w", "is_robot_indexable": true, "report_reasons": null, "author": "Scripter17", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/", "subreddit_subscribers": 654584, "created_utc": 1668738416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I am probably way out of my depth here but r/Twitter redirected me here when I asked this question. In case Twitter actually does go away I am trying to download the entire Twitter feed of my best friend who passed away 5 years ago. I\u2019m currently just screenshotting favorites in a panic and googling returns old workarounds that don\u2019t work anymore. Any advice at all is appreciated, thank you.", "author_fullname": "t2_eo84n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download An Entire Twitter Feed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy8dii", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668737540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am probably way out of my depth here but &lt;a href=\"/r/Twitter\"&gt;r/Twitter&lt;/a&gt; redirected me here when I asked this question. In case Twitter actually does go away I am trying to download the entire Twitter feed of my best friend who passed away 5 years ago. I\u2019m currently just screenshotting favorites in a panic and googling returns old workarounds that don\u2019t work anymore. Any advice at all is appreciated, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy8dii", "is_robot_indexable": true, "report_reasons": null, "author": "plaidtuxedo", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy8dii/download_an_entire_twitter_feed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy8dii/download_an_entire_twitter_feed/", "subreddit_subscribers": 654584, "created_utc": 1668737540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a thread for current blackfriday storage discounts like last year?", "author_fullname": "t2_4y7ra1ap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Black Friday thread", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyc1vh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668749117.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a thread for current blackfriday storage discounts like last year?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyc1vh", "is_robot_indexable": true, "report_reasons": null, "author": "HolUp-", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyc1vh/black_friday_thread/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yyc1vh/black_friday_thread/", "subreddit_subscribers": 654584, "created_utc": 1668749117.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there usually good deals for hard drives on these days? I'm finally building my first NAS and want to find good deals obviously.   Is it worth waiting for Black Friday / Cyber Monday deals or are they going to be just crap drives?\n\nThanks!", "author_fullname": "t2_140qwd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Black Friday/ Cyber Monday deals?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxw0kh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668705932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there usually good deals for hard drives on these days? I&amp;#39;m finally building my first NAS and want to find good deals obviously.   Is it worth waiting for Black Friday / Cyber Monday deals or are they going to be just crap drives?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxw0kh", "is_robot_indexable": true, "report_reasons": null, "author": "Mastasmoker", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxw0kh/black_friday_cyber_monday_deals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxw0kh/black_friday_cyber_monday_deals/", "subreddit_subscribers": 654584, "created_utc": 1668705932.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Lots of options to scrape your tweets, but I have a lot of bookmarks I also wanna back up.\n\nI found a way to download all the tweet URLs into a csv ([Dewey](https://getdewey.co/)) and a way to download videos and gifs independently ([youtube-dl](https://youtube-dl-helper.github.io/)) but I can't find a way to do it all at the same time.\n\nEverything into a XML file would be ideal. Also if possible it would be nice for it to keep threads (which should be possible if not annoying)\n\nI found [this](https://gist.github.com/CJKinni/3063070) but it's very old code and has basically no chance to work", "author_fullname": "t2_ykkhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get all my Twitter bookmarks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy978v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668739972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lots of options to scrape your tweets, but I have a lot of bookmarks I also wanna back up.&lt;/p&gt;\n\n&lt;p&gt;I found a way to download all the tweet URLs into a csv (&lt;a href=\"https://getdewey.co/\"&gt;Dewey&lt;/a&gt;) and a way to download videos and gifs independently (&lt;a href=\"https://youtube-dl-helper.github.io/\"&gt;youtube-dl&lt;/a&gt;) but I can&amp;#39;t find a way to do it all at the same time.&lt;/p&gt;\n\n&lt;p&gt;Everything into a XML file would be ideal. Also if possible it would be nice for it to keep threads (which should be possible if not annoying)&lt;/p&gt;\n\n&lt;p&gt;I found &lt;a href=\"https://gist.github.com/CJKinni/3063070\"&gt;this&lt;/a&gt; but it&amp;#39;s very old code and has basically no chance to work&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?auto=webp&amp;s=e2d60ad36c7afe27744a4b4f63f20d3181954d4b", "width": 1015, "height": 494}, "resolutions": [{"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4de019e984fdeee8a89ce7e525e828568019fd28", "width": 108, "height": 52}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06f53d6a2f1e4248b34fb2643cd2df9e85f63585", "width": 216, "height": 105}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=11aa0debd38927b0c86148157dd6fb2be3425330", "width": 320, "height": 155}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a008abc197d7c09c8a6d7c679a7a14dfeb6aff84", "width": 640, "height": 311}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8407e59962bf38175018cc891c88bea773d2b655", "width": 960, "height": 467}], "variants": {}, "id": "D83W6ubnaf9JIZcLbn0OsGoknMj1En17FJrGkylXYzs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy978v", "is_robot_indexable": true, "report_reasons": null, "author": "PowderPhysics", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy978v/how_to_get_all_my_twitter_bookmarks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy978v/how_to_get_all_my_twitter_bookmarks/", "subreddit_subscribers": 654584, "created_utc": 1668739972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "About 5 months ago, arguably the largest database of doujinshi and manga (Doujinshi.org) seemingly went down over night. The site had thousands of contributors collecting information on even the most obscure and most niche indie works over nearly a DECADE. Most of which was untranslated, so you can imagine the treasure trove of information. A shit ton of stuff that not even hardcore weebs know exists.\n\nThe owner made [this](https://twitter.com/tenetan/status/1550839774969208833) tweet the day the site went offline. This is the only update they have given, ever. No tweets since, absolutely nothing. There isn't a discord or a subreddit or anything either where one would be able to contact them. They do however still seem to [like drawings of anime girls on twitter](https://i.imgur.com/b3jFx1K.png), unbothered by the countless of people asking for updates.\n\nFor all we know they could be in financial trouble or be sick or whatever. We simply do not know. But what is pissing me off to no end is the fact that they are radio silent. If you host a database that is almost entirely dependant on the community, you have the obligation and responsibility to inform them of what's become of their effort if you ask me. They \\*owe\\* communcation. Because at that point, the site isn't even the owners anymore.\n\nNow, I have a question to my fellow data hoarders, and it's the reason why I'm posting this in the first place. What the hell are we supposed to? There's no archives or backups anywhere; the site wasn't all too popular, evident by the fact that no one is even talking about this despite it being the biggest ressource for Japanese fan works.\n\nTo be honest, I really doubt it's going to come back. And I fear this might be another Library of Alexandria scenario...", "author_fullname": "t2_d5g3mn4d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Largest Doujinshi and Manga Lexicon Went Down 5 Months Ago and No One Cares", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxwkd5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668707282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;About 5 months ago, arguably the largest database of doujinshi and manga (Doujinshi.org) seemingly went down over night. The site had thousands of contributors collecting information on even the most obscure and most niche indie works over nearly a DECADE. Most of which was untranslated, so you can imagine the treasure trove of information. A shit ton of stuff that not even hardcore weebs know exists.&lt;/p&gt;\n\n&lt;p&gt;The owner made &lt;a href=\"https://twitter.com/tenetan/status/1550839774969208833\"&gt;this&lt;/a&gt; tweet the day the site went offline. This is the only update they have given, ever. No tweets since, absolutely nothing. There isn&amp;#39;t a discord or a subreddit or anything either where one would be able to contact them. They do however still seem to &lt;a href=\"https://i.imgur.com/b3jFx1K.png\"&gt;like drawings of anime girls on twitter&lt;/a&gt;, unbothered by the countless of people asking for updates.&lt;/p&gt;\n\n&lt;p&gt;For all we know they could be in financial trouble or be sick or whatever. We simply do not know. But what is pissing me off to no end is the fact that they are radio silent. If you host a database that is almost entirely dependant on the community, you have the obligation and responsibility to inform them of what&amp;#39;s become of their effort if you ask me. They *owe* communcation. Because at that point, the site isn&amp;#39;t even the owners anymore.&lt;/p&gt;\n\n&lt;p&gt;Now, I have a question to my fellow data hoarders, and it&amp;#39;s the reason why I&amp;#39;m posting this in the first place. What the hell are we supposed to? There&amp;#39;s no archives or backups anywhere; the site wasn&amp;#39;t all too popular, evident by the fact that no one is even talking about this despite it being the biggest ressource for Japanese fan works.&lt;/p&gt;\n\n&lt;p&gt;To be honest, I really doubt it&amp;#39;s going to come back. And I fear this might be another Library of Alexandria scenario...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A39rPq03PqwgcbXWQppLdcN5AeSiXDvlH7tPcHsAh1Q.png?auto=webp&amp;s=879d33faecaf0361ff93445bf2f7bac5b4bcafc5", "width": 586, "height": 695}, "resolutions": [{"url": "https://external-preview.redd.it/A39rPq03PqwgcbXWQppLdcN5AeSiXDvlH7tPcHsAh1Q.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7c2c17a640d7412041f27536106544263a54ead6", "width": 108, "height": 128}, {"url": "https://external-preview.redd.it/A39rPq03PqwgcbXWQppLdcN5AeSiXDvlH7tPcHsAh1Q.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=841ba32d3ece1119693ee620ffc6858977a5d73d", "width": 216, "height": 256}, {"url": "https://external-preview.redd.it/A39rPq03PqwgcbXWQppLdcN5AeSiXDvlH7tPcHsAh1Q.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1177f2e748708d2001e85c789d08ed532347397f", "width": 320, "height": 379}], "variants": {}, "id": "WFdf4X6f7FV-nDYfv84KJrMGtgaGBMHKgAzMvk6EscM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yxwkd5", "is_robot_indexable": true, "report_reasons": null, "author": "scremixz566", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxwkd5/largest_doujinshi_and_manga_lexicon_went_down_5/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxwkd5/largest_doujinshi_and_manga_lexicon_went_down_5/", "subreddit_subscribers": 654584, "created_utc": 1668707282.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ihhqghp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just snagged this on Amazon UK Black Friday Sale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_yyg9f3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/t0QdmbTBFOZAyjfowwNYjHzCO76zS6S9QCtIRXq5XQ8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668765062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/pulb0zra3q0a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?auto=webp&amp;s=064dc021967fba1f97f6a98730f8780fd59ddfe3", "width": 1290, "height": 2796}, "resolutions": [{"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5639047d72e19e25073a3f3c2140ebe8085bc153", "width": 108, "height": 216}, {"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f439f75a13da20d16c4960144dcfa67c229334e", "width": 216, "height": 432}, {"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec9910faafefbed4dc9af1f9d1082bce7f97db6e", "width": 320, "height": 640}, {"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=86142c4fecd9aae10ef5121d62b444b9c6e11f0d", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8285610e80820165e22f600f2f4f7b9a156a13e2", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8aec81b4a6cd12f40e30865faf9c72aedee029ea", "width": 1080, "height": 2160}], "variants": {}, "id": "NpCU_UcXrABz_XqsWmyewmI77IdJg7pOzeHQxDQ3xOI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyg9f3", "is_robot_indexable": true, "report_reasons": null, "author": "arjan5", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyg9f3/just_snagged_this_on_amazon_uk_black_friday_sale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/pulb0zra3q0a1.jpg", "subreddit_subscribers": 654584, "created_utc": 1668765062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Putting together a small NAS with 6 x 4TB HGST SAS drives, HUS724040ALS640s, and was lucky enough to snag two of them as unused spares for \u00a330 a pop. The next four I've just picked up, and the first couple have come back with roughly 25,000 hours on them.\n\nObviously any reports are going to be anecdotal and not a guarantee, but I'm curious as to if any of you have experience with used enterprise drives and reliability in that region of use.\n\nFrom my own personal experience, it seems that if a drive is destined to die it tends to die relatively quickly, and the ones I've had in the past that made it to 10,000 hours typically lasted another 50k+.\n\nUltimately I spent \u00a330 a pop on each of these, so I'm tempted to just consider it a good deal regardless and put it out of my mind. Am I crazy?", "author_fullname": "t2_7cqcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Used Enterprise Drives and Power On Hours", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxsz7h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668698470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Putting together a small NAS with 6 x 4TB HGST SAS drives, HUS724040ALS640s, and was lucky enough to snag two of them as unused spares for \u00a330 a pop. The next four I&amp;#39;ve just picked up, and the first couple have come back with roughly 25,000 hours on them.&lt;/p&gt;\n\n&lt;p&gt;Obviously any reports are going to be anecdotal and not a guarantee, but I&amp;#39;m curious as to if any of you have experience with used enterprise drives and reliability in that region of use.&lt;/p&gt;\n\n&lt;p&gt;From my own personal experience, it seems that if a drive is destined to die it tends to die relatively quickly, and the ones I&amp;#39;ve had in the past that made it to 10,000 hours typically lasted another 50k+.&lt;/p&gt;\n\n&lt;p&gt;Ultimately I spent \u00a330 a pop on each of these, so I&amp;#39;m tempted to just consider it a good deal regardless and put it out of my mind. Am I crazy?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yxsz7h", "is_robot_indexable": true, "report_reasons": null, "author": "mdcdesign", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxsz7h/used_enterprise_drives_and_power_on_hours/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxsz7h/used_enterprise_drives_and_power_on_hours/", "subreddit_subscribers": 654584, "created_utc": 1668698470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Genuinely decent chance Twitter might pop in the near future.](https://twitter.com/alexeheath/status/1593399683086327808?s=20&amp;t=dfeufbahrPBgan8EqpYe1w) Does anyone know how to save tweets from an account en masse? Worried that some people might not archive their own things before it's too late. Sorry if this doesn't fit the sub?\n\nAll the tools I can find are pay-to-use which is wild, except archive.org which only saves the most recent page of tweets from an account? Like a few days' worth roughly.", "author_fullname": "t2_yeb0x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I archive other people's twitter accounts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy7yvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668736340.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://twitter.com/alexeheath/status/1593399683086327808?s=20&amp;amp;t=dfeufbahrPBgan8EqpYe1w\"&gt;Genuinely decent chance Twitter might pop in the near future.&lt;/a&gt; Does anyone know how to save tweets from an account en masse? Worried that some people might not archive their own things before it&amp;#39;s too late. Sorry if this doesn&amp;#39;t fit the sub?&lt;/p&gt;\n\n&lt;p&gt;All the tools I can find are pay-to-use which is wild, except archive.org which only saves the most recent page of tweets from an account? Like a few days&amp;#39; worth roughly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy7yvo", "is_robot_indexable": true, "report_reasons": null, "author": "SansFinalGuardian", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy7yvo/how_can_i_archive_other_peoples_twitter_accounts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy7yvo/how_can_i_archive_other_peoples_twitter_accounts/", "subreddit_subscribers": 654584, "created_utc": 1668736340.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Let's say\n\n\\- you have different purpose data (archive + volatile data)- too much to fit totally on local disk (both kinds)- backup over multiple companies, in case an account would be banned (there are enough reasons, even accidents, like banned google account because of youtube violence).\n\nI thought about the following concept:\n\nCold-Storage:\n- Backblaze\n- GCloud\n- one more?\n\nLocal (virtual) file system:  \n\n```\n/coldstorage (let it mostly unmounted, because of expensive read costs)\n  /backblaze\n    /dropbox-full-backup (incl. gdrive-backup)\n  /gcloud (cold tier)\n\n/dropbox (quiet fast)\n  /gdrive-backup (readonly)\n\n/gdrive (slow.....)  \n  /dropbox-backup  (readonly)\n```\n\nYou can also create an EncFS encrypted folder within the /dropbox folder (only for specific folders).\n\nSo, in this case you have two different volatile cloud storages, and they backup them each self to the other. Plus one big backup to the backblaze cold storage.\n\nCold Storage is also used for non volatile data like big ZIPS, archives, Camera Videos and so on.\n\nWhat do you think about this concept? It seems to too price, and i think, it's still performant. By the way, the sync between the multiple storages, i would them do within an small cloud computing instance, just for speed/internet speed bottle neck.", "author_fullname": "t2_44jxdyfx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redundant Cloud Storage Concept", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxwig0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668707564.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668707145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say&lt;/p&gt;\n\n&lt;p&gt;- you have different purpose data (archive + volatile data)- too much to fit totally on local disk (both kinds)- backup over multiple companies, in case an account would be banned (there are enough reasons, even accidents, like banned google account because of youtube violence).&lt;/p&gt;\n\n&lt;p&gt;I thought about the following concept:&lt;/p&gt;\n\n&lt;p&gt;Cold-Storage:\n- Backblaze\n- GCloud\n- one more?&lt;/p&gt;\n\n&lt;p&gt;Local (virtual) file system:  &lt;/p&gt;\n\n&lt;p&gt;```\n/coldstorage (let it mostly unmounted, because of expensive read costs)\n  /backblaze\n    /dropbox-full-backup (incl. gdrive-backup)\n  /gcloud (cold tier)&lt;/p&gt;\n\n&lt;p&gt;/dropbox (quiet fast)\n  /gdrive-backup (readonly)&lt;/p&gt;\n\n&lt;p&gt;/gdrive (slow.....)&lt;br/&gt;\n  /dropbox-backup  (readonly)\n```&lt;/p&gt;\n\n&lt;p&gt;You can also create an EncFS encrypted folder within the /dropbox folder (only for specific folders).&lt;/p&gt;\n\n&lt;p&gt;So, in this case you have two different volatile cloud storages, and they backup them each self to the other. Plus one big backup to the backblaze cold storage.&lt;/p&gt;\n\n&lt;p&gt;Cold Storage is also used for non volatile data like big ZIPS, archives, Camera Videos and so on.&lt;/p&gt;\n\n&lt;p&gt;What do you think about this concept? It seems to too price, and i think, it&amp;#39;s still performant. By the way, the sync between the multiple storages, i would them do within an small cloud computing instance, just for speed/internet speed bottle neck.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxwig0", "is_robot_indexable": true, "report_reasons": null, "author": "sebastian-loncar", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxwig0/redundant_cloud_storage_concept/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxwig0/redundant_cloud_storage_concept/", "subreddit_subscribers": 654584, "created_utc": 1668707145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A couple years ago i was really into old games and collected a lot of them from yard sales, ebay, traded them with friends. I knew what i already got for the most part, but i never really made a list/labeled them. In total i guess somewhere around 250 floppys &amp; 4-5000 cds.\n\nSince many of them are nearly 20 years some even older, i am looking for a way to digitalis them.\n\n**The first thing, is there an efficient way/program to convert them to isos?**\n\n**The second thing, are you familiar with any archive/collection management program thats selfhosted and where you could link the said isos?**   \n(Like Collectorz just Selfhosted, Browserbased &amp; OpenSource)\n\nThanks for any ideas", "author_fullname": "t2_8jnr5wv6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Efficient Way to Backup/Archive many Gamedisks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxns42", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668684298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A couple years ago i was really into old games and collected a lot of them from yard sales, ebay, traded them with friends. I knew what i already got for the most part, but i never really made a list/labeled them. In total i guess somewhere around 250 floppys &amp;amp; 4-5000 cds.&lt;/p&gt;\n\n&lt;p&gt;Since many of them are nearly 20 years some even older, i am looking for a way to digitalis them.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The first thing, is there an efficient way/program to convert them to isos?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The second thing, are you familiar with any archive/collection management program thats selfhosted and where you could link the said isos?&lt;/strong&gt;&lt;br/&gt;\n(Like Collectorz just Selfhosted, Browserbased &amp;amp; OpenSource)&lt;/p&gt;\n\n&lt;p&gt;Thanks for any ideas&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxns42", "is_robot_indexable": true, "report_reasons": null, "author": "Pommes254", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxns42/efficient_way_to_backuparchive_many_gamedisks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxns42/efficient_way_to_backuparchive_many_gamedisks/", "subreddit_subscribers": 654584, "created_utc": 1668684298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "*Note: I do have all the data I'm referring to backed up elsewhere*\n\nI'm on Windows 10 Pro and I've got two identical dynamic disks -- 3TB each with 2.4TB (or whatever) filled, each. I want to convert them to spanned or striped (haven't decided which, yet) but I don't seem to have the option -- it's greyed out in Disk Manager:\n\nhttps://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81\n\nDo I...need to shrink the disks to make room? I'm hesitant to try that without getting thoughts from others because it'll take the better part of a day, each disk. Do I...need to delete the volumes and create the array from scratch?\n\nI'd prefer a non-destructive option if possible. I'm willing to delete these because the data is backed up, but restoring it will be a PITA :)\n\nMuch appreciated.", "author_fullname": "t2_36jz3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows: Can dynamic disks not be converted to spanned/striped?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "media_metadata": {"b203wafbjm0a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94b6126cadbbbb1376df665d6a170fc771860b95"}, {"y": 135, "x": 216, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ea9718fef595c82c340ac3622c2b2a951e1052b"}, {"y": 200, "x": 320, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2265f302a5ff338b74ebf4de0a9b17ebb80d3252"}, {"y": 400, "x": 640, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7cd33c67ecfeec80a2ec3738e382e54eecd19144"}, {"y": 600, "x": 960, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=063565c5bab55cb108d470c859daba852a0a356d"}, {"y": 675, "x": 1080, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7893f3501b0345bcf7d35fda3017a05d217bb596"}], "s": {"y": 900, "x": 1440, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81"}, "id": "b203wafbjm0a1"}}, "name": "t3_yy99ds", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/vMb3XlrxBTmOW43DaRSvG5q-XrdleZUhBeYHO6lSV00.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668740163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Note: I do have all the data I&amp;#39;m referring to backed up elsewhere&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on Windows 10 Pro and I&amp;#39;ve got two identical dynamic disks -- 3TB each with 2.4TB (or whatever) filled, each. I want to convert them to spanned or striped (haven&amp;#39;t decided which, yet) but I don&amp;#39;t seem to have the option -- it&amp;#39;s greyed out in Disk Manager:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81\"&gt;https://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Do I...need to shrink the disks to make room? I&amp;#39;m hesitant to try that without getting thoughts from others because it&amp;#39;ll take the better part of a day, each disk. Do I...need to delete the volumes and create the array from scratch?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d prefer a non-destructive option if possible. I&amp;#39;m willing to delete these because the data is backed up, but restoring it will be a PITA :)&lt;/p&gt;\n\n&lt;p&gt;Much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy99ds", "is_robot_indexable": true, "report_reasons": null, "author": "eriksrx", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy99ds/windows_can_dynamic_disks_not_be_converted_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy99ds/windows_can_dynamic_disks_not_be_converted_to/", "subreddit_subscribers": 654584, "created_utc": 1668740163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My current offsite backup plan involves keeping a bunch of encrypted drives in a garage in the midwest, where temps can get well below zero.  \n\n\nI know minimum operating temps for drives tend to be around 40f, but haven't seen anything for non operating temps. Drives will be left to warm up overnight before they're powered on to update  \n\n\nAm I at risk of damaging the drives doing this?", "author_fullname": "t2_97alswd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storing hard drives in the cold?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy8voh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668739016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My current offsite backup plan involves keeping a bunch of encrypted drives in a garage in the midwest, where temps can get well below zero.  &lt;/p&gt;\n\n&lt;p&gt;I know minimum operating temps for drives tend to be around 40f, but haven&amp;#39;t seen anything for non operating temps. Drives will be left to warm up overnight before they&amp;#39;re powered on to update  &lt;/p&gt;\n\n&lt;p&gt;Am I at risk of damaging the drives doing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy8voh", "is_robot_indexable": true, "report_reasons": null, "author": "tunafishnobread", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy8voh/storing_hard_drives_in_the_cold/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy8voh/storing_hard_drives_in_the_cold/", "subreddit_subscribers": 654584, "created_utc": 1668739016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So one of my clients at work has an iMac that is used as a Data Server. They are in the process of upgrading it to an actual Apple Server but it is a slow process due to cost. \n\nThe problem is, they have 4 external hard drives that are used for backup purposes. Two are used for Time Machine backups, and the other two are used for data backups for a program they use. The old solution was that they were just swapping them out every week, but then we ran into the problem of corrupt drives (due to a USB hub not having enough power and failing intermittently). I now want all 4 plugged into the device at once to prevent this nightmare of a situation. The problem is, the iMac only has 4 USB ports on the back, and 3 are occupied and cannot be moved. One is used by a personal scanner which cannot be connected to a USB hub, another is the USB hub mentioned earlier, and a third is the Ethernet adapter which cannot be moved due to the actual Ethernet port on the iMac being broken.\n\nI need some ideas, because currently only one backup can really work properly as there is only one USB port available. Open to any ideas, thank you.", "author_fullname": "t2_f53sz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some way to have 4 external hard drives running backups on an iMac with minimal USB hubs available.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy6uf2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668732946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So one of my clients at work has an iMac that is used as a Data Server. They are in the process of upgrading it to an actual Apple Server but it is a slow process due to cost. &lt;/p&gt;\n\n&lt;p&gt;The problem is, they have 4 external hard drives that are used for backup purposes. Two are used for Time Machine backups, and the other two are used for data backups for a program they use. The old solution was that they were just swapping them out every week, but then we ran into the problem of corrupt drives (due to a USB hub not having enough power and failing intermittently). I now want all 4 plugged into the device at once to prevent this nightmare of a situation. The problem is, the iMac only has 4 USB ports on the back, and 3 are occupied and cannot be moved. One is used by a personal scanner which cannot be connected to a USB hub, another is the USB hub mentioned earlier, and a third is the Ethernet adapter which cannot be moved due to the actual Ethernet port on the iMac being broken.&lt;/p&gt;\n\n&lt;p&gt;I need some ideas, because currently only one backup can really work properly as there is only one USB port available. Open to any ideas, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy6uf2", "is_robot_indexable": true, "report_reasons": null, "author": "AH_BareGarrett", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy6uf2/need_some_way_to_have_4_external_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy6uf2/need_some_way_to_have_4_external_hard_drives/", "subreddit_subscribers": 654584, "created_utc": 1668732946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "is there a way to archive an email mailbox and make it searchable by a simple webinterface or even just a good export into text files + attachements.", "author_fullname": "t2_mdv6krfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "archive email mailbox and make it searchable", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy3bio", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668723359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;is there a way to archive an email mailbox and make it searchable by a simple webinterface or even just a good export into text files + attachements.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy3bio", "is_robot_indexable": true, "report_reasons": null, "author": "tillybowman", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy3bio/archive_email_mailbox_and_make_it_searchable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy3bio/archive_email_mailbox_and_make_it_searchable/", "subreddit_subscribers": 654584, "created_utc": 1668723359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sorry if this is a stupid question, I'm completely clueless about this stuff.\n\nI have a huge collection of obscure gaming books and materials. Including scans of rare developer interviews.\n\nI was reading the ArchiveTeam wiki and I found out that you can upload to archive.org using a torrent.\n\nhttps://wiki.archiveteam.org/index.php/Internet_Archive#Uploading_to_archive.org\n\n\"Torrent upload, useful if you need resume (for huge files or because your bandwidth is insufficient for upload in one go)\"\n\nI don't have the best equipment so I want to use this method in case I have some sort of interruption.\n\nIs it recommended to use torrent upload? Is there any file size limits to it? Can it handle 10gb+ uploads?", "author_fullname": "t2_8rcmiabz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about using torrent upload for Internet Archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxzvvs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668715128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is a stupid question, I&amp;#39;m completely clueless about this stuff.&lt;/p&gt;\n\n&lt;p&gt;I have a huge collection of obscure gaming books and materials. Including scans of rare developer interviews.&lt;/p&gt;\n\n&lt;p&gt;I was reading the ArchiveTeam wiki and I found out that you can upload to archive.org using a torrent.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://wiki.archiveteam.org/index.php/Internet_Archive#Uploading_to_archive.org\"&gt;https://wiki.archiveteam.org/index.php/Internet_Archive#Uploading_to_archive.org&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Torrent upload, useful if you need resume (for huge files or because your bandwidth is insufficient for upload in one go)&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have the best equipment so I want to use this method in case I have some sort of interruption.&lt;/p&gt;\n\n&lt;p&gt;Is it recommended to use torrent upload? Is there any file size limits to it? Can it handle 10gb+ uploads?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxzvvs", "is_robot_indexable": true, "report_reasons": null, "author": "CitronDestroys", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxzvvs/question_about_using_torrent_upload_for_internet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxzvvs/question_about_using_torrent_upload_for_internet/", "subreddit_subscribers": 654584, "created_utc": 1668715128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This cooking website that I have followed for years has a clean, printable version of every recipe on a very similar web address that all follow the same format.\n\n[https://www.halfbakedharvest.com/wprm\\_print/26284](https://www.halfbakedharvest.com/wprm_print/26284)\n\n[https://www.halfbakedharvest.com/wprm\\_print/108579](https://www.halfbakedharvest.com/wprm_print/108579)\n\n[https://www.halfbakedharvest.com/wprm\\_print/110135](https://www.halfbakedharvest.com/wprm_print/110135)\n\n[https://www.halfbakedharvest.com/wprm\\_print/118540](https://www.halfbakedharvest.com/wprm_print/118540)\n\nThere are hundreds of recipes, but the numbers at the end of the address jumps at random intervals (ex. 540, 588, 607, 690). Is there someway a scraper or something could go through and check every number to test if a page is there and then download the webpage?", "author_fullname": "t2_hd6cp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a way to save recipes webpages from similar web addresses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxx8pt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668708898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This cooking website that I have followed for years has a clean, printable version of every recipe on a very similar web address that all follow the same format.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.halfbakedharvest.com/wprm_print/26284\"&gt;https://www.halfbakedharvest.com/wprm_print/26284&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.halfbakedharvest.com/wprm_print/108579\"&gt;https://www.halfbakedharvest.com/wprm_print/108579&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.halfbakedharvest.com/wprm_print/110135\"&gt;https://www.halfbakedharvest.com/wprm_print/110135&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.halfbakedharvest.com/wprm_print/118540\"&gt;https://www.halfbakedharvest.com/wprm_print/118540&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There are hundreds of recipes, but the numbers at the end of the address jumps at random intervals (ex. 540, 588, 607, 690). Is there someway a scraper or something could go through and check every number to test if a page is there and then download the webpage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxx8pt", "is_robot_indexable": true, "report_reasons": null, "author": "1000yardstareslacker", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxx8pt/looking_for_a_way_to_save_recipes_webpages_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxx8pt/looking_for_a_way_to_save_recipes_webpages_from/", "subreddit_subscribers": 654584, "created_utc": 1668708898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As I started researching online backup solutions, I realized we all mostly talk about backing up, but **we rarely talking about restoring**. More specifically, we rarely talk about the process of restoring, if it works, how well it works, etc. I read many folks say they couldn't restore cause of a corrupted backup because of the backup tool they used -- *not cool*.\n\nSo, I'm wondering, for folks who have had to restore data **and it was successful**, what tool were you using to do the backup + restore? Comment with your thoughts/experience.\n\n[View Poll](https://www.reddit.com/poll/yxqwnz)", "author_fullname": "t2_5wpob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What back tool did you use to backup **AND RESTORE** and how well did it work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxqwnz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668693212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As I started researching online backup solutions, I realized we all mostly talk about backing up, but &lt;strong&gt;we rarely talking about restoring&lt;/strong&gt;. More specifically, we rarely talk about the process of restoring, if it works, how well it works, etc. I read many folks say they couldn&amp;#39;t restore cause of a corrupted backup because of the backup tool they used -- &lt;em&gt;not cool&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m wondering, for folks who have had to restore data &lt;strong&gt;and it was successful&lt;/strong&gt;, what tool were you using to do the backup + restore? Comment with your thoughts/experience.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/yxqwnz\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yxqwnz", "is_robot_indexable": true, "report_reasons": null, "author": "imthenachoman", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1669298012653, "options": [{"text": "Backblaze Personal", "id": "19865732"}, {"text": "rclone", "id": "19865733"}, {"text": "Duplicati", "id": "19865734"}, {"text": "Duplicacy", "id": "19865735"}, {"text": "ARQ", "id": "19865736"}, {"text": "Veeam", "id": "19865737"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 55, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxqwnz/what_back_tool_did_you_use_to_backup_and_restore/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/DataHoarder/comments/yxqwnz/what_back_tool_did_you_use_to_backup_and_restore/", "subreddit_subscribers": 654584, "created_utc": 1668693212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need a program for backup, with the ability to archive versions of files and store them forever. Surprisingly, even expensive  backup solutions cannot do this.  They all delete older versions of files as the copy job progresses.", "author_fullname": "t2_uce1w524", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What backup software keeps versions of files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxop6q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668687211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need a program for backup, with the ability to archive versions of files and store them forever. Surprisingly, even expensive  backup solutions cannot do this.  They all delete older versions of files as the copy job progresses.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxop6q", "is_robot_indexable": true, "report_reasons": null, "author": "Proper_History_9283", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxop6q/what_backup_software_keeps_versions_of_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxop6q/what_backup_software_keeps_versions_of_files/", "subreddit_subscribers": 654584, "created_utc": 1668687211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Perhaps I\u2019m just lucky and haven\u2019t lost a drive but I feel like between SMART monitoring and using single addressable drives my 24 bay media server using drives on their own is running well.\n\nI\u2019ve been considering some pooled sort of RAID like unraid or omv zfs for many years, and realize it means the loss of one drive for parity, but overall the bigger fear is that something will go wrong that will trash the entire pool of Files versus just one drives worth of content - tv and movie files which could be replaced. Anyone else have similar fears and how did you resolve? I just worry the implosion of say 100tb of a pool would be riskier than losing a regular 14tb drive. My drives are all of various size btw ranging from 5tb to 14tb all gutted from desktop cases so not bleeding edge. Also isn\u2019t raid going to run slower than jbod on my 4u Xeon rackmount?", "author_fullname": "t2_4jntazsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RAID Risks - Anyone left using only JBOD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxnrss", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668684268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Perhaps I\u2019m just lucky and haven\u2019t lost a drive but I feel like between SMART monitoring and using single addressable drives my 24 bay media server using drives on their own is running well.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been considering some pooled sort of RAID like unraid or omv zfs for many years, and realize it means the loss of one drive for parity, but overall the bigger fear is that something will go wrong that will trash the entire pool of Files versus just one drives worth of content - tv and movie files which could be replaced. Anyone else have similar fears and how did you resolve? I just worry the implosion of say 100tb of a pool would be riskier than losing a regular 14tb drive. My drives are all of various size btw ranging from 5tb to 14tb all gutted from desktop cases so not bleeding edge. Also isn\u2019t raid going to run slower than jbod on my 4u Xeon rackmount?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxnrss", "is_robot_indexable": true, "report_reasons": null, "author": "ExcitingDegree", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxnrss/raid_risks_anyone_left_using_only_jbod/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxnrss/raid_risks_anyone_left_using_only_jbod/", "subreddit_subscribers": 654584, "created_utc": 1668684268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_tlgcl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A database of paper airplanes with easy to follow folding instructions, video tutorials and printable folding plans. Find the best paper airplanes that fly the furthest and stay aloft the longest.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_yyhavo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/prZ5H2m6FUcoXaVYAgI4mTKRMuTFDWbsdOBELddMgW8.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668769050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "foldnfly.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.foldnfly.com/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?auto=webp&amp;s=2e319bc9673fc40f1aec8c70febd71b2f84b10cb", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d37c2a5e835ea4c51b538aef5cbbd90f4e401217", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4d339c5a12d9cf0b1cf1a15d74138abbb400754", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b622d929f9312bf9d0f6019c52870b890a6d4ad", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d65b5cd84e8524d5d83e756f6a19fffc433d5b12", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=69a20c6d882a8d34f6c8dea2fec83e9b577a5f29", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=94570fce0f309e51fd4ca001f841c5a99f97f58d", "width": 1080, "height": 567}], "variants": {}, "id": "uZ2zDM2Hy1FLstbjMCzVaGvaO0jNdvizg4F-yPk7isI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "40TB Synology DS1819+", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyhavo", "is_robot_indexable": true, "report_reasons": null, "author": "PaddleMonkey", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yyhavo/a_database_of_paper_airplanes_with_easy_to_follow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.foldnfly.com/", "subreddit_subscribers": 654584, "created_utc": 1668769050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hiya. I've bought an old 8 bay server that came with\n\n* 4x 1TB drives in it\n* Supermicro X9DRH-7TF/7F/iTF/iF motherboard\n* Xeon E5-2603 1.80 GHz\n* 8gb or DDR3 1333 Mhz ram\n* Areca Arc-1222 pcie raid controller\n* A proprietary modified debian 6 by a local it company\n\nIt was working fine but then I thought let's try adding more drives into the remaining bays and see how that works\n\nWell. It didn't. I managed to expand the raidset but the proprietary software that runs filesharing was pretty obfuscated and locked down with how and what it sees. I've contacted the company and they said 'sorry mate, that things been out of support for years. buy a new one'\n\nSo I jumped onto the next logical step. Let's wipe the whole thing and install something else on it. So I wiped it and tried installing openmediavault.\n\nBooted into installer, all looking good.Installer sees all my raidcard volumes, all good.\n\nInstalled it on a dedicated 10GB OS partition on the raidcard, all good.\n\nReboot\n\nSee the grub window\n\nThen black screen with the cursor for 10 minutes\n\nThen barrage of errors\n\nThought maybe faulty USB stick or smtn, tried reinstalling from another stick and a usb SSD drive. Same result.\n\nNext thing I had tried is installing mediavault not onto a RAID volume but onto USB stick instead and booting from that.\n\nSurprise, that booted in just fine!\n\nBUT\n\nOS doesn't see any of the RAID card volumes. Well it only sees them partially\n\nIn the OS raidcard volumes sda, sdb can\u2019t be accessed or mounted.\n\nThey appear listed with **lsblk** and **cat /proc/partitions** and are listed in **/dev/disk/by-path** (but not in by-id/by-partuuid/by-uuid).\n\nBut they don't appear when using **fdisk -l** and similar commands\n\nI could find messages like\n\n*I/O error, dev sda, sector 0 op 0x0:(READ) flags 0x0 phys\\_seg 1 prio class 0*\n\n*Nov 11 10:35:06 openmediavault kernel: Buffer I/O error on dev sda, logical block 0, async page read*\n\nI've tried installing raidcard driver I had found on the Areca website\n\n**arcmsr\\_1.50.0X.09-2-OMV6.0.24-k5.16.0-0.bpo.4-amd64.deb**\n\nRebooted, but still had same issue of raid volumes being only partially seen.\n\nI had also installed their raidcard monitoring tool which works fine but doesn't see any raid controllers in the web gui it comes with.\n\nI've had emailed Areca support a month ago about this but considering its a 10 year old legacy product I doubt they'll ever come back to me.\n\nI'm 90% leaning towards it being a raidcard driver issue. Anyone familiar with these older Areca cards? Do I need to use an older OS possibly for it to work?\n\nIdeally I'd like to just install any OS on a Raidcard volume that would let me share drives over SMB.\n\nI think motherboard itself might have 8 sata ports so i could potentially plug drives into that but I'd lose any hot swapping ability by doing that.\n\nIs it possible to save this at all? It was working fine for few weeks with the original 4 drives and software in place till I decided to 'upgrade'...\n\nHere's a link to journalctllog of when I boot into OS off USB\n\n[https://pastebin.com/A7pum7UW](https://pastebin.com/A7pum7UW)", "author_fullname": "t2_16yjgj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling with a bargain server i got : /", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyeyvz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668761335.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668759924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hiya. I&amp;#39;ve bought an old 8 bay server that came with&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;4x 1TB drives in it&lt;/li&gt;\n&lt;li&gt;Supermicro X9DRH-7TF/7F/iTF/iF motherboard&lt;/li&gt;\n&lt;li&gt;Xeon E5-2603 1.80 GHz&lt;/li&gt;\n&lt;li&gt;8gb or DDR3 1333 Mhz ram&lt;/li&gt;\n&lt;li&gt;Areca Arc-1222 pcie raid controller&lt;/li&gt;\n&lt;li&gt;A proprietary modified debian 6 by a local it company&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It was working fine but then I thought let&amp;#39;s try adding more drives into the remaining bays and see how that works&lt;/p&gt;\n\n&lt;p&gt;Well. It didn&amp;#39;t. I managed to expand the raidset but the proprietary software that runs filesharing was pretty obfuscated and locked down with how and what it sees. I&amp;#39;ve contacted the company and they said &amp;#39;sorry mate, that things been out of support for years. buy a new one&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;So I jumped onto the next logical step. Let&amp;#39;s wipe the whole thing and install something else on it. So I wiped it and tried installing openmediavault.&lt;/p&gt;\n\n&lt;p&gt;Booted into installer, all looking good.Installer sees all my raidcard volumes, all good.&lt;/p&gt;\n\n&lt;p&gt;Installed it on a dedicated 10GB OS partition on the raidcard, all good.&lt;/p&gt;\n\n&lt;p&gt;Reboot&lt;/p&gt;\n\n&lt;p&gt;See the grub window&lt;/p&gt;\n\n&lt;p&gt;Then black screen with the cursor for 10 minutes&lt;/p&gt;\n\n&lt;p&gt;Then barrage of errors&lt;/p&gt;\n\n&lt;p&gt;Thought maybe faulty USB stick or smtn, tried reinstalling from another stick and a usb SSD drive. Same result.&lt;/p&gt;\n\n&lt;p&gt;Next thing I had tried is installing mediavault not onto a RAID volume but onto USB stick instead and booting from that.&lt;/p&gt;\n\n&lt;p&gt;Surprise, that booted in just fine!&lt;/p&gt;\n\n&lt;p&gt;BUT&lt;/p&gt;\n\n&lt;p&gt;OS doesn&amp;#39;t see any of the RAID card volumes. Well it only sees them partially&lt;/p&gt;\n\n&lt;p&gt;In the OS raidcard volumes sda, sdb can\u2019t be accessed or mounted.&lt;/p&gt;\n\n&lt;p&gt;They appear listed with &lt;strong&gt;lsblk&lt;/strong&gt; and &lt;strong&gt;cat /proc/partitions&lt;/strong&gt; and are listed in &lt;strong&gt;/dev/disk/by-path&lt;/strong&gt; (but not in by-id/by-partuuid/by-uuid).&lt;/p&gt;\n\n&lt;p&gt;But they don&amp;#39;t appear when using &lt;strong&gt;fdisk -l&lt;/strong&gt; and similar commands&lt;/p&gt;\n\n&lt;p&gt;I could find messages like&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;I/O error, dev sda, sector 0 op 0x0:(READ) flags 0x0 phys_seg 1 prio class 0&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Nov 11 10:35:06 openmediavault kernel: Buffer I/O error on dev sda, logical block 0, async page read&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried installing raidcard driver I had found on the Areca website&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;arcmsr_1.50.0X.09-2-OMV6.0.24-k5.16.0-0.bpo.4-amd64.deb&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Rebooted, but still had same issue of raid volumes being only partially seen.&lt;/p&gt;\n\n&lt;p&gt;I had also installed their raidcard monitoring tool which works fine but doesn&amp;#39;t see any raid controllers in the web gui it comes with.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had emailed Areca support a month ago about this but considering its a 10 year old legacy product I doubt they&amp;#39;ll ever come back to me.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m 90% leaning towards it being a raidcard driver issue. Anyone familiar with these older Areca cards? Do I need to use an older OS possibly for it to work?&lt;/p&gt;\n\n&lt;p&gt;Ideally I&amp;#39;d like to just install any OS on a Raidcard volume that would let me share drives over SMB.&lt;/p&gt;\n\n&lt;p&gt;I think motherboard itself might have 8 sata ports so i could potentially plug drives into that but I&amp;#39;d lose any hot swapping ability by doing that.&lt;/p&gt;\n\n&lt;p&gt;Is it possible to save this at all? It was working fine for few weeks with the original 4 drives and software in place till I decided to &amp;#39;upgrade&amp;#39;...&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a link to journalctllog of when I boot into OS off USB&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pastebin.com/A7pum7UW\"&gt;https://pastebin.com/A7pum7UW&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?auto=webp&amp;s=07c121a0180003f7373863af66192b6ff6a937da", "width": 150, "height": 150}, "resolutions": [{"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df9c6a296446d05d873c629a30253398c4d29c1b", "width": 108, "height": 108}], "variants": {}, "id": "OgFzGCIRw1ZxjMOSkfV1OiH-_nQiZl8rzSonmOAuhGs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyeyvz", "is_robot_indexable": true, "report_reasons": null, "author": "poliver1988", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyeyvz/struggling_with_a_bargain_server_i_got/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yyeyvz/struggling_with_a_bargain_server_i_got/", "subreddit_subscribers": 654584, "created_utc": 1668759924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all. So, I have here about 15 drives, with an eye toward getting a lot more. Currently they are stored in a full tower case, and a sans digital 5-bay rack. However, I would like to have them all consolidated into one case/rack. I want it to be just a plain storage unit, not an external raid controller; all that is handled by linux. Basically just a rack that provides power and slots. What should I be looking for?\r\n\r\nPS: totally unrelated, but why the hell is it so hard to find a full tower that doesn't have a bunch of stupid RGB with it? Grrr.", "author_fullname": "t2_nyaisr0l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hardware suggestions for storing drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyecpx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668757761.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668757530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. So, I have here about 15 drives, with an eye toward getting a lot more. Currently they are stored in a full tower case, and a sans digital 5-bay rack. However, I would like to have them all consolidated into one case/rack. I want it to be just a plain storage unit, not an external raid controller; all that is handled by linux. Basically just a rack that provides power and slots. What should I be looking for?&lt;/p&gt;\n\n&lt;p&gt;PS: totally unrelated, but why the hell is it so hard to find a full tower that doesn&amp;#39;t have a bunch of stupid RGB with it? Grrr.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyecpx", "is_robot_indexable": true, "report_reasons": null, "author": "the_purple_goat", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyecpx/hardware_suggestions_for_storing_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yyecpx/hardware_suggestions_for_storing_drives/", "subreddit_subscribers": 654584, "created_utc": 1668757530.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}