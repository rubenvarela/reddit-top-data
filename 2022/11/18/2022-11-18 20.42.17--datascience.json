{"kind": "Listing", "data": {"after": null, "dist": 11, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There are quite a few recent posts about pros and cons of Jupyter in the DS field and I don't want to add to that. My question is more about tooling prevalent in corporate environments. Most of the time Python DS stack is limited to remotely hosted Jupyter lab instances (in contrast R stack includes paid subscriptions to Rstudio server, which is a great tool that can live in a browser!)\n\nWhat are the options available of one wants a hybrid IDE + Notebook workspace if ssh is blocked (almost all the time in corporate)\n\nI feel like Jupyter is hindering a push towards a Foss or paid alternate to Rstudio server for python. (I've heard VS code is attempting sever side tool which can be accessed through the browser but not sure if it's available for production (corporate use) yet. I know there can be workarounds using paid (new rstudio for python) options but since jupiter is so widespread its really hard to get the heads to spend money on something new and untested. \n\nI'm surprised that as a community we seem to be content with J.", "author_fullname": "t2_3dfr7klo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to escape Jupyter in a corporate environment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy42j2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 124, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 124, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668725301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are quite a few recent posts about pros and cons of Jupyter in the DS field and I don&amp;#39;t want to add to that. My question is more about tooling prevalent in corporate environments. Most of the time Python DS stack is limited to remotely hosted Jupyter lab instances (in contrast R stack includes paid subscriptions to Rstudio server, which is a great tool that can live in a browser!)&lt;/p&gt;\n\n&lt;p&gt;What are the options available of one wants a hybrid IDE + Notebook workspace if ssh is blocked (almost all the time in corporate)&lt;/p&gt;\n\n&lt;p&gt;I feel like Jupyter is hindering a push towards a Foss or paid alternate to Rstudio server for python. (I&amp;#39;ve heard VS code is attempting sever side tool which can be accessed through the browser but not sure if it&amp;#39;s available for production (corporate use) yet. I know there can be workarounds using paid (new rstudio for python) options but since jupiter is so widespread its really hard to get the heads to spend money on something new and untested. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m surprised that as a community we seem to be content with J.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yy42j2", "is_robot_indexable": true, "report_reasons": null, "author": "darthstargazer", "discussion_type": null, "num_comments": 75, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yy42j2/how_to_escape_jupyter_in_a_corporate_environment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yy42j2/how_to_escape_jupyter_in_a_corporate_environment/", "subreddit_subscribers": 820196, "created_utc": 1668725301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello! This is my first reddit post, so please forgive me if this is not the place to be posting asking for advice/guidance.\n\nI wanted to turn to this community as I feel extremely alone. I have virtually zero network. I have a few family members/family friends who work in the tech industry, but not in tech roles. Aside from that, I don't have any peers, coworkers, or friends that I know who would understand my situation.\n\nI graduated with my B.S. in Statistics back in March of this year. I've been working in a very small, non-tech start-up (\\~7 employees, I am the only technical employee) as a \"data scientist\" since graduating. I say \"data scientist\" in quotes because I don't feel like I'm doing work of a data scientist. I'm super part time, \\~15 hours per week, paid minimum wage. I live in the Bay Area, CA - so minimum wage doesn't cut it. I worked as an unpaid intern for the company for about 9 months before graduating, and I only accepted the offer because I had nothing else lined up. I figured I'd take it temporarily while I actively seek a full-time role elsewhere. \n\nThe company has all of their data in Google Workspace, primarily Google Sheets. No databases, no cloud infrastructure. The company doesn't want to transition to anything else because they don't see a need. I tried convincing my boss to let me set up a database and they can keep their data in Sheets as well, but he doesn't see a need so he doesn't want to pay me to do that. He basically has me doing busy work, barely any data science at all. Projects are scrappy with no structure. Most of the time it's my boss proposing a business problem to me, and me being like \"hmm, how can I solve this with python\". I come up with some what of a solution, that works for that one use case and that's it. It never becomes a fully finished project because I don't have the resources to take it any further.\n\nI thought I'd get a few projects under my belt, create a portfolio, and find another job. I feel like I'm worse off now than I was 9 months ago when I graduated. I haven't built a single model, no dashboards, no databases/SQL, no stats, etc. I haven't used my stats skills since school and I feel like I'm losing them. With that being said, I'm very far from being prepared to do any technical interviews. I've applied to countless positions over the past 6 or so months, had 2 technical interviews but I didn't make it past the SQL rounds. \n\nThis job is absolutely consuming me and I'm feeling hopeless. I don't have time to work on personal projects, prepare for technical interviews, etc. I get paid for 15 hours of work per week, but I work well over 40+ hours, I have no time for anything. On paper, I have the degree, I have the \"work experience\" and a good reference from an employer, but I'm severely lacking the technical skills required for entry level roles -  like being able to solve a problem end to end, push a model into production, deployment, etc. Not a single project I've worked on has came full circle, so as of right now I have nothing to show for the work I've done.\n\n&amp;#x200B;\n\nTLDR;\n\nI feel stuck because I don\u2019t have time to work on anything else outside of this job. I don\u2019t feel like I have the sufficient skills to land another job. It's just taking every ounce of my motivation and energy. I'm desperately seeking a job I can grow in, and one in which I have people to turn to for support. \n\nMy question is 1, what should I do right now, should I quit this job and fully focus on building my portfolio/preparing for technical interviews? \n\nAnd 2, should I find any job in a tech company, start from the bottom and try to get into DS role from internally? Or would that result in a similar situation that I'm in now because I wouldn't be practicing and learning new stats/ML skills?\n\nI apologize for this being so long, I've been holding that in for a long time lol. I'm hoping to reach anyone out there who has been in a similar position and can give me their two cents. Thank you for taking the time to read, cheers :)", "author_fullname": "t2_jmnpmehe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Seeking advice] I feel stuck in a mediocre \"DS\" job that pays me minimum wage and am really struggling to find an entry level role as a new grad.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yycqst", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 82, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 82, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668751538.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! This is my first reddit post, so please forgive me if this is not the place to be posting asking for advice/guidance.&lt;/p&gt;\n\n&lt;p&gt;I wanted to turn to this community as I feel extremely alone. I have virtually zero network. I have a few family members/family friends who work in the tech industry, but not in tech roles. Aside from that, I don&amp;#39;t have any peers, coworkers, or friends that I know who would understand my situation.&lt;/p&gt;\n\n&lt;p&gt;I graduated with my B.S. in Statistics back in March of this year. I&amp;#39;ve been working in a very small, non-tech start-up (~7 employees, I am the only technical employee) as a &amp;quot;data scientist&amp;quot; since graduating. I say &amp;quot;data scientist&amp;quot; in quotes because I don&amp;#39;t feel like I&amp;#39;m doing work of a data scientist. I&amp;#39;m super part time, ~15 hours per week, paid minimum wage. I live in the Bay Area, CA - so minimum wage doesn&amp;#39;t cut it. I worked as an unpaid intern for the company for about 9 months before graduating, and I only accepted the offer because I had nothing else lined up. I figured I&amp;#39;d take it temporarily while I actively seek a full-time role elsewhere. &lt;/p&gt;\n\n&lt;p&gt;The company has all of their data in Google Workspace, primarily Google Sheets. No databases, no cloud infrastructure. The company doesn&amp;#39;t want to transition to anything else because they don&amp;#39;t see a need. I tried convincing my boss to let me set up a database and they can keep their data in Sheets as well, but he doesn&amp;#39;t see a need so he doesn&amp;#39;t want to pay me to do that. He basically has me doing busy work, barely any data science at all. Projects are scrappy with no structure. Most of the time it&amp;#39;s my boss proposing a business problem to me, and me being like &amp;quot;hmm, how can I solve this with python&amp;quot;. I come up with some what of a solution, that works for that one use case and that&amp;#39;s it. It never becomes a fully finished project because I don&amp;#39;t have the resources to take it any further.&lt;/p&gt;\n\n&lt;p&gt;I thought I&amp;#39;d get a few projects under my belt, create a portfolio, and find another job. I feel like I&amp;#39;m worse off now than I was 9 months ago when I graduated. I haven&amp;#39;t built a single model, no dashboards, no databases/SQL, no stats, etc. I haven&amp;#39;t used my stats skills since school and I feel like I&amp;#39;m losing them. With that being said, I&amp;#39;m very far from being prepared to do any technical interviews. I&amp;#39;ve applied to countless positions over the past 6 or so months, had 2 technical interviews but I didn&amp;#39;t make it past the SQL rounds. &lt;/p&gt;\n\n&lt;p&gt;This job is absolutely consuming me and I&amp;#39;m feeling hopeless. I don&amp;#39;t have time to work on personal projects, prepare for technical interviews, etc. I get paid for 15 hours of work per week, but I work well over 40+ hours, I have no time for anything. On paper, I have the degree, I have the &amp;quot;work experience&amp;quot; and a good reference from an employer, but I&amp;#39;m severely lacking the technical skills required for entry level roles -  like being able to solve a problem end to end, push a model into production, deployment, etc. Not a single project I&amp;#39;ve worked on has came full circle, so as of right now I have nothing to show for the work I&amp;#39;ve done.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;TLDR;&lt;/p&gt;\n\n&lt;p&gt;I feel stuck because I don\u2019t have time to work on anything else outside of this job. I don\u2019t feel like I have the sufficient skills to land another job. It&amp;#39;s just taking every ounce of my motivation and energy. I&amp;#39;m desperately seeking a job I can grow in, and one in which I have people to turn to for support. &lt;/p&gt;\n\n&lt;p&gt;My question is 1, what should I do right now, should I quit this job and fully focus on building my portfolio/preparing for technical interviews? &lt;/p&gt;\n\n&lt;p&gt;And 2, should I find any job in a tech company, start from the bottom and try to get into DS role from internally? Or would that result in a similar situation that I&amp;#39;m in now because I wouldn&amp;#39;t be practicing and learning new stats/ML skills?&lt;/p&gt;\n\n&lt;p&gt;I apologize for this being so long, I&amp;#39;ve been holding that in for a long time lol. I&amp;#39;m hoping to reach anyone out there who has been in a similar position and can give me their two cents. Thank you for taking the time to read, cheers :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "yycqst", "is_robot_indexable": true, "report_reasons": null, "author": "soph_py", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yycqst/seeking_advice_i_feel_stuck_in_a_mediocre_ds_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yycqst/seeking_advice_i_feel_stuck_in_a_mediocre_ds_job/", "subreddit_subscribers": 820196, "created_utc": 1668751538.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone so I recently had an interview for a mid-level DS role. Throughout the interview they kept it pretty generic and high level just trying to gauge an understanding of how you thought about a problem and solved it.\n\n&amp;#x200B;\n\nWe did get to a point however where we reached a case study style of \"walk us how you would solve this\". The problem was that essentially customers of the service abuse certain offers costing the business money, and how would we go about solving this problem and training a machine learning model to do this for us.\n\n&amp;#x200B;\n\nGiven that they described it as a supervised problem with a known set of a customers, I said let's go about taking that and making a training set. Since it was imbalanced with a 1% target we could upsample our minority class and have a reasonable 10/90 or 80/20 split for use with our models. The actual model training itself is such a minor step I just said lets go with XGBoost.\n\n&amp;#x200B;\n\nIn terms of feature engineering I discussed a potential idea with a ratio of customer deposits to withdrawals as an indicator of any customers potentially abusing their service. Given that they described in this scenario having a full customer 365 view they started to lead me towards (what I believe) were more automated ways of doing feature engineering such as forward selection which I briefly touched on but didn't go into more detail about. \n\n&amp;#x200B;\n\nIn all honesty I left the interview feeling like I didn't answer these questions as well as I could have, and inevitably was right when I got the call back that they had passed on me. It essentially came down to a lack of technical detail.\n\n&amp;#x200B;\n\nIn my career I've always definitely felt more of an applied data scientist (coming from a CompSCI background) where I understand how to move data through the right steps correctly, and even feel comfortable performing EDA and questioning the data that I have available. That being said clearly I'm only at the surface of what these interviewers were looking for.\n\n&amp;#x200B;\n\nI'm hoping to pass the question onto the community in terms of how you would've answered this with some more technical detail as a learning experience and a way to point me in the direction of more areas to focus on.", "author_fullname": "t2_udnwzx6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DS Interview - Having a better answer/understanding for technical detail?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy6blv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668731405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone so I recently had an interview for a mid-level DS role. Throughout the interview they kept it pretty generic and high level just trying to gauge an understanding of how you thought about a problem and solved it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;We did get to a point however where we reached a case study style of &amp;quot;walk us how you would solve this&amp;quot;. The problem was that essentially customers of the service abuse certain offers costing the business money, and how would we go about solving this problem and training a machine learning model to do this for us.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Given that they described it as a supervised problem with a known set of a customers, I said let&amp;#39;s go about taking that and making a training set. Since it was imbalanced with a 1% target we could upsample our minority class and have a reasonable 10/90 or 80/20 split for use with our models. The actual model training itself is such a minor step I just said lets go with XGBoost.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In terms of feature engineering I discussed a potential idea with a ratio of customer deposits to withdrawals as an indicator of any customers potentially abusing their service. Given that they described in this scenario having a full customer 365 view they started to lead me towards (what I believe) were more automated ways of doing feature engineering such as forward selection which I briefly touched on but didn&amp;#39;t go into more detail about. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In all honesty I left the interview feeling like I didn&amp;#39;t answer these questions as well as I could have, and inevitably was right when I got the call back that they had passed on me. It essentially came down to a lack of technical detail.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In my career I&amp;#39;ve always definitely felt more of an applied data scientist (coming from a CompSCI background) where I understand how to move data through the right steps correctly, and even feel comfortable performing EDA and questioning the data that I have available. That being said clearly I&amp;#39;m only at the surface of what these interviewers were looking for.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping to pass the question onto the community in terms of how you would&amp;#39;ve answered this with some more technical detail as a learning experience and a way to point me in the direction of more areas to focus on.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yy6blv", "is_robot_indexable": true, "report_reasons": null, "author": "dsisaster", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yy6blv/ds_interview_having_a_better_answerunderstanding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yy6blv/ds_interview_having_a_better_answerunderstanding/", "subreddit_subscribers": 820196, "created_utc": 1668731405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_amfdjuba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Q] When is a pearson correlation of 0.1 convincing for any storytelling usage or specific use case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyo3io", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668788729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yyo3io", "is_robot_indexable": true, "report_reasons": null, "author": "limedove", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yyo3io/q_when_is_a_pearson_correlation_of_01_convincing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yyo3io/q_when_is_a_pearson_correlation_of_01_convincing/", "subreddit_subscribers": 820196, "created_utc": 1668788729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Good evening everyone,\n\nI have a task with a client, he gave me a dataset full of hotel description and I must add tags to them. A tag can be \"own\\_outdoor\\_pool\", \"close\\_to\\_beach\", \"luxe\" just to give some examples. As it is real world data, we cannot do supervised ML or DL as the dataset is not labelled with those tags. What I do right now, is to do a subsentence segmentation with a DL model, I build an \"initialisation file\" where I give for the tag an initialisation sentence, let's have the tag \"own\\_outdoor\\_pool\" some initialisation sentences could be for exemple \"outdoor pool in the hotel\", \"a pool located outside\", \"you can find a pool in the garden\", and I do this for every tag. Then I do sentence embedding with a NLP model for the subsentences of each description and each initialisation sentence and I compute a cosine distance of each subsentence of the description of the hotel with all the initialisation sentences for each tag. It works pretty well, the highest distance gives the good tag usually, I also put a treshold aroung 0.55 to avoid useless tag for not relevant subsentences. The issue that I have is with overlapping tag such as \"heated\\_pool\", \"indoor\\_pool\", outdoor\\_pool\". As the initialisation sentences for these 3 tags are similar, the distance with subsentences of a given hotel description that have pool in them will have a high cosine distance with these 3 tags. A subsentences with heated pool will have high cosine similarity with the two tags \"indoor\\_pool\" and \"outdoor\\_pool\" where I want to get the tag \"heated\\_pool\".\n\nI am thinking to use the inverse of a penalty, meaning that I would like to increase the significance of a word such as indoor, outdoor or heated to get the proper tag. Yet, I do not know how to do it. Do anyone here can give me a hint? Some ressources available? Thank you in advance.\n\nNB: Sorry for my english, not my lative language.", "author_fullname": "t2_5gzyxvce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to add more importance to a given word for sentence similarity ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy20qv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668720177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good evening everyone,&lt;/p&gt;\n\n&lt;p&gt;I have a task with a client, he gave me a dataset full of hotel description and I must add tags to them. A tag can be &amp;quot;own_outdoor_pool&amp;quot;, &amp;quot;close_to_beach&amp;quot;, &amp;quot;luxe&amp;quot; just to give some examples. As it is real world data, we cannot do supervised ML or DL as the dataset is not labelled with those tags. What I do right now, is to do a subsentence segmentation with a DL model, I build an &amp;quot;initialisation file&amp;quot; where I give for the tag an initialisation sentence, let&amp;#39;s have the tag &amp;quot;own_outdoor_pool&amp;quot; some initialisation sentences could be for exemple &amp;quot;outdoor pool in the hotel&amp;quot;, &amp;quot;a pool located outside&amp;quot;, &amp;quot;you can find a pool in the garden&amp;quot;, and I do this for every tag. Then I do sentence embedding with a NLP model for the subsentences of each description and each initialisation sentence and I compute a cosine distance of each subsentence of the description of the hotel with all the initialisation sentences for each tag. It works pretty well, the highest distance gives the good tag usually, I also put a treshold aroung 0.55 to avoid useless tag for not relevant subsentences. The issue that I have is with overlapping tag such as &amp;quot;heated_pool&amp;quot;, &amp;quot;indoor_pool&amp;quot;, outdoor_pool&amp;quot;. As the initialisation sentences for these 3 tags are similar, the distance with subsentences of a given hotel description that have pool in them will have a high cosine distance with these 3 tags. A subsentences with heated pool will have high cosine similarity with the two tags &amp;quot;indoor_pool&amp;quot; and &amp;quot;outdoor_pool&amp;quot; where I want to get the tag &amp;quot;heated_pool&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I am thinking to use the inverse of a penalty, meaning that I would like to increase the significance of a word such as indoor, outdoor or heated to get the proper tag. Yet, I do not know how to do it. Do anyone here can give me a hint? Some ressources available? Thank you in advance.&lt;/p&gt;\n\n&lt;p&gt;NB: Sorry for my english, not my lative language.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yy20qv", "is_robot_indexable": true, "report_reasons": null, "author": "dekozr", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yy20qv/how_to_add_more_importance_to_a_given_word_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yy20qv/how_to_add_more_importance_to_a_given_word_for/", "subreddit_subscribers": 820196, "created_utc": 1668720177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, I am freelancing from over a few years now and now starting my own consulting firm. I posted about this here I think long ago, too. But this time I am more closer.\n\nI need some suggestions: I think I can grow easily and I'd need someone onboard. Do you think it is best to hire \"freelancers\" to partner with? I was creating the website and I feel except 'me being me' I have nothing to say about it lol. \n\nwhat do you suggest? Anyway if I will get big projects in near future I'd need to do some stuff to hire people, but is it best to contact one or two freelancers and add them to teams page or I am being unrealistic? (idk how much it will matter, but so far I was freelancing alone).\n\nThanks!", "author_fullname": "t2_9vrsvrcf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting my own consulting firm, do having a stable team matter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyjqut", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668777099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am freelancing from over a few years now and now starting my own consulting firm. I posted about this here I think long ago, too. But this time I am more closer.&lt;/p&gt;\n\n&lt;p&gt;I need some suggestions: I think I can grow easily and I&amp;#39;d need someone onboard. Do you think it is best to hire &amp;quot;freelancers&amp;quot; to partner with? I was creating the website and I feel except &amp;#39;me being me&amp;#39; I have nothing to say about it lol. &lt;/p&gt;\n\n&lt;p&gt;what do you suggest? Anyway if I will get big projects in near future I&amp;#39;d need to do some stuff to hire people, but is it best to contact one or two freelancers and add them to teams page or I am being unrealistic? (idk how much it will matter, but so far I was freelancing alone).&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yyjqut", "is_robot_indexable": true, "report_reasons": null, "author": "homoeconomicus1", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yyjqut/starting_my_own_consulting_firm_do_having_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yyjqut/starting_my_own_consulting_firm_do_having_a/", "subreddit_subscribers": 820196, "created_utc": 1668777099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I usually use STATA and Excel to make pictographs. But I wanted to know if there are better resources out there that are used by researchers.\n\nPS: Matplotlib, Bokeh...etc are too difficult to customise", "author_fullname": "t2_6lvuczf0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for data visualization (free &amp; paid) for scientific publications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy19mj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668718369.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I usually use STATA and Excel to make pictographs. But I wanted to know if there are better resources out there that are used by researchers.&lt;/p&gt;\n\n&lt;p&gt;PS: Matplotlib, Bokeh...etc are too difficult to customise&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yy19mj", "is_robot_indexable": true, "report_reasons": null, "author": "RexFury101", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yy19mj/resources_for_data_visualization_free_paid_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yy19mj/resources_for_data_visualization_free_paid_for/", "subreddit_subscribers": 820196, "created_utc": 1668718369.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "When it comes to developing some data product, my workflow has firmly established in Jupyter, where I can run-evaluate-rerun small blocks of code, until I'm happy with result, package it in a function and move on to next problem, rinse and repeat.\n\nHowever, at some point you have to move out of experimental sandbox to a proper code base, that integrates with broader stack of entire product. Refactoring into clean, documented functions, tests, I/O contracts, deployment strategy, wrapping in service.\n\nIt does feel like it's not optimal, as I often tend to revert back to Jupyter to debug or change things up, then returning back to proper project. So I tried cutting Jupyter out of the workflow, to have everything in the same place and avoiding unnecessary extra steps. But it just doesn't work for me! I can't seem to figure it out whether it's the force of habbit, or I'm missing something important. I end up having several sandbox scripts, every py file has some random 'if name==main' block to try out functions contained within, etc. \n\nWhat is your workflow, as the project gets more complex and you have to maintain a stable code base?", "author_fullname": "t2_4eomeg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Development workflow in Python (prototype to product)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yydh9b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668754197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When it comes to developing some data product, my workflow has firmly established in Jupyter, where I can run-evaluate-rerun small blocks of code, until I&amp;#39;m happy with result, package it in a function and move on to next problem, rinse and repeat.&lt;/p&gt;\n\n&lt;p&gt;However, at some point you have to move out of experimental sandbox to a proper code base, that integrates with broader stack of entire product. Refactoring into clean, documented functions, tests, I/O contracts, deployment strategy, wrapping in service.&lt;/p&gt;\n\n&lt;p&gt;It does feel like it&amp;#39;s not optimal, as I often tend to revert back to Jupyter to debug or change things up, then returning back to proper project. So I tried cutting Jupyter out of the workflow, to have everything in the same place and avoiding unnecessary extra steps. But it just doesn&amp;#39;t work for me! I can&amp;#39;t seem to figure it out whether it&amp;#39;s the force of habbit, or I&amp;#39;m missing something important. I end up having several sandbox scripts, every py file has some random &amp;#39;if name==main&amp;#39; block to try out functions contained within, etc. &lt;/p&gt;\n\n&lt;p&gt;What is your workflow, as the project gets more complex and you have to maintain a stable code base?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yydh9b", "is_robot_indexable": true, "report_reasons": null, "author": "statespace37", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yydh9b/development_workflow_in_python_prototype_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yydh9b/development_workflow_in_python_prototype_to/", "subreddit_subscribers": 820196, "created_utc": 1668754197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am doing some data science problems for practice, and this is the question I'm currently tackling:\n\n&amp;#x200B;\n\n&gt;Given a list of L values generated independently by some unknown process, we will use the mean of L to predict unseen values generated by the same process. Use leave-one-out cross-validation to estimate the mean absolute error (MAE) of this process.  \n&gt;  \n&gt;Input: An array of floats *arr*  \n&gt;  \n&gt;Output: A float score  \n&gt;  \n&gt;Example:  \n&gt;  \n&gt;\\- arr = \\[1,2,3\\],  \n&gt;  \n&gt;\\- score = 1.0\n\nNow, usually, the input variables (X) and target variable (y) have the same number of rows. But in this case, since it says \"we will use the mean of L to predict unseen values\", what does y look like? Because in the given example, X has just one column, so if we take the mean of X, we will get a scalar value, which gives error when trying to do cross-validation:\n\n&amp;#x200B;\n\n    from sklearn.model_selection import LeaveOneOut, cross_val_score\n    from sklearn.linear_model import LinearRegression\n    import numpy as np\n    \n    # input list of values\n    x = [[2, 5, 4, 3, 4, 6, 7, 5, 8, 9]]\n    \n    # define the output as the mean of the inputs, as specified in the question\n    y = [np.mean(x)]\n    \n    # build multiple linear regression model\n    model = LinearRegression()\n    \n    # define cross-validation method to use\n    cv = LeaveOneOut()\n    \n    # use LOOCV to evaluate model\n    scores = cross_val_score(model, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n    \n    # view mean absolute error\n    np.mean(np.absolute(scores))\n    \n    &gt;&gt;&gt; \n    ---------------------------------------------------------------------------\n    Empty                                     Traceback (most recent call last)\n    File ~/miniforge3/lib/python3.10/site-packages/joblib/parallel.py:862, in Parallel.dispatch_one_batch(self, iterator)\n        861 try:\n    --&gt; 862     tasks = self._ready_batches.get(block=False)\n        863 except queue.Empty:\n        864     # slice the iterator n_jobs * batchsize items at a time. If the\n        865     # slice returns less than that, then the current batchsize puts\n       (...)\n        868     # accordingly to distribute evenly the last items between all\n        869     # workers.\n    \n    File ~/miniforge3/lib/python3.10/queue.py:168, in Queue.get(self, block, timeout)\n        167     if not self._qsize():\n    --&gt; 168         raise Empty\n        169 elif timeout is None:\n    \n    Empty: \n    \n    During handling of the above exception, another exception occurred:\n    \n    ValueError                                Traceback (most recent call last)\n    Input In [70], in &lt;cell line: 18&gt;()\n         15 cv = LeaveOneOut()\n         17 # use LOOCV to evaluate model\n    ---&gt; 18 scores = cross_val_score(model, x, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n         20 # view mean absolute error\n         21 np.mean(np.absolute(scores))\n    \n    File ~/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:515, in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\n        512 # To ensure multimetric format is not supported\n        513 scorer = check_scoring(estimator, scoring=scoring)\n    --&gt; 515 cv_results = cross_validate(\n        516     estimator=estimator,\n        517     X=X,\n        518     y=y,\n        519     groups=groups,\n        520     scoring={\"score\": scorer},\n        521     cv=cv,\n        522     n_jobs=n_jobs,\n        523     verbose=verbose,\n        524     fit_params=fit_params,\n        525     pre_dispatch=pre_dispatch,\n        526     error_score=error_score,\n        527 )\n        528 return cv_results[\"test_score\"]\n    \n    File ~/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266, in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\n        263 # We clone the estimator to make sure that all the folds are\n        264 # independent, and that it is pickle-able.\n        265 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n    --&gt; 266 results = parallel(\n        267     delayed(_fit_and_score)(\n        268         clone(estimator),\n        269         X,\n        270         y,\n        271         scorers,\n        272         train,\n        273         test,\n        274         verbose,\n        275         None,\n        276         fit_params,\n        277         return_train_score=return_train_score,\n        278         return_times=True,\n        279         return_estimator=return_estimator,\n        280         error_score=error_score,\n        281     )\n        282     for train, test in cv.split(X, y, groups)\n        283 )\n        285 _warn_or_raise_about_fit_failures(results, error_score)\n        287 # For callabe scoring, the return type is only know after calling. If the\n        288 # return type is a dictionary, the error scores can now be inserted with\n        289 # the correct key.\n    \n    File ~/miniforge3/lib/python3.10/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable)\n       1076 try:\n       1077     # Only set self._iterating to True if at least a batch\n       1078     # was dispatched. In particular this covers the edge\n       (...)\n       1082     # was very quick and its callback already dispatched all the\n       1083     # remaining jobs.\n       1084     self._iterating = False\n    -&gt; 1085     if self.dispatch_one_batch(iterator):\n       1086         self._iterating = self._original_iterator is not None\n       1088     while self.dispatch_one_batch(iterator):\n    \n    File ~/miniforge3/lib/python3.10/site-packages/joblib/parallel.py:873, in Parallel.dispatch_one_batch(self, iterator)\n        870 n_jobs = self._cached_effective_n_jobs\n        871 big_batch_size = batch_size * n_jobs\n    --&gt; 873 islice = list(itertools.islice(iterator, big_batch_size))\n        874 if len(islice) == 0:\n        875     return False\n    \n    File ~/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266, in &lt;genexpr&gt;(.0)\n        263 # We clone the estimator to make sure that all the folds are\n        264 # independent, and that it is pickle-able.\n        265 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n    --&gt; 266 results = parallel(\n        267     delayed(_fit_and_score)(\n        268         clone(estimator),\n        269         X,\n        270         y,\n        271         scorers,\n        272         train,\n        273         test,\n        274         verbose,\n        275         None,\n        276         fit_params,\n        277         return_train_score=return_train_score,\n        278         return_times=True,\n        279         return_estimator=return_estimator,\n        280         error_score=error_score,\n        281     )\n        282     for train, test in cv.split(X, y, groups)\n        283 )\n        285 _warn_or_raise_about_fit_failures(results, error_score)\n        287 # For callabe scoring, the return type is only know after calling. If the\n        288 # return type is a dictionary, the error scores can now be inserted with\n        289 # the correct key.\n    \n    File ~/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_split.py:86, in BaseCrossValidator.split(self, X, y, groups)\n         84 X, y, groups = indexable(X, y, groups)\n         85 indices = np.arange(_num_samples(X))\n    ---&gt; 86 for test_index in self._iter_test_masks(X, y, groups):\n         87     train_index = indices[np.logical_not(test_index)]\n         88     test_index = indices[test_index]\n    \n    File ~/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_split.py:98, in BaseCrossValidator._iter_test_masks(self, X, y, groups)\n         93 def _iter_test_masks(self, X=None, y=None, groups=None):\n         94     \"\"\"Generates boolean masks corresponding to test sets.\n         95 \n         96     By default, delegates to _iter_test_indices(X, y, groups)\n         97     \"\"\"\n    ---&gt; 98     for test_index in self._iter_test_indices(X, y, groups):\n         99         test_mask = np.zeros(_num_samples(X), dtype=bool)\n        100         test_mask[test_index] = True\n    \n    File ~/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_split.py:163, in LeaveOneOut._iter_test_indices(self, X, y, groups)\n        161 n_samples = _num_samples(X)\n        162 if n_samples &lt;= 1:\n    --&gt; 163     raise ValueError(\n        164         \"Cannot perform LeaveOneOut with n_samples={}.\".format(n_samples)\n        165     )\n        166 return range(n_samples)\n    \n    ValueError: Cannot perform LeaveOneOut with n_samples=1.\n\nCuriously, if I duplicate the contents of X and y, the error goes away, and a \\`score\\` of \\`0.0\\` is outputted:\n\n    # input list of values\n    x = [[2, 5, 4, 3, 4, 6, 7, 5, 8, 9], [2, 5, 4, 3, 4, 6, 7, 5, 8, 9]]\n    \n    # define the output as the mean of the inputs, as specified in the question\n    y = [np.mean(x),np.mean(x)]\n    ...\n    ...\n    ...\n    \n    &gt;&gt;&gt; 0.0\n\nWhy is that?", "author_fullname": "t2_v0d24k7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should the target variable (y) look like here?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyklp6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668780400.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668779565.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am doing some data science problems for practice, and this is the question I&amp;#39;m currently tackling:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Given a list of L values generated independently by some unknown process, we will use the mean of L to predict unseen values generated by the same process. Use leave-one-out cross-validation to estimate the mean absolute error (MAE) of this process.  &lt;/p&gt;\n\n&lt;p&gt;Input: An array of floats &lt;em&gt;arr&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;Output: A float score  &lt;/p&gt;\n\n&lt;p&gt;Example:  &lt;/p&gt;\n\n&lt;p&gt;- arr = [1,2,3],  &lt;/p&gt;\n\n&lt;p&gt;- score = 1.0&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Now, usually, the input variables (X) and target variable (y) have the same number of rows. But in this case, since it says &amp;quot;we will use the mean of L to predict unseen values&amp;quot;, what does y look like? Because in the given example, X has just one column, so if we take the mean of X, we will get a scalar value, which gives error when trying to do cross-validation:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import LeaveOneOut, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# input list of values\nx = [[2, 5, 4, 3, 4, 6, 7, 5, 8, 9]]\n\n# define the output as the mean of the inputs, as specified in the question\ny = [np.mean(x)]\n\n# build multiple linear regression model\nmodel = LinearRegression()\n\n# define cross-validation method to use\ncv = LeaveOneOut()\n\n# use LOOCV to evaluate model\nscores = cross_val_score(model, x, y, scoring=&amp;#39;neg_mean_absolute_error&amp;#39;, cv=cv, n_jobs=-1)\n\n# view mean absolute error\nnp.mean(np.absolute(scores))\n\n&amp;gt;&amp;gt;&amp;gt; \n---------------------------------------------------------------------------\nEmpty                                     Traceback (most recent call last)\nFile ~/miniforge3/lib/python3.10/site-packages/joblib/parallel.py:862, in Parallel.dispatch_one_batch(self, iterator)\n    861 try:\n--&amp;gt; 862     tasks = self._ready_batches.get(block=False)\n    863 except queue.Empty:\n    864     # slice the iterator n_jobs * batchsize items at a time. If the\n    865     # slice returns less than that, then the current batchsize puts\n   (...)\n    868     # accordingly to distribute evenly the last items between all\n    869     # workers.\n\nFile ~/miniforge3/lib/python3.10/queue.py:168, in Queue.get(self, block, timeout)\n    167     if not self._qsize():\n--&amp;gt; 168         raise Empty\n    169 elif timeout is None:\n\nEmpty: \n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nInput In [70], in &amp;lt;cell line: 18&amp;gt;()\n     15 cv = LeaveOneOut()\n     17 # use LOOCV to evaluate model\n---&amp;gt; 18 scores = cross_val_score(model, x, y, scoring=&amp;#39;neg_mean_absolute_error&amp;#39;, cv=cv, n_jobs=-1)\n     20 # view mean absolute error\n     21 np.mean(np.absolute(scores))\n\nFile ~/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:515, in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\n    512 # To ensure multimetric format is not supported\n    513 scorer = check_scoring(estimator, scoring=scoring)\n--&amp;gt; 515 cv_results = cross_validate(\n    516     estimator=estimator,\n    517     X=X,\n    518     y=y,\n    519     groups=groups,\n    520     scoring={&amp;quot;score&amp;quot;: scorer},\n    521     cv=cv,\n    522     n_jobs=n_jobs,\n    523     verbose=verbose,\n    524     fit_params=fit_params,\n    525     pre_dispatch=pre_dispatch,\n    526     error_score=error_score,\n    527 )\n    528 return cv_results[&amp;quot;test_score&amp;quot;]\n\nFile ~/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266, in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\n    263 # We clone the estimator to make sure that all the folds are\n    264 # independent, and that it is pickle-able.\n    265 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n--&amp;gt; 266 results = parallel(\n    267     delayed(_fit_and_score)(\n    268         clone(estimator),\n    269         X,\n    270         y,\n    271         scorers,\n    272         train,\n    273         test,\n    274         verbose,\n    275         None,\n    276         fit_params,\n    277         return_train_score=return_train_score,\n    278         return_times=True,\n    279         return_estimator=return_estimator,\n    280         error_score=error_score,\n    281     )\n    282     for train, test in cv.split(X, y, groups)\n    283 )\n    285 _warn_or_raise_about_fit_failures(results, error_score)\n    287 # For callabe scoring, the return type is only know after calling. If the\n    288 # return type is a dictionary, the error scores can now be inserted with\n    289 # the correct key.\n\nFile ~/miniforge3/lib/python3.10/site-packages/joblib/parallel.py:1085, in Parallel.__call__(self, iterable)\n   1076 try:\n   1077     # Only set self._iterating to True if at least a batch\n   1078     # was dispatched. In particular this covers the edge\n   (...)\n   1082     # was very quick and its callback already dispatched all the\n   1083     # remaining jobs.\n   1084     self._iterating = False\n-&amp;gt; 1085     if self.dispatch_one_batch(iterator):\n   1086         self._iterating = self._original_iterator is not None\n   1088     while self.dispatch_one_batch(iterator):\n\nFile ~/miniforge3/lib/python3.10/site-packages/joblib/parallel.py:873, in Parallel.dispatch_one_batch(self, iterator)\n    870 n_jobs = self._cached_effective_n_jobs\n    871 big_batch_size = batch_size * n_jobs\n--&amp;gt; 873 islice = list(itertools.islice(iterator, big_batch_size))\n    874 if len(islice) == 0:\n    875     return False\n\nFile ~/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266, in &amp;lt;genexpr&amp;gt;(.0)\n    263 # We clone the estimator to make sure that all the folds are\n    264 # independent, and that it is pickle-able.\n    265 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n--&amp;gt; 266 results = parallel(\n    267     delayed(_fit_and_score)(\n    268         clone(estimator),\n    269         X,\n    270         y,\n    271         scorers,\n    272         train,\n    273         test,\n    274         verbose,\n    275         None,\n    276         fit_params,\n    277         return_train_score=return_train_score,\n    278         return_times=True,\n    279         return_estimator=return_estimator,\n    280         error_score=error_score,\n    281     )\n    282     for train, test in cv.split(X, y, groups)\n    283 )\n    285 _warn_or_raise_about_fit_failures(results, error_score)\n    287 # For callabe scoring, the return type is only know after calling. If the\n    288 # return type is a dictionary, the error scores can now be inserted with\n    289 # the correct key.\n\nFile ~/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_split.py:86, in BaseCrossValidator.split(self, X, y, groups)\n     84 X, y, groups = indexable(X, y, groups)\n     85 indices = np.arange(_num_samples(X))\n---&amp;gt; 86 for test_index in self._iter_test_masks(X, y, groups):\n     87     train_index = indices[np.logical_not(test_index)]\n     88     test_index = indices[test_index]\n\nFile ~/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_split.py:98, in BaseCrossValidator._iter_test_masks(self, X, y, groups)\n     93 def _iter_test_masks(self, X=None, y=None, groups=None):\n     94     &amp;quot;&amp;quot;&amp;quot;Generates boolean masks corresponding to test sets.\n     95 \n     96     By default, delegates to _iter_test_indices(X, y, groups)\n     97     &amp;quot;&amp;quot;&amp;quot;\n---&amp;gt; 98     for test_index in self._iter_test_indices(X, y, groups):\n     99         test_mask = np.zeros(_num_samples(X), dtype=bool)\n    100         test_mask[test_index] = True\n\nFile ~/miniforge3/lib/python3.10/site-packages/sklearn/model_selection/_split.py:163, in LeaveOneOut._iter_test_indices(self, X, y, groups)\n    161 n_samples = _num_samples(X)\n    162 if n_samples &amp;lt;= 1:\n--&amp;gt; 163     raise ValueError(\n    164         &amp;quot;Cannot perform LeaveOneOut with n_samples={}.&amp;quot;.format(n_samples)\n    165     )\n    166 return range(n_samples)\n\nValueError: Cannot perform LeaveOneOut with n_samples=1.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Curiously, if I duplicate the contents of X and y, the error goes away, and a `score` of `0.0` is outputted:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# input list of values\nx = [[2, 5, 4, 3, 4, 6, 7, 5, 8, 9], [2, 5, 4, 3, 4, 6, 7, 5, 8, 9]]\n\n# define the output as the mean of the inputs, as specified in the question\ny = [np.mean(x),np.mean(x)]\n...\n...\n...\n\n&amp;gt;&amp;gt;&amp;gt; 0.0\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Why is that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yyklp6", "is_robot_indexable": true, "report_reasons": null, "author": "kristada673", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yyklp6/what_should_the_target_variable_y_look_like_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yyklp6/what_should_the_target_variable_y_look_like_here/", "subreddit_subscribers": 820196, "created_utc": 1668779565.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Are data scientists at major sns companies allowed to view/look at the search history and profile views of specific individuals?", "author_fullname": "t2_6kdtsvf8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can data scientests a major sns companies view profile search history", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy9kuh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.11, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668741119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are data scientists at major sns companies allowed to view/look at the search history and profile views of specific individuals?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yy9kuh", "is_robot_indexable": true, "report_reasons": null, "author": "FunClothes4", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yy9kuh/can_data_scientests_a_major_sns_companies_view/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yy9kuh/can_data_scientests_a_major_sns_companies_view/", "subreddit_subscribers": 820196, "created_utc": 1668741119.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm looking to do some freelancing / start a small data science company. What are some good buzz words / company names ? \n\nThanks !!", "author_fullname": "t2_7roblynl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science Company Names?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy0zld", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.21, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668717682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to do some freelancing / start a small data science company. What are some good buzz words / company names ? &lt;/p&gt;\n\n&lt;p&gt;Thanks !!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yy0zld", "is_robot_indexable": true, "report_reasons": null, "author": "Top_Height4917", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yy0zld/data_science_company_names/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yy0zld/data_science_company_names/", "subreddit_subscribers": 820196, "created_utc": 1668717682.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}