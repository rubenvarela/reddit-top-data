{"kind": "Listing", "data": {"after": "t3_yy978v", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://www.justice.gov/usao-edny/pr/two-russian-nationals-charged-running-massive-e-book-piracy-website", "author_fullname": "t2_7wwsf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The operators of Z-Library arrested in Argentina ti be extradited to the US", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxmtad", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1150, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 1150, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668680985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.justice.gov/usao-edny/pr/two-russian-nationals-charged-running-massive-e-book-piracy-website\"&gt;https://www.justice.gov/usao-edny/pr/two-russian-nationals-charged-running-massive-e-book-piracy-website&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/w93zRzaRJxd5t8UZ0qmwxNnQ42WUkGACAVu4F6kYt5s.jpg?auto=webp&amp;s=b79b5e4c727a2a33f42a03386f69f7f956238969", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/w93zRzaRJxd5t8UZ0qmwxNnQ42WUkGACAVu4F6kYt5s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc20e1aec4443077677bd4c317ecb1150b621862", "width": 108, "height": 108}], "variants": {}, "id": "Xl1_YVzU4YW2LwbKsSkm5qq8NUF5LNfXk_KIiT4bR6w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxmtad", "is_robot_indexable": true, "report_reasons": null, "author": "espero", "discussion_type": null, "num_comments": 331, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxmtad/the_operators_of_zlibrary_arrested_in_argentina/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxmtad/the_operators_of_zlibrary_arrested_in_argentina/", "subreddit_subscribers": 654520, "created_utc": 1668680985.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5s2jw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RIP unlimited Google workspace for education so sad", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_yxuyof", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 200, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 200, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZJzgS5JeIqSiM-S4S_ZMfo9x_ok42HTD7BQN1dBqruA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668703414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.imgur.com/WIq41oo.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?auto=webp&amp;s=0735b911b2f5092d8e3a6f537ab26c58dfdc480b", "width": 1440, "height": 3120}, "resolutions": [{"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f556e8d409447f35d120f43223d24293101359cd", "width": 108, "height": 216}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7b195ecb97e447e8fb122db7364de94d876fd94a", "width": 216, "height": 432}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b630ff166942e4ba407be4f0d2a3262dbc86c259", "width": 320, "height": 640}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5d9d5f1acf434fafac965e8287ab8c8f05f14959", "width": 640, "height": 1280}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ea00e99830f8772720df410dbc757afae117f6e7", "width": 960, "height": 1920}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f3d8a3b27fc330b38d13e49555f63d6a1318a08", "width": 1080, "height": 2160}], "variants": {}, "id": "tXJO2nqpeXoWEfbeINadLiA5MRKOwAiwWes_Q_YJ3iM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxuyof", "is_robot_indexable": true, "report_reasons": null, "author": "UACEENGR", "discussion_type": null, "num_comments": 76, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxuyof/rip_unlimited_google_workspace_for_education_so/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.imgur.com/WIq41oo.jpg", "subreddit_subscribers": 654520, "created_utc": 1668703414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey folks, long time lurker here. I have a bunch of loose external drives that are frequently in use. I have plans to get cats soon and I'm worried they'll be attracted to the noise/heat and knock them all about / on to the ground. Does anybody have any good techniques / strategies / equipment to protect sensitive data from cat attacks?\n\nPS: I already tried \"man cat 1\" in the terminal but didn't find anything useful.", "author_fullname": "t2_3agoq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Catproofing strategies for external drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxig2y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668665424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, long time lurker here. I have a bunch of loose external drives that are frequently in use. I have plans to get cats soon and I&amp;#39;m worried they&amp;#39;ll be attracted to the noise/heat and knock them all about / on to the ground. Does anybody have any good techniques / strategies / equipment to protect sensitive data from cat attacks?&lt;/p&gt;\n\n&lt;p&gt;PS: I already tried &amp;quot;man cat 1&amp;quot; in the terminal but didn&amp;#39;t find anything useful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxig2y", "is_robot_indexable": true, "report_reasons": null, "author": "nzodd", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxig2y/catproofing_strategies_for_external_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxig2y/catproofing_strategies_for_external_drives/", "subreddit_subscribers": 654520, "created_utc": 1668665424.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&gt;Twitter has emailed staffers: \"Hi, Effective immediately, we are temporarily closing our office buildings and all badge access will be suspended. Offices will reopen on Monday, November 21st. .. We look forward to working with you on Twitter\u2019s exciting future.\"\n\n&amp;#x200B;\n\n&gt;Story to be updated soon with more: Am hearing that several \u201ccritical\u201d infra engineering teams at Twitter have completely resigned. \u201cYou cannot run Twitter without this team,\u201d one current engineer tells me of one such group. Also, Twitter has shut off badge access to its offices.\n\n&amp;#x200B;\n\n&gt;What I\u2019m hearing from Twitter employees; It looks like roughly 75% of the remaining 3,700ish Twitter employees have not opted to stay after the \u201chardcore\u201d email.  \n&gt;  \n&gt;Even though the deadline has passed, everyone still has access to their systems.\n\n&amp;#x200B;\n\n&gt;\u201cI know of six critical systems (like \u2018serving tweets\u2019 levels of critical) which no longer have any engineers,\" the former employee said. \"There is no longer even a skeleton crew manning the system. It will continue to coast until it runs into something, and then it will stop.\u201d  \n&gt;  \n&gt;Resignations and departures were already taking a toll on Twitter\u2019s service, employees said. \u201cBreakages are already happening slowly and accumulating,\u201d one said. \u201cIf you want to export your tweets, do it now.\u201d\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Link 1](https://twitter.com/oliverdarcy/status/1593394621627138048)\n\n[Link 2](https://twitter.com/alexeheath/status/1593399683086327808)\n\n[Link 3](https://twitter.com/kyliebytes/status/1593391167718113280)\n\n[Link 4](https://www.washingtonpost.com/technology/2022/11/17/twitter-musk-easing-rto-order/)\n\n&amp;#x200B;\n\nEdit:\n\n[twitter-scraper (github no api-key needed)](https://github.com/n0madic/twitter-scraper)\n\n[twitter-media-downloader (github no api-key needed)](https://github.com/mmpx12/twitter-media-downloader)\n\n&amp;#x200B;\n\nEdit2:\n\n[https://github.com/markowanga/stweet](https://github.com/markowanga/stweet)\n\n&amp;#x200B;", "author_fullname": "t2_ioi0j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup twitter now! Multiple critical infra teams have resigned", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy7tig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 56, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 56, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668737805.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668735892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Twitter has emailed staffers: &amp;quot;Hi, Effective immediately, we are temporarily closing our office buildings and all badge access will be suspended. Offices will reopen on Monday, November 21st. .. We look forward to working with you on Twitter\u2019s exciting future.&amp;quot;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Story to be updated soon with more: Am hearing that several \u201ccritical\u201d infra engineering teams at Twitter have completely resigned. \u201cYou cannot run Twitter without this team,\u201d one current engineer tells me of one such group. Also, Twitter has shut off badge access to its offices.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;What I\u2019m hearing from Twitter employees; It looks like roughly 75% of the remaining 3,700ish Twitter employees have not opted to stay after the \u201chardcore\u201d email.  &lt;/p&gt;\n\n&lt;p&gt;Even though the deadline has passed, everyone still has access to their systems.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;\u201cI know of six critical systems (like \u2018serving tweets\u2019 levels of critical) which no longer have any engineers,&amp;quot; the former employee said. &amp;quot;There is no longer even a skeleton crew manning the system. It will continue to coast until it runs into something, and then it will stop.\u201d  &lt;/p&gt;\n\n&lt;p&gt;Resignations and departures were already taking a toll on Twitter\u2019s service, employees said. \u201cBreakages are already happening slowly and accumulating,\u201d one said. \u201cIf you want to export your tweets, do it now.\u201d&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/oliverdarcy/status/1593394621627138048\"&gt;Link 1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/alexeheath/status/1593399683086327808\"&gt;Link 2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/kyliebytes/status/1593391167718113280\"&gt;Link 3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.washingtonpost.com/technology/2022/11/17/twitter-musk-easing-rto-order/\"&gt;Link 4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/n0madic/twitter-scraper\"&gt;twitter-scraper (github no api-key needed)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mmpx12/twitter-media-downloader\"&gt;twitter-media-downloader (github no api-key needed)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit2:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/markowanga/stweet\"&gt;https://github.com/markowanga/stweet&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yy7tig", "is_robot_indexable": true, "report_reasons": null, "author": "fourDnet", "discussion_type": null, "num_comments": 67, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy7tig/backup_twitter_now_multiple_critical_infra_teams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy7tig/backup_twitter_now_multiple_critical_infra_teams/", "subreddit_subscribers": 654520, "created_utc": 1668735892.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I am probably way out of my depth here but r/Twitter redirected me here when I asked this question. In case Twitter actually does go away I am trying to download the entire Twitter feed of my best friend who passed away 5 years ago. I\u2019m currently just screenshotting favorites in a panic and googling returns old workarounds that don\u2019t work anymore. Any advice at all is appreciated, thank you.", "author_fullname": "t2_eo84n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download An Entire Twitter Feed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy8dii", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668737540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am probably way out of my depth here but &lt;a href=\"/r/Twitter\"&gt;r/Twitter&lt;/a&gt; redirected me here when I asked this question. In case Twitter actually does go away I am trying to download the entire Twitter feed of my best friend who passed away 5 years ago. I\u2019m currently just screenshotting favorites in a panic and googling returns old workarounds that don\u2019t work anymore. Any advice at all is appreciated, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy8dii", "is_robot_indexable": true, "report_reasons": null, "author": "plaidtuxedo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy8dii/download_an_entire_twitter_feed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy8dii/download_an_entire_twitter_feed/", "subreddit_subscribers": 654520, "created_utc": 1668737540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to figure out how I want to backup the data of my daily driver. It's about 1.5 TB worth of documents and family pictures/videos. I don't need to access the data daily. I'd only ever need it if I lost all my data and needed to restore/recover.\n\nI don't need or want to backup everything on my disk. I have specific folders I'd select and then would, ideally, want to exclude certain sub-folders like `.git` or `node_modules`.\n\nI've been researching this for days and ... I just need some outside perspective. Here are my thoughts:\n\n* BackBlaze Personal Backup is $7 a month for unlimited backups. But the backup client backups everything and you have to specify all the excludes. The UI is cumbersome as hell and I'll have to edit an XML file to get it to do what I want (like exclude any `.git` folder). I'd only have to edit the file once, probably, but it's still a PITA. \n* I could use something like Duplicati or Duplicacy with B2 or S3 Glacier. I don't have experience with either so I'm not sure.\n* S3 Glacier seems cheaper but I'm not sure if that's necessarily a good thing over B2.\n\nI'm pretty tech savvy but I want something as simple as possible here. Ideally I'd select the root level folders I want to back up, set some kind of encryption, and then have them auto back up, with versions.\n\nBeing able to remotely access files when I'm not at home is an added bonus but not  a must. If I really needed that I'd just install Nextcloud or something.", "author_fullname": "t2_5wpob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need help with my analysis paralysis: BackBlaze unlimited vs Duplicati or Duplicacy with B2 or S3 Glacier", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxfwx3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668657651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to figure out how I want to backup the data of my daily driver. It&amp;#39;s about 1.5 TB worth of documents and family pictures/videos. I don&amp;#39;t need to access the data daily. I&amp;#39;d only ever need it if I lost all my data and needed to restore/recover.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t need or want to backup everything on my disk. I have specific folders I&amp;#39;d select and then would, ideally, want to exclude certain sub-folders like &lt;code&gt;.git&lt;/code&gt; or &lt;code&gt;node_modules&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been researching this for days and ... I just need some outside perspective. Here are my thoughts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;BackBlaze Personal Backup is $7 a month for unlimited backups. But the backup client backups everything and you have to specify all the excludes. The UI is cumbersome as hell and I&amp;#39;ll have to edit an XML file to get it to do what I want (like exclude any &lt;code&gt;.git&lt;/code&gt; folder). I&amp;#39;d only have to edit the file once, probably, but it&amp;#39;s still a PITA. &lt;/li&gt;\n&lt;li&gt;I could use something like Duplicati or Duplicacy with B2 or S3 Glacier. I don&amp;#39;t have experience with either so I&amp;#39;m not sure.&lt;/li&gt;\n&lt;li&gt;S3 Glacier seems cheaper but I&amp;#39;m not sure if that&amp;#39;s necessarily a good thing over B2.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m pretty tech savvy but I want something as simple as possible here. Ideally I&amp;#39;d select the root level folders I want to back up, set some kind of encryption, and then have them auto back up, with versions.&lt;/p&gt;\n\n&lt;p&gt;Being able to remotely access files when I&amp;#39;m not at home is an added bonus but not  a must. If I really needed that I&amp;#39;d just install Nextcloud or something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxfwx3", "is_robot_indexable": true, "report_reasons": null, "author": "imthenachoman", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxfwx3/i_need_help_with_my_analysis_paralysis_backblaze/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxfwx3/i_need_help_with_my_analysis_paralysis_backblaze/", "subreddit_subscribers": 654520, "created_utc": 1668657651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there usually good deals for hard drives on these days? I'm finally building my first NAS and want to find good deals obviously.   Is it worth waiting for Black Friday / Cyber Monday deals or are they going to be just crap drives?\n\nThanks!", "author_fullname": "t2_140qwd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Black Friday/ Cyber Monday deals?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxw0kh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668705932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there usually good deals for hard drives on these days? I&amp;#39;m finally building my first NAS and want to find good deals obviously.   Is it worth waiting for Black Friday / Cyber Monday deals or are they going to be just crap drives?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxw0kh", "is_robot_indexable": true, "report_reasons": null, "author": "Mastasmoker", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxw0kh/black_friday_cyber_monday_deals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxw0kh/black_friday_cyber_monday_deals/", "subreddit_subscribers": 654520, "created_utc": 1668705932.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "hello my gaming pc from 2016  has been very slow the past 2 years i was wondering if my HDD being at 100% in the task manager had anything to do with it so I've been researching and got to crystaldiskinfo. i am looking and it says i have 662 reallocated sectors and 712 current pending sectors i was wondering what any of this means.", "author_fullname": "t2_208j5c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "reallocated sector", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxjjwf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668669104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hello my gaming pc from 2016  has been very slow the past 2 years i was wondering if my HDD being at 100% in the task manager had anything to do with it so I&amp;#39;ve been researching and got to crystaldiskinfo. i am looking and it says i have 662 reallocated sectors and 712 current pending sectors i was wondering what any of this means.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxjjwf", "is_robot_indexable": true, "report_reasons": null, "author": "IWILLREFORM", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxjjwf/reallocated_sector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxjjwf/reallocated_sector/", "subreddit_subscribers": 654520, "created_utc": 1668669104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "# UPDATE 1: I FORGOT THE PART THAT MAKES TEXT-TWEETS ACTUALLY WORK\n\nI put this together in a hurry (frankly I should've done this a month ago) but I've been archiving a ton of stuff with gallery-dl for the better part of 2 years so I think I know what I'm doing\n\nI'll be providing tech support in the comments so if anyone wants help please don't be afraid to ask. I was where you are now.\n\n## For the config.json:\n\nTo get text-only tweets, you need to set `\"text-tweets\"` to true **AND** have the following\n\n    \"postprocessors\":[\n        {\"name\": \"metadata\", \"event\": \"post\", \"filename\": \"{tweet_id}_main.json\"}\n    ]\n\n- Set `\"replies\"` to true\n- Set `\"retweets\"` and `\"quoted\"` to true if you want retweets and quote retweets\n- Set `\"logout\"` to `true` if you want to backup someone who has you blocked\n\n[Use browser cookies](https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst#extractorcookies). This makes life with gallery-dl so much easier\n\n## URLs:\n\nBecause the twitter API frankly just sucks, you gotta get all of these URLs to make sure you get most of the person you want\n\nYou'll need to pass `-o skip=true` into the command to make it work right\n\n- `https://www.twitter.com/USER`\n- `https://www.twitter.com/USER/media`\n- `https://twitter.com/search?q=from:USER`\n\nIt still won't get everything but it'll get most of it. Do each a few times to get what it missed\n\n***ALWAYS*** use `--write-metadata`. Even with the postprocessor for text-only tweets, this flag gets per-image metadata\n\nYour config.json should look like this\n\n    {\n        \"extractor\":{\n            \"twitter\":{\n                \"text-tweets\":true,\n                \"retweets\":true,\n                \"qouted\":true,\n                \"logout\":true,\n                \"replies\":true,\n                \"postprocessors\":[\n                    {\"name\": \"metadata\", \"event\": \"post\", \"filename\": \"{tweet_id}_main.json\"}\n                ]\n            }\n        }\n    }\n\nAnd the commands you're running should look like `gallery-dl URL --write-metadata -o skip=true`\n\nIf you can, make a `.bat` or `.sh` or whatever file that has the above command for everyone you want to archive and just let it run overnight\n\nIf you have [`jq`](https://stedolan.github.io/jq/) installed you can run `gallery-dl https://twitter.com/YOUR HANDLE/following --dump-json | jq \".[][2].legacy.screen_name|\"\"https://twitter.com/\"\"\"\"+.\" -r` to get a list of everyone you follow\n\nBecause both windows command prompt just kinda sucks at handling quotation marks, that'll only work on windows. Also if I try to make it generate the full command jq just complains so uh, sorry. If your text editor has a regex replace feature just replace `^` with `gallery-dl \"` and then `$` with `\" --write-metadata -o skip=true`", "author_fullname": "t2_yj3jz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For everyone using gallery-dl to backup twitter: Make sure you do it right", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy8o9w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668742778.0, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668738416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;UPDATE 1: I FORGOT THE PART THAT MAKES TEXT-TWEETS ACTUALLY WORK&lt;/h1&gt;\n\n&lt;p&gt;I put this together in a hurry (frankly I should&amp;#39;ve done this a month ago) but I&amp;#39;ve been archiving a ton of stuff with gallery-dl for the better part of 2 years so I think I know what I&amp;#39;m doing&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll be providing tech support in the comments so if anyone wants help please don&amp;#39;t be afraid to ask. I was where you are now.&lt;/p&gt;\n\n&lt;h2&gt;For the config.json:&lt;/h2&gt;\n\n&lt;p&gt;To get text-only tweets, you need to set &lt;code&gt;&amp;quot;text-tweets&amp;quot;&lt;/code&gt; to true &lt;strong&gt;AND&lt;/strong&gt; have the following&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;quot;postprocessors&amp;quot;:[\n    {&amp;quot;name&amp;quot;: &amp;quot;metadata&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;post&amp;quot;, &amp;quot;filename&amp;quot;: &amp;quot;{tweet_id}_main.json&amp;quot;}\n]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Set &lt;code&gt;&amp;quot;replies&amp;quot;&lt;/code&gt; to true&lt;/li&gt;\n&lt;li&gt;Set &lt;code&gt;&amp;quot;retweets&amp;quot;&lt;/code&gt; and &lt;code&gt;&amp;quot;quoted&amp;quot;&lt;/code&gt; to true if you want retweets and quote retweets&lt;/li&gt;\n&lt;li&gt;Set &lt;code&gt;&amp;quot;logout&amp;quot;&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; if you want to backup someone who has you blocked&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst#extractorcookies\"&gt;Use browser cookies&lt;/a&gt;. This makes life with gallery-dl so much easier&lt;/p&gt;\n\n&lt;h2&gt;URLs:&lt;/h2&gt;\n\n&lt;p&gt;Because the twitter API frankly just sucks, you gotta get all of these URLs to make sure you get most of the person you want&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;ll need to pass &lt;code&gt;-o skip=true&lt;/code&gt; into the command to make it work right&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;https://www.twitter.com/USER&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;https://www.twitter.com/USER/media&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;https://twitter.com/search?q=from:USER&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It still won&amp;#39;t get everything but it&amp;#39;ll get most of it. Do each a few times to get what it missed&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;ALWAYS&lt;/em&gt;&lt;/strong&gt; use &lt;code&gt;--write-metadata&lt;/code&gt;. Even with the postprocessor for text-only tweets, this flag gets per-image metadata&lt;/p&gt;\n\n&lt;p&gt;Your config.json should look like this&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n    &amp;quot;extractor&amp;quot;:{\n        &amp;quot;twitter&amp;quot;:{\n            &amp;quot;text-tweets&amp;quot;:true,\n            &amp;quot;retweets&amp;quot;:true,\n            &amp;quot;qouted&amp;quot;:true,\n            &amp;quot;logout&amp;quot;:true,\n            &amp;quot;replies&amp;quot;:true,\n            &amp;quot;postprocessors&amp;quot;:[\n                {&amp;quot;name&amp;quot;: &amp;quot;metadata&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;post&amp;quot;, &amp;quot;filename&amp;quot;: &amp;quot;{tweet_id}_main.json&amp;quot;}\n            ]\n        }\n    }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And the commands you&amp;#39;re running should look like &lt;code&gt;gallery-dl URL --write-metadata -o skip=true&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;If you can, make a &lt;code&gt;.bat&lt;/code&gt; or &lt;code&gt;.sh&lt;/code&gt; or whatever file that has the above command for everyone you want to archive and just let it run overnight&lt;/p&gt;\n\n&lt;p&gt;If you have &lt;a href=\"https://stedolan.github.io/jq/\"&gt;&lt;code&gt;jq&lt;/code&gt;&lt;/a&gt; installed you can run &lt;code&gt;gallery-dl https://twitter.com/YOUR HANDLE/following --dump-json | jq &amp;quot;.[][2].legacy.screen_name|&amp;quot;&amp;quot;https://twitter.com/&amp;quot;&amp;quot;&amp;quot;&amp;quot;+.&amp;quot; -r&lt;/code&gt; to get a list of everyone you follow&lt;/p&gt;\n\n&lt;p&gt;Because both windows command prompt just kinda sucks at handling quotation marks, that&amp;#39;ll only work on windows. Also if I try to make it generate the full command jq just complains so uh, sorry. If your text editor has a regex replace feature just replace &lt;code&gt;^&lt;/code&gt; with &lt;code&gt;gallery-dl &amp;quot;&lt;/code&gt; and then &lt;code&gt;$&lt;/code&gt; with &lt;code&gt;&amp;quot; --write-metadata -o skip=true&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/73z_pwHg8hEVhsVVYQC_gqIj3BM2IwuVKgJj-1RXKoA.jpg?auto=webp&amp;s=c511b5d7bc447e6922b8c20997d7833ab62ebc43", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/73z_pwHg8hEVhsVVYQC_gqIj3BM2IwuVKgJj-1RXKoA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dac22edf6320acff6c40a952134deca92841f592", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/73z_pwHg8hEVhsVVYQC_gqIj3BM2IwuVKgJj-1RXKoA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0faec43b6aeb14c38d88ee6028ae03dc4968aad4", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/73z_pwHg8hEVhsVVYQC_gqIj3BM2IwuVKgJj-1RXKoA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2afd51ba49db6b5fbc776dc87883707fdd0c44a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/73z_pwHg8hEVhsVVYQC_gqIj3BM2IwuVKgJj-1RXKoA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3fc956fe02d9dbfacb9ae96e4dbd14f3109e588c", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/73z_pwHg8hEVhsVVYQC_gqIj3BM2IwuVKgJj-1RXKoA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a4a7d493128ce7f3aa4511792bb180f1f6afa10", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/73z_pwHg8hEVhsVVYQC_gqIj3BM2IwuVKgJj-1RXKoA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bdf4a91627ccdb210bdbc4de1103f8742f6ed0b3", "width": 1080, "height": 540}], "variants": {}, "id": "jni-9Krbw5obF8RU44n83V1dZE1FU1weixmQmu5Kb_E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "The sexiest data storage medium", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy8o9w", "is_robot_indexable": true, "report_reasons": null, "author": "Scripter17", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/", "subreddit_subscribers": 654520, "created_utc": 1668738416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Putting together a small NAS with 6 x 4TB HGST SAS drives, HUS724040ALS640s, and was lucky enough to snag two of them as unused spares for \u00a330 a pop. The next four I've just picked up, and the first couple have come back with roughly 25,000 hours on them.\n\nObviously any reports are going to be anecdotal and not a guarantee, but I'm curious as to if any of you have experience with used enterprise drives and reliability in that region of use.\n\nFrom my own personal experience, it seems that if a drive is destined to die it tends to die relatively quickly, and the ones I've had in the past that made it to 10,000 hours typically lasted another 50k+.\n\nUltimately I spent \u00a330 a pop on each of these, so I'm tempted to just consider it a good deal regardless and put it out of my mind. Am I crazy?", "author_fullname": "t2_7cqcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Used Enterprise Drives and Power On Hours", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxsz7h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668698470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Putting together a small NAS with 6 x 4TB HGST SAS drives, HUS724040ALS640s, and was lucky enough to snag two of them as unused spares for \u00a330 a pop. The next four I&amp;#39;ve just picked up, and the first couple have come back with roughly 25,000 hours on them.&lt;/p&gt;\n\n&lt;p&gt;Obviously any reports are going to be anecdotal and not a guarantee, but I&amp;#39;m curious as to if any of you have experience with used enterprise drives and reliability in that region of use.&lt;/p&gt;\n\n&lt;p&gt;From my own personal experience, it seems that if a drive is destined to die it tends to die relatively quickly, and the ones I&amp;#39;ve had in the past that made it to 10,000 hours typically lasted another 50k+.&lt;/p&gt;\n\n&lt;p&gt;Ultimately I spent \u00a330 a pop on each of these, so I&amp;#39;m tempted to just consider it a good deal regardless and put it out of my mind. Am I crazy?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yxsz7h", "is_robot_indexable": true, "report_reasons": null, "author": "mdcdesign", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxsz7h/used_enterprise_drives_and_power_on_hours/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxsz7h/used_enterprise_drives_and_power_on_hours/", "subreddit_subscribers": 654520, "created_utc": 1668698470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This cooking website that I have followed for years has a clean, printable version of every recipe on a very similar web address that all follow the same format.\n\n[https://www.halfbakedharvest.com/wprm\\_print/26284](https://www.halfbakedharvest.com/wprm_print/26284)\n\n[https://www.halfbakedharvest.com/wprm\\_print/108579](https://www.halfbakedharvest.com/wprm_print/108579)\n\n[https://www.halfbakedharvest.com/wprm\\_print/110135](https://www.halfbakedharvest.com/wprm_print/110135)\n\n[https://www.halfbakedharvest.com/wprm\\_print/118540](https://www.halfbakedharvest.com/wprm_print/118540)\n\nThere are hundreds of recipes, but the numbers at the end of the address jumps at random intervals (ex. 540, 588, 607, 690). Is there someway a scraper or something could go through and check every number to test if a page is there and then download the webpage?", "author_fullname": "t2_hd6cp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a way to save recipes webpages from similar web addresses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxx8pt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668708898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This cooking website that I have followed for years has a clean, printable version of every recipe on a very similar web address that all follow the same format.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.halfbakedharvest.com/wprm_print/26284\"&gt;https://www.halfbakedharvest.com/wprm_print/26284&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.halfbakedharvest.com/wprm_print/108579\"&gt;https://www.halfbakedharvest.com/wprm_print/108579&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.halfbakedharvest.com/wprm_print/110135\"&gt;https://www.halfbakedharvest.com/wprm_print/110135&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.halfbakedharvest.com/wprm_print/118540\"&gt;https://www.halfbakedharvest.com/wprm_print/118540&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There are hundreds of recipes, but the numbers at the end of the address jumps at random intervals (ex. 540, 588, 607, 690). Is there someway a scraper or something could go through and check every number to test if a page is there and then download the webpage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxx8pt", "is_robot_indexable": true, "report_reasons": null, "author": "1000yardstareslacker", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxx8pt/looking_for_a_way_to_save_recipes_webpages_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxx8pt/looking_for_a_way_to_save_recipes_webpages_from/", "subreddit_subscribers": 654520, "created_utc": 1668708898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A couple years ago i was really into old games and collected a lot of them from yard sales, ebay, traded them with friends. I knew what i already got for the most part, but i never really made a list/labeled them. In total i guess somewhere around 250 floppys &amp; 4-5000 cds.\n\nSince many of them are nearly 20 years some even older, i am looking for a way to digitalis them.\n\n**The first thing, is there an efficient way/program to convert them to isos?**\n\n**The second thing, are you familiar with any archive/collection management program thats selfhosted and where you could link the said isos?**   \n(Like Collectorz just Selfhosted, Browserbased &amp; OpenSource)\n\nThanks for any ideas", "author_fullname": "t2_8jnr5wv6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Efficient Way to Backup/Archive many Gamedisks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxns42", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668684298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A couple years ago i was really into old games and collected a lot of them from yard sales, ebay, traded them with friends. I knew what i already got for the most part, but i never really made a list/labeled them. In total i guess somewhere around 250 floppys &amp;amp; 4-5000 cds.&lt;/p&gt;\n\n&lt;p&gt;Since many of them are nearly 20 years some even older, i am looking for a way to digitalis them.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The first thing, is there an efficient way/program to convert them to isos?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The second thing, are you familiar with any archive/collection management program thats selfhosted and where you could link the said isos?&lt;/strong&gt;&lt;br/&gt;\n(Like Collectorz just Selfhosted, Browserbased &amp;amp; OpenSource)&lt;/p&gt;\n\n&lt;p&gt;Thanks for any ideas&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxns42", "is_robot_indexable": true, "report_reasons": null, "author": "Pommes254", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxns42/efficient_way_to_backuparchive_many_gamedisks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxns42/efficient_way_to_backuparchive_many_gamedisks/", "subreddit_subscribers": 654520, "created_utc": 1668684298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Perhaps I\u2019m just lucky and haven\u2019t lost a drive but I feel like between SMART monitoring and using single addressable drives my 24 bay media server using drives on their own is running well.\n\nI\u2019ve been considering some pooled sort of RAID like unraid or omv zfs for many years, and realize it means the loss of one drive for parity, but overall the bigger fear is that something will go wrong that will trash the entire pool of Files versus just one drives worth of content - tv and movie files which could be replaced. Anyone else have similar fears and how did you resolve? I just worry the implosion of say 100tb of a pool would be riskier than losing a regular 14tb drive. My drives are all of various size btw ranging from 5tb to 14tb all gutted from desktop cases so not bleeding edge. Also isn\u2019t raid going to run slower than jbod on my 4u Xeon rackmount?", "author_fullname": "t2_4jntazsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RAID Risks - Anyone left using only JBOD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxnrss", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668684268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Perhaps I\u2019m just lucky and haven\u2019t lost a drive but I feel like between SMART monitoring and using single addressable drives my 24 bay media server using drives on their own is running well.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been considering some pooled sort of RAID like unraid or omv zfs for many years, and realize it means the loss of one drive for parity, but overall the bigger fear is that something will go wrong that will trash the entire pool of Files versus just one drives worth of content - tv and movie files which could be replaced. Anyone else have similar fears and how did you resolve? I just worry the implosion of say 100tb of a pool would be riskier than losing a regular 14tb drive. My drives are all of various size btw ranging from 5tb to 14tb all gutted from desktop cases so not bleeding edge. Also isn\u2019t raid going to run slower than jbod on my 4u Xeon rackmount?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxnrss", "is_robot_indexable": true, "report_reasons": null, "author": "ExcitingDegree", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxnrss/raid_risks_anyone_left_using_only_jbod/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxnrss/raid_risks_anyone_left_using_only_jbod/", "subreddit_subscribers": 654520, "created_utc": 1668684268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1egq3y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding the UBI File System in Embedded Devices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_yxm2v5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/p2KwVnIXS5RIWyEbUk0K8spJZGi2P-R5tF-a9cMQObE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668678336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "serhack.me", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://serhack.me/articles/understanding-ubi-file-system-embedded-devices-reolink/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Na7cLVk3-hR2SoZo2UIPnR0MXEW43mRO_x09p4aHf74.jpg?auto=webp&amp;s=00078a38d692ba8754d89738ee89ea6e4b9e8c19", "width": 800, "height": 450}, "resolutions": [{"url": "https://external-preview.redd.it/Na7cLVk3-hR2SoZo2UIPnR0MXEW43mRO_x09p4aHf74.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49133ab284e90d376be0f0b56abc666023dcfeaa", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/Na7cLVk3-hR2SoZo2UIPnR0MXEW43mRO_x09p4aHf74.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=365be787bd7e64ffb8060bab75adadd2059c7267", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/Na7cLVk3-hR2SoZo2UIPnR0MXEW43mRO_x09p4aHf74.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0192f4547be6c29445b1d2376d14a324d9cf0109", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/Na7cLVk3-hR2SoZo2UIPnR0MXEW43mRO_x09p4aHf74.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=38415530d6caba0e9880f6a29b456efdf0d969d6", "width": 640, "height": 360}], "variants": {}, "id": "HfVseBBMzCg0VO3uFfUhboKSYGovmAWqQX0RMkMeYVk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yxm2v5", "is_robot_indexable": true, "report_reasons": null, "author": "serhack", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxm2v5/understanding_the_ubi_file_system_in_embedded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://serhack.me/articles/understanding-ubi-file-system-embedded-devices-reolink/", "subreddit_subscribers": 654520, "created_utc": 1668678336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is the standard recommendation basically Fractal Design Define 7 XL?\n\nI looked at the Hardware Wiki and it looks like it is in bad need for some love and updates since a lot of the links don't even work!\n\nI'd like to know what most people are running/recommending here for \\~12x 3.5\" and probably at least 2x 5.25\" and 2x 2.5\". Hot swap is a nice-to-have, but not required.\n\nNote that I'm not interested in server U cases. I'd rather it take more space vertically than horizontally. Plus in my experience of owning some enterprise gears, they tend to be loud and I'm trying to steer away from that.", "author_fullname": "t2_5xyrdccu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good tower case in 2022", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxk1ew", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668670897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is the standard recommendation basically Fractal Design Define 7 XL?&lt;/p&gt;\n\n&lt;p&gt;I looked at the Hardware Wiki and it looks like it is in bad need for some love and updates since a lot of the links don&amp;#39;t even work!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to know what most people are running/recommending here for ~12x 3.5&amp;quot; and probably at least 2x 5.25&amp;quot; and 2x 2.5&amp;quot;. Hot swap is a nice-to-have, but not required.&lt;/p&gt;\n\n&lt;p&gt;Note that I&amp;#39;m not interested in server U cases. I&amp;#39;d rather it take more space vertically than horizontally. Plus in my experience of owning some enterprise gears, they tend to be loud and I&amp;#39;m trying to steer away from that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yxk1ew", "is_robot_indexable": true, "report_reasons": null, "author": "whattteva", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxk1ew/good_tower_case_in_2022/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxk1ew/good_tower_case_in_2022/", "subreddit_subscribers": 654520, "created_utc": 1668670897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all.\n\nI am thinking of building a DIY DAS, with a SilverStone DS380 and 8xHDD, but I need something odd that maybe does not exist.\n\nI would need to connect my DAS through an USB interface (USB3.0 is enough for mechanical HDD), so I am looking for an interface that convert the SATA or MiniSAS connectors to an external USB connector, preferably with a PCI bracket to fit the PCI slots of the case.\n\nTo sum up, I need something like a SSF8087 to SSF8088 adapter (pic below) but SSF8087 to USB instead.\n\nhttps://tinypic.host/i/yhZkj\n\nI do not find anything similar to it, but thinking about all the commercial DAS (Yottamaster, IcyBox, Mediasonic...), they all have an external USB interface, that expose the SATA/SAS HDD in the interior, so the card must exist. \n\nThanks in advance.", "author_fullname": "t2_14jgu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SATA or MiniSAS (internal) to USB (external) adapter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy2qyi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668722266.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668721975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all.&lt;/p&gt;\n\n&lt;p&gt;I am thinking of building a DIY DAS, with a SilverStone DS380 and 8xHDD, but I need something odd that maybe does not exist.&lt;/p&gt;\n\n&lt;p&gt;I would need to connect my DAS through an USB interface (USB3.0 is enough for mechanical HDD), so I am looking for an interface that convert the SATA or MiniSAS connectors to an external USB connector, preferably with a PCI bracket to fit the PCI slots of the case.&lt;/p&gt;\n\n&lt;p&gt;To sum up, I need something like a SSF8087 to SSF8088 adapter (pic below) but SSF8087 to USB instead.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://tinypic.host/i/yhZkj\"&gt;https://tinypic.host/i/yhZkj&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I do not find anything similar to it, but thinking about all the commercial DAS (Yottamaster, IcyBox, Mediasonic...), they all have an external USB interface, that expose the SATA/SAS HDD in the interior, so the card must exist. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rr7MNbCppznpfkih2duPrin_J0xv-DiwE8KMSvWtDjA.jpg?auto=webp&amp;s=9b207d0abedb366d4381989cda212ee9d7574de3", "width": 1080, "height": 653}, "resolutions": [{"url": "https://external-preview.redd.it/rr7MNbCppznpfkih2duPrin_J0xv-DiwE8KMSvWtDjA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c756591550f48c6bce45456d7cb4acb5a8d804af", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/rr7MNbCppznpfkih2duPrin_J0xv-DiwE8KMSvWtDjA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=63b9020eea214892cd1125b63e546c36bac2af0c", "width": 216, "height": 130}, {"url": "https://external-preview.redd.it/rr7MNbCppznpfkih2duPrin_J0xv-DiwE8KMSvWtDjA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4d2d4a0d43b5d9a6a877d265f8dca65ecbdf88b", "width": 320, "height": 193}, {"url": "https://external-preview.redd.it/rr7MNbCppznpfkih2duPrin_J0xv-DiwE8KMSvWtDjA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6afe2d78de2ed2c55554560aea202cc6feb55873", "width": 640, "height": 386}, {"url": "https://external-preview.redd.it/rr7MNbCppznpfkih2duPrin_J0xv-DiwE8KMSvWtDjA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=51e614fa4c38d2671d98e93289a003bd373a449f", "width": 960, "height": 580}, {"url": "https://external-preview.redd.it/rr7MNbCppznpfkih2duPrin_J0xv-DiwE8KMSvWtDjA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=10feb3bb48f86b851b6c59cde4d15e743f8df4ca", "width": 1080, "height": 653}], "variants": {}, "id": "DQVGuU-HCHY8fxeMmUx1dMx7UmiZDidB6gsy-HQmVRc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy2qyi", "is_robot_indexable": true, "report_reasons": null, "author": "jfromeo", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy2qyi/sata_or_minisas_internal_to_usb_external_adapter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy2qyi/sata_or_minisas_internal_to_usb_external_adapter/", "subreddit_subscribers": 654520, "created_utc": 1668721975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've never done RAID/ZFS before, but now I need to keep files and have them with higher availability so not just backed up. Until now I was simply copying the data to 2 separate disks because the availability was not that important \n\nBut now I want to make sure that I can always access the files because I need to serve them from a web server\n\nWhat solution do I need? ZFS? RAID?\n\nI need to be able to access the files as if I would access regular disk on the system\n\nI was planning to install it on a separate VM on the network (It's a little private project I'm doing, nothing too important)", "author_fullname": "t2_58tud67t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I need ZFS or RAID in this case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy2qb4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668721930.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve never done RAID/ZFS before, but now I need to keep files and have them with higher availability so not just backed up. Until now I was simply copying the data to 2 separate disks because the availability was not that important &lt;/p&gt;\n\n&lt;p&gt;But now I want to make sure that I can always access the files because I need to serve them from a web server&lt;/p&gt;\n\n&lt;p&gt;What solution do I need? ZFS? RAID?&lt;/p&gt;\n\n&lt;p&gt;I need to be able to access the files as if I would access regular disk on the system&lt;/p&gt;\n\n&lt;p&gt;I was planning to install it on a separate VM on the network (It&amp;#39;s a little private project I&amp;#39;m doing, nothing too important)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy2qb4", "is_robot_indexable": true, "report_reasons": null, "author": "ligonsker", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy2qb4/do_i_need_zfs_or_raid_in_this_case/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy2qb4/do_i_need_zfs_or_raid_in_this_case/", "subreddit_subscribers": 654520, "created_utc": 1668721930.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "About 5 months ago, arguably the largest database of doujinshi and manga (Doujinshi.org) seemingly went down over night. The site had thousands of contributors collecting information on even the most obscure and most niche indie works over nearly a DECADE. Most of which was untranslated, so you can imagine the treasure trove of information. A shit ton of stuff that not even hardcore weebs know exists.\n\nThe owner made [this](https://twitter.com/tenetan/status/1550839774969208833) tweet the day the site went offline. This is the only update they have given, ever. No tweets since, absolutely nothing. There isn't a discord or a subreddit or anything either where one would be able to contact them. They do however still seem to [like drawings of anime girls on twitter](https://i.imgur.com/b3jFx1K.png), unbothered by the countless of people asking for updates.\n\nFor all we know they could be in financial trouble or be sick or whatever. We simply do not know. But what is pissing me off to no end is the fact that they are radio silent. If you host a database that is almost entirely dependant on the community, you have the obligation and responsibility to inform them of what's become of their effort if you ask me. They \\*owe\\* communcation. Because at that point, the site isn't even the owners anymore.\n\nNow, I have a question to my fellow data hoarders, and it's the reason why I'm posting this in the first place. What the hell are we supposed to? There's no archives or backups anywhere; the site wasn't all too popular, evident by the fact that no one is even talking about this despite it being the biggest ressource for Japanese fan works.\n\nTo be honest, I really doubt it's going to come back. And I fear this might be another Library of Alexandria scenario...", "author_fullname": "t2_d5g3mn4d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Largest Doujinshi and Manga Lexicon Went Down 5 Months Ago and No One Cares", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxwkd5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668707282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;About 5 months ago, arguably the largest database of doujinshi and manga (Doujinshi.org) seemingly went down over night. The site had thousands of contributors collecting information on even the most obscure and most niche indie works over nearly a DECADE. Most of which was untranslated, so you can imagine the treasure trove of information. A shit ton of stuff that not even hardcore weebs know exists.&lt;/p&gt;\n\n&lt;p&gt;The owner made &lt;a href=\"https://twitter.com/tenetan/status/1550839774969208833\"&gt;this&lt;/a&gt; tweet the day the site went offline. This is the only update they have given, ever. No tweets since, absolutely nothing. There isn&amp;#39;t a discord or a subreddit or anything either where one would be able to contact them. They do however still seem to &lt;a href=\"https://i.imgur.com/b3jFx1K.png\"&gt;like drawings of anime girls on twitter&lt;/a&gt;, unbothered by the countless of people asking for updates.&lt;/p&gt;\n\n&lt;p&gt;For all we know they could be in financial trouble or be sick or whatever. We simply do not know. But what is pissing me off to no end is the fact that they are radio silent. If you host a database that is almost entirely dependant on the community, you have the obligation and responsibility to inform them of what&amp;#39;s become of their effort if you ask me. They *owe* communcation. Because at that point, the site isn&amp;#39;t even the owners anymore.&lt;/p&gt;\n\n&lt;p&gt;Now, I have a question to my fellow data hoarders, and it&amp;#39;s the reason why I&amp;#39;m posting this in the first place. What the hell are we supposed to? There&amp;#39;s no archives or backups anywhere; the site wasn&amp;#39;t all too popular, evident by the fact that no one is even talking about this despite it being the biggest ressource for Japanese fan works.&lt;/p&gt;\n\n&lt;p&gt;To be honest, I really doubt it&amp;#39;s going to come back. And I fear this might be another Library of Alexandria scenario...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A39rPq03PqwgcbXWQppLdcN5AeSiXDvlH7tPcHsAh1Q.png?auto=webp&amp;s=879d33faecaf0361ff93445bf2f7bac5b4bcafc5", "width": 586, "height": 695}, "resolutions": [{"url": "https://external-preview.redd.it/A39rPq03PqwgcbXWQppLdcN5AeSiXDvlH7tPcHsAh1Q.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7c2c17a640d7412041f27536106544263a54ead6", "width": 108, "height": 128}, {"url": "https://external-preview.redd.it/A39rPq03PqwgcbXWQppLdcN5AeSiXDvlH7tPcHsAh1Q.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=841ba32d3ece1119693ee620ffc6858977a5d73d", "width": 216, "height": 256}, {"url": "https://external-preview.redd.it/A39rPq03PqwgcbXWQppLdcN5AeSiXDvlH7tPcHsAh1Q.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1177f2e748708d2001e85c789d08ed532347397f", "width": 320, "height": 379}], "variants": {}, "id": "WFdf4X6f7FV-nDYfv84KJrMGtgaGBMHKgAzMvk6EscM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yxwkd5", "is_robot_indexable": true, "report_reasons": null, "author": "scremixz566", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxwkd5/largest_doujinshi_and_manga_lexicon_went_down_5/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxwkd5/largest_doujinshi_and_manga_lexicon_went_down_5/", "subreddit_subscribers": 654520, "created_utc": 1668707282.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Edit to answer FAQ: hardware choices are for cost and versatility, the whole budget for this build is under $350. Raid config is RAID 10 + 1 hot spare and four more drives on hand to swap in for failures. I am not afraid to cut holes in this case. \n\nI've also posted this over on homeserver, but the level of jank I'm striving for fits a lot better here I think. \n\nI'm currently working on building a home file server that could potentially host both Plex and Pi-Hole. I've already got the drives on the way but I have not ordered the rest of the hardware. Right now I am bidding on an auction for an Optiplex 7040 MT, i7-6700 with 8GB RAM. There will be a dedicated SSD for the OS. I know that the specs will be sufficient for any of the individual uses, but will they be enough for all three at once? Also, what would be the best way to install 11 drives? I'm not afraid of jank, so \"creative\" solutions are welcome. My array plans are for 11 2TB drives with 10 TB logical storage for full parity with 1 hot spare. I will also have 4 cold spares. \nHere are the solutions I'm looking for ideas on:\n1. software. Currently thinking about Samba but would be open to any other free options\n\n2. power. Will the 240w dell power supply be enough for 11 spinning platters or do I need another solution?\n\n3. connection. Do I need to use a specific PCIe expansion card to make sure my OS can see all the drives?\n\n4. physical mounting. How the heck can I get 11 drives connected to this thing?\n\nTIA!", "author_fullname": "t2_2vd74d5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for Optiplex Server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxuv9c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668713641.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668703184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edit to answer FAQ: hardware choices are for cost and versatility, the whole budget for this build is under $350. Raid config is RAID 10 + 1 hot spare and four more drives on hand to swap in for failures. I am not afraid to cut holes in this case. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also posted this over on homeserver, but the level of jank I&amp;#39;m striving for fits a lot better here I think. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on building a home file server that could potentially host both Plex and Pi-Hole. I&amp;#39;ve already got the drives on the way but I have not ordered the rest of the hardware. Right now I am bidding on an auction for an Optiplex 7040 MT, i7-6700 with 8GB RAM. There will be a dedicated SSD for the OS. I know that the specs will be sufficient for any of the individual uses, but will they be enough for all three at once? Also, what would be the best way to install 11 drives? I&amp;#39;m not afraid of jank, so &amp;quot;creative&amp;quot; solutions are welcome. My array plans are for 11 2TB drives with 10 TB logical storage for full parity with 1 hot spare. I will also have 4 cold spares. \nHere are the solutions I&amp;#39;m looking for ideas on:\n1. software. Currently thinking about Samba but would be open to any other free options&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;power. Will the 240w dell power supply be enough for 11 spinning platters or do I need another solution?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;connection. Do I need to use a specific PCIe expansion card to make sure my OS can see all the drives?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;physical mounting. How the heck can I get 11 drives connected to this thing?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxuv9c", "is_robot_indexable": true, "report_reasons": null, "author": "OutfoxHyperion", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxuv9c/advice_for_optiplex_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxuv9c/advice_for_optiplex_server/", "subreddit_subscribers": 654520, "created_utc": 1668703184.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey, can anyone help me download this video? [Video](https://d1z78r8i505acl.cloudfront.net/media/BVCi593Eyp0AB/c0019c94/video_1491_720_630.mp4). Every time I download it gives me a blank screen. It is from an educational website I've subscribed to but want to download for offline viewing. Thanks", "author_fullname": "t2_9guxqr8p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me download an embedded video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxtyno", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668700948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, can anyone help me download this video? &lt;a href=\"https://d1z78r8i505acl.cloudfront.net/media/BVCi593Eyp0AB/c0019c94/video_1491_720_630.mp4\"&gt;Video&lt;/a&gt;. Every time I download it gives me a blank screen. It is from an educational website I&amp;#39;ve subscribed to but want to download for offline viewing. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxtyno", "is_robot_indexable": true, "report_reasons": null, "author": "imahahaTAN", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxtyno/help_me_download_an_embedded_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxtyno/help_me_download_an_embedded_video/", "subreddit_subscribers": 654520, "created_utc": 1668700948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've tried Onedrive and Google Drive, but neither of them allows me to roll back a whole folder at once. Only a single file at a time. Backblaze costs money per-device, but I have multiple devices so that solution doesn't work for me either. I also don't need more than 1TB.\n\nI'd like it to be as streamlined as possible (so no manual pushing/pulling, and preferably no complicated setups).\n\nCan anyone help?", "author_fullname": "t2_m7d9qp32", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a cloud storage that allows me to roll back entire folders at once?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxrgzi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668694805.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668694590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried Onedrive and Google Drive, but neither of them allows me to roll back a whole folder at once. Only a single file at a time. Backblaze costs money per-device, but I have multiple devices so that solution doesn&amp;#39;t work for me either. I also don&amp;#39;t need more than 1TB.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like it to be as streamlined as possible (so no manual pushing/pulling, and preferably no complicated setups).&lt;/p&gt;\n\n&lt;p&gt;Can anyone help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxrgzi", "is_robot_indexable": true, "report_reasons": null, "author": "CutiePatootieLootie", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxrgzi/is_there_a_cloud_storage_that_allows_me_to_roll/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxrgzi/is_there_a_cloud_storage_that_allows_me_to_roll/", "subreddit_subscribers": 654520, "created_utc": 1668694590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As I started researching online backup solutions, I realized we all mostly talk about backing up, but **we rarely talking about restoring**. More specifically, we rarely talk about the process of restoring, if it works, how well it works, etc. I read many folks say they couldn't restore cause of a corrupted backup because of the backup tool they used -- *not cool*.\n\nSo, I'm wondering, for folks who have had to restore data **and it was successful**, what tool were you using to do the backup + restore? Comment with your thoughts/experience.\n\n[View Poll](https://www.reddit.com/poll/yxqwnz)", "author_fullname": "t2_5wpob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What back tool did you use to backup **AND RESTORE** and how well did it work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxqwnz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668693212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As I started researching online backup solutions, I realized we all mostly talk about backing up, but &lt;strong&gt;we rarely talking about restoring&lt;/strong&gt;. More specifically, we rarely talk about the process of restoring, if it works, how well it works, etc. I read many folks say they couldn&amp;#39;t restore cause of a corrupted backup because of the backup tool they used -- &lt;em&gt;not cool&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m wondering, for folks who have had to restore data &lt;strong&gt;and it was successful&lt;/strong&gt;, what tool were you using to do the backup + restore? Comment with your thoughts/experience.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/yxqwnz\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yxqwnz", "is_robot_indexable": true, "report_reasons": null, "author": "imthenachoman", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1669298012653, "options": [{"text": "Backblaze Personal", "id": "19865732"}, {"text": "rclone", "id": "19865733"}, {"text": "Duplicati", "id": "19865734"}, {"text": "Duplicacy", "id": "19865735"}, {"text": "ARQ", "id": "19865736"}, {"text": "Veeam", "id": "19865737"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 53, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxqwnz/what_back_tool_did_you_use_to_backup_and_restore/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/DataHoarder/comments/yxqwnz/what_back_tool_did_you_use_to_backup_and_restore/", "subreddit_subscribers": 654520, "created_utc": 1668693212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I wrote a little blog post about my journey of migrating all of my cloud files away from OneDrive! Eventually, I found a managed Nextcloud instance to be working really well for me, but I\u2019ve also compared some other cloud storage providers in the blog post. Maybe it helps some of you, or is at least an interesting read!\n\n[https://herrherrmann.net/migrating-1-terabyte-of-files-from-onedrive-to-nextcloud/](https://herrherrmann.net/migrating-1-terabyte-of-files-from-onedrive-to-nextcloud/)\n\nAnd please let me know if I missed anything!", "author_fullname": "t2_snlb8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating 1 terabyte of files from OneDrive to Nextcloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxn81k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668682432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote a little blog post about my journey of migrating all of my cloud files away from OneDrive! Eventually, I found a managed Nextcloud instance to be working really well for me, but I\u2019ve also compared some other cloud storage providers in the blog post. Maybe it helps some of you, or is at least an interesting read!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://herrherrmann.net/migrating-1-terabyte-of-files-from-onedrive-to-nextcloud/\"&gt;https://herrherrmann.net/migrating-1-terabyte-of-files-from-onedrive-to-nextcloud/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And please let me know if I missed anything!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ssxpdJV9WRYM88wr_Db_-DOFs6riFnJxrqjMBr1VAO0.jpg?auto=webp&amp;s=88a062633dedf9ccf452448d8fc2db2dad601c1f", "width": 1280, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/ssxpdJV9WRYM88wr_Db_-DOFs6riFnJxrqjMBr1VAO0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=98d55d59103bd7b567864dba6260fd09315eb9ed", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/ssxpdJV9WRYM88wr_Db_-DOFs6riFnJxrqjMBr1VAO0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e34219664abdc1aa38d76b56c777f3772f425052", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/ssxpdJV9WRYM88wr_Db_-DOFs6riFnJxrqjMBr1VAO0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec4b0f7ab74821b6ac125f46b3118523fbc8000b", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/ssxpdJV9WRYM88wr_Db_-DOFs6riFnJxrqjMBr1VAO0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3401a41d681fdfc6b2ef8e92e6678d6312a20951", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/ssxpdJV9WRYM88wr_Db_-DOFs6riFnJxrqjMBr1VAO0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d402ce1f8f6ebb81425114a6c8044a702a4ec2d", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/ssxpdJV9WRYM88wr_Db_-DOFs6riFnJxrqjMBr1VAO0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d33df6ce715275bb50796c44663dec38ee80c099", "width": 1080, "height": 675}], "variants": {}, "id": "hSLDBk73_NSVA14RgE0rWWy7p2PyXnFjDL3BnIToUjA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxn81k", "is_robot_indexable": true, "report_reasons": null, "author": "herrherrmann", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxn81k/migrating_1_terabyte_of_files_from_onedrive_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxn81k/migrating_1_terabyte_of_files_from_onedrive_to/", "subreddit_subscribers": 654520, "created_utc": 1668682432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "*Note: I do have all the data I'm referring to backed up elsewhere*\n\nI'm on Windows 10 Pro and I've got two identical dynamic disks -- 3TB each with 2.4TB (or whatever) filled, each. I want to convert them to spanned or striped (haven't decided which, yet) but I don't seem to have the option -- it's greyed out in Disk Manager:\n\nhttps://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81\n\nDo I...need to shrink the disks to make room? I'm hesitant to try that without getting thoughts from others because it'll take the better part of a day, each disk. Do I...need to delete the volumes and create the array from scratch?\n\nI'd prefer a non-destructive option if possible. I'm willing to delete these because the data is backed up, but restoring it will be a PITA :)\n\nMuch appreciated.", "author_fullname": "t2_36jz3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows: Can dynamic disks not be converted to spanned/striped?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": true, "media_metadata": {"b203wafbjm0a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94b6126cadbbbb1376df665d6a170fc771860b95"}, {"y": 135, "x": 216, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ea9718fef595c82c340ac3622c2b2a951e1052b"}, {"y": 200, "x": 320, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2265f302a5ff338b74ebf4de0a9b17ebb80d3252"}, {"y": 400, "x": 640, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7cd33c67ecfeec80a2ec3738e382e54eecd19144"}, {"y": 600, "x": 960, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=063565c5bab55cb108d470c859daba852a0a356d"}, {"y": 675, "x": 1080, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7893f3501b0345bcf7d35fda3017a05d217bb596"}], "s": {"y": 900, "x": 1440, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81"}, "id": "b203wafbjm0a1"}}, "name": "t3_yy99ds", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/vMb3XlrxBTmOW43DaRSvG5q-XrdleZUhBeYHO6lSV00.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668740163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Note: I do have all the data I&amp;#39;m referring to backed up elsewhere&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on Windows 10 Pro and I&amp;#39;ve got two identical dynamic disks -- 3TB each with 2.4TB (or whatever) filled, each. I want to convert them to spanned or striped (haven&amp;#39;t decided which, yet) but I don&amp;#39;t seem to have the option -- it&amp;#39;s greyed out in Disk Manager:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81\"&gt;https://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Do I...need to shrink the disks to make room? I&amp;#39;m hesitant to try that without getting thoughts from others because it&amp;#39;ll take the better part of a day, each disk. Do I...need to delete the volumes and create the array from scratch?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d prefer a non-destructive option if possible. I&amp;#39;m willing to delete these because the data is backed up, but restoring it will be a PITA :)&lt;/p&gt;\n\n&lt;p&gt;Much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy99ds", "is_robot_indexable": true, "report_reasons": null, "author": "eriksrx", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy99ds/windows_can_dynamic_disks_not_be_converted_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy99ds/windows_can_dynamic_disks_not_be_converted_to/", "subreddit_subscribers": 654520, "created_utc": 1668740163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Lots of options to scrape your tweets, but I have a lot of bookmarks I also wanna back up.\n\nI found a way to download all the tweet URLs into a csv ([Dewey](https://getdewey.co/)) and a way to download videos and gifs independently ([youtube-dl](https://youtube-dl-helper.github.io/)) but I can't find a way to do it all at the same time.\n\nEverything into a XML file would be ideal. Also if possible it would be nice for it to keep threads (which should be possible if not annoying)\n\nI found [this](https://gist.github.com/CJKinni/3063070) but it's very old code and has basically no chance to work", "author_fullname": "t2_ykkhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get all my Twitter bookmarks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yy978v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668739972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lots of options to scrape your tweets, but I have a lot of bookmarks I also wanna back up.&lt;/p&gt;\n\n&lt;p&gt;I found a way to download all the tweet URLs into a csv (&lt;a href=\"https://getdewey.co/\"&gt;Dewey&lt;/a&gt;) and a way to download videos and gifs independently (&lt;a href=\"https://youtube-dl-helper.github.io/\"&gt;youtube-dl&lt;/a&gt;) but I can&amp;#39;t find a way to do it all at the same time.&lt;/p&gt;\n\n&lt;p&gt;Everything into a XML file would be ideal. Also if possible it would be nice for it to keep threads (which should be possible if not annoying)&lt;/p&gt;\n\n&lt;p&gt;I found &lt;a href=\"https://gist.github.com/CJKinni/3063070\"&gt;this&lt;/a&gt; but it&amp;#39;s very old code and has basically no chance to work&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?auto=webp&amp;s=e2d60ad36c7afe27744a4b4f63f20d3181954d4b", "width": 1015, "height": 494}, "resolutions": [{"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4de019e984fdeee8a89ce7e525e828568019fd28", "width": 108, "height": 52}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06f53d6a2f1e4248b34fb2643cd2df9e85f63585", "width": 216, "height": 105}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=11aa0debd38927b0c86148157dd6fb2be3425330", "width": 320, "height": 155}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a008abc197d7c09c8a6d7c679a7a14dfeb6aff84", "width": 640, "height": 311}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8407e59962bf38175018cc891c88bea773d2b655", "width": 960, "height": 467}], "variants": {}, "id": "D83W6ubnaf9JIZcLbn0OsGoknMj1En17FJrGkylXYzs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy978v", "is_robot_indexable": true, "report_reasons": null, "author": "PowderPhysics", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy978v/how_to_get_all_my_twitter_bookmarks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy978v/how_to_get_all_my_twitter_bookmarks/", "subreddit_subscribers": 654520, "created_utc": 1668739972.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}