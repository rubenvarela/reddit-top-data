{"kind": "Listing", "data": {"after": "t3_yynwbg", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&gt;Twitter has emailed staffers: \"Hi, Effective immediately, we are temporarily closing our office buildings and all badge access will be suspended. Offices will reopen on Monday, November 21st. .. We look forward to working with you on Twitter\u2019s exciting future.\"\n\n&amp;#x200B;\n\n&gt;Story to be updated soon with more: Am hearing that several \u201ccritical\u201d infra engineering teams at Twitter have completely resigned. \u201cYou cannot run Twitter without this team,\u201d one current engineer tells me of one such group. Also, Twitter has shut off badge access to its offices.\n\n&amp;#x200B;\n\n&gt;What I\u2019m hearing from Twitter employees; It looks like roughly 75% of the remaining 3,700ish Twitter employees have not opted to stay after the \u201chardcore\u201d email.  \n&gt;  \n&gt;Even though the deadline has passed, everyone still has access to their systems.\n\n&amp;#x200B;\n\n&gt;\u201cI know of six critical systems (like \u2018serving tweets\u2019 levels of critical) which no longer have any engineers,\" the former employee said. \"There is no longer even a skeleton crew manning the system. It will continue to coast until it runs into something, and then it will stop.\u201d  \n&gt;  \n&gt;Resignations and departures were already taking a toll on Twitter\u2019s service, employees said. \u201cBreakages are already happening slowly and accumulating,\u201d one said. \u201cIf you want to export your tweets, do it now.\u201d\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Link 1](https://twitter.com/oliverdarcy/status/1593394621627138048)\n\n[Link 2](https://twitter.com/alexeheath/status/1593399683086327808)\n\n[Link 3](https://twitter.com/kyliebytes/status/1593391167718113280)\n\n[Link 4](https://www.washingtonpost.com/technology/2022/11/17/twitter-musk-easing-rto-order/)\n\n&amp;#x200B;\n\nEdit:\n\n[twitter-scraper (github no api-key needed)](https://github.com/n0madic/twitter-scraper)\n\n[twitter-media-downloader (github no api-key needed)](https://github.com/mmpx12/twitter-media-downloader)\n\n&amp;#x200B;\n\nEdit2:\n\n[https://github.com/markowanga/stweet](https://github.com/markowanga/stweet)\n\n&amp;#x200B;\n\nEdit3:\n\n[gallery-dl guide by /u/Scripter17](https://www.reddit.com/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/)\n\n&amp;#x200B;\n\nEdit4:\n\n[Twitter Media Downloader](https://chrome.google.com/webstore/detail/twitter-media-downloader/cblpjenafgeohmnjknfhpdbdljfkndig?hl=en)\n\n&amp;#x200B;\n\nEdit5:  \n[https://github.com/JustAnotherArchivist/snscrape](https://github.com/JustAnotherArchivist/snscrape)", "author_fullname": "t2_ioi0j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup twitter now! Multiple critical infra teams have resigned", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy7tig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 853, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 853, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668783041.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668735892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Twitter has emailed staffers: &amp;quot;Hi, Effective immediately, we are temporarily closing our office buildings and all badge access will be suspended. Offices will reopen on Monday, November 21st. .. We look forward to working with you on Twitter\u2019s exciting future.&amp;quot;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Story to be updated soon with more: Am hearing that several \u201ccritical\u201d infra engineering teams at Twitter have completely resigned. \u201cYou cannot run Twitter without this team,\u201d one current engineer tells me of one such group. Also, Twitter has shut off badge access to its offices.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;What I\u2019m hearing from Twitter employees; It looks like roughly 75% of the remaining 3,700ish Twitter employees have not opted to stay after the \u201chardcore\u201d email.  &lt;/p&gt;\n\n&lt;p&gt;Even though the deadline has passed, everyone still has access to their systems.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;\u201cI know of six critical systems (like \u2018serving tweets\u2019 levels of critical) which no longer have any engineers,&amp;quot; the former employee said. &amp;quot;There is no longer even a skeleton crew manning the system. It will continue to coast until it runs into something, and then it will stop.\u201d  &lt;/p&gt;\n\n&lt;p&gt;Resignations and departures were already taking a toll on Twitter\u2019s service, employees said. \u201cBreakages are already happening slowly and accumulating,\u201d one said. \u201cIf you want to export your tweets, do it now.\u201d&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/oliverdarcy/status/1593394621627138048\"&gt;Link 1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/alexeheath/status/1593399683086327808\"&gt;Link 2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/kyliebytes/status/1593391167718113280\"&gt;Link 3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.washingtonpost.com/technology/2022/11/17/twitter-musk-easing-rto-order/\"&gt;Link 4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/n0madic/twitter-scraper\"&gt;twitter-scraper (github no api-key needed)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mmpx12/twitter-media-downloader\"&gt;twitter-media-downloader (github no api-key needed)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit2:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/markowanga/stweet\"&gt;https://github.com/markowanga/stweet&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit3:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/\"&gt;gallery-dl guide by /u/Scripter17&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit4:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://chrome.google.com/webstore/detail/twitter-media-downloader/cblpjenafgeohmnjknfhpdbdljfkndig?hl=en\"&gt;Twitter Media Downloader&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit5:&lt;br/&gt;\n&lt;a href=\"https://github.com/JustAnotherArchivist/snscrape\"&gt;https://github.com/JustAnotherArchivist/snscrape&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yy7tig", "is_robot_indexable": true, "report_reasons": null, "author": "fourDnet", "discussion_type": null, "num_comments": 330, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy7tig/backup_twitter_now_multiple_critical_infra_teams/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/yy7tig/backup_twitter_now_multiple_critical_infra_teams/", "subreddit_subscribers": 654755, "created_utc": 1668735892.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_d48yz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google and Amazon Helped the FBI Identify Z-Library's Operators", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyj1dc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 286, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 286, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1668774878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "torrentfreak.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://torrentfreak.com/how-google-and-amazon-helped-the-fbi-identify-z-librarys-operators-221117/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyj1dc", "is_robot_indexable": true, "report_reasons": null, "author": "Prometheus720", "discussion_type": null, "num_comments": 102, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyj1dc/google_and_amazon_helped_the_fbi_identify/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://torrentfreak.com/how-google-and-amazon-helped-the-fbi-identify-z-librarys-operators-221117/", "subreddit_subscribers": 654755, "created_utc": 1668774878.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_tlgcl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A database of paper airplanes with easy to follow folding instructions, video tutorials and printable folding plans. Find the best paper airplanes that fly the furthest and stay aloft the longest.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_yyhavo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 117, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 117, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/prZ5H2m6FUcoXaVYAgI4mTKRMuTFDWbsdOBELddMgW8.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668769050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "foldnfly.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.foldnfly.com/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?auto=webp&amp;s=2e319bc9673fc40f1aec8c70febd71b2f84b10cb", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d37c2a5e835ea4c51b538aef5cbbd90f4e401217", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4d339c5a12d9cf0b1cf1a15d74138abbb400754", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b622d929f9312bf9d0f6019c52870b890a6d4ad", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d65b5cd84e8524d5d83e756f6a19fffc433d5b12", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=69a20c6d882a8d34f6c8dea2fec83e9b577a5f29", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=94570fce0f309e51fd4ca001f841c5a99f97f58d", "width": 1080, "height": 567}], "variants": {}, "id": "uZ2zDM2Hy1FLstbjMCzVaGvaO0jNdvizg4F-yPk7isI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "40TB Synology DS1819+", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyhavo", "is_robot_indexable": true, "report_reasons": null, "author": "PaddleMonkey", "discussion_type": null, "num_comments": 9, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yyhavo/a_database_of_paper_airplanes_with_easy_to_follow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.foldnfly.com/", "subreddit_subscribers": 654755, "created_utc": 1668769050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Rewritten for clarity because speedrunning a post like this tends to leave questions\n\nHow to get started:\n\n1. Install [Python](https://www.python.org/). There is a standalone .exe but this just makes it easier to upgrade and all that\n\n2. Run `pip install gallery-dl` in command prompt (windows) or Bash (Linux)\n\n3. From there running `gallery-dl &lt;url&gt;` in the same command line should download the url's contents\n\n## config.json\n\nThe config.json is located at `%APPDATA%\\gallery-dl\\config.json` (windows) and `/etc/gallery-dl.conf` (Linux)\n\nIf the folder/file doesn't exist, just making it yourself should work\n\nThe basic config I recommend is this. If this is your first time with gallery-dl it's safe to just replace the entire file with this. If it's not your first time you should know how to transplant this into your existing config\n\n    {\n        \"extractor\":{\n            \"cookies\": [\"&lt;your browser (firefox, chromium, etc)&gt;\"],\n            \"twitter\":{\n                \"users\": \"https://twitter.com/{legacy[screen_name]}\",\n                \"text-tweets\":true,\n                \"retweets\":true,\n                \"quoted\":true,\n                \"logout\":true,\n                \"replies\":true,\n                \"postprocessors\":[\n                    {\"name\": \"metadata\", \"event\": \"post\", \"filename\": \"{tweet_id}_main.json\"}\n                ]\n            }\n        }\n    }\n\nThe documentation for the config.json is [here](https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst) and the specific part about getting cookies from your browser is [here](https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst#extractorcookies)\n\nCurrently supplying your login as a username/password combo seems to be broken. Idk if this is an issue with twitter or gallery-dl but using browser cookies is just easier in the long run\n\n## URLs:\n\n[The twitter API limits getting a user's page to the latest ~3200 tweets](https://github.com/mikf/gallery-dl/issues/2226). To get the as much as possible I recommend getting the main tab, the media tab, *and* the URL when you search for `from:&lt;user&gt;`\n\nTo make downloading the media tab not immediately exit when it sees a duplicate image, you'll want to add `-o skip=true` to the command you put in the command line. This can also be specified in the config. I have mine set to 20 when I'm just updating an existing download. If it sees 20 known images in a row then it moves on to the next one.\n\nThe 3 URLs I recommend downloading are:\n\n- `https://www.twitter.com/&lt;user&gt;`\n- `https://www.twitter.com/&lt;user&gt;/media`\n- `https://twitter.com/search?q=from:&lt;user&gt;`\n\nTo get someone's likes the URL is `https://www.twitter.com/&lt;user&gt;/likes`\n\nTo get your bookmarks the URL is `https://twitter.com/i/bookmarks`\n\n**Note**: Because twitter honestly just sucks and has for quite a while, you should run each download a few times (again with `-o skip=true`) to make sure you get everything\n\n## Commands:\n\nAnd the commands you're running should look like `gallery-dl &lt;url&gt; --write-metadata -o skip=true`\n\n`--write-metadata` saves `.json` files with metadata about each image. the `\"postprocessors\"` part of the config already writes the metadata for the tweet itself but the per-image metadata has some extra stuff\n\nIf you run `gallery-dl -g https://twitter.com/&lt;your handle&gt;/following` you can get a list of everyone you follow.\n\n### Windows:\n\nIf you have a text editor that supports regex replacement (CTRL+H in Sublime Text. Enable the button that looks like a .*), you can paste the list gallery-dl gave you and replace `(.+\\/)([^/\\r\\n]+)` with `gallery-dl $1$2               --write-metadata -o skip=true\\ngallery-dl $1$2/media         --write-metadata -o skip=true\\ngallery-dl $1search?q=from:$2 --write-metadata -o skip=true -o \"directory=[\"\"twitter\"\",\"\"{$2}\"\"]\"`\n\nYou should see something along the lines of\n\n    gallery-dl https://twitter.com/test1               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test1/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test1 --write-metadata -o skip=true -o \"directory=[\"\"twitter\"\",\"\"{test1}\"\"]\"\n    gallery-dl https://twitter.com/test2               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test2/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test2 --write-metadata -o skip=true -o \"directory=[\"\"twitter\"\",\"\"{test2}\"\"]\"\n    gallery-dl https://twitter.com/test3               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test3/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test3 --write-metadata -o skip=true -o \"directory=[\"\"twitter\"\",\"\"{test3}\"\"]\"\n\nThen put an `@echo off` at the top of the file and save it as a `.bat`\n\n### Linux:\n\nIf you have a text editor that supports regex replacement, you can paste the list gallery-dl gave you and replace `(.+\\/)([^/\\r\\n]+)` with `gallery-dl $1$2               --write-metadata -o skip=true\\ngallery-dl $1$2/media         --write-metadata -o skip=true\\ngallery-dl $1search?q=from:$2 --write-metadata -o skip=true -o \"directory=[\\\"twitter\\\",\\\"{$2}\\\"]\"`\n\nYou should see something along the lines of\n\n    gallery-dl https://twitter.com/test1               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test1/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test1 --write-metadata -o skip=true -o \"directory=[\\\"twitter\\\",\\\"{test1}\\\"]\"\n    gallery-dl https://twitter.com/test2               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test2/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test2 --write-metadata -o skip=true -o \"directory=[\\\"twitter\\\",\\\"{test2}\\\"]\"\n    gallery-dl https://twitter.com/test3               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test3/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test3 --write-metadata -o skip=true -o \"directory=[\\\"twitter\\\",\\\"{test3}\\\"]\"\n\nThen save it as a `.sh` file\n\nIf, on either OS, the resulting commands has a bunch of `$1` and `$2` in it, replace the `$`s in the replacement string with `\\`s and do it again.\n\nAfter that, running the file should (assuming I got all the steps right) download everyone you follow\n\n.\n\nNow, if you excuse me, it's almost 6am and I need to sleep. I really hope I haven't made any catastrophic errors", "author_fullname": "t2_yj3jz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For everyone using gallery-dl to backup twitter: Make sure you do it right", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy8o9w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 72, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 72, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668768882.0, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668738416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Rewritten for clarity because speedrunning a post like this tends to leave questions&lt;/p&gt;\n\n&lt;p&gt;How to get started:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Install &lt;a href=\"https://www.python.org/\"&gt;Python&lt;/a&gt;. There is a standalone .exe but this just makes it easier to upgrade and all that&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run &lt;code&gt;pip install gallery-dl&lt;/code&gt; in command prompt (windows) or Bash (Linux)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;From there running &lt;code&gt;gallery-dl &amp;lt;url&amp;gt;&lt;/code&gt; in the same command line should download the url&amp;#39;s contents&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h2&gt;config.json&lt;/h2&gt;\n\n&lt;p&gt;The config.json is located at &lt;code&gt;%APPDATA%\\gallery-dl\\config.json&lt;/code&gt; (windows) and &lt;code&gt;/etc/gallery-dl.conf&lt;/code&gt; (Linux)&lt;/p&gt;\n\n&lt;p&gt;If the folder/file doesn&amp;#39;t exist, just making it yourself should work&lt;/p&gt;\n\n&lt;p&gt;The basic config I recommend is this. If this is your first time with gallery-dl it&amp;#39;s safe to just replace the entire file with this. If it&amp;#39;s not your first time you should know how to transplant this into your existing config&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n    &amp;quot;extractor&amp;quot;:{\n        &amp;quot;cookies&amp;quot;: [&amp;quot;&amp;lt;your browser (firefox, chromium, etc)&amp;gt;&amp;quot;],\n        &amp;quot;twitter&amp;quot;:{\n            &amp;quot;users&amp;quot;: &amp;quot;https://twitter.com/{legacy[screen_name]}&amp;quot;,\n            &amp;quot;text-tweets&amp;quot;:true,\n            &amp;quot;retweets&amp;quot;:true,\n            &amp;quot;quoted&amp;quot;:true,\n            &amp;quot;logout&amp;quot;:true,\n            &amp;quot;replies&amp;quot;:true,\n            &amp;quot;postprocessors&amp;quot;:[\n                {&amp;quot;name&amp;quot;: &amp;quot;metadata&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;post&amp;quot;, &amp;quot;filename&amp;quot;: &amp;quot;{tweet_id}_main.json&amp;quot;}\n            ]\n        }\n    }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The documentation for the config.json is &lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst\"&gt;here&lt;/a&gt; and the specific part about getting cookies from your browser is &lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst#extractorcookies\"&gt;here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Currently supplying your login as a username/password combo seems to be broken. Idk if this is an issue with twitter or gallery-dl but using browser cookies is just easier in the long run&lt;/p&gt;\n\n&lt;h2&gt;URLs:&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mikf/gallery-dl/issues/2226\"&gt;The twitter API limits getting a user&amp;#39;s page to the latest ~3200 tweets&lt;/a&gt;. To get the as much as possible I recommend getting the main tab, the media tab, &lt;em&gt;and&lt;/em&gt; the URL when you search for &lt;code&gt;from:&amp;lt;user&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;To make downloading the media tab not immediately exit when it sees a duplicate image, you&amp;#39;ll want to add &lt;code&gt;-o skip=true&lt;/code&gt; to the command you put in the command line. This can also be specified in the config. I have mine set to 20 when I&amp;#39;m just updating an existing download. If it sees 20 known images in a row then it moves on to the next one.&lt;/p&gt;\n\n&lt;p&gt;The 3 URLs I recommend downloading are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;https://www.twitter.com/&amp;lt;user&amp;gt;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;https://www.twitter.com/&amp;lt;user&amp;gt;/media&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;https://twitter.com/search?q=from:&amp;lt;user&amp;gt;&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To get someone&amp;#39;s likes the URL is &lt;code&gt;https://www.twitter.com/&amp;lt;user&amp;gt;/likes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;To get your bookmarks the URL is &lt;code&gt;https://twitter.com/i/bookmarks&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Because twitter honestly just sucks and has for quite a while, you should run each download a few times (again with &lt;code&gt;-o skip=true&lt;/code&gt;) to make sure you get everything&lt;/p&gt;\n\n&lt;h2&gt;Commands:&lt;/h2&gt;\n\n&lt;p&gt;And the commands you&amp;#39;re running should look like &lt;code&gt;gallery-dl &amp;lt;url&amp;gt; --write-metadata -o skip=true&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;--write-metadata&lt;/code&gt; saves &lt;code&gt;.json&lt;/code&gt; files with metadata about each image. the &lt;code&gt;&amp;quot;postprocessors&amp;quot;&lt;/code&gt; part of the config already writes the metadata for the tweet itself but the per-image metadata has some extra stuff&lt;/p&gt;\n\n&lt;p&gt;If you run &lt;code&gt;gallery-dl -g https://twitter.com/&amp;lt;your handle&amp;gt;/following&lt;/code&gt; you can get a list of everyone you follow.&lt;/p&gt;\n\n&lt;h3&gt;Windows:&lt;/h3&gt;\n\n&lt;p&gt;If you have a text editor that supports regex replacement (CTRL+H in Sublime Text. Enable the button that looks like a .*), you can paste the list gallery-dl gave you and replace &lt;code&gt;(.+\\/)([^/\\r\\n]+)&lt;/code&gt; with &lt;code&gt;gallery-dl $1$2               --write-metadata -o skip=true\\ngallery-dl $1$2/media         --write-metadata -o skip=true\\ngallery-dl $1search?q=from:$2 --write-metadata -o skip=true -o &amp;quot;directory=[&amp;quot;&amp;quot;twitter&amp;quot;&amp;quot;,&amp;quot;&amp;quot;{$2}&amp;quot;&amp;quot;]&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;You should see something along the lines of&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gallery-dl https://twitter.com/test1               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test1/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test1 --write-metadata -o skip=true -o &amp;quot;directory=[&amp;quot;&amp;quot;twitter&amp;quot;&amp;quot;,&amp;quot;&amp;quot;{test1}&amp;quot;&amp;quot;]&amp;quot;\ngallery-dl https://twitter.com/test2               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test2/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test2 --write-metadata -o skip=true -o &amp;quot;directory=[&amp;quot;&amp;quot;twitter&amp;quot;&amp;quot;,&amp;quot;&amp;quot;{test2}&amp;quot;&amp;quot;]&amp;quot;\ngallery-dl https://twitter.com/test3               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test3/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test3 --write-metadata -o skip=true -o &amp;quot;directory=[&amp;quot;&amp;quot;twitter&amp;quot;&amp;quot;,&amp;quot;&amp;quot;{test3}&amp;quot;&amp;quot;]&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then put an &lt;code&gt;@echo off&lt;/code&gt; at the top of the file and save it as a &lt;code&gt;.bat&lt;/code&gt;&lt;/p&gt;\n\n&lt;h3&gt;Linux:&lt;/h3&gt;\n\n&lt;p&gt;If you have a text editor that supports regex replacement, you can paste the list gallery-dl gave you and replace &lt;code&gt;(.+\\/)([^/\\r\\n]+)&lt;/code&gt; with &lt;code&gt;gallery-dl $1$2               --write-metadata -o skip=true\\ngallery-dl $1$2/media         --write-metadata -o skip=true\\ngallery-dl $1search?q=from:$2 --write-metadata -o skip=true -o &amp;quot;directory=[\\&amp;quot;twitter\\&amp;quot;,\\&amp;quot;{$2}\\&amp;quot;]&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;You should see something along the lines of&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gallery-dl https://twitter.com/test1               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test1/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test1 --write-metadata -o skip=true -o &amp;quot;directory=[\\&amp;quot;twitter\\&amp;quot;,\\&amp;quot;{test1}\\&amp;quot;]&amp;quot;\ngallery-dl https://twitter.com/test2               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test2/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test2 --write-metadata -o skip=true -o &amp;quot;directory=[\\&amp;quot;twitter\\&amp;quot;,\\&amp;quot;{test2}\\&amp;quot;]&amp;quot;\ngallery-dl https://twitter.com/test3               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test3/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test3 --write-metadata -o skip=true -o &amp;quot;directory=[\\&amp;quot;twitter\\&amp;quot;,\\&amp;quot;{test3}\\&amp;quot;]&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then save it as a &lt;code&gt;.sh&lt;/code&gt; file&lt;/p&gt;\n\n&lt;p&gt;If, on either OS, the resulting commands has a bunch of &lt;code&gt;$1&lt;/code&gt; and &lt;code&gt;$2&lt;/code&gt; in it, replace the &lt;code&gt;$&lt;/code&gt;s in the replacement string with &lt;code&gt;\\&lt;/code&gt;s and do it again.&lt;/p&gt;\n\n&lt;p&gt;After that, running the file should (assuming I got all the steps right) download everyone you follow&lt;/p&gt;\n\n&lt;p&gt;.&lt;/p&gt;\n\n&lt;p&gt;Now, if you excuse me, it&amp;#39;s almost 6am and I need to sleep. I really hope I haven&amp;#39;t made any catastrophic errors&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XACIlvLRgD0CrsIsxO7yWQMICHPINiGesu3WjxQxeXs.jpg?auto=webp&amp;s=c7f0d77306cb94adced1c514958fdf68f575791c", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/XACIlvLRgD0CrsIsxO7yWQMICHPINiGesu3WjxQxeXs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0a156ee41a904137a12d7727f03ba51aa2a31c7", "width": 108, "height": 108}], "variants": {}, "id": "_QTobzuJkr1Zm6t-xAciOuvRRUG3sFX1cl1tVTmHCMU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "The sexiest data storage medium", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy8o9w", "is_robot_indexable": true, "report_reasons": null, "author": "Scripter17", "discussion_type": null, "num_comments": 59, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/", "subreddit_subscribers": 654755, "created_utc": 1668738416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I am probably way out of my depth here but r/Twitter redirected me here when I asked this question. In case Twitter actually does go away I am trying to download the entire Twitter feed of my best friend who passed away 5 years ago. I\u2019m currently just screenshotting favorites in a panic and googling returns old workarounds that don\u2019t work anymore. Any advice at all is appreciated, thank you.", "author_fullname": "t2_eo84n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download An Entire Twitter Feed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy8dii", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668737540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am probably way out of my depth here but &lt;a href=\"/r/Twitter\"&gt;r/Twitter&lt;/a&gt; redirected me here when I asked this question. In case Twitter actually does go away I am trying to download the entire Twitter feed of my best friend who passed away 5 years ago. I\u2019m currently just screenshotting favorites in a panic and googling returns old workarounds that don\u2019t work anymore. Any advice at all is appreciated, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy8dii", "is_robot_indexable": true, "report_reasons": null, "author": "plaidtuxedo", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy8dii/download_an_entire_twitter_feed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy8dii/download_an_entire_twitter_feed/", "subreddit_subscribers": 654755, "created_utc": 1668737540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a thread for current blackfriday storage discounts like last year?", "author_fullname": "t2_4y7ra1ap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Black Friday thread", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyc1vh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668749117.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a thread for current blackfriday storage discounts like last year?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyc1vh", "is_robot_indexable": true, "report_reasons": null, "author": "HolUp-", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyc1vh/black_friday_thread/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yyc1vh/black_friday_thread/", "subreddit_subscribers": 654755, "created_utc": 1668749117.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_16ljkt33", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Well I guess there\u2019s a first time for everyone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_yymt49", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZbfaJL2A04kigIvAfyli2o1vVNQh3VGu_QN4cP9SVsE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668785375.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/jgn8k3cprr0a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/jgn8k3cprr0a1.jpg?auto=webp&amp;s=9c7d9d7a0a06582b4e311f39efd681cadfb01e77", "width": 1105, "height": 1632}, "resolutions": [{"url": "https://preview.redd.it/jgn8k3cprr0a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f931b06fc41138c9b2c3f54645f061124f05666f", "width": 108, "height": 159}, {"url": "https://preview.redd.it/jgn8k3cprr0a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b439c243abb4bd3fbb01a67d4ab5aa93ee90574f", "width": 216, "height": 319}, {"url": "https://preview.redd.it/jgn8k3cprr0a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=837eccd77e3417c0ff37e6657b807dc9a33e0821", "width": 320, "height": 472}, {"url": "https://preview.redd.it/jgn8k3cprr0a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eab28429ca47f669cebd02eefd7c2d67c9312fe6", "width": 640, "height": 945}, {"url": "https://preview.redd.it/jgn8k3cprr0a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=179673b47e79d29e4b57f4c86e34cea65a58da30", "width": 960, "height": 1417}, {"url": "https://preview.redd.it/jgn8k3cprr0a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16fa7c62fd867b4d191341381b1da98ae39ac430", "width": 1080, "height": 1595}], "variants": {}, "id": "ZNagWy__7ZPBH4futKvhBx4Oid-stmGTw1lkVZWWw-4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yymt49", "is_robot_indexable": true, "report_reasons": null, "author": "ddrfraser1", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yymt49/well_i_guess_theres_a_first_time_for_everyone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/jgn8k3cprr0a1.jpg", "subreddit_subscribers": 654755, "created_utc": 1668785375.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Lots of options to scrape your tweets, but I have a lot of bookmarks I also wanna back up.\n\nI found a way to download all the tweet URLs into a csv ([Dewey](https://getdewey.co/)) and a way to download videos and gifs independently ([youtube-dl](https://youtube-dl-helper.github.io/)) but I can't find a way to do it all at the same time.\n\nEverything into a XML file would be ideal. Also if possible it would be nice for it to keep threads (which should be possible if not annoying)\n\nI found [this](https://gist.github.com/CJKinni/3063070) but it's very old code and has basically no chance to work", "author_fullname": "t2_ykkhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get all my Twitter bookmarks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy978v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668739972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lots of options to scrape your tweets, but I have a lot of bookmarks I also wanna back up.&lt;/p&gt;\n\n&lt;p&gt;I found a way to download all the tweet URLs into a csv (&lt;a href=\"https://getdewey.co/\"&gt;Dewey&lt;/a&gt;) and a way to download videos and gifs independently (&lt;a href=\"https://youtube-dl-helper.github.io/\"&gt;youtube-dl&lt;/a&gt;) but I can&amp;#39;t find a way to do it all at the same time.&lt;/p&gt;\n\n&lt;p&gt;Everything into a XML file would be ideal. Also if possible it would be nice for it to keep threads (which should be possible if not annoying)&lt;/p&gt;\n\n&lt;p&gt;I found &lt;a href=\"https://gist.github.com/CJKinni/3063070\"&gt;this&lt;/a&gt; but it&amp;#39;s very old code and has basically no chance to work&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?auto=webp&amp;s=e2d60ad36c7afe27744a4b4f63f20d3181954d4b", "width": 1015, "height": 494}, "resolutions": [{"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4de019e984fdeee8a89ce7e525e828568019fd28", "width": 108, "height": 52}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06f53d6a2f1e4248b34fb2643cd2df9e85f63585", "width": 216, "height": 105}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=11aa0debd38927b0c86148157dd6fb2be3425330", "width": 320, "height": 155}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a008abc197d7c09c8a6d7c679a7a14dfeb6aff84", "width": 640, "height": 311}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8407e59962bf38175018cc891c88bea773d2b655", "width": 960, "height": 467}], "variants": {}, "id": "D83W6ubnaf9JIZcLbn0OsGoknMj1En17FJrGkylXYzs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy978v", "is_robot_indexable": true, "report_reasons": null, "author": "PowderPhysics", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy978v/how_to_get_all_my_twitter_bookmarks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy978v/how_to_get_all_my_twitter_bookmarks/", "subreddit_subscribers": 654755, "created_utc": 1668739972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_7w4k6ew8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Users urged to archive tweets amid rumors of Twitter implosion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_yymkm3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/eALEhho84LazRxOlfZEM6k9800EAMqAzryV6fTLtZc8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668784757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "theguardian.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.theguardian.com/technology/2022/nov/17/twitter-archive-tweets-company-shuts", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ip5DRtw-aLUv8vdwCuNwFZx6-c4vgkuFjAhEUWk1x_I.jpg?auto=webp&amp;s=1a1c0054b85a7f182043ecc83a24c61ed68b81da", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Ip5DRtw-aLUv8vdwCuNwFZx6-c4vgkuFjAhEUWk1x_I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=28e937736e790e72762917e976d26195cfe61fd0", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Ip5DRtw-aLUv8vdwCuNwFZx6-c4vgkuFjAhEUWk1x_I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5be5a1ef1f03e74076ada75bb970130fe7d027e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Ip5DRtw-aLUv8vdwCuNwFZx6-c4vgkuFjAhEUWk1x_I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ae5b4b15e0e9348a170bccc6ee4291f4410491b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Ip5DRtw-aLUv8vdwCuNwFZx6-c4vgkuFjAhEUWk1x_I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=774d8fc94fb5c43dea6ec1a6270926be8c2df19a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Ip5DRtw-aLUv8vdwCuNwFZx6-c4vgkuFjAhEUWk1x_I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7384756fc81363733e6c438330eac0e52e25f5ae", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Ip5DRtw-aLUv8vdwCuNwFZx6-c4vgkuFjAhEUWk1x_I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d5898eeaaa637f96d892c11a4b505afe236bc039", "width": 1080, "height": 567}], "variants": {}, "id": "e73AQNWJSsqrckdWSHBK6HJXAG6Ufrm3I_6DS13ZpN8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yymkm3", "is_robot_indexable": true, "report_reasons": null, "author": "Buddy_Deep", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yymkm3/users_urged_to_archive_tweets_amid_rumors_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.theguardian.com/technology/2022/nov/17/twitter-archive-tweets-company-shuts", "subreddit_subscribers": 654755, "created_utc": 1668784757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Should i get 5 of these for my NAS? Curious if its worth it to use these or bite the bullet and get the Reds for a but more money.  \n\n\nThey are currently $199.99 at BestBuy\n\nWD - easystore 14TB External USB 3.0 Hard Drive - Black\n\nModel:WDBAMA0140HBK-NESNSKU:6425303\n\n[https://www.bestbuy.com/site/wd-easystore-14tb-external-usb-3-0-hard-drive-black/6425303.p?skuId=6425303](https://www.bestbuy.com/site/wd-easystore-14tb-external-usb-3-0-hard-drive-black/6425303.p?skuId=6425303)", "author_fullname": "t2_8wu8x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD - easystore 14TB External USB 3.0 Hard Drive For NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyk7mz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Shuckable NAS Drives", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668778440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Should i get 5 of these for my NAS? Curious if its worth it to use these or bite the bullet and get the Reds for a but more money.  &lt;/p&gt;\n\n&lt;p&gt;They are currently $199.99 at BestBuy&lt;/p&gt;\n\n&lt;p&gt;WD - easystore 14TB External USB 3.0 Hard Drive - Black&lt;/p&gt;\n\n&lt;p&gt;Model:WDBAMA0140HBK-NESNSKU:6425303&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.bestbuy.com/site/wd-easystore-14tb-external-usb-3-0-hard-drive-black/6425303.p?skuId=6425303\"&gt;https://www.bestbuy.com/site/wd-easystore-14tb-external-usb-3-0-hard-drive-black/6425303.p?skuId=6425303&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZseKVRgX3jeJqr848TEUnFsXmb36I5tfKDi8Zy3BG-o.jpg?auto=webp&amp;s=0d9dc09b0ad8bb581b3f12d19a61f850933bb13d", "width": 1452, "height": 5012}, "resolutions": [{"url": "https://external-preview.redd.it/ZseKVRgX3jeJqr848TEUnFsXmb36I5tfKDi8Zy3BG-o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f13038664e7cac7334520eb6f6219ebcaa06077d", "width": 108, "height": 216}, {"url": "https://external-preview.redd.it/ZseKVRgX3jeJqr848TEUnFsXmb36I5tfKDi8Zy3BG-o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ced7daed466de03f936545afae108a381fbd5548", "width": 216, "height": 432}, {"url": "https://external-preview.redd.it/ZseKVRgX3jeJqr848TEUnFsXmb36I5tfKDi8Zy3BG-o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=99ce2e22c9b74024d7f6948409a3403bb9821faf", "width": 320, "height": 640}, {"url": "https://external-preview.redd.it/ZseKVRgX3jeJqr848TEUnFsXmb36I5tfKDi8Zy3BG-o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a06827eb64f82327b663eeb4bcc50b34736b2469", "width": 640, "height": 1280}, {"url": "https://external-preview.redd.it/ZseKVRgX3jeJqr848TEUnFsXmb36I5tfKDi8Zy3BG-o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=959de205f67d48894b89a59f2999a355aea021a3", "width": 960, "height": 1920}, {"url": "https://external-preview.redd.it/ZseKVRgX3jeJqr848TEUnFsXmb36I5tfKDi8Zy3BG-o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f171001b6b2b3e2579548bf4f45f34f0ee94f5cb", "width": 1080, "height": 2160}], "variants": {}, "id": "v3nUqdM-I-azEYHHZp7B5iu55trREQB_4lBrJlrXTo0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yyk7mz", "is_robot_indexable": true, "report_reasons": null, "author": "ironman52885", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyk7mz/wd_easystore_14tb_external_usb_30_hard_drive_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yyk7mz/wd_easystore_14tb_external_usb_30_hard_drive_for/", "subreddit_subscribers": 654755, "created_utc": 1668778440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "like on the title. i want to archive everything from the artists that I followed on Twitter just in case...", "author_fullname": "t2_60xzq29b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I download all media (pictures and videos) from everyone that I follow on Twitter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyi25r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668771657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;like on the title. i want to archive everything from the artists that I followed on Twitter just in case...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "newbie | 4TB ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyi25r", "is_robot_indexable": true, "report_reasons": null, "author": "HoangDung007", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yyi25r/how_can_i_download_all_media_pictures_and_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yyi25r/how_can_i_download_all_media_pictures_and_videos/", "subreddit_subscribers": 654755, "created_utc": 1668771657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "*Note: I do have all the data I'm referring to backed up elsewhere*\n\nI'm on Windows 10 Pro and I've got two identical dynamic disks -- 3TB each with 2.4TB (or whatever) filled, each. I want to convert them to spanned or striped (haven't decided which, yet) but I don't seem to have the option -- it's greyed out in Disk Manager:\n\nhttps://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81\n\nDo I...need to shrink the disks to make room? I'm hesitant to try that without getting thoughts from others because it'll take the better part of a day, each disk. Do I...need to delete the volumes and create the array from scratch?\n\nI'd prefer a non-destructive option if possible. I'm willing to delete these because the data is backed up, but restoring it will be a PITA :)\n\nMuch appreciated.", "author_fullname": "t2_36jz3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows: Can dynamic disks not be converted to spanned/striped?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "media_metadata": {"b203wafbjm0a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94b6126cadbbbb1376df665d6a170fc771860b95"}, {"y": 135, "x": 216, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ea9718fef595c82c340ac3622c2b2a951e1052b"}, {"y": 200, "x": 320, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2265f302a5ff338b74ebf4de0a9b17ebb80d3252"}, {"y": 400, "x": 640, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7cd33c67ecfeec80a2ec3738e382e54eecd19144"}, {"y": 600, "x": 960, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=063565c5bab55cb108d470c859daba852a0a356d"}, {"y": 675, "x": 1080, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7893f3501b0345bcf7d35fda3017a05d217bb596"}], "s": {"y": 900, "x": 1440, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81"}, "id": "b203wafbjm0a1"}}, "name": "t3_yy99ds", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/vMb3XlrxBTmOW43DaRSvG5q-XrdleZUhBeYHO6lSV00.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668740163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Note: I do have all the data I&amp;#39;m referring to backed up elsewhere&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on Windows 10 Pro and I&amp;#39;ve got two identical dynamic disks -- 3TB each with 2.4TB (or whatever) filled, each. I want to convert them to spanned or striped (haven&amp;#39;t decided which, yet) but I don&amp;#39;t seem to have the option -- it&amp;#39;s greyed out in Disk Manager:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81\"&gt;https://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Do I...need to shrink the disks to make room? I&amp;#39;m hesitant to try that without getting thoughts from others because it&amp;#39;ll take the better part of a day, each disk. Do I...need to delete the volumes and create the array from scratch?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d prefer a non-destructive option if possible. I&amp;#39;m willing to delete these because the data is backed up, but restoring it will be a PITA :)&lt;/p&gt;\n\n&lt;p&gt;Much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy99ds", "is_robot_indexable": true, "report_reasons": null, "author": "eriksrx", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy99ds/windows_can_dynamic_disks_not_be_converted_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy99ds/windows_can_dynamic_disks_not_be_converted_to/", "subreddit_subscribers": 654755, "created_utc": 1668740163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Genuinely decent chance Twitter might pop in the near future.](https://twitter.com/alexeheath/status/1593399683086327808?s=20&amp;t=dfeufbahrPBgan8EqpYe1w) Does anyone know how to save tweets from an account en masse? Worried that some people might not archive their own things before it's too late. Sorry if this doesn't fit the sub?\n\nAll the tools I can find are pay-to-use which is wild, except archive.org which only saves the most recent page of tweets from an account? Like a few days' worth roughly.", "author_fullname": "t2_yeb0x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I archive other people's twitter accounts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy7yvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668736340.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://twitter.com/alexeheath/status/1593399683086327808?s=20&amp;amp;t=dfeufbahrPBgan8EqpYe1w\"&gt;Genuinely decent chance Twitter might pop in the near future.&lt;/a&gt; Does anyone know how to save tweets from an account en masse? Worried that some people might not archive their own things before it&amp;#39;s too late. Sorry if this doesn&amp;#39;t fit the sub?&lt;/p&gt;\n\n&lt;p&gt;All the tools I can find are pay-to-use which is wild, except archive.org which only saves the most recent page of tweets from an account? Like a few days&amp;#39; worth roughly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy7yvo", "is_robot_indexable": true, "report_reasons": null, "author": "SansFinalGuardian", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy7yvo/how_can_i_archive_other_peoples_twitter_accounts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy7yvo/how_can_i_archive_other_peoples_twitter_accounts/", "subreddit_subscribers": 654755, "created_utc": 1668736340.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've never done RAID/ZFS before, but now I need to keep files and have them with higher availability so not just backed up. Until now I was simply copying the data to 2 separate disks because the availability was not that important \n\nBut now I want to make sure that I can always access the files because I need to serve them from a web server\n\nWhat solution do I need? ZFS? RAID?\n\nI need to be able to access the files as if I would access regular disk on the system\n\nI was planning to install it on a separate VM on the network (It's a little private project I'm doing, nothing too important)", "author_fullname": "t2_58tud67t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I need ZFS or RAID in this case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy2qb4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668721930.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve never done RAID/ZFS before, but now I need to keep files and have them with higher availability so not just backed up. Until now I was simply copying the data to 2 separate disks because the availability was not that important &lt;/p&gt;\n\n&lt;p&gt;But now I want to make sure that I can always access the files because I need to serve them from a web server&lt;/p&gt;\n\n&lt;p&gt;What solution do I need? ZFS? RAID?&lt;/p&gt;\n\n&lt;p&gt;I need to be able to access the files as if I would access regular disk on the system&lt;/p&gt;\n\n&lt;p&gt;I was planning to install it on a separate VM on the network (It&amp;#39;s a little private project I&amp;#39;m doing, nothing too important)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy2qb4", "is_robot_indexable": true, "report_reasons": null, "author": "ligonsker", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy2qb4/do_i_need_zfs_or_raid_in_this_case/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy2qb4/do_i_need_zfs_or_raid_in_this_case/", "subreddit_subscribers": 654755, "created_utc": 1668721930.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I used to have a reddit account years ago that I deleted. I regret deleting it and want to view my old posts but don't know how to access it.", "author_fullname": "t2_toe2sph2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to find deleted posts from a reddit user?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yync9m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668786804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to have a reddit account years ago that I deleted. I regret deleting it and want to view my old posts but don&amp;#39;t know how to access it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yync9m", "is_robot_indexable": true, "report_reasons": null, "author": "throwawayaccount29_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yync9m/how_to_find_deleted_posts_from_a_reddit_user/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yync9m/how_to_find_deleted_posts_from_a_reddit_user/", "subreddit_subscribers": 654755, "created_utc": 1668786804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hiya. I've bought an old 8 bay server that came with\n\n* 4x 1TB drives in it\n* Supermicro X9DRH-7TF/7F/iTF/iF motherboard\n* Xeon E5-2603 1.80 GHz\n* 8gb or DDR3 1333 Mhz ram\n* Areca Arc-1222 pcie raid controller\n* A proprietary modified debian 6 by a local it company\n\nIt was working fine but then I thought let's try adding more drives into the remaining bays and see how that works\n\nWell. It didn't. I managed to expand the raidset but the proprietary software that runs filesharing was pretty obfuscated and locked down with how and what it sees. I've contacted the company and they said 'sorry mate, that things been out of support for years. buy a new one'\n\nSo I jumped onto the next logical step. Let's wipe the whole thing and install something else on it. So I wiped it and tried installing openmediavault.\n\nBooted into installer, all looking good.Installer sees all my raidcard volumes, all good.\n\nInstalled it on a dedicated 10GB OS partition on the raidcard, all good.\n\nReboot\n\nSee the grub window\n\nThen black screen with the cursor for 10 minutes\n\nThen barrage of errors\n\nThought maybe faulty USB stick or smtn, tried reinstalling from another stick and a usb SSD drive. Same result.\n\nNext thing I had tried is installing mediavault not onto a RAID volume but onto USB stick instead and booting from that.\n\nSurprise, that booted in just fine!\n\nBUT\n\nOS doesn't see any of the RAID card volumes. Well it only sees them partially\n\nIn the OS raidcard volumes sda, sdb can\u2019t be accessed or mounted.\n\nThey appear listed with **lsblk** and **cat /proc/partitions** and are listed in **/dev/disk/by-path** (but not in by-id/by-partuuid/by-uuid).\n\nBut they don't appear when using **fdisk -l** and similar commands\n\nI could find messages like\n\n*I/O error, dev sda, sector 0 op 0x0:(READ) flags 0x0 phys\\_seg 1 prio class 0*\n\n*Nov 11 10:35:06 openmediavault kernel: Buffer I/O error on dev sda, logical block 0, async page read*\n\nI've tried installing raidcard driver I had found on the Areca website\n\n**arcmsr\\_1.50.0X.09-2-OMV6.0.24-k5.16.0-0.bpo.4-amd64.deb**\n\nRebooted, but still had same issue of raid volumes being only partially seen.\n\nI had also installed their raidcard monitoring tool which works fine but doesn't see any raid controllers in the web gui it comes with.\n\nI've had emailed Areca support a month ago about this but considering its a 10 year old legacy product I doubt they'll ever come back to me.\n\nI'm 90% leaning towards it being a raidcard driver issue. Anyone familiar with these older Areca cards? Do I need to use an older OS possibly for it to work?\n\nIdeally I'd like to just install any OS on a Raidcard volume that would let me share drives over SMB.\n\nI think motherboard itself might have 8 sata ports so i could potentially plug drives into that but I'd lose any hot swapping ability by doing that.\n\nIs it possible to save this at all? It was working fine for few weeks with the original 4 drives and software in place till I decided to 'upgrade'...\n\nHere's a link to journalctllog of when I boot into OS off USB\n\n[https://pastebin.com/A7pum7UW](https://pastebin.com/A7pum7UW)", "author_fullname": "t2_16yjgj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling with a bargain server i got : /", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyeyvz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668761335.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668759924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hiya. I&amp;#39;ve bought an old 8 bay server that came with&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;4x 1TB drives in it&lt;/li&gt;\n&lt;li&gt;Supermicro X9DRH-7TF/7F/iTF/iF motherboard&lt;/li&gt;\n&lt;li&gt;Xeon E5-2603 1.80 GHz&lt;/li&gt;\n&lt;li&gt;8gb or DDR3 1333 Mhz ram&lt;/li&gt;\n&lt;li&gt;Areca Arc-1222 pcie raid controller&lt;/li&gt;\n&lt;li&gt;A proprietary modified debian 6 by a local it company&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It was working fine but then I thought let&amp;#39;s try adding more drives into the remaining bays and see how that works&lt;/p&gt;\n\n&lt;p&gt;Well. It didn&amp;#39;t. I managed to expand the raidset but the proprietary software that runs filesharing was pretty obfuscated and locked down with how and what it sees. I&amp;#39;ve contacted the company and they said &amp;#39;sorry mate, that things been out of support for years. buy a new one&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;So I jumped onto the next logical step. Let&amp;#39;s wipe the whole thing and install something else on it. So I wiped it and tried installing openmediavault.&lt;/p&gt;\n\n&lt;p&gt;Booted into installer, all looking good.Installer sees all my raidcard volumes, all good.&lt;/p&gt;\n\n&lt;p&gt;Installed it on a dedicated 10GB OS partition on the raidcard, all good.&lt;/p&gt;\n\n&lt;p&gt;Reboot&lt;/p&gt;\n\n&lt;p&gt;See the grub window&lt;/p&gt;\n\n&lt;p&gt;Then black screen with the cursor for 10 minutes&lt;/p&gt;\n\n&lt;p&gt;Then barrage of errors&lt;/p&gt;\n\n&lt;p&gt;Thought maybe faulty USB stick or smtn, tried reinstalling from another stick and a usb SSD drive. Same result.&lt;/p&gt;\n\n&lt;p&gt;Next thing I had tried is installing mediavault not onto a RAID volume but onto USB stick instead and booting from that.&lt;/p&gt;\n\n&lt;p&gt;Surprise, that booted in just fine!&lt;/p&gt;\n\n&lt;p&gt;BUT&lt;/p&gt;\n\n&lt;p&gt;OS doesn&amp;#39;t see any of the RAID card volumes. Well it only sees them partially&lt;/p&gt;\n\n&lt;p&gt;In the OS raidcard volumes sda, sdb can\u2019t be accessed or mounted.&lt;/p&gt;\n\n&lt;p&gt;They appear listed with &lt;strong&gt;lsblk&lt;/strong&gt; and &lt;strong&gt;cat /proc/partitions&lt;/strong&gt; and are listed in &lt;strong&gt;/dev/disk/by-path&lt;/strong&gt; (but not in by-id/by-partuuid/by-uuid).&lt;/p&gt;\n\n&lt;p&gt;But they don&amp;#39;t appear when using &lt;strong&gt;fdisk -l&lt;/strong&gt; and similar commands&lt;/p&gt;\n\n&lt;p&gt;I could find messages like&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;I/O error, dev sda, sector 0 op 0x0:(READ) flags 0x0 phys_seg 1 prio class 0&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Nov 11 10:35:06 openmediavault kernel: Buffer I/O error on dev sda, logical block 0, async page read&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried installing raidcard driver I had found on the Areca website&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;arcmsr_1.50.0X.09-2-OMV6.0.24-k5.16.0-0.bpo.4-amd64.deb&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Rebooted, but still had same issue of raid volumes being only partially seen.&lt;/p&gt;\n\n&lt;p&gt;I had also installed their raidcard monitoring tool which works fine but doesn&amp;#39;t see any raid controllers in the web gui it comes with.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had emailed Areca support a month ago about this but considering its a 10 year old legacy product I doubt they&amp;#39;ll ever come back to me.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m 90% leaning towards it being a raidcard driver issue. Anyone familiar with these older Areca cards? Do I need to use an older OS possibly for it to work?&lt;/p&gt;\n\n&lt;p&gt;Ideally I&amp;#39;d like to just install any OS on a Raidcard volume that would let me share drives over SMB.&lt;/p&gt;\n\n&lt;p&gt;I think motherboard itself might have 8 sata ports so i could potentially plug drives into that but I&amp;#39;d lose any hot swapping ability by doing that.&lt;/p&gt;\n\n&lt;p&gt;Is it possible to save this at all? It was working fine for few weeks with the original 4 drives and software in place till I decided to &amp;#39;upgrade&amp;#39;...&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a link to journalctllog of when I boot into OS off USB&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pastebin.com/A7pum7UW\"&gt;https://pastebin.com/A7pum7UW&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?auto=webp&amp;s=07c121a0180003f7373863af66192b6ff6a937da", "width": 150, "height": 150}, "resolutions": [{"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df9c6a296446d05d873c629a30253398c4d29c1b", "width": 108, "height": 108}], "variants": {}, "id": "OgFzGCIRw1ZxjMOSkfV1OiH-_nQiZl8rzSonmOAuhGs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyeyvz", "is_robot_indexable": true, "report_reasons": null, "author": "poliver1988", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyeyvz/struggling_with_a_bargain_server_i_got/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yyeyvz/struggling_with_a_bargain_server_i_got/", "subreddit_subscribers": 654755, "created_utc": 1668759924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all. So, I have here about 15 drives, with an eye toward getting a lot more. Currently they are stored in a full tower case, and a sans digital 5-bay rack. However, I would like to have them all consolidated into one case/rack. I want it to be just a plain storage unit, not an external raid controller; all that is handled by linux. Basically just a rack that provides power and slots. What should I be looking for?\r\n\r\nPS: totally unrelated, but why the hell is it so hard to find a full tower that doesn't have a bunch of stupid RGB with it? Grrr.", "author_fullname": "t2_nyaisr0l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hardware suggestions for storing drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyecpx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668757761.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668757530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. So, I have here about 15 drives, with an eye toward getting a lot more. Currently they are stored in a full tower case, and a sans digital 5-bay rack. However, I would like to have them all consolidated into one case/rack. I want it to be just a plain storage unit, not an external raid controller; all that is handled by linux. Basically just a rack that provides power and slots. What should I be looking for?&lt;/p&gt;\n\n&lt;p&gt;PS: totally unrelated, but why the hell is it so hard to find a full tower that doesn&amp;#39;t have a bunch of stupid RGB with it? Grrr.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyecpx", "is_robot_indexable": true, "report_reasons": null, "author": "the_purple_goat", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyecpx/hardware_suggestions_for_storing_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yyecpx/hardware_suggestions_for_storing_drives/", "subreddit_subscribers": 654755, "created_utc": 1668757530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have two needs:\n\n1. backup 1.5 TB of data on my main machine to an S3 bucket (B2, S3, etc...)\n2. sync a sub folder (of the 1.5 TB) between my laptop and main machine\n\nI've always used SyncThing to sync my two machines (#2). But then I came across GoodSync and see that it can do both: P2P syncing and backing up.\n\nI looked around for other products that do both but can't find any. I thought I would check here before I pull the trigger.", "author_fullname": "t2_5wpob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is GoodSync the only product that backup to S3 buckets AND P2P sync with another machine on the LAN?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy9y67", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668742248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have two needs:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;backup 1.5 TB of data on my main machine to an S3 bucket (B2, S3, etc...)&lt;/li&gt;\n&lt;li&gt;sync a sub folder (of the 1.5 TB) between my laptop and main machine&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ve always used SyncThing to sync my two machines (#2). But then I came across GoodSync and see that it can do both: P2P syncing and backing up.&lt;/p&gt;\n\n&lt;p&gt;I looked around for other products that do both but can&amp;#39;t find any. I thought I would check here before I pull the trigger.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy9y67", "is_robot_indexable": true, "report_reasons": null, "author": "imthenachoman", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy9y67/is_goodsync_the_only_product_that_backup_to_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy9y67/is_goodsync_the_only_product_that_backup_to_s3/", "subreddit_subscribers": 654755, "created_utc": 1668742248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "is there a way to archive an email mailbox and make it searchable by a simple webinterface or even just a good export into text files + attachements.", "author_fullname": "t2_mdv6krfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "archive email mailbox and make it searchable", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy3bio", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668723359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;is there a way to archive an email mailbox and make it searchable by a simple webinterface or even just a good export into text files + attachements.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy3bio", "is_robot_indexable": true, "report_reasons": null, "author": "tillybowman", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy3bio/archive_email_mailbox_and_make_it_searchable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy3bio/archive_email_mailbox_and_make_it_searchable/", "subreddit_subscribers": 654755, "created_utc": 1668723359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What I am trying to achieve:\n\nI have some raw video files (uncompressed and without any codecs). Is it possible to compress the raw video file and then decompress it? I don't necessarily need to watch the compressed video, and therefore does not matter how bad it looks after compression. My goal is to store the raw file in the least amount of data (I know this sentence somewhat contradict itself). You can think of it like using zip. Basically compressing it to make it smaller and using it after extracting/decompressing.\n\nMy reason:\n\nLet's say you have some important \"raw\" video files, but they are too large, and you want to keep as much on a specific SSD or HDD.\n\nResearch:\n\nWhenever I search video and codecs it basically comes back to, lossy being able to reduce the size tremendously but looking as good as the original file and lossless being able to preserve the quality of a video while reducing the size. I read that the point of a lossless compression is that you can decompress it to its original file size, and that some lossless video codecs are able to reduce the size about 50%. If that's true, why don't people use lossless codecs like you would use ZIP or RAR? I cannot find a clear answer no matter how long I search, but I am probably just stupid.\n\nThanks for any replies.", "author_fullname": "t2_avg2w1xt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "compress and decompress raw video file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyr8ka", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668797706.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668796757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What I am trying to achieve:&lt;/p&gt;\n\n&lt;p&gt;I have some raw video files (uncompressed and without any codecs). Is it possible to compress the raw video file and then decompress it? I don&amp;#39;t necessarily need to watch the compressed video, and therefore does not matter how bad it looks after compression. My goal is to store the raw file in the least amount of data (I know this sentence somewhat contradict itself). You can think of it like using zip. Basically compressing it to make it smaller and using it after extracting/decompressing.&lt;/p&gt;\n\n&lt;p&gt;My reason:&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say you have some important &amp;quot;raw&amp;quot; video files, but they are too large, and you want to keep as much on a specific SSD or HDD.&lt;/p&gt;\n\n&lt;p&gt;Research:&lt;/p&gt;\n\n&lt;p&gt;Whenever I search video and codecs it basically comes back to, lossy being able to reduce the size tremendously but looking as good as the original file and lossless being able to preserve the quality of a video while reducing the size. I read that the point of a lossless compression is that you can decompress it to its original file size, and that some lossless video codecs are able to reduce the size about 50%. If that&amp;#39;s true, why don&amp;#39;t people use lossless codecs like you would use ZIP or RAR? I cannot find a clear answer no matter how long I search, but I am probably just stupid.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any replies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyr8ka", "is_robot_indexable": true, "report_reasons": null, "author": "UredNuubjuz", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyr8ka/compress_and_decompress_raw_video_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yyr8ka/compress_and_decompress_raw_video_file/", "subreddit_subscribers": 654755, "created_utc": 1668796757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Time for another giveaway! This time around, we are giving away an **IronWolf Pro 125 960GB SSD** to one lucky winner in this thread!\n\nHappy Holidays. We have reached out to the moderators regarding another giveaway because this subreddit is awesome and we love running activities with you all!\n\n**The prize is: one IronWolf Pro 125 960GB SSD**\n\nHow to enter:\n\nJust reply to this post once with a comment about what you are thankful for. **We ask entrants to please include the terms RunWithIronWolf and Seagate in your comment to be considered for the prize drawing.**\n\nFeel free to let us know just what you would do with the extra drive!\n\nSelection process/rules\n\nOne entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. **Entries are open until November 30th 2022, 23:59 UTC**. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, and not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.\n\nGeographic restrictions:\n\nOur policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions (e.g. the obvious shipping restrictions to Russia and Belarus currently)\n\nUS\n\nCanada (exc. Quebec and will require a basic skills-based question if winner is chosen by law)\n\nBrazil\n\nSouth America\n\nUnited Kingdom\n\nGermany\n\nFrance\n\nIberia\n\nAustralia\n\nNew Zealand\n\nKorea\n\nIndia\n\nMalaysia\n\nSingapore\n\nChina", "author_fullname": "t2_16nn7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Official Giveaway: November Seagate IronWolf Giveaway!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyqmrx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": "", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "OFFICIAL", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668795165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Time for another giveaway! This time around, we are giving away an &lt;strong&gt;IronWolf Pro 125 960GB SSD&lt;/strong&gt; to one lucky winner in this thread!&lt;/p&gt;\n\n&lt;p&gt;Happy Holidays. We have reached out to the moderators regarding another giveaway because this subreddit is awesome and we love running activities with you all!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The prize is: one IronWolf Pro 125 960GB SSD&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;How to enter:&lt;/p&gt;\n\n&lt;p&gt;Just reply to this post once with a comment about what you are thankful for. &lt;strong&gt;We ask entrants to please include the terms RunWithIronWolf and Seagate in your comment to be considered for the prize drawing.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Feel free to let us know just what you would do with the extra drive!&lt;/p&gt;\n\n&lt;p&gt;Selection process/rules&lt;/p&gt;\n\n&lt;p&gt;One entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. &lt;strong&gt;Entries are open until November 30th 2022, 23:59 UTC&lt;/strong&gt;. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, and not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.&lt;/p&gt;\n\n&lt;p&gt;Geographic restrictions:&lt;/p&gt;\n\n&lt;p&gt;Our policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions (e.g. the obvious shipping restrictions to Russia and Belarus currently)&lt;/p&gt;\n\n&lt;p&gt;US&lt;/p&gt;\n\n&lt;p&gt;Canada (exc. Quebec and will require a basic skills-based question if winner is chosen by law)&lt;/p&gt;\n\n&lt;p&gt;Brazil&lt;/p&gt;\n\n&lt;p&gt;South America&lt;/p&gt;\n\n&lt;p&gt;United Kingdom&lt;/p&gt;\n\n&lt;p&gt;Germany&lt;/p&gt;\n\n&lt;p&gt;France&lt;/p&gt;\n\n&lt;p&gt;Iberia&lt;/p&gt;\n\n&lt;p&gt;Australia&lt;/p&gt;\n\n&lt;p&gt;New Zealand&lt;/p&gt;\n\n&lt;p&gt;Korea&lt;/p&gt;\n\n&lt;p&gt;India&lt;/p&gt;\n\n&lt;p&gt;Malaysia&lt;/p&gt;\n\n&lt;p&gt;Singapore&lt;/p&gt;\n\n&lt;p&gt;China&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "81f0a58e-b3f5-11ea-95d7-0e4db8ecc231", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "OFFICIAL SEAGATE", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "yyqmrx", "is_robot_indexable": true, "report_reasons": null, "author": "Seagate_Surfer", "discussion_type": null, "num_comments": 29, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yyqmrx/official_giveaway_november_seagate_ironwolf/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/yyqmrx/official_giveaway_november_seagate_ironwolf/", "subreddit_subscribers": 654755, "created_utc": 1668795165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey, i'm on windows with a 1tb ssd plus a 4tb hdd for data.\n\nMy goal is to be able to not lose files in the event of one drive dying, with little maintenance and cost. All i care about is that if a drive ceases to function, the other will contain all data. Or at least a select few folders.\n\nI'm not sure whether i should opt for an internal or external storage option and i'm not sure whether 1 extra hdd can keep a backup of my other two drives.\n\nI also am not sure which software may be most suited for me, ease of use would be my focus, ideally with the backup drive only copying the needed files once every few days for example.\n\nI did check the wiki and i see a few options but i'm still not sure which route to go for and i hope y'all can help me out!\n\nThank you all in advance! &lt;3", "author_fullname": "t2_d36f1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have an SSD + HDD, what's a simple way to always have local backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyqk9f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668794988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, i&amp;#39;m on windows with a 1tb ssd plus a 4tb hdd for data.&lt;/p&gt;\n\n&lt;p&gt;My goal is to be able to not lose files in the event of one drive dying, with little maintenance and cost. All i care about is that if a drive ceases to function, the other will contain all data. Or at least a select few folders.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure whether i should opt for an internal or external storage option and i&amp;#39;m not sure whether 1 extra hdd can keep a backup of my other two drives.&lt;/p&gt;\n\n&lt;p&gt;I also am not sure which software may be most suited for me, ease of use would be my focus, ideally with the backup drive only copying the needed files once every few days for example.&lt;/p&gt;\n\n&lt;p&gt;I did check the wiki and i see a few options but i&amp;#39;m still not sure which route to go for and i hope y&amp;#39;all can help me out!&lt;/p&gt;\n\n&lt;p&gt;Thank you all in advance! &amp;lt;3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyqk9f", "is_robot_indexable": true, "report_reasons": null, "author": "iv2b", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyqk9f/i_have_an_ssd_hdd_whats_a_simple_way_to_always/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yyqk9f/i_have_an_ssd_hdd_whats_a_simple_way_to_always/", "subreddit_subscribers": 654755, "created_utc": 1668794988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_jtbkb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TerraMaster NAS hardware... is it a fly by night company? Their website has been down for a week", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_yyqbgo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TXK4QUpjyZadnRIyVaqBIorKlgjQFU217D3Uu2ZkMzc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668794379.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zbq5c7nv0r0a1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zbq5c7nv0r0a1.png?auto=webp&amp;s=da64268d9d3fa326628fc862c18c08c59c3219ec", "width": 1191, "height": 1267}, "resolutions": [{"url": "https://preview.redd.it/zbq5c7nv0r0a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6ac0b16ef087dbb4cd92905669e9f73c2ccb595", "width": 108, "height": 114}, {"url": "https://preview.redd.it/zbq5c7nv0r0a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=873e228e609f98d6457b2b69c3dd2df68c34305f", "width": 216, "height": 229}, {"url": "https://preview.redd.it/zbq5c7nv0r0a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d39c5a661452de1b5be2ed89d475ff90bb09494e", "width": 320, "height": 340}, {"url": "https://preview.redd.it/zbq5c7nv0r0a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=135580c44f779241c3ddc1cbc7736b3009b6a881", "width": 640, "height": 680}, {"url": "https://preview.redd.it/zbq5c7nv0r0a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6fbd4f2bf90a26001bb4a7cf9aa8b10bac96bb71", "width": 960, "height": 1021}, {"url": "https://preview.redd.it/zbq5c7nv0r0a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e489cd4e41ea572f09dded9bd2ea89856ff45313", "width": 1080, "height": 1148}], "variants": {}, "id": "QGwBMRTPc5ttUQs3ZBri2eyVMLQx_0g0LjpbNPlDqUQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yyqbgo", "is_robot_indexable": true, "report_reasons": null, "author": "Intelg", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyqbgo/terramaster_nas_hardware_is_it_a_fly_by_night/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zbq5c7nv0r0a1.png", "subreddit_subscribers": 654755, "created_utc": 1668794379.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I\u2019m finally installing two shucked 14TBs as I get ready to reconfigure my server over turkey break and I discovered a small annoyance/issue. Both drives are the same model (WD140EDGZ), support a physical sector size of 4096 bytes, and have the same firmware. The annoyance/problem is that one supports a logical sector size of 512/4096 bytes and the other says it only supports 256/2048 bytes. Both disks are connected to the same controller, so I don\u2019t think it an issue with that. \n\nIs this some odd Linux kernel issue or did WD limit the sector size to half for some odd reason? \n\nDoes one dive being set to 2048 mess with snapraid if all the others are 4096? \n\nIs there a way to fix it so it reports 4096 like the rest of my drives?", "author_fullname": "t2_5k304h3q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Firmware issue / bad shucking lottery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyoxy4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668790888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I\u2019m finally installing two shucked 14TBs as I get ready to reconfigure my server over turkey break and I discovered a small annoyance/issue. Both drives are the same model (WD140EDGZ), support a physical sector size of 4096 bytes, and have the same firmware. The annoyance/problem is that one supports a logical sector size of 512/4096 bytes and the other says it only supports 256/2048 bytes. Both disks are connected to the same controller, so I don\u2019t think it an issue with that. &lt;/p&gt;\n\n&lt;p&gt;Is this some odd Linux kernel issue or did WD limit the sector size to half for some odd reason? &lt;/p&gt;\n\n&lt;p&gt;Does one dive being set to 2048 mess with snapraid if all the others are 4096? &lt;/p&gt;\n\n&lt;p&gt;Is there a way to fix it so it reports 4096 like the rest of my drives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyoxy4", "is_robot_indexable": true, "report_reasons": null, "author": "nfg42", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyoxy4/firmware_issue_bad_shucking_lottery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yyoxy4/firmware_issue_bad_shucking_lottery/", "subreddit_subscribers": 654755, "created_utc": 1668790888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi !\n\nI currently own a raspberry pi 4b and I would like to extend the storage capacity with a 3.5\" hard drive of 4to (and be able to upgrade with by adding a new drive) to build a jellyfin (Plex) and backup server \n\nI found the argon eon case which looks great but is expensive (around 180$ in France with shipping).\n\nI also found this 2 bay docking station for around 50$:  https://www.amazon.fr/Sabrent-Ec-dflt-Drive-Dock-External/dp/B0759567JT/\n\nIs there major differences between what those can do in terms of specs ?\nI will probably buy an iron wolf 4to drive with CMR (for around 90$)", "author_fullname": "t2_4ytzqq2l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which Raspberry pi docking station to choose?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yynwbg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668788220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi !&lt;/p&gt;\n\n&lt;p&gt;I currently own a raspberry pi 4b and I would like to extend the storage capacity with a 3.5&amp;quot; hard drive of 4to (and be able to upgrade with by adding a new drive) to build a jellyfin (Plex) and backup server &lt;/p&gt;\n\n&lt;p&gt;I found the argon eon case which looks great but is expensive (around 180$ in France with shipping).&lt;/p&gt;\n\n&lt;p&gt;I also found this 2 bay docking station for around 50$:  &lt;a href=\"https://www.amazon.fr/Sabrent-Ec-dflt-Drive-Dock-External/dp/B0759567JT/\"&gt;https://www.amazon.fr/Sabrent-Ec-dflt-Drive-Dock-External/dp/B0759567JT/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Is there major differences between what those can do in terms of specs ?\nI will probably buy an iron wolf 4to drive with CMR (for around 90$)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yynwbg", "is_robot_indexable": true, "report_reasons": null, "author": "sarlaytos284", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yynwbg/which_raspberry_pi_docking_station_to_choose/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yynwbg/which_raspberry_pi_docking_station_to_choose/", "subreddit_subscribers": 654755, "created_utc": 1668788220.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}