{"kind": "Listing", "data": {"after": "t3_yyj1dc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&gt;Twitter has emailed staffers: \"Hi, Effective immediately, we are temporarily closing our office buildings and all badge access will be suspended. Offices will reopen on Monday, November 21st. .. We look forward to working with you on Twitter\u2019s exciting future.\"\n\n&amp;#x200B;\n\n&gt;Story to be updated soon with more: Am hearing that several \u201ccritical\u201d infra engineering teams at Twitter have completely resigned. \u201cYou cannot run Twitter without this team,\u201d one current engineer tells me of one such group. Also, Twitter has shut off badge access to its offices.\n\n&amp;#x200B;\n\n&gt;What I\u2019m hearing from Twitter employees; It looks like roughly 75% of the remaining 3,700ish Twitter employees have not opted to stay after the \u201chardcore\u201d email.  \n&gt;  \n&gt;Even though the deadline has passed, everyone still has access to their systems.\n\n&amp;#x200B;\n\n&gt;\u201cI know of six critical systems (like \u2018serving tweets\u2019 levels of critical) which no longer have any engineers,\" the former employee said. \"There is no longer even a skeleton crew manning the system. It will continue to coast until it runs into something, and then it will stop.\u201d  \n&gt;  \n&gt;Resignations and departures were already taking a toll on Twitter\u2019s service, employees said. \u201cBreakages are already happening slowly and accumulating,\u201d one said. \u201cIf you want to export your tweets, do it now.\u201d\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Link 1](https://twitter.com/oliverdarcy/status/1593394621627138048)\n\n[Link 2](https://twitter.com/alexeheath/status/1593399683086327808)\n\n[Link 3](https://twitter.com/kyliebytes/status/1593391167718113280)\n\n[Link 4](https://www.washingtonpost.com/technology/2022/11/17/twitter-musk-easing-rto-order/)\n\n&amp;#x200B;\n\nEdit:\n\n[twitter-scraper (github no api-key needed)](https://github.com/n0madic/twitter-scraper)\n\n[twitter-media-downloader (github no api-key needed)](https://github.com/mmpx12/twitter-media-downloader)\n\n&amp;#x200B;\n\nEdit2:\n\n[https://github.com/markowanga/stweet](https://github.com/markowanga/stweet)\n\n&amp;#x200B;\n\nEdit3:\n\n[gallery-dl guide by /u/Scripter17](https://www.reddit.com/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/)\n\n&amp;#x200B;\n\nEdit4:\n\n[Twitter Media Downloader](https://chrome.google.com/webstore/detail/twitter-media-downloader/cblpjenafgeohmnjknfhpdbdljfkndig?hl=en)", "author_fullname": "t2_ioi0j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup twitter now! Multiple critical infra teams have resigned", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy7tig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 550, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 550, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668776690.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668735892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Twitter has emailed staffers: &amp;quot;Hi, Effective immediately, we are temporarily closing our office buildings and all badge access will be suspended. Offices will reopen on Monday, November 21st. .. We look forward to working with you on Twitter\u2019s exciting future.&amp;quot;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Story to be updated soon with more: Am hearing that several \u201ccritical\u201d infra engineering teams at Twitter have completely resigned. \u201cYou cannot run Twitter without this team,\u201d one current engineer tells me of one such group. Also, Twitter has shut off badge access to its offices.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;What I\u2019m hearing from Twitter employees; It looks like roughly 75% of the remaining 3,700ish Twitter employees have not opted to stay after the \u201chardcore\u201d email.  &lt;/p&gt;\n\n&lt;p&gt;Even though the deadline has passed, everyone still has access to their systems.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;\u201cI know of six critical systems (like \u2018serving tweets\u2019 levels of critical) which no longer have any engineers,&amp;quot; the former employee said. &amp;quot;There is no longer even a skeleton crew manning the system. It will continue to coast until it runs into something, and then it will stop.\u201d  &lt;/p&gt;\n\n&lt;p&gt;Resignations and departures were already taking a toll on Twitter\u2019s service, employees said. \u201cBreakages are already happening slowly and accumulating,\u201d one said. \u201cIf you want to export your tweets, do it now.\u201d&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/oliverdarcy/status/1593394621627138048\"&gt;Link 1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/alexeheath/status/1593399683086327808\"&gt;Link 2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/kyliebytes/status/1593391167718113280\"&gt;Link 3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.washingtonpost.com/technology/2022/11/17/twitter-musk-easing-rto-order/\"&gt;Link 4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/n0madic/twitter-scraper\"&gt;twitter-scraper (github no api-key needed)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mmpx12/twitter-media-downloader\"&gt;twitter-media-downloader (github no api-key needed)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit2:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/markowanga/stweet\"&gt;https://github.com/markowanga/stweet&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit3:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/\"&gt;gallery-dl guide by /u/Scripter17&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit4:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://chrome.google.com/webstore/detail/twitter-media-downloader/cblpjenafgeohmnjknfhpdbdljfkndig?hl=en\"&gt;Twitter Media Downloader&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yy7tig", "is_robot_indexable": true, "report_reasons": null, "author": "fourDnet", "discussion_type": null, "num_comments": 213, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy7tig/backup_twitter_now_multiple_critical_infra_teams/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/yy7tig/backup_twitter_now_multiple_critical_infra_teams/", "subreddit_subscribers": 654616, "created_utc": 1668735892.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5s2jw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RIP unlimited Google workspace for education so sad", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_yxuyof", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 326, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 326, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZJzgS5JeIqSiM-S4S_ZMfo9x_ok42HTD7BQN1dBqruA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668703414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.imgur.com/WIq41oo.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?auto=webp&amp;s=0735b911b2f5092d8e3a6f537ab26c58dfdc480b", "width": 1440, "height": 3120}, "resolutions": [{"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f556e8d409447f35d120f43223d24293101359cd", "width": 108, "height": 216}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7b195ecb97e447e8fb122db7364de94d876fd94a", "width": 216, "height": 432}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b630ff166942e4ba407be4f0d2a3262dbc86c259", "width": 320, "height": 640}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5d9d5f1acf434fafac965e8287ab8c8f05f14959", "width": 640, "height": 1280}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ea00e99830f8772720df410dbc757afae117f6e7", "width": 960, "height": 1920}, {"url": "https://external-preview.redd.it/yX6To4rdytjMfe6or_usFNmQ_HIdy3vH0xhkqoRGm-U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f3d8a3b27fc330b38d13e49555f63d6a1318a08", "width": 1080, "height": 2160}], "variants": {}, "id": "tXJO2nqpeXoWEfbeINadLiA5MRKOwAiwWes_Q_YJ3iM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxuyof", "is_robot_indexable": true, "report_reasons": null, "author": "UACEENGR", "discussion_type": null, "num_comments": 111, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxuyof/rip_unlimited_google_workspace_for_education_so/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.imgur.com/WIq41oo.jpg", "subreddit_subscribers": 654616, "created_utc": 1668703414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Rewritten for clarity because speedrunning a post like this tends to leave questions\n\nHow to get started:\n\n1. Install [Python](https://www.python.org/). There is a standalone .exe but this just makes it easier to upgrade and all that\n\n2. Run `pip install gallery-dl` in command prompt (windows) or Bash (Linux)\n\n3. From there running `gallery-dl &lt;url&gt;` in the same command line should download the url's contents\n\n## config.json\n\nThe config.json is located at `%APPDATA%\\gallery-dl\\config.json` (windows) and `/etc/gallery-dl.conf` (Linux)\n\nIf the folder/file doesn't exist, just making it yourself should work\n\nThe basic config I recommend is this. If this is your first time with gallery-dl it's safe to just replace the entire file with this. If it's not your first time you should know how to transplant this into your existing config\n\n    {\n        \"extractor\":{\n            \"cookies\": [\"&lt;your browser (firefox, chromium, etc)&gt;\"],\n            \"twitter\":{\n                \"users\": \"https://twitter.com/{legacy[screen_name]}\",\n                \"text-tweets\":true,\n                \"retweets\":true,\n                \"quoted\":true,\n                \"logout\":true,\n                \"replies\":true,\n                \"postprocessors\":[\n                    {\"name\": \"metadata\", \"event\": \"post\", \"filename\": \"{tweet_id}_main.json\"}\n                ]\n            }\n        }\n    }\n\nThe documentation for the config.json is [here](https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst) and the specific part about getting cookies from your browser is [here](https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst#extractorcookies)\n\nCurrently supplying your login as a username/password combo seems to be broken. Idk if this is an issue with twitter or gallery-dl but using browser cookies is just easier in the long run\n\n## URLs:\n\n[The twitter API limits getting a user's page to the latest ~3200 tweets](https://github.com/mikf/gallery-dl/issues/2226). To get the as much as possible I recommend getting the main tab, the media tab, *and* the URL when you search for `from:&lt;user&gt;`\n\nTo make downloading the media tab not immediately exit when it sees a duplicate image, you'll want to add `-o skip=true` to the command you put in the command line. This can also be specified in the config. I have mine set to 20 when I'm just updating an existing download. If it sees 20 known images in a row then it moves on to the next one.\n\nThe 3 URLs I recommend downloading are:\n\n- `https://www.twitter.com/&lt;user&gt;`\n- `https://www.twitter.com/&lt;user&gt;/media`\n- `https://twitter.com/search?q=from:&lt;user&gt;`\n\nTo get someone's likes the URL is `https://www.twitter.com/&lt;user&gt;/likes`\n\nTo get your bookmarks the URL is `https://twitter.com/i/bookmarks`\n\n**Note**: Because twitter honestly just sucks and has for quite a while, you should run each download a few times (again with `-o skip=true`) to make sure you get everything\n\n## Commands:\n\nAnd the commands you're running should look like `gallery-dl &lt;url&gt; --write-metadata -o skip=true`\n\n`--write-metadata` saves `.json` files with metadata about each image. the `\"postprocessors\"` part of the config already writes the metadata for the tweet itself but the per-image metadata has some extra stuff\n\nIf you run `gallery-dl -g https://twitter.com/&lt;your handle&gt;/following` you can get a list of everyone you follow.\n\n### Windows:\n\nIf you have a text editor that supports regex replacement (CTRL+H in Sublime Text. Enable the button that looks like a .*), you can paste the list gallery-dl gave you and replace `(.+\\/)([^/\\r\\n]+)` with `gallery-dl $1$2               --write-metadata -o skip=true\\ngallery-dl $1$2/media         --write-metadata -o skip=true\\ngallery-dl $1search?q=from:$2 --write-metadata -o skip=true -o \"directory=[\"\"twitter\"\",\"\"{$2}\"\"]\"`\n\nYou should see something along the lines of\n\n    gallery-dl https://twitter.com/test1               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test1/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test1 --write-metadata -o skip=true -o \"directory=[\"\"twitter\"\",\"\"{test1}\"\"]\"\n    gallery-dl https://twitter.com/test2               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test2/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test2 --write-metadata -o skip=true -o \"directory=[\"\"twitter\"\",\"\"{test2}\"\"]\"\n    gallery-dl https://twitter.com/test3               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test3/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test3 --write-metadata -o skip=true -o \"directory=[\"\"twitter\"\",\"\"{test3}\"\"]\"\n\nThen put an `@echo off` at the top of the file and save it as a `.bat`\n\n### Linux:\n\nIf you have a text editor that supports regex replacement, you can paste the list gallery-dl gave you and replace `(.+\\/)([^/\\r\\n]+)` with `gallery-dl $1$2               --write-metadata -o skip=true\\ngallery-dl $1$2/media         --write-metadata -o skip=true\\ngallery-dl $1search?q=from:$2 --write-metadata -o skip=true -o \"directory=[\\\"twitter\\\",\\\"{$2}\\\"]\"`\n\nYou should see something along the lines of\n\n    gallery-dl https://twitter.com/test1               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test1/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test1 --write-metadata -o skip=true -o \"directory=[\\\"twitter\\\",\\\"{test1}\\\"]\"\n    gallery-dl https://twitter.com/test2               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test2/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test2 --write-metadata -o skip=true -o \"directory=[\\\"twitter\\\",\\\"{test2}\\\"]\"\n    gallery-dl https://twitter.com/test3               --write-metadata -o skip=true\n    gallery-dl https://twitter.com/test3/media         --write-metadata -o skip=true\n    gallery-dl https://twitter.com/search?q=from:test3 --write-metadata -o skip=true -o \"directory=[\\\"twitter\\\",\\\"{test3}\\\"]\"\n\nThen save it as a `.sh` file\n\nIf, on either OS, the resulting commands has a bunch of `$1` and `$2` in it, replace the `$`s in the replacement string with `\\`s and do it again.\n\nAfter that, running the file should (assuming I got all the steps right) download everyone you follow\n\n.\n\nNow, if you excuse me, it's almost 6am and I need to sleep. I really hope I haven't made any catastrophic errors", "author_fullname": "t2_yj3jz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For everyone using gallery-dl to backup twitter: Make sure you do it right", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy8o9w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668768882.0, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668738416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Rewritten for clarity because speedrunning a post like this tends to leave questions&lt;/p&gt;\n\n&lt;p&gt;How to get started:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Install &lt;a href=\"https://www.python.org/\"&gt;Python&lt;/a&gt;. There is a standalone .exe but this just makes it easier to upgrade and all that&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run &lt;code&gt;pip install gallery-dl&lt;/code&gt; in command prompt (windows) or Bash (Linux)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;From there running &lt;code&gt;gallery-dl &amp;lt;url&amp;gt;&lt;/code&gt; in the same command line should download the url&amp;#39;s contents&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h2&gt;config.json&lt;/h2&gt;\n\n&lt;p&gt;The config.json is located at &lt;code&gt;%APPDATA%\\gallery-dl\\config.json&lt;/code&gt; (windows) and &lt;code&gt;/etc/gallery-dl.conf&lt;/code&gt; (Linux)&lt;/p&gt;\n\n&lt;p&gt;If the folder/file doesn&amp;#39;t exist, just making it yourself should work&lt;/p&gt;\n\n&lt;p&gt;The basic config I recommend is this. If this is your first time with gallery-dl it&amp;#39;s safe to just replace the entire file with this. If it&amp;#39;s not your first time you should know how to transplant this into your existing config&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n    &amp;quot;extractor&amp;quot;:{\n        &amp;quot;cookies&amp;quot;: [&amp;quot;&amp;lt;your browser (firefox, chromium, etc)&amp;gt;&amp;quot;],\n        &amp;quot;twitter&amp;quot;:{\n            &amp;quot;users&amp;quot;: &amp;quot;https://twitter.com/{legacy[screen_name]}&amp;quot;,\n            &amp;quot;text-tweets&amp;quot;:true,\n            &amp;quot;retweets&amp;quot;:true,\n            &amp;quot;quoted&amp;quot;:true,\n            &amp;quot;logout&amp;quot;:true,\n            &amp;quot;replies&amp;quot;:true,\n            &amp;quot;postprocessors&amp;quot;:[\n                {&amp;quot;name&amp;quot;: &amp;quot;metadata&amp;quot;, &amp;quot;event&amp;quot;: &amp;quot;post&amp;quot;, &amp;quot;filename&amp;quot;: &amp;quot;{tweet_id}_main.json&amp;quot;}\n            ]\n        }\n    }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The documentation for the config.json is &lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst\"&gt;here&lt;/a&gt; and the specific part about getting cookies from your browser is &lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/configuration.rst#extractorcookies\"&gt;here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Currently supplying your login as a username/password combo seems to be broken. Idk if this is an issue with twitter or gallery-dl but using browser cookies is just easier in the long run&lt;/p&gt;\n\n&lt;h2&gt;URLs:&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mikf/gallery-dl/issues/2226\"&gt;The twitter API limits getting a user&amp;#39;s page to the latest ~3200 tweets&lt;/a&gt;. To get the as much as possible I recommend getting the main tab, the media tab, &lt;em&gt;and&lt;/em&gt; the URL when you search for &lt;code&gt;from:&amp;lt;user&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;To make downloading the media tab not immediately exit when it sees a duplicate image, you&amp;#39;ll want to add &lt;code&gt;-o skip=true&lt;/code&gt; to the command you put in the command line. This can also be specified in the config. I have mine set to 20 when I&amp;#39;m just updating an existing download. If it sees 20 known images in a row then it moves on to the next one.&lt;/p&gt;\n\n&lt;p&gt;The 3 URLs I recommend downloading are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;https://www.twitter.com/&amp;lt;user&amp;gt;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;https://www.twitter.com/&amp;lt;user&amp;gt;/media&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;https://twitter.com/search?q=from:&amp;lt;user&amp;gt;&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To get someone&amp;#39;s likes the URL is &lt;code&gt;https://www.twitter.com/&amp;lt;user&amp;gt;/likes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;To get your bookmarks the URL is &lt;code&gt;https://twitter.com/i/bookmarks&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Because twitter honestly just sucks and has for quite a while, you should run each download a few times (again with &lt;code&gt;-o skip=true&lt;/code&gt;) to make sure you get everything&lt;/p&gt;\n\n&lt;h2&gt;Commands:&lt;/h2&gt;\n\n&lt;p&gt;And the commands you&amp;#39;re running should look like &lt;code&gt;gallery-dl &amp;lt;url&amp;gt; --write-metadata -o skip=true&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;--write-metadata&lt;/code&gt; saves &lt;code&gt;.json&lt;/code&gt; files with metadata about each image. the &lt;code&gt;&amp;quot;postprocessors&amp;quot;&lt;/code&gt; part of the config already writes the metadata for the tweet itself but the per-image metadata has some extra stuff&lt;/p&gt;\n\n&lt;p&gt;If you run &lt;code&gt;gallery-dl -g https://twitter.com/&amp;lt;your handle&amp;gt;/following&lt;/code&gt; you can get a list of everyone you follow.&lt;/p&gt;\n\n&lt;h3&gt;Windows:&lt;/h3&gt;\n\n&lt;p&gt;If you have a text editor that supports regex replacement (CTRL+H in Sublime Text. Enable the button that looks like a .*), you can paste the list gallery-dl gave you and replace &lt;code&gt;(.+\\/)([^/\\r\\n]+)&lt;/code&gt; with &lt;code&gt;gallery-dl $1$2               --write-metadata -o skip=true\\ngallery-dl $1$2/media         --write-metadata -o skip=true\\ngallery-dl $1search?q=from:$2 --write-metadata -o skip=true -o &amp;quot;directory=[&amp;quot;&amp;quot;twitter&amp;quot;&amp;quot;,&amp;quot;&amp;quot;{$2}&amp;quot;&amp;quot;]&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;You should see something along the lines of&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gallery-dl https://twitter.com/test1               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test1/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test1 --write-metadata -o skip=true -o &amp;quot;directory=[&amp;quot;&amp;quot;twitter&amp;quot;&amp;quot;,&amp;quot;&amp;quot;{test1}&amp;quot;&amp;quot;]&amp;quot;\ngallery-dl https://twitter.com/test2               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test2/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test2 --write-metadata -o skip=true -o &amp;quot;directory=[&amp;quot;&amp;quot;twitter&amp;quot;&amp;quot;,&amp;quot;&amp;quot;{test2}&amp;quot;&amp;quot;]&amp;quot;\ngallery-dl https://twitter.com/test3               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test3/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test3 --write-metadata -o skip=true -o &amp;quot;directory=[&amp;quot;&amp;quot;twitter&amp;quot;&amp;quot;,&amp;quot;&amp;quot;{test3}&amp;quot;&amp;quot;]&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then put an &lt;code&gt;@echo off&lt;/code&gt; at the top of the file and save it as a &lt;code&gt;.bat&lt;/code&gt;&lt;/p&gt;\n\n&lt;h3&gt;Linux:&lt;/h3&gt;\n\n&lt;p&gt;If you have a text editor that supports regex replacement, you can paste the list gallery-dl gave you and replace &lt;code&gt;(.+\\/)([^/\\r\\n]+)&lt;/code&gt; with &lt;code&gt;gallery-dl $1$2               --write-metadata -o skip=true\\ngallery-dl $1$2/media         --write-metadata -o skip=true\\ngallery-dl $1search?q=from:$2 --write-metadata -o skip=true -o &amp;quot;directory=[\\&amp;quot;twitter\\&amp;quot;,\\&amp;quot;{$2}\\&amp;quot;]&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;You should see something along the lines of&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gallery-dl https://twitter.com/test1               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test1/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test1 --write-metadata -o skip=true -o &amp;quot;directory=[\\&amp;quot;twitter\\&amp;quot;,\\&amp;quot;{test1}\\&amp;quot;]&amp;quot;\ngallery-dl https://twitter.com/test2               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test2/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test2 --write-metadata -o skip=true -o &amp;quot;directory=[\\&amp;quot;twitter\\&amp;quot;,\\&amp;quot;{test2}\\&amp;quot;]&amp;quot;\ngallery-dl https://twitter.com/test3               --write-metadata -o skip=true\ngallery-dl https://twitter.com/test3/media         --write-metadata -o skip=true\ngallery-dl https://twitter.com/search?q=from:test3 --write-metadata -o skip=true -o &amp;quot;directory=[\\&amp;quot;twitter\\&amp;quot;,\\&amp;quot;{test3}\\&amp;quot;]&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then save it as a &lt;code&gt;.sh&lt;/code&gt; file&lt;/p&gt;\n\n&lt;p&gt;If, on either OS, the resulting commands has a bunch of &lt;code&gt;$1&lt;/code&gt; and &lt;code&gt;$2&lt;/code&gt; in it, replace the &lt;code&gt;$&lt;/code&gt;s in the replacement string with &lt;code&gt;\\&lt;/code&gt;s and do it again.&lt;/p&gt;\n\n&lt;p&gt;After that, running the file should (assuming I got all the steps right) download everyone you follow&lt;/p&gt;\n\n&lt;p&gt;.&lt;/p&gt;\n\n&lt;p&gt;Now, if you excuse me, it&amp;#39;s almost 6am and I need to sleep. I really hope I haven&amp;#39;t made any catastrophic errors&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XACIlvLRgD0CrsIsxO7yWQMICHPINiGesu3WjxQxeXs.jpg?auto=webp&amp;s=c7f0d77306cb94adced1c514958fdf68f575791c", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/XACIlvLRgD0CrsIsxO7yWQMICHPINiGesu3WjxQxeXs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0a156ee41a904137a12d7727f03ba51aa2a31c7", "width": 108, "height": 108}], "variants": {}, "id": "_QTobzuJkr1Zm6t-xAciOuvRRUG3sFX1cl1tVTmHCMU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "The sexiest data storage medium", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy8o9w", "is_robot_indexable": true, "report_reasons": null, "author": "Scripter17", "discussion_type": null, "num_comments": 51, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy8o9w/for_everyone_using_gallerydl_to_backup_twitter/", "subreddit_subscribers": 654616, "created_utc": 1668738416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I am probably way out of my depth here but r/Twitter redirected me here when I asked this question. In case Twitter actually does go away I am trying to download the entire Twitter feed of my best friend who passed away 5 years ago. I\u2019m currently just screenshotting favorites in a panic and googling returns old workarounds that don\u2019t work anymore. Any advice at all is appreciated, thank you.", "author_fullname": "t2_eo84n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download An Entire Twitter Feed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy8dii", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668737540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am probably way out of my depth here but &lt;a href=\"/r/Twitter\"&gt;r/Twitter&lt;/a&gt; redirected me here when I asked this question. In case Twitter actually does go away I am trying to download the entire Twitter feed of my best friend who passed away 5 years ago. I\u2019m currently just screenshotting favorites in a panic and googling returns old workarounds that don\u2019t work anymore. Any advice at all is appreciated, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy8dii", "is_robot_indexable": true, "report_reasons": null, "author": "plaidtuxedo", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy8dii/download_an_entire_twitter_feed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy8dii/download_an_entire_twitter_feed/", "subreddit_subscribers": 654616, "created_utc": 1668737540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a thread for current blackfriday storage discounts like last year?", "author_fullname": "t2_4y7ra1ap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Black Friday thread", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yyc1vh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668749117.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a thread for current blackfriday storage discounts like last year?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyc1vh", "is_robot_indexable": true, "report_reasons": null, "author": "HolUp-", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyc1vh/black_friday_thread/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yyc1vh/black_friday_thread/", "subreddit_subscribers": 654616, "created_utc": 1668749117.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there usually good deals for hard drives on these days? I'm finally building my first NAS and want to find good deals obviously.   Is it worth waiting for Black Friday / Cyber Monday deals or are they going to be just crap drives?\n\nThanks!", "author_fullname": "t2_140qwd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Black Friday/ Cyber Monday deals?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxw0kh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668705932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there usually good deals for hard drives on these days? I&amp;#39;m finally building my first NAS and want to find good deals obviously.   Is it worth waiting for Black Friday / Cyber Monday deals or are they going to be just crap drives?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxw0kh", "is_robot_indexable": true, "report_reasons": null, "author": "Mastasmoker", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxw0kh/black_friday_cyber_monday_deals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxw0kh/black_friday_cyber_monday_deals/", "subreddit_subscribers": 654616, "created_utc": 1668705932.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "About 5 months ago, arguably the largest database of doujinshi and manga (Doujinshi.org) seemingly went down over night. The site had thousands of contributors collecting information on even the most obscure and most niche indie works over nearly a DECADE. Most of which was untranslated, so you can imagine the treasure trove of information. A shit ton of stuff that not even hardcore weebs know exists.\n\nThe owner made [this](https://twitter.com/tenetan/status/1550839774969208833) tweet the day the site went offline. This is the only update they have given, ever. No tweets since, absolutely nothing. There isn't a discord or a subreddit or anything either where one would be able to contact them. They do however still seem to [like drawings of anime girls on twitter](https://i.imgur.com/b3jFx1K.png), unbothered by the countless of people asking for updates.\n\nFor all we know they could be in financial trouble or be sick or whatever. We simply do not know. But what is pissing me off to no end is the fact that they are radio silent. If you host a database that is almost entirely dependant on the community, you have the obligation and responsibility to inform them of what's become of their effort if you ask me. They \\*owe\\* communcation. Because at that point, the site isn't even the owners anymore.\n\nNow, I have a question to my fellow data hoarders, and it's the reason why I'm posting this in the first place. What the hell are we supposed to? There's no archives or backups anywhere; the site wasn't all too popular, evident by the fact that no one is even talking about this despite it being the biggest ressource for Japanese fan works.\n\nTo be honest, I really doubt it's going to come back. And I fear this might be another Library of Alexandria scenario...", "author_fullname": "t2_d5g3mn4d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Largest Doujinshi and Manga Lexicon Went Down 5 Months Ago and No One Cares", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxwkd5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668707282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;About 5 months ago, arguably the largest database of doujinshi and manga (Doujinshi.org) seemingly went down over night. The site had thousands of contributors collecting information on even the most obscure and most niche indie works over nearly a DECADE. Most of which was untranslated, so you can imagine the treasure trove of information. A shit ton of stuff that not even hardcore weebs know exists.&lt;/p&gt;\n\n&lt;p&gt;The owner made &lt;a href=\"https://twitter.com/tenetan/status/1550839774969208833\"&gt;this&lt;/a&gt; tweet the day the site went offline. This is the only update they have given, ever. No tweets since, absolutely nothing. There isn&amp;#39;t a discord or a subreddit or anything either where one would be able to contact them. They do however still seem to &lt;a href=\"https://i.imgur.com/b3jFx1K.png\"&gt;like drawings of anime girls on twitter&lt;/a&gt;, unbothered by the countless of people asking for updates.&lt;/p&gt;\n\n&lt;p&gt;For all we know they could be in financial trouble or be sick or whatever. We simply do not know. But what is pissing me off to no end is the fact that they are radio silent. If you host a database that is almost entirely dependant on the community, you have the obligation and responsibility to inform them of what&amp;#39;s become of their effort if you ask me. They *owe* communcation. Because at that point, the site isn&amp;#39;t even the owners anymore.&lt;/p&gt;\n\n&lt;p&gt;Now, I have a question to my fellow data hoarders, and it&amp;#39;s the reason why I&amp;#39;m posting this in the first place. What the hell are we supposed to? There&amp;#39;s no archives or backups anywhere; the site wasn&amp;#39;t all too popular, evident by the fact that no one is even talking about this despite it being the biggest ressource for Japanese fan works.&lt;/p&gt;\n\n&lt;p&gt;To be honest, I really doubt it&amp;#39;s going to come back. And I fear this might be another Library of Alexandria scenario...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A39rPq03PqwgcbXWQppLdcN5AeSiXDvlH7tPcHsAh1Q.png?auto=webp&amp;s=879d33faecaf0361ff93445bf2f7bac5b4bcafc5", "width": 586, "height": 695}, "resolutions": [{"url": "https://external-preview.redd.it/A39rPq03PqwgcbXWQppLdcN5AeSiXDvlH7tPcHsAh1Q.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7c2c17a640d7412041f27536106544263a54ead6", "width": 108, "height": 128}, {"url": "https://external-preview.redd.it/A39rPq03PqwgcbXWQppLdcN5AeSiXDvlH7tPcHsAh1Q.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=841ba32d3ece1119693ee620ffc6858977a5d73d", "width": 216, "height": 256}, {"url": "https://external-preview.redd.it/A39rPq03PqwgcbXWQppLdcN5AeSiXDvlH7tPcHsAh1Q.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1177f2e748708d2001e85c789d08ed532347397f", "width": 320, "height": 379}], "variants": {}, "id": "WFdf4X6f7FV-nDYfv84KJrMGtgaGBMHKgAzMvk6EscM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yxwkd5", "is_robot_indexable": true, "report_reasons": null, "author": "scremixz566", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxwkd5/largest_doujinshi_and_manga_lexicon_went_down_5/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxwkd5/largest_doujinshi_and_manga_lexicon_went_down_5/", "subreddit_subscribers": 654616, "created_utc": 1668707282.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Lots of options to scrape your tweets, but I have a lot of bookmarks I also wanna back up.\n\nI found a way to download all the tweet URLs into a csv ([Dewey](https://getdewey.co/)) and a way to download videos and gifs independently ([youtube-dl](https://youtube-dl-helper.github.io/)) but I can't find a way to do it all at the same time.\n\nEverything into a XML file would be ideal. Also if possible it would be nice for it to keep threads (which should be possible if not annoying)\n\nI found [this](https://gist.github.com/CJKinni/3063070) but it's very old code and has basically no chance to work", "author_fullname": "t2_ykkhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get all my Twitter bookmarks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy978v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668739972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lots of options to scrape your tweets, but I have a lot of bookmarks I also wanna back up.&lt;/p&gt;\n\n&lt;p&gt;I found a way to download all the tweet URLs into a csv (&lt;a href=\"https://getdewey.co/\"&gt;Dewey&lt;/a&gt;) and a way to download videos and gifs independently (&lt;a href=\"https://youtube-dl-helper.github.io/\"&gt;youtube-dl&lt;/a&gt;) but I can&amp;#39;t find a way to do it all at the same time.&lt;/p&gt;\n\n&lt;p&gt;Everything into a XML file would be ideal. Also if possible it would be nice for it to keep threads (which should be possible if not annoying)&lt;/p&gt;\n\n&lt;p&gt;I found &lt;a href=\"https://gist.github.com/CJKinni/3063070\"&gt;this&lt;/a&gt; but it&amp;#39;s very old code and has basically no chance to work&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?auto=webp&amp;s=e2d60ad36c7afe27744a4b4f63f20d3181954d4b", "width": 1015, "height": 494}, "resolutions": [{"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4de019e984fdeee8a89ce7e525e828568019fd28", "width": 108, "height": 52}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06f53d6a2f1e4248b34fb2643cd2df9e85f63585", "width": 216, "height": 105}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=11aa0debd38927b0c86148157dd6fb2be3425330", "width": 320, "height": 155}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a008abc197d7c09c8a6d7c679a7a14dfeb6aff84", "width": 640, "height": 311}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8407e59962bf38175018cc891c88bea773d2b655", "width": 960, "height": 467}], "variants": {}, "id": "D83W6ubnaf9JIZcLbn0OsGoknMj1En17FJrGkylXYzs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy978v", "is_robot_indexable": true, "report_reasons": null, "author": "PowderPhysics", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy978v/how_to_get_all_my_twitter_bookmarks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy978v/how_to_get_all_my_twitter_bookmarks/", "subreddit_subscribers": 654616, "created_utc": 1668739972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_tlgcl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A database of paper airplanes with easy to follow folding instructions, video tutorials and printable folding plans. Find the best paper airplanes that fly the furthest and stay aloft the longest.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_yyhavo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/prZ5H2m6FUcoXaVYAgI4mTKRMuTFDWbsdOBELddMgW8.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668769050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "foldnfly.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.foldnfly.com/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?auto=webp&amp;s=2e319bc9673fc40f1aec8c70febd71b2f84b10cb", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d37c2a5e835ea4c51b538aef5cbbd90f4e401217", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4d339c5a12d9cf0b1cf1a15d74138abbb400754", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b622d929f9312bf9d0f6019c52870b890a6d4ad", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d65b5cd84e8524d5d83e756f6a19fffc433d5b12", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=69a20c6d882a8d34f6c8dea2fec83e9b577a5f29", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/WVp0m9OUTYQ0kDtjcDJDARG5lVeC4REtkVmsdLIvLJ8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=94570fce0f309e51fd4ca001f841c5a99f97f58d", "width": 1080, "height": 567}], "variants": {}, "id": "uZ2zDM2Hy1FLstbjMCzVaGvaO0jNdvizg4F-yPk7isI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "40TB Synology DS1819+", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyhavo", "is_robot_indexable": true, "report_reasons": null, "author": "PaddleMonkey", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/yyhavo/a_database_of_paper_airplanes_with_easy_to_follow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.foldnfly.com/", "subreddit_subscribers": 654616, "created_utc": 1668769050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ihhqghp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just snagged this on Amazon UK Black Friday Sale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_yyg9f3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/t0QdmbTBFOZAyjfowwNYjHzCO76zS6S9QCtIRXq5XQ8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668765062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/pulb0zra3q0a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?auto=webp&amp;s=064dc021967fba1f97f6a98730f8780fd59ddfe3", "width": 1290, "height": 2796}, "resolutions": [{"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5639047d72e19e25073a3f3c2140ebe8085bc153", "width": 108, "height": 216}, {"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f439f75a13da20d16c4960144dcfa67c229334e", "width": 216, "height": 432}, {"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec9910faafefbed4dc9af1f9d1082bce7f97db6e", "width": 320, "height": 640}, {"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=86142c4fecd9aae10ef5121d62b444b9c6e11f0d", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8285610e80820165e22f600f2f4f7b9a156a13e2", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/pulb0zra3q0a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8aec81b4a6cd12f40e30865faf9c72aedee029ea", "width": 1080, "height": 2160}], "variants": {}, "id": "NpCU_UcXrABz_XqsWmyewmI77IdJg7pOzeHQxDQ3xOI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyg9f3", "is_robot_indexable": true, "report_reasons": null, "author": "arjan5", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyg9f3/just_snagged_this_on_amazon_uk_black_friday_sale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/pulb0zra3q0a1.jpg", "subreddit_subscribers": 654616, "created_utc": 1668765062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Genuinely decent chance Twitter might pop in the near future.](https://twitter.com/alexeheath/status/1593399683086327808?s=20&amp;t=dfeufbahrPBgan8EqpYe1w) Does anyone know how to save tweets from an account en masse? Worried that some people might not archive their own things before it's too late. Sorry if this doesn't fit the sub?\n\nAll the tools I can find are pay-to-use which is wild, except archive.org which only saves the most recent page of tweets from an account? Like a few days' worth roughly.", "author_fullname": "t2_yeb0x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I archive other people's twitter accounts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy7yvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668736340.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://twitter.com/alexeheath/status/1593399683086327808?s=20&amp;amp;t=dfeufbahrPBgan8EqpYe1w\"&gt;Genuinely decent chance Twitter might pop in the near future.&lt;/a&gt; Does anyone know how to save tweets from an account en masse? Worried that some people might not archive their own things before it&amp;#39;s too late. Sorry if this doesn&amp;#39;t fit the sub?&lt;/p&gt;\n\n&lt;p&gt;All the tools I can find are pay-to-use which is wild, except archive.org which only saves the most recent page of tweets from an account? Like a few days&amp;#39; worth roughly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy7yvo", "is_robot_indexable": true, "report_reasons": null, "author": "SansFinalGuardian", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy7yvo/how_can_i_archive_other_peoples_twitter_accounts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy7yvo/how_can_i_archive_other_peoples_twitter_accounts/", "subreddit_subscribers": 654616, "created_utc": 1668736340.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Putting together a small NAS with 6 x 4TB HGST SAS drives, HUS724040ALS640s, and was lucky enough to snag two of them as unused spares for \u00a330 a pop. The next four I've just picked up, and the first couple have come back with roughly 25,000 hours on them.\n\nObviously any reports are going to be anecdotal and not a guarantee, but I'm curious as to if any of you have experience with used enterprise drives and reliability in that region of use.\n\nFrom my own personal experience, it seems that if a drive is destined to die it tends to die relatively quickly, and the ones I've had in the past that made it to 10,000 hours typically lasted another 50k+.\n\nUltimately I spent \u00a330 a pop on each of these, so I'm tempted to just consider it a good deal regardless and put it out of my mind. Am I crazy?", "author_fullname": "t2_7cqcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Used Enterprise Drives and Power On Hours", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxsz7h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668698470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Putting together a small NAS with 6 x 4TB HGST SAS drives, HUS724040ALS640s, and was lucky enough to snag two of them as unused spares for \u00a330 a pop. The next four I&amp;#39;ve just picked up, and the first couple have come back with roughly 25,000 hours on them.&lt;/p&gt;\n\n&lt;p&gt;Obviously any reports are going to be anecdotal and not a guarantee, but I&amp;#39;m curious as to if any of you have experience with used enterprise drives and reliability in that region of use.&lt;/p&gt;\n\n&lt;p&gt;From my own personal experience, it seems that if a drive is destined to die it tends to die relatively quickly, and the ones I&amp;#39;ve had in the past that made it to 10,000 hours typically lasted another 50k+.&lt;/p&gt;\n\n&lt;p&gt;Ultimately I spent \u00a330 a pop on each of these, so I&amp;#39;m tempted to just consider it a good deal regardless and put it out of my mind. Am I crazy?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yxsz7h", "is_robot_indexable": true, "report_reasons": null, "author": "mdcdesign", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxsz7h/used_enterprise_drives_and_power_on_hours/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxsz7h/used_enterprise_drives_and_power_on_hours/", "subreddit_subscribers": 654616, "created_utc": 1668698470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Let's say\n\n- you have different purpose data (archive + volatile data)\n- too much to fit totally on local disk (both kinds)\n- backup over multiple companies, in case an account would be banned (there are enough reasons, even accidents, like banned google account because of youtube violence).\n\nI thought about the following concept:\n\nCold-Storage:\n- Backblaze\n- GCloud\n- one more?\n\nLocal (virtual) file system:  \n\n```\n/coldstorage (let it mostly unmounted, because of expensive read costs)\n  /backblaze\n    /dropbox-full-backup (incl. gdrive-backup)\n  /gcloud (cold tier)\n\n/dropbox (quiet fast)\n  /gdrive-backup (readonly)\n\n/gdrive (slow.....)  \n  /dropbox-backup  (readonly)\n```\n\nYou can also create an EncFS encrypted folder within the /dropbox folder (only for specific folders).\n\nSo, in this case you have two different volatile cloud storages, and they backup them each self to the other. Plus one big backup to the backblaze cold storage.\n\nCold Storage is also used for non volatile data like big ZIPS, archives, Camera Videos and so on.\n\nWhat do you think about this concept? It seems to too price, and i think, it's still performant. By the way, the sync between the multiple storages, i would them do within an small cloud computing instance, just for speed/internet speed bottle neck.", "author_fullname": "t2_44jxdyfx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redundant Cloud Storage Concept", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxwig0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668773499.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668707145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;you have different purpose data (archive + volatile data)&lt;/li&gt;\n&lt;li&gt;too much to fit totally on local disk (both kinds)&lt;/li&gt;\n&lt;li&gt;backup over multiple companies, in case an account would be banned (there are enough reasons, even accidents, like banned google account because of youtube violence).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I thought about the following concept:&lt;/p&gt;\n\n&lt;p&gt;Cold-Storage:\n- Backblaze\n- GCloud\n- one more?&lt;/p&gt;\n\n&lt;p&gt;Local (virtual) file system:  &lt;/p&gt;\n\n&lt;p&gt;```\n/coldstorage (let it mostly unmounted, because of expensive read costs)\n  /backblaze\n    /dropbox-full-backup (incl. gdrive-backup)\n  /gcloud (cold tier)&lt;/p&gt;\n\n&lt;p&gt;/dropbox (quiet fast)\n  /gdrive-backup (readonly)&lt;/p&gt;\n\n&lt;p&gt;/gdrive (slow.....)&lt;br/&gt;\n  /dropbox-backup  (readonly)\n```&lt;/p&gt;\n\n&lt;p&gt;You can also create an EncFS encrypted folder within the /dropbox folder (only for specific folders).&lt;/p&gt;\n\n&lt;p&gt;So, in this case you have two different volatile cloud storages, and they backup them each self to the other. Plus one big backup to the backblaze cold storage.&lt;/p&gt;\n\n&lt;p&gt;Cold Storage is also used for non volatile data like big ZIPS, archives, Camera Videos and so on.&lt;/p&gt;\n\n&lt;p&gt;What do you think about this concept? It seems to too price, and i think, it&amp;#39;s still performant. By the way, the sync between the multiple storages, i would them do within an small cloud computing instance, just for speed/internet speed bottle neck.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxwig0", "is_robot_indexable": true, "report_reasons": null, "author": "sebastian-loncar", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxwig0/redundant_cloud_storage_concept/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxwig0/redundant_cloud_storage_concept/", "subreddit_subscribers": 654616, "created_utc": 1668707145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So recently I heard plenty of rumours saying twitter will shut down, and so I decided to secure all my bookmarks. I already have my bookmarks synced in [dewey](https://getdewey.co/how-to-use/export-bookmarks/) and [BirdBear](https://birdbear.app/) yet I'm not sure if I'm going to be able to access the content after god forbid Twitter is gone.\n\nIs there a way to download them or store them in a cloud storage or whatever (i'm not experienced in this data stuff..)?", "author_fullname": "t2_65e2rxyh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to gain access to my twitter bookmarks forever even in case of Twitter shutdown?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yyiot5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668774562.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668773746.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So recently I heard plenty of rumours saying twitter will shut down, and so I decided to secure all my bookmarks. I already have my bookmarks synced in &lt;a href=\"https://getdewey.co/how-to-use/export-bookmarks/\"&gt;dewey&lt;/a&gt; and &lt;a href=\"https://birdbear.app/\"&gt;BirdBear&lt;/a&gt; yet I&amp;#39;m not sure if I&amp;#39;m going to be able to access the content after god forbid Twitter is gone.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to download them or store them in a cloud storage or whatever (i&amp;#39;m not experienced in this data stuff..)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?auto=webp&amp;s=e2d60ad36c7afe27744a4b4f63f20d3181954d4b", "width": 1015, "height": 494}, "resolutions": [{"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4de019e984fdeee8a89ce7e525e828568019fd28", "width": 108, "height": 52}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06f53d6a2f1e4248b34fb2643cd2df9e85f63585", "width": 216, "height": 105}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=11aa0debd38927b0c86148157dd6fb2be3425330", "width": 320, "height": 155}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a008abc197d7c09c8a6d7c679a7a14dfeb6aff84", "width": 640, "height": 311}, {"url": "https://external-preview.redd.it/AEdc_3tJelQQvC6DWwF19CLav4eOuAbCnyD661amR-4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8407e59962bf38175018cc891c88bea773d2b655", "width": 960, "height": 467}], "variants": {}, "id": "D83W6ubnaf9JIZcLbn0OsGoknMj1En17FJrGkylXYzs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyiot5", "is_robot_indexable": true, "report_reasons": null, "author": "AYMAAAAAAAAAAAAAAAAN", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyiot5/is_there_a_way_to_gain_access_to_my_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yyiot5/is_there_a_way_to_gain_access_to_my_twitter/", "subreddit_subscribers": 654616, "created_utc": 1668773746.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have two needs:\n\n1. backup 1.5 TB of data on my main machine to an S3 bucket (B2, S3, etc...)\n2. sync a sub folder (of the 1.5 TB) between my laptop and main machine\n\nI've always used SyncThing to sync my two machines (#2). But then I came across GoodSync and see that it can do both: P2P syncing and backing up.\n\nI looked around for other products that do both but can't find any. I thought I would check here before I pull the trigger.", "author_fullname": "t2_5wpob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is GoodSync the only product that backup to S3 buckets AND P2P sync with another machine on the LAN?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy9y67", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668742248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have two needs:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;backup 1.5 TB of data on my main machine to an S3 bucket (B2, S3, etc...)&lt;/li&gt;\n&lt;li&gt;sync a sub folder (of the 1.5 TB) between my laptop and main machine&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ve always used SyncThing to sync my two machines (#2). But then I came across GoodSync and see that it can do both: P2P syncing and backing up.&lt;/p&gt;\n\n&lt;p&gt;I looked around for other products that do both but can&amp;#39;t find any. I thought I would check here before I pull the trigger.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy9y67", "is_robot_indexable": true, "report_reasons": null, "author": "imthenachoman", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy9y67/is_goodsync_the_only_product_that_backup_to_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy9y67/is_goodsync_the_only_product_that_backup_to_s3/", "subreddit_subscribers": 654616, "created_utc": 1668742248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "*Note: I do have all the data I'm referring to backed up elsewhere*\n\nI'm on Windows 10 Pro and I've got two identical dynamic disks -- 3TB each with 2.4TB (or whatever) filled, each. I want to convert them to spanned or striped (haven't decided which, yet) but I don't seem to have the option -- it's greyed out in Disk Manager:\n\nhttps://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81\n\nDo I...need to shrink the disks to make room? I'm hesitant to try that without getting thoughts from others because it'll take the better part of a day, each disk. Do I...need to delete the volumes and create the array from scratch?\n\nI'd prefer a non-destructive option if possible. I'm willing to delete these because the data is backed up, but restoring it will be a PITA :)\n\nMuch appreciated.", "author_fullname": "t2_36jz3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows: Can dynamic disks not be converted to spanned/striped?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "media_metadata": {"b203wafbjm0a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94b6126cadbbbb1376df665d6a170fc771860b95"}, {"y": 135, "x": 216, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ea9718fef595c82c340ac3622c2b2a951e1052b"}, {"y": 200, "x": 320, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2265f302a5ff338b74ebf4de0a9b17ebb80d3252"}, {"y": 400, "x": 640, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7cd33c67ecfeec80a2ec3738e382e54eecd19144"}, {"y": 600, "x": 960, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=063565c5bab55cb108d470c859daba852a0a356d"}, {"y": 675, "x": 1080, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7893f3501b0345bcf7d35fda3017a05d217bb596"}], "s": {"y": 900, "x": 1440, "u": "https://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81"}, "id": "b203wafbjm0a1"}}, "name": "t3_yy99ds", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/vMb3XlrxBTmOW43DaRSvG5q-XrdleZUhBeYHO6lSV00.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668740163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Note: I do have all the data I&amp;#39;m referring to backed up elsewhere&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on Windows 10 Pro and I&amp;#39;ve got two identical dynamic disks -- 3TB each with 2.4TB (or whatever) filled, each. I want to convert them to spanned or striped (haven&amp;#39;t decided which, yet) but I don&amp;#39;t seem to have the option -- it&amp;#39;s greyed out in Disk Manager:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81\"&gt;https://preview.redd.it/b203wafbjm0a1.png?width=1440&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b57714bbcb5fb9e4fdc5ca8832838a0716c36e81&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Do I...need to shrink the disks to make room? I&amp;#39;m hesitant to try that without getting thoughts from others because it&amp;#39;ll take the better part of a day, each disk. Do I...need to delete the volumes and create the array from scratch?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d prefer a non-destructive option if possible. I&amp;#39;m willing to delete these because the data is backed up, but restoring it will be a PITA :)&lt;/p&gt;\n\n&lt;p&gt;Much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy99ds", "is_robot_indexable": true, "report_reasons": null, "author": "eriksrx", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy99ds/windows_can_dynamic_disks_not_be_converted_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy99ds/windows_can_dynamic_disks_not_be_converted_to/", "subreddit_subscribers": 654616, "created_utc": 1668740163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My current offsite backup plan involves keeping a bunch of encrypted drives in a garage in the midwest, where temps can get well below zero.  \n\n\nI know minimum operating temps for drives tend to be around 40f, but haven't seen anything for non operating temps. Drives will be left to warm up overnight before they're powered on to update  \n\n\nAm I at risk of damaging the drives doing this?", "author_fullname": "t2_97alswd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storing hard drives in the cold?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy8voh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668739016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My current offsite backup plan involves keeping a bunch of encrypted drives in a garage in the midwest, where temps can get well below zero.  &lt;/p&gt;\n\n&lt;p&gt;I know minimum operating temps for drives tend to be around 40f, but haven&amp;#39;t seen anything for non operating temps. Drives will be left to warm up overnight before they&amp;#39;re powered on to update  &lt;/p&gt;\n\n&lt;p&gt;Am I at risk of damaging the drives doing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy8voh", "is_robot_indexable": true, "report_reasons": null, "author": "tunafishnobread", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy8voh/storing_hard_drives_in_the_cold/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy8voh/storing_hard_drives_in_the_cold/", "subreddit_subscribers": 654616, "created_utc": 1668739016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So one of my clients at work has an iMac that is used as a Data Server. They are in the process of upgrading it to an actual Apple Server but it is a slow process due to cost. \n\nThe problem is, they have 4 external hard drives that are used for backup purposes. Two are used for Time Machine backups, and the other two are used for data backups for a program they use. The old solution was that they were just swapping them out every week, but then we ran into the problem of corrupt drives (due to a USB hub not having enough power and failing intermittently). I now want all 4 plugged into the device at once to prevent this nightmare of a situation. The problem is, the iMac only has 4 USB ports on the back, and 3 are occupied and cannot be moved. One is used by a personal scanner which cannot be connected to a USB hub, another is the USB hub mentioned earlier, and a third is the Ethernet adapter which cannot be moved due to the actual Ethernet port on the iMac being broken.\n\nI need some ideas, because currently only one backup can really work properly as there is only one USB port available. Open to any ideas, thank you.", "author_fullname": "t2_f53sz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some way to have 4 external hard drives running backups on an iMac with minimal USB hubs available.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy6uf2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668732946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So one of my clients at work has an iMac that is used as a Data Server. They are in the process of upgrading it to an actual Apple Server but it is a slow process due to cost. &lt;/p&gt;\n\n&lt;p&gt;The problem is, they have 4 external hard drives that are used for backup purposes. Two are used for Time Machine backups, and the other two are used for data backups for a program they use. The old solution was that they were just swapping them out every week, but then we ran into the problem of corrupt drives (due to a USB hub not having enough power and failing intermittently). I now want all 4 plugged into the device at once to prevent this nightmare of a situation. The problem is, the iMac only has 4 USB ports on the back, and 3 are occupied and cannot be moved. One is used by a personal scanner which cannot be connected to a USB hub, another is the USB hub mentioned earlier, and a third is the Ethernet adapter which cannot be moved due to the actual Ethernet port on the iMac being broken.&lt;/p&gt;\n\n&lt;p&gt;I need some ideas, because currently only one backup can really work properly as there is only one USB port available. Open to any ideas, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy6uf2", "is_robot_indexable": true, "report_reasons": null, "author": "AH_BareGarrett", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy6uf2/need_some_way_to_have_4_external_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy6uf2/need_some_way_to_have_4_external_hard_drives/", "subreddit_subscribers": 654616, "created_utc": 1668732946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "is there a way to archive an email mailbox and make it searchable by a simple webinterface or even just a good export into text files + attachements.", "author_fullname": "t2_mdv6krfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "archive email mailbox and make it searchable", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy3bio", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668723359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;is there a way to archive an email mailbox and make it searchable by a simple webinterface or even just a good export into text files + attachements.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy3bio", "is_robot_indexable": true, "report_reasons": null, "author": "tillybowman", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy3bio/archive_email_mailbox_and_make_it_searchable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy3bio/archive_email_mailbox_and_make_it_searchable/", "subreddit_subscribers": 654616, "created_utc": 1668723359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've never done RAID/ZFS before, but now I need to keep files and have them with higher availability so not just backed up. Until now I was simply copying the data to 2 separate disks because the availability was not that important \n\nBut now I want to make sure that I can always access the files because I need to serve them from a web server\n\nWhat solution do I need? ZFS? RAID?\n\nI need to be able to access the files as if I would access regular disk on the system\n\nI was planning to install it on a separate VM on the network (It's a little private project I'm doing, nothing too important)", "author_fullname": "t2_58tud67t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I need ZFS or RAID in this case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yy2qb4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668721930.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve never done RAID/ZFS before, but now I need to keep files and have them with higher availability so not just backed up. Until now I was simply copying the data to 2 separate disks because the availability was not that important &lt;/p&gt;\n\n&lt;p&gt;But now I want to make sure that I can always access the files because I need to serve them from a web server&lt;/p&gt;\n\n&lt;p&gt;What solution do I need? ZFS? RAID?&lt;/p&gt;\n\n&lt;p&gt;I need to be able to access the files as if I would access regular disk on the system&lt;/p&gt;\n\n&lt;p&gt;I was planning to install it on a separate VM on the network (It&amp;#39;s a little private project I&amp;#39;m doing, nothing too important)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yy2qb4", "is_robot_indexable": true, "report_reasons": null, "author": "ligonsker", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yy2qb4/do_i_need_zfs_or_raid_in_this_case/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yy2qb4/do_i_need_zfs_or_raid_in_this_case/", "subreddit_subscribers": 654616, "created_utc": 1668721930.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sorry if this is a stupid question, I'm completely clueless about this stuff.\n\nI have a huge collection of obscure gaming books and materials. Including scans of rare developer interviews.\n\nI was reading the ArchiveTeam wiki and I found out that you can upload to archive.org using a torrent.\n\nhttps://wiki.archiveteam.org/index.php/Internet_Archive#Uploading_to_archive.org\n\n\"Torrent upload, useful if you need resume (for huge files or because your bandwidth is insufficient for upload in one go)\"\n\nI don't have the best equipment so I want to use this method in case I have some sort of interruption.\n\nIs it recommended to use torrent upload? Is there any file size limits to it? Can it handle 10gb+ uploads?", "author_fullname": "t2_8rcmiabz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about using torrent upload for Internet Archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxzvvs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668715128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is a stupid question, I&amp;#39;m completely clueless about this stuff.&lt;/p&gt;\n\n&lt;p&gt;I have a huge collection of obscure gaming books and materials. Including scans of rare developer interviews.&lt;/p&gt;\n\n&lt;p&gt;I was reading the ArchiveTeam wiki and I found out that you can upload to archive.org using a torrent.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://wiki.archiveteam.org/index.php/Internet_Archive#Uploading_to_archive.org\"&gt;https://wiki.archiveteam.org/index.php/Internet_Archive#Uploading_to_archive.org&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Torrent upload, useful if you need resume (for huge files or because your bandwidth is insufficient for upload in one go)&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have the best equipment so I want to use this method in case I have some sort of interruption.&lt;/p&gt;\n\n&lt;p&gt;Is it recommended to use torrent upload? Is there any file size limits to it? Can it handle 10gb+ uploads?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxzvvs", "is_robot_indexable": true, "report_reasons": null, "author": "CitronDestroys", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxzvvs/question_about_using_torrent_upload_for_internet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxzvvs/question_about_using_torrent_upload_for_internet/", "subreddit_subscribers": 654616, "created_utc": 1668715128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This cooking website that I have followed for years has a clean, printable version of every recipe on a very similar web address that all follow the same format.\n\n[https://www.halfbakedharvest.com/wprm\\_print/26284](https://www.halfbakedharvest.com/wprm_print/26284)\n\n[https://www.halfbakedharvest.com/wprm\\_print/108579](https://www.halfbakedharvest.com/wprm_print/108579)\n\n[https://www.halfbakedharvest.com/wprm\\_print/110135](https://www.halfbakedharvest.com/wprm_print/110135)\n\n[https://www.halfbakedharvest.com/wprm\\_print/118540](https://www.halfbakedharvest.com/wprm_print/118540)\n\nThere are hundreds of recipes, but the numbers at the end of the address jumps at random intervals (ex. 540, 588, 607, 690). Is there someway a scraper or something could go through and check every number to test if a page is there and then download the webpage?", "author_fullname": "t2_hd6cp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a way to save recipes webpages from similar web addresses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxx8pt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668708898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This cooking website that I have followed for years has a clean, printable version of every recipe on a very similar web address that all follow the same format.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.halfbakedharvest.com/wprm_print/26284\"&gt;https://www.halfbakedharvest.com/wprm_print/26284&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.halfbakedharvest.com/wprm_print/108579\"&gt;https://www.halfbakedharvest.com/wprm_print/108579&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.halfbakedharvest.com/wprm_print/110135\"&gt;https://www.halfbakedharvest.com/wprm_print/110135&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.halfbakedharvest.com/wprm_print/118540\"&gt;https://www.halfbakedharvest.com/wprm_print/118540&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There are hundreds of recipes, but the numbers at the end of the address jumps at random intervals (ex. 540, 588, 607, 690). Is there someway a scraper or something could go through and check every number to test if a page is there and then download the webpage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxx8pt", "is_robot_indexable": true, "report_reasons": null, "author": "1000yardstareslacker", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxx8pt/looking_for_a_way_to_save_recipes_webpages_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxx8pt/looking_for_a_way_to_save_recipes_webpages_from/", "subreddit_subscribers": 654616, "created_utc": 1668708898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I tried OneDrive and Google Drive, but they don't suit my needs, primarily because any changes I make in the cloud are synced to my PC. Ideally, I'm looking for a service that is relatively reputable/secure/private and offers scheduled backups. I'm on Windows 11 and I work with lots of files. I'd appreciate any recommendations you guys have, thanks!", "author_fullname": "t2_dilor0ea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need a solid backup solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxtw4i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668700804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried OneDrive and Google Drive, but they don&amp;#39;t suit my needs, primarily because any changes I make in the cloud are synced to my PC. Ideally, I&amp;#39;m looking for a service that is relatively reputable/secure/private and offers scheduled backups. I&amp;#39;m on Windows 11 and I work with lots of files. I&amp;#39;d appreciate any recommendations you guys have, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yxtw4i", "is_robot_indexable": true, "report_reasons": null, "author": "nottapsycho", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxtw4i/i_need_a_solid_backup_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yxtw4i/i_need_a_solid_backup_solution/", "subreddit_subscribers": 654616, "created_utc": 1668700804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As I started researching online backup solutions, I realized we all mostly talk about backing up, but **we rarely talking about restoring**. More specifically, we rarely talk about the process of restoring, if it works, how well it works, etc. I read many folks say they couldn't restore cause of a corrupted backup because of the backup tool they used -- *not cool*.\n\nSo, I'm wondering, for folks who have had to restore data **and it was successful**, what tool were you using to do the backup + restore? Comment with your thoughts/experience.\n\n[View Poll](https://www.reddit.com/poll/yxqwnz)", "author_fullname": "t2_5wpob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What back tool did you use to backup **AND RESTORE** and how well did it work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yxqwnz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668693212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As I started researching online backup solutions, I realized we all mostly talk about backing up, but &lt;strong&gt;we rarely talking about restoring&lt;/strong&gt;. More specifically, we rarely talk about the process of restoring, if it works, how well it works, etc. I read many folks say they couldn&amp;#39;t restore cause of a corrupted backup because of the backup tool they used -- &lt;em&gt;not cool&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m wondering, for folks who have had to restore data &lt;strong&gt;and it was successful&lt;/strong&gt;, what tool were you using to do the backup + restore? Comment with your thoughts/experience.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/yxqwnz\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "yxqwnz", "is_robot_indexable": true, "report_reasons": null, "author": "imthenachoman", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1669298012653, "options": [{"text": "Backblaze Personal", "id": "19865732"}, {"text": "rclone", "id": "19865733"}, {"text": "Duplicati", "id": "19865734"}, {"text": "Duplicacy", "id": "19865735"}, {"text": "ARQ", "id": "19865736"}, {"text": "Veeam", "id": "19865737"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 55, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yxqwnz/what_back_tool_did_you_use_to_backup_and_restore/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/DataHoarder/comments/yxqwnz/what_back_tool_did_you_use_to_backup_and_restore/", "subreddit_subscribers": 654616, "created_utc": 1668693212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_d48yz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google and Amazon Helped the FBI Identify Z-Library's Operators", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yyj1dc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1668774878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "torrentfreak.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://torrentfreak.com/how-google-and-amazon-helped-the-fbi-identify-z-librarys-operators-221117/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yyj1dc", "is_robot_indexable": true, "report_reasons": null, "author": "Prometheus720", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yyj1dc/google_and_amazon_helped_the_fbi_identify/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://torrentfreak.com/how-google-and-amazon-helped-the-fbi-identify-z-librarys-operators-221117/", "subreddit_subscribers": 654616, "created_utc": 1668774878.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}