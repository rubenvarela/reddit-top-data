{"kind": "Listing", "data": {"after": null, "dist": 13, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm new to the data engineering world and I want to learn more about *good* ways to do this. I feel like the approach my team uses is a little convoluted, but I don't know a better way to do it just yet.\n\n\nMy team owns several data products and one in particular I think could be better. It consists of ~10 data sources in delta format that have foreign key values to join them in a relational model. There are also a lot of transformations happening on the datasets before &amp; after the joins. The joins are basically being done randomly throughout the code and it feels very inconsistent and hard to navigate as it is.\n\n\nIf these data sources were tables in a SQL database, I'd say to just use some kind of ORM and have all the joins in place when we load the tables as objects in the app. Is there an equivalent to that in the data engineering world? Or maybe a better way to do it? For context, I'm coming from a backend Java point of view so I guess I'm trying to relate it to that, but I also don't want to tunnel-vision towards something like that as an end goal if that's not the industry standard approach.\n\n\nAny advice on where to start? I'm looking for best practices and ideally code/project examples that are creating similar data products to the one I described.", "author_fullname": "t2_229opcl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for joining multiple datasets in a data pipeline (Pyspark)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysmdk9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668199124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to the data engineering world and I want to learn more about &lt;em&gt;good&lt;/em&gt; ways to do this. I feel like the approach my team uses is a little convoluted, but I don&amp;#39;t know a better way to do it just yet.&lt;/p&gt;\n\n&lt;p&gt;My team owns several data products and one in particular I think could be better. It consists of ~10 data sources in delta format that have foreign key values to join them in a relational model. There are also a lot of transformations happening on the datasets before &amp;amp; after the joins. The joins are basically being done randomly throughout the code and it feels very inconsistent and hard to navigate as it is.&lt;/p&gt;\n\n&lt;p&gt;If these data sources were tables in a SQL database, I&amp;#39;d say to just use some kind of ORM and have all the joins in place when we load the tables as objects in the app. Is there an equivalent to that in the data engineering world? Or maybe a better way to do it? For context, I&amp;#39;m coming from a backend Java point of view so I guess I&amp;#39;m trying to relate it to that, but I also don&amp;#39;t want to tunnel-vision towards something like that as an end goal if that&amp;#39;s not the industry standard approach.&lt;/p&gt;\n\n&lt;p&gt;Any advice on where to start? I&amp;#39;m looking for best practices and ideally code/project examples that are creating similar data products to the one I described.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ysmdk9", "is_robot_indexable": true, "report_reasons": null, "author": "y8MAC", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysmdk9/best_practices_for_joining_multiple_datasets_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysmdk9/best_practices_for_joining_multiple_datasets_in_a/", "subreddit_subscribers": 79689, "created_utc": 1668199124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Question regarding union. I know how it works but i am struggling with coming up with a practical use of it. Also it seems you can get the same results with just doing joins. \n\nWhen would you do a union function on a data set instead of a join(s).", "author_fullname": "t2_iafvx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Union / Union All operator", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysdmch", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668180497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Question regarding union. I know how it works but i am struggling with coming up with a practical use of it. Also it seems you can get the same results with just doing joins. &lt;/p&gt;\n\n&lt;p&gt;When would you do a union function on a data set instead of a join(s).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ysdmch", "is_robot_indexable": true, "report_reasons": null, "author": "duckvirus", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysdmch/union_union_all_operator/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysdmch/union_union_all_operator/", "subreddit_subscribers": 79689, "created_utc": 1668180497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just wondering if anyone have some good resources on how to choose cluster size for spark jobs. \n\nRecently i am faced with a task to extract data from a hive table than run some very simple processes on it before storing it in blob storage. These are huge tables with 30-40 Mil rows per year \n\ni am not sure how to pick my spark cluster size. Like # of executors, core and memory for thr executor and drivers.\n\nAnyone has some tips or resources on that. \n\nLike given the size of the data and thr partitions . How big should my cluster be?", "author_fullname": "t2_c3yqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Cluster Sizing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysgftu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668186442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wondering if anyone have some good resources on how to choose cluster size for spark jobs. &lt;/p&gt;\n\n&lt;p&gt;Recently i am faced with a task to extract data from a hive table than run some very simple processes on it before storing it in blob storage. These are huge tables with 30-40 Mil rows per year &lt;/p&gt;\n\n&lt;p&gt;i am not sure how to pick my spark cluster size. Like # of executors, core and memory for thr executor and drivers.&lt;/p&gt;\n\n&lt;p&gt;Anyone has some tips or resources on that. &lt;/p&gt;\n\n&lt;p&gt;Like given the size of the data and thr partitions . How big should my cluster be?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ysgftu", "is_robot_indexable": true, "report_reasons": null, "author": "543254447", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysgftu/spark_cluster_sizing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysgftu/spark_cluster_sizing/", "subreddit_subscribers": 79689, "created_utc": 1668186442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "we've got a main dataflow that handles/outputs about 1Million rows.  We've reached a point where, at best, we  schedule our dataflows and they end up running about every 2.25 hours.  My question is: is there a better (faster) solution that would allow us to transform the data and send back to salesforce that can perform faster?  I know it's not a lot of info here so I'm happy to answer any questions...", "author_fullname": "t2_59xk7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Salesforce/ Einstein/ CRAM Analytics, etc... alternatives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysu4da", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668219810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;we&amp;#39;ve got a main dataflow that handles/outputs about 1Million rows.  We&amp;#39;ve reached a point where, at best, we  schedule our dataflows and they end up running about every 2.25 hours.  My question is: is there a better (faster) solution that would allow us to transform the data and send back to salesforce that can perform faster?  I know it&amp;#39;s not a lot of info here so I&amp;#39;m happy to answer any questions...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ysu4da", "is_robot_indexable": true, "report_reasons": null, "author": "RawTuna", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysu4da/salesforce_einstein_cram_analytics_etc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysu4da/salesforce_einstein_cram_analytics_etc/", "subreddit_subscribers": 79689, "created_utc": 1668219810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I interviewed for a position as a Software Engineer II, although given the job description the position seems more like a data engineering position. For my interview, I presented two projects I finished and one project I was working on. The project I was working on was a machine-learning algorithm for 3D image segmentation. When they asked me questions about the project I finished, I answered well, but when asked questions about my ML algorithm, there were a few questions I couldn't answer (for instance, the accuracy of my algorithm compared to other algorithms) because I haven't got around to answering those questions. \n\nTwo days after the interview, however, I developed an algorithm to measure the accuracy of my ML algorithm. I got 98% accuracy. I'm going to calculate the accuracy of other, competing algorithms. I expect that they'll be a lot less accurate. Should I send this information to the hiring manager as a follow-up? I felt that the weakest part of my interview was the moments when I had to answer questions about my ML algorithm.\n\nThank you!", "author_fullname": "t2_2ve4vyvh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I follow up on an interview with information on a current project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysmqxu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668199917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I interviewed for a position as a Software Engineer II, although given the job description the position seems more like a data engineering position. For my interview, I presented two projects I finished and one project I was working on. The project I was working on was a machine-learning algorithm for 3D image segmentation. When they asked me questions about the project I finished, I answered well, but when asked questions about my ML algorithm, there were a few questions I couldn&amp;#39;t answer (for instance, the accuracy of my algorithm compared to other algorithms) because I haven&amp;#39;t got around to answering those questions. &lt;/p&gt;\n\n&lt;p&gt;Two days after the interview, however, I developed an algorithm to measure the accuracy of my ML algorithm. I got 98% accuracy. I&amp;#39;m going to calculate the accuracy of other, competing algorithms. I expect that they&amp;#39;ll be a lot less accurate. Should I send this information to the hiring manager as a follow-up? I felt that the weakest part of my interview was the moments when I had to answer questions about my ML algorithm.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "ysmqxu", "is_robot_indexable": true, "report_reasons": null, "author": "statius9", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysmqxu/should_i_follow_up_on_an_interview_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysmqxu/should_i_follow_up_on_an_interview_with/", "subreddit_subscribers": 79689, "created_utc": 1668199917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AWS just announced that now [Amazon RDS supports gp3 volumes](https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-rds-general-purpose-gp3-storage-volumes/). This is very exciting for a single reason: you could save up to 20% on costs when you move from gp2 to gp3 volumes. You can read more here: [https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/](https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/)\n\nIn order to see how much money you could save, you could use this calculator: [https://aws.amazon.com/ebs/resources/](https://aws.amazon.com/ebs/resources/)", "author_fullname": "t2_ugjew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon RDS now supports new General Purpose gp3 storage volumes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yshbtk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668188327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AWS just announced that now &lt;a href=\"https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-rds-general-purpose-gp3-storage-volumes/\"&gt;Amazon RDS supports gp3 volumes&lt;/a&gt;. This is very exciting for a single reason: you could save up to 20% on costs when you move from gp2 to gp3 volumes. You can read more here: &lt;a href=\"https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/\"&gt;https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In order to see how much money you could save, you could use this calculator: &lt;a href=\"https://aws.amazon.com/ebs/resources/\"&gt;https://aws.amazon.com/ebs/resources/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?auto=webp&amp;s=8e3eb77ba905bb641af80fcf3efe1de0190ac8c2", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b015da4f990706696f7d06ac19bc75b807d90200", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44756ae9c6e1724356ccaef8214086d7d0cc95da", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a5696e6599c7d56b3770650b416341ba2102fd8", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4610f9bb7893259c61ba4fda892295f0da1a05ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9979842391099359ecaa7d0ce4c8c31f1e3bead7", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fe7c4eb58196f897578137b50f669a4707c9902", "width": 1080, "height": 567}], "variants": {}, "id": "RUqh18uQTwuGJocqdUcC-6UfvfWS63SRDdr8AQqU3uM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yshbtk", "is_robot_indexable": true, "report_reasons": null, "author": "marcosluis2186", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yshbtk/amazon_rds_now_supports_new_general_purpose_gp3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yshbtk/amazon_rds_now_supports_new_general_purpose_gp3/", "subreddit_subscribers": 79689, "created_utc": 1668188327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Based on what should i choose my number of repartitions for example when writing dataframe to cluster, most of the internet says 4 times number of cores, but noone says why?What are the most important criteria here?Spark(pyspark)", "author_fullname": "t2_sx1wry60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "repartitions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysij46", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668191061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based on what should i choose my number of repartitions for example when writing dataframe to cluster, most of the internet says 4 times number of cores, but noone says why?What are the most important criteria here?Spark(pyspark)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ysij46", "is_robot_indexable": true, "report_reasons": null, "author": "AcceptableProcess772", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysij46/repartitions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysij46/repartitions/", "subreddit_subscribers": 79689, "created_utc": 1668191061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All! I'm new to this sub and data engineering in general. I have a background in software engineering and have used jupyter notebooks and pandas (or similar) for most things data related. I have used AWS S3 services via CLI and GUI, but have only recently discovered CDK and Infrastructure as Code.   \nI was wondering if someone could point me towards good resources for learning how to data cleanse from an S3 using Lambda functions? I would appreciate if it were in a pythonic way, similar to the CDK Workshop. I tried following the Data Cleansing/ETL script portion of the AWS Glue Workshop, but honestly did not understand it well. If that is the best approach, any friendly walkthroughs or guidance would also be appreciated. Thanks!", "author_fullname": "t2_5jsvrams", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for Data Cleansing with AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yspcka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668206160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All! I&amp;#39;m new to this sub and data engineering in general. I have a background in software engineering and have used jupyter notebooks and pandas (or similar) for most things data related. I have used AWS S3 services via CLI and GUI, but have only recently discovered CDK and Infrastructure as Code.&lt;br/&gt;\nI was wondering if someone could point me towards good resources for learning how to data cleanse from an S3 using Lambda functions? I would appreciate if it were in a pythonic way, similar to the CDK Workshop. I tried following the Data Cleansing/ETL script portion of the AWS Glue Workshop, but honestly did not understand it well. If that is the best approach, any friendly walkthroughs or guidance would also be appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yspcka", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden-Mango", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yspcka/resources_for_data_cleansing_with_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yspcka/resources_for_data_cleansing_with_aws/", "subreddit_subscribers": 79689, "created_utc": 1668206160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a free certification exam voucher from Microsoft for Exam PL-300: Microsoft Power BI Data Analyst, which expires in December 31 2022. I'd just finished some DE courses and was about to start building projects. \n\nI'd like to know if having a Power BI cert counts as something towards becoming a DE.\n\nSo, I'm a little confused here, should I purse my DE project and study for the Power BI exam or I should just put my focus on my projects?", "author_fullname": "t2_jvuhrmb0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Power BI certification, yay or nay?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysm1tt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668198435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a free certification exam voucher from Microsoft for Exam PL-300: Microsoft Power BI Data Analyst, which expires in December 31 2022. I&amp;#39;d just finished some DE courses and was about to start building projects. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to know if having a Power BI cert counts as something towards becoming a DE.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m a little confused here, should I purse my DE project and study for the Power BI exam or I should just put my focus on my projects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ysm1tt", "is_robot_indexable": true, "report_reasons": null, "author": "Necessary-Factor8861", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysm1tt/power_bi_certification_yay_or_nay/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysm1tt/power_bi_certification_yay_or_nay/", "subreddit_subscribers": 79689, "created_utc": 1668198435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Besides being a more senior data engineer or engineering manager, what other types of positions can a data engineer move into?\n\n\n\nI'm particularly interested in devops.  Is that type of career change possible?  I'm also interested in data engineering that is less SQL and more spark development.", "author_fullname": "t2_3v6ob2bf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the different types of career progression available to data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysim7f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668191234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Besides being a more senior data engineer or engineering manager, what other types of positions can a data engineer move into?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m particularly interested in devops.  Is that type of career change possible?  I&amp;#39;m also interested in data engineering that is less SQL and more spark development.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ysim7f", "is_robot_indexable": true, "report_reasons": null, "author": "data_preprocessing", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysim7f/what_are_the_different_types_of_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysim7f/what_are_the_different_types_of_career/", "subreddit_subscribers": 79689, "created_utc": 1668191234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to do a data engineering project on aws. I tried aws glue before using tutorials but that pretty much just get data from s3 transform it and load into some target like s3 or redshift.\nBut this time I want to create an automated ETL pipeline on aws.\nBut I don't know how to get with all the steps.\nMy main source of data would be an api like reddit or Twitter. Should I use lambda or glue to load data into s3.\nI want to know the steps from getting data from api to generating some dashboards.\nAnd at the end how to automate all that stuff so I can run it daily or weekly.\nIf someone can breakdown the process for me that would be very helpful.\nThanks", "author_fullname": "t2_r509bej6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "need help with aws project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yseqms", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668182872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to do a data engineering project on aws. I tried aws glue before using tutorials but that pretty much just get data from s3 transform it and load into some target like s3 or redshift.\nBut this time I want to create an automated ETL pipeline on aws.\nBut I don&amp;#39;t know how to get with all the steps.\nMy main source of data would be an api like reddit or Twitter. Should I use lambda or glue to load data into s3.\nI want to know the steps from getting data from api to generating some dashboards.\nAnd at the end how to automate all that stuff so I can run it daily or weekly.\nIf someone can breakdown the process for me that would be very helpful.\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yseqms", "is_robot_indexable": true, "report_reasons": null, "author": "mediocrX", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yseqms/need_help_with_aws_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yseqms/need_help_with_aws_project/", "subreddit_subscribers": 79689, "created_utc": 1668182872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently use everyaction which is a CRM, it works well, can search for clients easily and add notes. But can't search for business names, user interface is messy, hard to export data, hard to get data into a spreadsheet. Plus, it's mainly for fundraising.\n\nDo you know any good alternatives that's simple to use? I would use Excel/Sheets, but it seems to get slow after thousands of rows of data. Ideally my whole organization of 30 ppl would have access to this.", "author_fullname": "t2_6qasjhd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good database for nonprofit? Need a good CRM that's like a spreadsheet. Airtable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yt14ct", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668242324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently use everyaction which is a CRM, it works well, can search for clients easily and add notes. But can&amp;#39;t search for business names, user interface is messy, hard to export data, hard to get data into a spreadsheet. Plus, it&amp;#39;s mainly for fundraising.&lt;/p&gt;\n\n&lt;p&gt;Do you know any good alternatives that&amp;#39;s simple to use? I would use Excel/Sheets, but it seems to get slow after thousands of rows of data. Ideally my whole organization of 30 ppl would have access to this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yt14ct", "is_robot_indexable": true, "report_reasons": null, "author": "unamity1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yt14ct/good_database_for_nonprofit_need_a_good_crm_thats/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yt14ct/good_database_for_nonprofit_need_a_good_crm_thats/", "subreddit_subscribers": 79689, "created_utc": 1668242324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_10v76s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An Open Letter to Data Ninjas - Yes, You Need To Implement Data Contract System - I wrote about two personas I met in software engineering; \"The Reliability Ninja\" and our very own \"The Data Ninja.\" what kind of a Ninja are you?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_ysnavn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.29, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ZKZJZHh3UjEfbDzJwvtMY73RsANx0gHKgsyYVyOmd_4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668201088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringweekly.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dataengineeringweekly.com/p/an-open-letter-to-data-ninjas-yes", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/x9VoAMNPgUWw7_HFjYS9Q1ZDdadR6QPP_nHhzokO-GE.jpg?auto=webp&amp;s=25524d6e3148f7d9d97baf21a101f48a2e347b2f", "width": 600, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/x9VoAMNPgUWw7_HFjYS9Q1ZDdadR6QPP_nHhzokO-GE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=472b47681a508be3ba12a30be3efd44526e7398b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/x9VoAMNPgUWw7_HFjYS9Q1ZDdadR6QPP_nHhzokO-GE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cdada9738ffb3daaa4cb2e53347674b8a3cd1023", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/x9VoAMNPgUWw7_HFjYS9Q1ZDdadR6QPP_nHhzokO-GE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d1518f53d26e502e837e23594b13c8831ffed786", "width": 320, "height": 320}], "variants": {}, "id": "w8BWsxJSphv-JF7ZK0JvqyOaa1jlEHi8WOPWvych5Kg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "ysnavn", "is_robot_indexable": true, "report_reasons": null, "author": "vananth22", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysnavn/an_open_letter_to_data_ninjas_yes_you_need_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dataengineeringweekly.com/p/an-open-letter-to-data-ninjas-yes", "subreddit_subscribers": 79689, "created_utc": 1668201088.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}