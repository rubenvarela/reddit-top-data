{"kind": "Listing", "data": {"after": null, "dist": 18, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm new to the data engineering world and I want to learn more about *good* ways to do this. I feel like the approach my team uses is a little convoluted, but I don't know a better way to do it just yet.\n\n\nMy team owns several data products and one in particular I think could be better. It consists of ~10 data sources in delta format that have foreign key values to join them in a relational model. There are also a lot of transformations happening on the datasets before &amp; after the joins. The joins are basically being done randomly throughout the code and it feels very inconsistent and hard to navigate as it is.\n\n\nIf these data sources were tables in a SQL database, I'd say to just use some kind of ORM and have all the joins in place when we load the tables as objects in the app. Is there an equivalent to that in the data engineering world? Or maybe a better way to do it? For context, I'm coming from a backend Java point of view so I guess I'm trying to relate it to that, but I also don't want to tunnel-vision towards something like that as an end goal if that's not the industry standard approach.\n\n\nAny advice on where to start? I'm looking for best practices and ideally code/project examples that are creating similar data products to the one I described.", "author_fullname": "t2_229opcl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for joining multiple datasets in a data pipeline (Pyspark)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysmdk9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668199124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to the data engineering world and I want to learn more about &lt;em&gt;good&lt;/em&gt; ways to do this. I feel like the approach my team uses is a little convoluted, but I don&amp;#39;t know a better way to do it just yet.&lt;/p&gt;\n\n&lt;p&gt;My team owns several data products and one in particular I think could be better. It consists of ~10 data sources in delta format that have foreign key values to join them in a relational model. There are also a lot of transformations happening on the datasets before &amp;amp; after the joins. The joins are basically being done randomly throughout the code and it feels very inconsistent and hard to navigate as it is.&lt;/p&gt;\n\n&lt;p&gt;If these data sources were tables in a SQL database, I&amp;#39;d say to just use some kind of ORM and have all the joins in place when we load the tables as objects in the app. Is there an equivalent to that in the data engineering world? Or maybe a better way to do it? For context, I&amp;#39;m coming from a backend Java point of view so I guess I&amp;#39;m trying to relate it to that, but I also don&amp;#39;t want to tunnel-vision towards something like that as an end goal if that&amp;#39;s not the industry standard approach.&lt;/p&gt;\n\n&lt;p&gt;Any advice on where to start? I&amp;#39;m looking for best practices and ideally code/project examples that are creating similar data products to the one I described.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ysmdk9", "is_robot_indexable": true, "report_reasons": null, "author": "y8MAC", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysmdk9/best_practices_for_joining_multiple_datasets_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysmdk9/best_practices_for_joining_multiple_datasets_in_a/", "subreddit_subscribers": 79724, "created_utc": 1668199124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Question regarding union. I know how it works but i am struggling with coming up with a practical use of it. Also it seems you can get the same results with just doing joins. \n\nWhen would you do a union function on a data set instead of a join(s).", "author_fullname": "t2_iafvx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Union / Union All operator", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysdmch", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668180497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Question regarding union. I know how it works but i am struggling with coming up with a practical use of it. Also it seems you can get the same results with just doing joins. &lt;/p&gt;\n\n&lt;p&gt;When would you do a union function on a data set instead of a join(s).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ysdmch", "is_robot_indexable": true, "report_reasons": null, "author": "duckvirus", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysdmch/union_union_all_operator/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysdmch/union_union_all_operator/", "subreddit_subscribers": 79724, "created_utc": 1668180497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just wondering if anyone have some good resources on how to choose cluster size for spark jobs. \n\nRecently i am faced with a task to extract data from a hive table than run some very simple processes on it before storing it in blob storage. These are huge tables with 30-40 Mil rows per year \n\ni am not sure how to pick my spark cluster size. Like # of executors, core and memory for thr executor and drivers.\n\nAnyone has some tips or resources on that. \n\nLike given the size of the data and thr partitions . How big should my cluster be?", "author_fullname": "t2_c3yqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Cluster Sizing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysgftu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668186442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wondering if anyone have some good resources on how to choose cluster size for spark jobs. &lt;/p&gt;\n\n&lt;p&gt;Recently i am faced with a task to extract data from a hive table than run some very simple processes on it before storing it in blob storage. These are huge tables with 30-40 Mil rows per year &lt;/p&gt;\n\n&lt;p&gt;i am not sure how to pick my spark cluster size. Like # of executors, core and memory for thr executor and drivers.&lt;/p&gt;\n\n&lt;p&gt;Anyone has some tips or resources on that. &lt;/p&gt;\n\n&lt;p&gt;Like given the size of the data and thr partitions . How big should my cluster be?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ysgftu", "is_robot_indexable": true, "report_reasons": null, "author": "543254447", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysgftu/spark_cluster_sizing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysgftu/spark_cluster_sizing/", "subreddit_subscribers": 79724, "created_utc": 1668186442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_49cfbl1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A step-by-step tutorial on how to build a Web application, combining the Streamlit Python library and Versatile Data Kit", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_yt56bf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JSJ1at-aug_8Wr09JC-n8v5ArAv5sCv6YsmL9sJvL9U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668254590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "towardsdatascience.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://towardsdatascience.com/how-to-build-a-web-app-with-data-ingested-through-versatile-data-kit-ddae43b5f62d", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/O-BxnYA4LNFlgSeabIA4y22yTXBCn89My10u-_9LV5w.jpg?auto=webp&amp;s=09f119e8de087cd14fc803fd6c665041b459fd9e", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/O-BxnYA4LNFlgSeabIA4y22yTXBCn89My10u-_9LV5w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ac68c1926ee2f578a2748054dac7fee04808446", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/O-BxnYA4LNFlgSeabIA4y22yTXBCn89My10u-_9LV5w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=386933e4392d9bf87804d07e0a0c41fb6b290365", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/O-BxnYA4LNFlgSeabIA4y22yTXBCn89My10u-_9LV5w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7979389df95242ecda3b1f3cfced5bead603442e", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/O-BxnYA4LNFlgSeabIA4y22yTXBCn89My10u-_9LV5w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e8218b0e4c49a2654c3d531be2e467ac09c5bbd5", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/O-BxnYA4LNFlgSeabIA4y22yTXBCn89My10u-_9LV5w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b5389be094df87c82b368662bc8667b0a55751b", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/O-BxnYA4LNFlgSeabIA4y22yTXBCn89My10u-_9LV5w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d104a7ea27c69be40dcfd0adc96fa62eab661653", "width": 1080, "height": 720}], "variants": {}, "id": "CCkZnnkqPoVdK4oYReBfNPZZg2UGzVKlhwVORcrjUFk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yt56bf", "is_robot_indexable": true, "report_reasons": null, "author": "zverulacis", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yt56bf/a_stepbystep_tutorial_on_how_to_build_a_web/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://towardsdatascience.com/how-to-build-a-web-app-with-data-ingested-through-versatile-data-kit-ddae43b5f62d", "subreddit_subscribers": 79724, "created_utc": 1668254590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Edit:  \"CRM Analytics\" not CRAM\n\nwe've got a main dataflow that handles/outputs about 1Million rows.  We've reached a point where, at best, we  schedule our dataflows and they end up running about every 2.25 hours.  My question is: is there a better (faster) solution that would allow us to transform the data and send back to salesforce that can perform faster?  I know it's not a lot of info here so I'm happy to answer any questions...", "author_fullname": "t2_59xk7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Salesforce/ Einstein/ CRAM Analytics, etc... alternatives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysu4da", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668259176.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668219810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edit:  &amp;quot;CRM Analytics&amp;quot; not CRAM&lt;/p&gt;\n\n&lt;p&gt;we&amp;#39;ve got a main dataflow that handles/outputs about 1Million rows.  We&amp;#39;ve reached a point where, at best, we  schedule our dataflows and they end up running about every 2.25 hours.  My question is: is there a better (faster) solution that would allow us to transform the data and send back to salesforce that can perform faster?  I know it&amp;#39;s not a lot of info here so I&amp;#39;m happy to answer any questions...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ysu4da", "is_robot_indexable": true, "report_reasons": null, "author": "RawTuna", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysu4da/salesforce_einstein_cram_analytics_etc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysu4da/salesforce_einstein_cram_analytics_etc/", "subreddit_subscribers": 79724, "created_utc": 1668219810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I interviewed for a position as a Software Engineer II, although given the job description the position seems more like a data engineering position. For my interview, I presented two projects I finished and one project I was working on. The project I was working on was a machine-learning algorithm for 3D image segmentation. When they asked me questions about the project I finished, I answered well, but when asked questions about my ML algorithm, there were a few questions I couldn't answer (for instance, the accuracy of my algorithm compared to other algorithms) because I haven't got around to answering those questions. \n\nTwo days after the interview, however, I developed an algorithm to measure the accuracy of my ML algorithm. I got 98% accuracy. I'm going to calculate the accuracy of other, competing algorithms. I expect that they'll be a lot less accurate. Should I send this information to the hiring manager as a follow-up? I felt that the weakest part of my interview was the moments when I had to answer questions about my ML algorithm.\n\nThank you!", "author_fullname": "t2_2ve4vyvh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I follow up on an interview with information on a current project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysmqxu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668199917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I interviewed for a position as a Software Engineer II, although given the job description the position seems more like a data engineering position. For my interview, I presented two projects I finished and one project I was working on. The project I was working on was a machine-learning algorithm for 3D image segmentation. When they asked me questions about the project I finished, I answered well, but when asked questions about my ML algorithm, there were a few questions I couldn&amp;#39;t answer (for instance, the accuracy of my algorithm compared to other algorithms) because I haven&amp;#39;t got around to answering those questions. &lt;/p&gt;\n\n&lt;p&gt;Two days after the interview, however, I developed an algorithm to measure the accuracy of my ML algorithm. I got 98% accuracy. I&amp;#39;m going to calculate the accuracy of other, competing algorithms. I expect that they&amp;#39;ll be a lot less accurate. Should I send this information to the hiring manager as a follow-up? I felt that the weakest part of my interview was the moments when I had to answer questions about my ML algorithm.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "ysmqxu", "is_robot_indexable": true, "report_reasons": null, "author": "statius9", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysmqxu/should_i_follow_up_on_an_interview_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysmqxu/should_i_follow_up_on_an_interview_with/", "subreddit_subscribers": 79724, "created_utc": 1668199917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AWS just announced that now [Amazon RDS supports gp3 volumes](https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-rds-general-purpose-gp3-storage-volumes/). This is very exciting for a single reason: you could save up to 20% on costs when you move from gp2 to gp3 volumes. You can read more here: [https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/](https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/)\n\nIn order to see how much money you could save, you could use this calculator: [https://aws.amazon.com/ebs/resources/](https://aws.amazon.com/ebs/resources/)", "author_fullname": "t2_ugjew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon RDS now supports new General Purpose gp3 storage volumes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yshbtk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1668188327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AWS just announced that now &lt;a href=\"https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-rds-general-purpose-gp3-storage-volumes/\"&gt;Amazon RDS supports gp3 volumes&lt;/a&gt;. This is very exciting for a single reason: you could save up to 20% on costs when you move from gp2 to gp3 volumes. You can read more here: &lt;a href=\"https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/\"&gt;https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In order to see how much money you could save, you could use this calculator: &lt;a href=\"https://aws.amazon.com/ebs/resources/\"&gt;https://aws.amazon.com/ebs/resources/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?auto=webp&amp;s=8e3eb77ba905bb641af80fcf3efe1de0190ac8c2", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b015da4f990706696f7d06ac19bc75b807d90200", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44756ae9c6e1724356ccaef8214086d7d0cc95da", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a5696e6599c7d56b3770650b416341ba2102fd8", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4610f9bb7893259c61ba4fda892295f0da1a05ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9979842391099359ecaa7d0ce4c8c31f1e3bead7", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fe7c4eb58196f897578137b50f669a4707c9902", "width": 1080, "height": 567}], "variants": {}, "id": "RUqh18uQTwuGJocqdUcC-6UfvfWS63SRDdr8AQqU3uM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yshbtk", "is_robot_indexable": true, "report_reasons": null, "author": "marcosluis2186", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yshbtk/amazon_rds_now_supports_new_general_purpose_gp3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yshbtk/amazon_rds_now_supports_new_general_purpose_gp3/", "subreddit_subscribers": 79724, "created_utc": 1668188327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any architectural design patterns that follow the reactive programming paradigm for building data pipelines?\n\nI am aware of event-driven architectures. But what about reactivity?", "author_fullname": "t2_404pimvo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the reactive programming paradigm a thing in data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yt4gzd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668252521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any architectural design patterns that follow the reactive programming paradigm for building data pipelines?&lt;/p&gt;\n\n&lt;p&gt;I am aware of event-driven architectures. But what about reactivity?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yt4gzd", "is_robot_indexable": true, "report_reasons": null, "author": "unskilledexplorer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yt4gzd/is_the_reactive_programming_paradigm_a_thing_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yt4gzd/is_the_reactive_programming_paradigm_a_thing_in/", "subreddit_subscribers": 79724, "created_utc": 1668252521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Based on what should i choose my number of repartitions for example when writing dataframe to cluster, most of the internet says 4 times number of cores, but noone says why?What are the most important criteria here?Spark(pyspark)", "author_fullname": "t2_sx1wry60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "repartitions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysij46", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668191061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based on what should i choose my number of repartitions for example when writing dataframe to cluster, most of the internet says 4 times number of cores, but noone says why?What are the most important criteria here?Spark(pyspark)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ysij46", "is_robot_indexable": true, "report_reasons": null, "author": "AcceptableProcess772", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysij46/repartitions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysij46/repartitions/", "subreddit_subscribers": 79724, "created_utc": 1668191061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've had many jobs and interviews over the years, but for my current project (major federal agency) I had to get a low level security clearance (\"public trust\", the lowest one).\n\nAm I still allowed to say \"I used such and such data warehouse and orchestration tool at this organization\"? Does federal government care? ***Obviously*** I wouldn't share specifics about data. \n\nThey're behind the tech curve anyway (which is why I'm leaving!) so there's not much to discuss but I could see them being uptight about it. Anyway I just want to be careful.", "author_fullname": "t2_2o0q5m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When interviewing, how much are you allowed to say about the tools/systems you used?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yt8wpg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668264162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve had many jobs and interviews over the years, but for my current project (major federal agency) I had to get a low level security clearance (&amp;quot;public trust&amp;quot;, the lowest one).&lt;/p&gt;\n\n&lt;p&gt;Am I still allowed to say &amp;quot;I used such and such data warehouse and orchestration tool at this organization&amp;quot;? Does federal government care? &lt;strong&gt;&lt;em&gt;Obviously&lt;/em&gt;&lt;/strong&gt; I wouldn&amp;#39;t share specifics about data. &lt;/p&gt;\n\n&lt;p&gt;They&amp;#39;re behind the tech curve anyway (which is why I&amp;#39;m leaving!) so there&amp;#39;s not much to discuss but I could see them being uptight about it. Anyway I just want to be careful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yt8wpg", "is_robot_indexable": true, "report_reasons": null, "author": "rotterdamn8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yt8wpg/when_interviewing_how_much_are_you_allowed_to_say/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yt8wpg/when_interviewing_how_much_are_you_allowed_to_say/", "subreddit_subscribers": 79724, "created_utc": 1668264162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How are we all preparing for the recession? Do we see huge impact on data engineerings jobs?\nAs DE's What secondary skills to aquire to stay in the game?", "author_fullname": "t2_6kmo2ecy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recession impact!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yt6xft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668259323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are we all preparing for the recession? Do we see huge impact on data engineerings jobs?\nAs DE&amp;#39;s What secondary skills to aquire to stay in the game?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yt6xft", "is_robot_indexable": true, "report_reasons": null, "author": "soujoshi", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yt6xft/recession_impact/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yt6xft/recession_impact/", "subreddit_subscribers": 79724, "created_utc": 1668259323.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All! I'm new to this sub and data engineering in general. I have a background in software engineering and have used jupyter notebooks and pandas (or similar) for most things data related. I have used AWS S3 services via CLI and GUI, but have only recently discovered CDK and Infrastructure as Code.   \nI was wondering if someone could point me towards good resources for learning how to data cleanse from an S3 using Lambda functions? I would appreciate if it were in a pythonic way, similar to the CDK Workshop. I tried following the Data Cleansing/ETL script portion of the AWS Glue Workshop, but honestly did not understand it well. If that is the best approach, any friendly walkthroughs or guidance would also be appreciated. Thanks!", "author_fullname": "t2_5jsvrams", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for Data Cleansing with AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yspcka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668206160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All! I&amp;#39;m new to this sub and data engineering in general. I have a background in software engineering and have used jupyter notebooks and pandas (or similar) for most things data related. I have used AWS S3 services via CLI and GUI, but have only recently discovered CDK and Infrastructure as Code.&lt;br/&gt;\nI was wondering if someone could point me towards good resources for learning how to data cleanse from an S3 using Lambda functions? I would appreciate if it were in a pythonic way, similar to the CDK Workshop. I tried following the Data Cleansing/ETL script portion of the AWS Glue Workshop, but honestly did not understand it well. If that is the best approach, any friendly walkthroughs or guidance would also be appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yspcka", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden-Mango", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yspcka/resources_for_data_cleansing_with_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yspcka/resources_for_data_cleansing_with_aws/", "subreddit_subscribers": 79724, "created_utc": 1668206160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to understand the salting technique to remove skew data, but quite cant understand how exactly to implement it", "author_fullname": "t2_6nf1ab9x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to implement Salting Technique to remove data skewness?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yt75iw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668259888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to understand the salting technique to remove skew data, but quite cant understand how exactly to implement it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yt75iw", "is_robot_indexable": true, "report_reasons": null, "author": "jayeshneyyan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yt75iw/how_to_implement_salting_technique_to_remove_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yt75iw/how_to_implement_salting_technique_to_remove_data/", "subreddit_subscribers": 79724, "created_utc": 1668259888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am stuck at this problem where we have a 3rd party system giving us parquet files to process. They are converting json to parquet to save network cost, we run Glue job to process those parquet files. \n\nThe issue is that there is an address column, which is of array type, if the data is present in address field then Spark job runs fine, but if address is not present, the schema shows Int type instead of array type(for address) and I cant bulk process my parquet files. \n\nI tried to use mapPartitions to coerce my schema, but it too is failing. \n\nThe brute force way is to read files sequentially and keep on appending the dataframe, but I want to know if there is a better way to handle it.", "author_fullname": "t2_166reh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to handle parquet files of partially matching schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yt4n5z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668253018.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am stuck at this problem where we have a 3rd party system giving us parquet files to process. They are converting json to parquet to save network cost, we run Glue job to process those parquet files. &lt;/p&gt;\n\n&lt;p&gt;The issue is that there is an address column, which is of array type, if the data is present in address field then Spark job runs fine, but if address is not present, the schema shows Int type instead of array type(for address) and I cant bulk process my parquet files. &lt;/p&gt;\n\n&lt;p&gt;I tried to use mapPartitions to coerce my schema, but it too is failing. &lt;/p&gt;\n\n&lt;p&gt;The brute force way is to read files sequentially and keep on appending the dataframe, but I want to know if there is a better way to handle it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yt4n5z", "is_robot_indexable": true, "report_reasons": null, "author": "ezio20", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yt4n5z/best_way_to_handle_parquet_files_of_partially/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yt4n5z/best_way_to_handle_parquet_files_of_partially/", "subreddit_subscribers": 79724, "created_utc": 1668253018.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a free certification exam voucher from Microsoft for Exam PL-300: Microsoft Power BI Data Analyst, which expires in December 31 2022. I'd just finished some DE courses and was about to start building projects. \n\nI'd like to know if having a Power BI cert counts as something towards becoming a DE.\n\nSo, I'm a little confused here, should I purse my DE project and study for the Power BI exam or I should just put my focus on my projects?", "author_fullname": "t2_jvuhrmb0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Power BI certification, yay or nay?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysm1tt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668198435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a free certification exam voucher from Microsoft for Exam PL-300: Microsoft Power BI Data Analyst, which expires in December 31 2022. I&amp;#39;d just finished some DE courses and was about to start building projects. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to know if having a Power BI cert counts as something towards becoming a DE.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m a little confused here, should I purse my DE project and study for the Power BI exam or I should just put my focus on my projects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ysm1tt", "is_robot_indexable": true, "report_reasons": null, "author": "Necessary-Factor8861", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysm1tt/power_bi_certification_yay_or_nay/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysm1tt/power_bi_certification_yay_or_nay/", "subreddit_subscribers": 79724, "created_utc": 1668198435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Besides being a more senior data engineer or engineering manager, what other types of positions can a data engineer move into?\n\n\n\nI'm particularly interested in devops.  Is that type of career change possible?  I'm also interested in data engineering that is less SQL and more spark development.", "author_fullname": "t2_3v6ob2bf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the different types of career progression available to data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ysim7f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668191234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Besides being a more senior data engineer or engineering manager, what other types of positions can a data engineer move into?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m particularly interested in devops.  Is that type of career change possible?  I&amp;#39;m also interested in data engineering that is less SQL and more spark development.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ysim7f", "is_robot_indexable": true, "report_reasons": null, "author": "data_preprocessing", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysim7f/what_are_the_different_types_of_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ysim7f/what_are_the_different_types_of_career/", "subreddit_subscribers": 79724, "created_utc": 1668191234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to do a data engineering project on aws. I tried aws glue before using tutorials but that pretty much just get data from s3 transform it and load into some target like s3 or redshift.\nBut this time I want to create an automated ETL pipeline on aws.\nBut I don't know how to get with all the steps.\nMy main source of data would be an api like reddit or Twitter. Should I use lambda or glue to load data into s3.\nI want to know the steps from getting data from api to generating some dashboards.\nAnd at the end how to automate all that stuff so I can run it daily or weekly.\nIf someone can breakdown the process for me that would be very helpful.\nThanks", "author_fullname": "t2_r509bej6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "need help with aws project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yseqms", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668182872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to do a data engineering project on aws. I tried aws glue before using tutorials but that pretty much just get data from s3 transform it and load into some target like s3 or redshift.\nBut this time I want to create an automated ETL pipeline on aws.\nBut I don&amp;#39;t know how to get with all the steps.\nMy main source of data would be an api like reddit or Twitter. Should I use lambda or glue to load data into s3.\nI want to know the steps from getting data from api to generating some dashboards.\nAnd at the end how to automate all that stuff so I can run it daily or weekly.\nIf someone can breakdown the process for me that would be very helpful.\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yseqms", "is_robot_indexable": true, "report_reasons": null, "author": "mediocrX", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yseqms/need_help_with_aws_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yseqms/need_help_with_aws_project/", "subreddit_subscribers": 79724, "created_utc": 1668182872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_10v76s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An Open Letter to Data Ninjas - Yes, You Need To Implement Data Contract System - I wrote about two personas I met in software engineering; \"The Reliability Ninja\" and our very own \"The Data Ninja.\" what kind of a Ninja are you?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_ysnavn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.29, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ZKZJZHh3UjEfbDzJwvtMY73RsANx0gHKgsyYVyOmd_4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1668201088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringweekly.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dataengineeringweekly.com/p/an-open-letter-to-data-ninjas-yes", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/x9VoAMNPgUWw7_HFjYS9Q1ZDdadR6QPP_nHhzokO-GE.jpg?auto=webp&amp;s=25524d6e3148f7d9d97baf21a101f48a2e347b2f", "width": 600, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/x9VoAMNPgUWw7_HFjYS9Q1ZDdadR6QPP_nHhzokO-GE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=472b47681a508be3ba12a30be3efd44526e7398b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/x9VoAMNPgUWw7_HFjYS9Q1ZDdadR6QPP_nHhzokO-GE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cdada9738ffb3daaa4cb2e53347674b8a3cd1023", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/x9VoAMNPgUWw7_HFjYS9Q1ZDdadR6QPP_nHhzokO-GE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d1518f53d26e502e837e23594b13c8831ffed786", "width": 320, "height": 320}], "variants": {}, "id": "w8BWsxJSphv-JF7ZK0JvqyOaa1jlEHi8WOPWvych5Kg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "ysnavn", "is_robot_indexable": true, "report_reasons": null, "author": "vananth22", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ysnavn/an_open_letter_to_data_ninjas_yes_you_need_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dataengineeringweekly.com/p/an-open-letter-to-data-ninjas-yes", "subreddit_subscribers": 79724, "created_utc": 1668201088.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}