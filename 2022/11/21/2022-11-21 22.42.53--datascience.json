{"kind": "Listing", "data": {"after": null, "dist": 9, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I observed a few company where one of the person on the hiring panel (often a fellow data scientist and not a person of managerial position) rapidly ask technical questions. I am not sure if it is pre-mediated to stress test a candidate, but the experience left a sour taste in my mouth. What do you think about this interview strategy?\n\nUnrelated, I got a little revenge when I get to rapidly fire the panel for their take on the negative reviews I gathered on glassdoor.", "author_fullname": "t2_35uzbd5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "rapid fire technical questioning during an interview a red flag?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z11itd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 105, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 105, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669044476.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669043997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I observed a few company where one of the person on the hiring panel (often a fellow data scientist and not a person of managerial position) rapidly ask technical questions. I am not sure if it is pre-mediated to stress test a candidate, but the experience left a sour taste in my mouth. What do you think about this interview strategy?&lt;/p&gt;\n\n&lt;p&gt;Unrelated, I got a little revenge when I get to rapidly fire the panel for their take on the negative reviews I gathered on glassdoor.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "z11itd", "is_robot_indexable": true, "report_reasons": null, "author": "fanhui3", "discussion_type": null, "num_comments": 67, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z11itd/rapid_fire_technical_questioning_during_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z11itd/rapid_fire_technical_questioning_during_an/", "subreddit_subscribers": 820880, "created_utc": 1669043997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Weekly Entering &amp; Transitioning - Thread 21 Nov, 2022 - 28 Nov, 2022", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0q0yl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669006869.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;\n&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;\n&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;\n&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;\n&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=\"https://www.reddit.com/r/datascience/wiki/frequently-asked-questions\"&gt;FAQ&lt;/a&gt; and Resources pages on our wiki. You can also search for answers in &lt;a href=\"https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new\"&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "new", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z0q0yl", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 14, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z0q0yl/weekly_entering_transitioning_thread_21_nov_2022/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/datascience/comments/z0q0yl/weekly_entering_transitioning_thread_21_nov_2022/", "subreddit_subscribers": 820880, "created_utc": 1669006869.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Google Sheets have useful ImportXML() and ImportHTML() functions which allow to extract data from external websites and put them to sheet cells. If you never used it, try it now! Put this in Google Sheets cell: =IMPORTXML(\"  \n[https://en.wikipedia.org/wiki/Moon\\_landing](https://en.wikipedia.org/wiki/Moon_landing%60%60)\", \"//a/@href\")\n\nIt is possible to do a lot of things with ImportXML, but there are a lot of cons as well:\n\n* If the target website data requires some cleanup post-processing, it's getting very complicated since you are now \"programming with excel formulas\", rather painful process compared to regular code writing in conventional programming languages\n* There is no proper launch &amp; cache control so the function can be triggered occasionally and if the HTTP request fails, cells will be populated with ERR! values\n* The approach only works with most basic websites (no SPAs rendered in browsers can be scraped this way, any basic web scraping protection or connectivity issue breaks the process, no control over HTTP request geo location, or number of retries)\n\nWhen ImportXML() fails, the second approach to web scraping in Google Sheets is usually to write some custom [Google Apps Script](https://www.google.com/script/start/). This approach is much more flexible, just write Javascript code and deploy it as Google Sheets addon, but it takes a lot of time, and is not too easy to debug and iterate over - definitely not low code.\n\nSo, I have recently discovered an interesting approach of web scraping to Google Sheets using Make.com/Zapier alternative called Pipedream.com, and this approach is pretty flexible, reliable AND low code. It requires two external web services and Google Sheets is basically used as data store only, all the logic is done in Pipedream, which is a low code automation platform for developers, and all the web scraping heavy lifting (data retrieval, extracting data from HTML, data post processing) is done in ScrapeNinja.net low code sandbox. The best part of the Pipedream is that this approach is infinitely more flexible and reliable than plain Google Sheets, as it's basically a direct Zapier competitor (but with better free plan, and low-code more than no-code, so some Javascript knowledge might be required).\n\nHere is the video of using Pipedream and [ScrapeNinja.net](https://scrapeninja.net/) to extract HackerNews titles to Google Sheets every hour:\n\n[https://youtu.be/uBC752CWTew](https://youtu.be/uBC752CWTew)\n\nHow do you do web scraping in Google Sheets?", "author_fullname": "t2_13hqmc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web scraping in Google Sheets: a low-code alternative to ImportXML() approach", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0tums", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669020047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google Sheets have useful ImportXML() and ImportHTML() functions which allow to extract data from external websites and put them to sheet cells. If you never used it, try it now! Put this in Google Sheets cell: =IMPORTXML(&amp;quot;&lt;br/&gt;\n&lt;a href=\"https://en.wikipedia.org/wiki/Moon_landing%60%60\"&gt;https://en.wikipedia.org/wiki/Moon_landing&lt;/a&gt;&amp;quot;, &amp;quot;//a/@href&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;It is possible to do a lot of things with ImportXML, but there are a lot of cons as well:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If the target website data requires some cleanup post-processing, it&amp;#39;s getting very complicated since you are now &amp;quot;programming with excel formulas&amp;quot;, rather painful process compared to regular code writing in conventional programming languages&lt;/li&gt;\n&lt;li&gt;There is no proper launch &amp;amp; cache control so the function can be triggered occasionally and if the HTTP request fails, cells will be populated with ERR! values&lt;/li&gt;\n&lt;li&gt;The approach only works with most basic websites (no SPAs rendered in browsers can be scraped this way, any basic web scraping protection or connectivity issue breaks the process, no control over HTTP request geo location, or number of retries)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;When ImportXML() fails, the second approach to web scraping in Google Sheets is usually to write some custom &lt;a href=\"https://www.google.com/script/start/\"&gt;Google Apps Script&lt;/a&gt;. This approach is much more flexible, just write Javascript code and deploy it as Google Sheets addon, but it takes a lot of time, and is not too easy to debug and iterate over - definitely not low code.&lt;/p&gt;\n\n&lt;p&gt;So, I have recently discovered an interesting approach of web scraping to Google Sheets using Make.com/Zapier alternative called Pipedream.com, and this approach is pretty flexible, reliable AND low code. It requires two external web services and Google Sheets is basically used as data store only, all the logic is done in Pipedream, which is a low code automation platform for developers, and all the web scraping heavy lifting (data retrieval, extracting data from HTML, data post processing) is done in ScrapeNinja.net low code sandbox. The best part of the Pipedream is that this approach is infinitely more flexible and reliable than plain Google Sheets, as it&amp;#39;s basically a direct Zapier competitor (but with better free plan, and low-code more than no-code, so some Javascript knowledge might be required).&lt;/p&gt;\n\n&lt;p&gt;Here is the video of using Pipedream and &lt;a href=\"https://scrapeninja.net/\"&gt;ScrapeNinja.net&lt;/a&gt; to extract HackerNews titles to Google Sheets every hour:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/uBC752CWTew\"&gt;https://youtu.be/uBC752CWTew&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;How do you do web scraping in Google Sheets?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z0tums", "is_robot_indexable": true, "report_reasons": null, "author": "superjet1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z0tums/web_scraping_in_google_sheets_a_lowcode/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z0tums/web_scraping_in_google_sheets_a_lowcode/", "subreddit_subscribers": 820880, "created_utc": 1669020047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have an engineering background. I am exploring a career move into data science. Was offered a senior data scientist role at a large IT firm. My core responsibilities include \"data exploration and providing structured data for analysis\", which to me sounds like a data analyst role and not data scientist.\n\nNeed some suggestions if I should proceed. From what I read online, data scientists do spend a lot of time exploring right data sources and structuring documents for optimum training and visualizations. How much time do you typically spend in these activities (in terms of % of work per week)? I want to see how much time I will get for learning \"cool\" stuff like training models and playing with algorithms. \n\nThanks!", "author_fullname": "t2_r0jobtqp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much time do you typically spend in exploring and structuring data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0q4jp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669007186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an engineering background. I am exploring a career move into data science. Was offered a senior data scientist role at a large IT firm. My core responsibilities include &amp;quot;data exploration and providing structured data for analysis&amp;quot;, which to me sounds like a data analyst role and not data scientist.&lt;/p&gt;\n\n&lt;p&gt;Need some suggestions if I should proceed. From what I read online, data scientists do spend a lot of time exploring right data sources and structuring documents for optimum training and visualizations. How much time do you typically spend in these activities (in terms of % of work per week)? I want to see how much time I will get for learning &amp;quot;cool&amp;quot; stuff like training models and playing with algorithms. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z0q4jp", "is_robot_indexable": true, "report_reasons": null, "author": "Professor1441", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z0q4jp/how_much_time_do_you_typically_spend_in_exploring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z0q4jp/how_much_time_do_you_typically_spend_in_exploring/", "subreddit_subscribers": 820880, "created_utc": 1669007186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'd like to be able to make some interactive plots and tables in Jupyter to share with non coding colleagues. Most should be using Python.\n\nIdeally, sharing a single html is wonderful, or alternatively I can point them to a private URL from gitlab. But I have no idea of what's the easiest way to do this, any suggestions to get started?\n\nThe important things here are that everything needs to remain private, so \"mybinder\" and others cannot be used since they require public repositories\n\nMany thanks for all the suggestions", "author_fullname": "t2_7a5gkn8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Standalone Interactive html from jupyter notebooks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0i38w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668985020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to be able to make some interactive plots and tables in Jupyter to share with non coding colleagues. Most should be using Python.&lt;/p&gt;\n\n&lt;p&gt;Ideally, sharing a single html is wonderful, or alternatively I can point them to a private URL from gitlab. But I have no idea of what&amp;#39;s the easiest way to do this, any suggestions to get started?&lt;/p&gt;\n\n&lt;p&gt;The important things here are that everything needs to remain private, so &amp;quot;mybinder&amp;quot; and others cannot be used since they require public repositories&lt;/p&gt;\n\n&lt;p&gt;Many thanks for all the suggestions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z0i38w", "is_robot_indexable": true, "report_reasons": null, "author": "Kiwi_Major", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z0i38w/standalone_interactive_html_from_jupyter_notebooks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z0i38w/standalone_interactive_html_from_jupyter_notebooks/", "subreddit_subscribers": 820880, "created_utc": 1668985020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This [paper](https://igi-web.tugraz.at/PDF/kdd08.pdf), as well as many others (e.g. these [paper](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf) [paper](https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009_BPC_BigChaos.pdf) from the winners of the Netflix Prize) that came out of the Netflix Prize contest recommend that you remove global effects one by one as a preprocessing step. However, when I try this, it only seems to increase my RMSE.\n\nFor example, if I train a model with the following optimization problem:  \n\nhttps://preview.redd.it/u6nkqddknd1a1.png?width=429&amp;format=png&amp;auto=webp&amp;s=b628883edb2c1729f47568b903449fa3a6ea26d0\n\nand then use those residuals to train a latent factor model with this optimization problem: \n\nhttps://preview.redd.it/wlyjab0mnd1a1.png?width=495&amp;format=png&amp;auto=webp&amp;s=1031b65d6d18661fb7b89a88116c73f0567ed8c2\n\nI get no improvement from the first model. Is this the wrong optimization problem to train on residuals with? I use hyper-parameters very similar to the ones delineated in the papers I linked, and the number of latent factors I choose has relatively little impact on my RMSE.\n\nAdditionally, I have tried combining the user/items biases with the pq interaction into one LFM, and it is worse than the one with only biases. Even adding implicit bias (SVD++) only increases RMSE, so I am sort of confused about why this is happening and how to fix it. My best RMSE I can get is \\~1.0011 and is from the most basic bias-only model. However, the papers I linked get RMSE well below 1. I know I am not factoring in time, but my RMSE should still be much lower with out those things.\n\nAlso note, I am using the [Yelp dataset](https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset) from kaggle instead of the Netflix dataset.", "author_fullname": "t2_2pqawytk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to remove global effects for latent factor recommender?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 13, "top_awarded_type": null, "hide_score": true, "media_metadata": {"u6nkqddknd1a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 10, "x": 108, "u": "https://preview.redd.it/u6nkqddknd1a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=792ee36dd7b6ccf2e25ee8439990e3946879f500"}, {"y": 21, "x": 216, "u": "https://preview.redd.it/u6nkqddknd1a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e89f0d429a43f96de439c45d4610b78b3662e23"}, {"y": 31, "x": 320, "u": "https://preview.redd.it/u6nkqddknd1a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8f06d505c2b16789369eb6aca317e18832ac3ca"}], "s": {"y": 42, "x": 429, "u": "https://preview.redd.it/u6nkqddknd1a1.png?width=429&amp;format=png&amp;auto=webp&amp;s=b628883edb2c1729f47568b903449fa3a6ea26d0"}, "id": "u6nkqddknd1a1"}, "wlyjab0mnd1a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 9, "x": 108, "u": "https://preview.redd.it/wlyjab0mnd1a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=74bcbc80b9a88cc7b9b76c0e66e97b348be2a896"}, {"y": 18, "x": 216, "u": "https://preview.redd.it/wlyjab0mnd1a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fcdbb0952d42cefa92e73f370c7c4a3dbfbf54e0"}, {"y": 27, "x": 320, "u": "https://preview.redd.it/wlyjab0mnd1a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=70ae4a234b2cfdb1b042a262dc2b7bd99d8b5f0a"}], "s": {"y": 42, "x": 495, "u": "https://preview.redd.it/wlyjab0mnd1a1.png?width=495&amp;format=png&amp;auto=webp&amp;s=1031b65d6d18661fb7b89a88116c73f0567ed8c2"}, "id": "wlyjab0mnd1a1"}}, "name": "t3_z1bhzc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/eB4HBKlyGHh-E2WTHgbnJZfntH3zOKVxx20RmOqSQqM.jpg", "edited": 1669068356.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669067634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This &lt;a href=\"https://igi-web.tugraz.at/PDF/kdd08.pdf\"&gt;paper&lt;/a&gt;, as well as many others (e.g. these &lt;a href=\"https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf\"&gt;paper&lt;/a&gt; &lt;a href=\"https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009_BPC_BigChaos.pdf\"&gt;paper&lt;/a&gt; from the winners of the Netflix Prize) that came out of the Netflix Prize contest recommend that you remove global effects one by one as a preprocessing step. However, when I try this, it only seems to increase my RMSE.&lt;/p&gt;\n\n&lt;p&gt;For example, if I train a model with the following optimization problem:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u6nkqddknd1a1.png?width=429&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b628883edb2c1729f47568b903449fa3a6ea26d0\"&gt;https://preview.redd.it/u6nkqddknd1a1.png?width=429&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b628883edb2c1729f47568b903449fa3a6ea26d0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;and then use those residuals to train a latent factor model with this optimization problem: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wlyjab0mnd1a1.png?width=495&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1031b65d6d18661fb7b89a88116c73f0567ed8c2\"&gt;https://preview.redd.it/wlyjab0mnd1a1.png?width=495&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1031b65d6d18661fb7b89a88116c73f0567ed8c2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I get no improvement from the first model. Is this the wrong optimization problem to train on residuals with? I use hyper-parameters very similar to the ones delineated in the papers I linked, and the number of latent factors I choose has relatively little impact on my RMSE.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I have tried combining the user/items biases with the pq interaction into one LFM, and it is worse than the one with only biases. Even adding implicit bias (SVD++) only increases RMSE, so I am sort of confused about why this is happening and how to fix it. My best RMSE I can get is ~1.0011 and is from the most basic bias-only model. However, the papers I linked get RMSE well below 1. I know I am not factoring in time, but my RMSE should still be much lower with out those things.&lt;/p&gt;\n\n&lt;p&gt;Also note, I am using the &lt;a href=\"https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset\"&gt;Yelp dataset&lt;/a&gt; from kaggle instead of the Netflix dataset.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z1bhzc", "is_robot_indexable": true, "report_reasons": null, "author": "Sentientlog", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z1bhzc/how_to_remove_global_effects_for_latent_factor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z1bhzc/how_to_remove_global_effects_for_latent_factor/", "subreddit_subscribers": 820880, "created_utc": 1669067634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Tldr; How to convince stakeholders that your product is working as intended and the can trust the output of it?\n\nBackground: I have build this rather simple data pipeline which transforms the input data, applies some simple aggregations (not even the harmonic mean), some simple filters and produces plots of the data. Now, the stakeholders which are the sales team and my boss don't trust the output of the whole thing (anymore).\n\nMeasures I took to build trust:\n\n* included unit tests with test cases in order to verify aggregation and construction of metrics\n* separate aggregation and plotting into different steps of the pipeline\n* made a diagram with data flow and all definitions\n* rely mostly on well tested third party libraries (pandas and matplotlib)\n* Versioning and documenting the code and the input to have reproducible pipeline runs\n\nReasons why I think they don't trust it:\n* I am the only person developing it, so there is no review process and I could test my own logical errors with the unit tests\n* Communication between the sales team and me was suboptimal, i.e. the meaning of some metrics was not clear to them and I did not always get what the expected of some metrics were\n* The underling input data quality was poor and changing often but was out of my control. In the end it was blamed on the product even it was not it's fault\n* Development in production due to changing requirements, tight deadlines and feature request, which lead to minor bugs\n\nI am interested in your thoughts how I can improve the situation and how I can avoid similar problems in the future.", "author_fullname": "t2_c1bt3c4m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get stakeholders to trust your product", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z16efm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669055810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tldr; How to convince stakeholders that your product is working as intended and the can trust the output of it?&lt;/p&gt;\n\n&lt;p&gt;Background: I have build this rather simple data pipeline which transforms the input data, applies some simple aggregations (not even the harmonic mean), some simple filters and produces plots of the data. Now, the stakeholders which are the sales team and my boss don&amp;#39;t trust the output of the whole thing (anymore).&lt;/p&gt;\n\n&lt;p&gt;Measures I took to build trust:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;included unit tests with test cases in order to verify aggregation and construction of metrics&lt;/li&gt;\n&lt;li&gt;separate aggregation and plotting into different steps of the pipeline&lt;/li&gt;\n&lt;li&gt;made a diagram with data flow and all definitions&lt;/li&gt;\n&lt;li&gt;rely mostly on well tested third party libraries (pandas and matplotlib)&lt;/li&gt;\n&lt;li&gt;Versioning and documenting the code and the input to have reproducible pipeline runs&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Reasons why I think they don&amp;#39;t trust it:\n* I am the only person developing it, so there is no review process and I could test my own logical errors with the unit tests\n* Communication between the sales team and me was suboptimal, i.e. the meaning of some metrics was not clear to them and I did not always get what the expected of some metrics were\n* The underling input data quality was poor and changing often but was out of my control. In the end it was blamed on the product even it was not it&amp;#39;s fault\n* Development in production due to changing requirements, tight deadlines and feature request, which lead to minor bugs&lt;/p&gt;\n\n&lt;p&gt;I am interested in your thoughts how I can improve the situation and how I can avoid similar problems in the future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z16efm", "is_robot_indexable": true, "report_reasons": null, "author": "Life_Ad_6195", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z16efm/how_to_get_stakeholders_to_trust_your_product/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z16efm/how_to_get_stakeholders_to_trust_your_product/", "subreddit_subscribers": 820880, "created_utc": 1669055810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_26vasc8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just wanted to share the best typo I'll ever come across in an academic paper. Right in the acknowledgments, too.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 63, "top_awarded_type": null, "hide_score": true, "name": "t3_z1bb5z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GHKkiK09k74d1wdHYGuEwqVcqUW0kI6afx9FzJrp_zA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669067190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/5qabzetyjd1a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/5qabzetyjd1a1.png?auto=webp&amp;s=cde95a96bd10392a89a2ddb5aea562bdc98b1282", "width": 2065, "height": 939}, "resolutions": [{"url": "https://preview.redd.it/5qabzetyjd1a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e6ce10f75c0c9f5befe5f9cd6e2974a8bbcf9fc", "width": 108, "height": 49}, {"url": "https://preview.redd.it/5qabzetyjd1a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ee7155fc04f2a2a37cd5ddbbad57d53d5d49e44", "width": 216, "height": 98}, {"url": "https://preview.redd.it/5qabzetyjd1a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=06dfb3ef8fbea0a6aca16b71f61e7475d1a8e73f", "width": 320, "height": 145}, {"url": "https://preview.redd.it/5qabzetyjd1a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=23f6ec6e50c1a78f006b151713b04a87412034db", "width": 640, "height": 291}, {"url": "https://preview.redd.it/5qabzetyjd1a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=95f6d06150d3dd1f374073a040f0d24fc25dadbb", "width": 960, "height": 436}, {"url": "https://preview.redd.it/5qabzetyjd1a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2cb713a0420f6cafcc0e45648aca50c0ee3feba9", "width": 1080, "height": 491}], "variants": {}, "id": "cvCS17NPuAJL2zmo-4XCZpZ4NZ3e2t5CicU0qlqbTYw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "z1bb5z", "is_robot_indexable": true, "report_reasons": null, "author": "icanelectoo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z1bb5z/just_wanted_to_share_the_best_typo_ill_ever_come/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/5qabzetyjd1a1.png", "subreddit_subscribers": 820880, "created_utc": 1669067190.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "[https://www.reddit.com/r/quantum\\_Mach\\_Learning?utm\\_medium=android\\_app&amp;utm\\_source=share](https://www.reddit.com/r/quantum_Mach_Learning?utm_medium=android_app&amp;utm_source=share)\n\n&amp;#x200B;\n\nHi all,\n\nSo this is an invite to a quantum machine learning community which as the name says is aimed for people who are interested in this promising field to ineract and network with one another....\n\nMay you be an undergrad or a grad student or a professional working in another field....you are welcome to join.\n\nHope you do join and make it a thriving and lively place...\n\nSee you all great people there....", "author_fullname": "t2_ucr73atc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "COME ONE COME ALL..... Invite to a quantum machine learing community...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z127u8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Networking", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669045704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/quantum_Mach_Learning?utm_medium=android_app&amp;amp;utm_source=share\"&gt;https://www.reddit.com/r/quantum_Mach_Learning?utm_medium=android_app&amp;amp;utm_source=share&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;So this is an invite to a quantum machine learning community which as the name says is aimed for people who are interested in this promising field to ineract and network with one another....&lt;/p&gt;\n\n&lt;p&gt;May you be an undergrad or a grad student or a professional working in another field....you are welcome to join.&lt;/p&gt;\n\n&lt;p&gt;Hope you do join and make it a thriving and lively place...&lt;/p&gt;\n\n&lt;p&gt;See you all great people there....&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z127u8", "is_robot_indexable": true, "report_reasons": null, "author": "thejay147", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z127u8/come_one_come_all_invite_to_a_quantum_machine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z127u8/come_one_come_all_invite_to_a_quantum_machine/", "subreddit_subscribers": 820880, "created_utc": 1669045704.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}