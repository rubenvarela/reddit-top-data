{"kind": "Listing", "data": {"after": "t3_z0wzng", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can anybody please explain to me why people are obsessed with working for FAANG+? These companies are notorious for being morally dubious, having shitty working environments and high burnout churn. Plenty of more attractive options out there. Just curious as I see a lot of posts on here and other subs with people asking for FAANG+ specific interview prep advice and it just seems odd. You wouldn't want to work for SPECTRE would you, very pass\u00e9.", "author_fullname": "t2_8p2u7cmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why FAANG+", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0srs8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669016067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anybody please explain to me why people are obsessed with working for FAANG+? These companies are notorious for being morally dubious, having shitty working environments and high burnout churn. Plenty of more attractive options out there. Just curious as I see a lot of posts on here and other subs with people asking for FAANG+ specific interview prep advice and it just seems odd. You wouldn&amp;#39;t want to work for SPECTRE would you, very pass\u00e9.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z0srs8", "is_robot_indexable": true, "report_reasons": null, "author": "Jamese0", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0srs8/why_faang/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0srs8/why_faang/", "subreddit_subscribers": 80570, "created_utc": 1669016067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I love this community because there is so much good and practical advice on nearly every topic of data engineering. I'm curious on inverting the question as one can and should also learn this way. What is some of the worst advice y'all have been given that you may have tried applying to your careers and it did not work out?", "author_fullname": "t2_3p620rl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bad Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0bbs8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668968778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I love this community because there is so much good and practical advice on nearly every topic of data engineering. I&amp;#39;m curious on inverting the question as one can and should also learn this way. What is some of the worst advice y&amp;#39;all have been given that you may have tried applying to your careers and it did not work out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer At Lyft/Author of Ace DE Interview", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z0bbs8", "is_robot_indexable": true, "report_reasons": null, "author": "chrisgarzon19", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/z0bbs8/bad_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0bbs8/bad_advice/", "subreddit_subscribers": 80570, "created_utc": 1668968778.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to know how DE(Data Engineering) community make use of Airflow on AWS. \n\nMWAA(Amazon Managed Workflows for Apache Airflow) is ruled out due to cost considerations as we are a very small team.\n\nCan I install Airflow on EC2 instance and explore? Thanks.\n\nWant to learn form the community about how they use Airflow in their projects on AWS. Please through some examples.", "author_fullname": "t2_eozceps7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0pa71", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669004580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to know how DE(Data Engineering) community make use of Airflow on AWS. &lt;/p&gt;\n\n&lt;p&gt;MWAA(Amazon Managed Workflows for Apache Airflow) is ruled out due to cost considerations as we are a very small team.&lt;/p&gt;\n\n&lt;p&gt;Can I install Airflow on EC2 instance and explore? Thanks.&lt;/p&gt;\n\n&lt;p&gt;Want to learn form the community about how they use Airflow in their projects on AWS. Please through some examples.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z0pa71", "is_robot_indexable": true, "report_reasons": null, "author": "Liily_07", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0pa71/airflow_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0pa71/airflow_on_aws/", "subreddit_subscribers": 80570, "created_utc": 1669004580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve recently moved into the field of data engineering (6 months) after two years as a Business Analyst in NZ. Was wondering what the data engineering salaries were in NZ so that I can a plan on what to ask for our upcoming salary review. Thanks heaps", "author_fullname": "t2_if7fad4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are Data Engineer salaries in New Zealand?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0qw22", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669009601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve recently moved into the field of data engineering (6 months) after two years as a Business Analyst in NZ. Was wondering what the data engineering salaries were in NZ so that I can a plan on what to ask for our upcoming salary review. Thanks heaps&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z0qw22", "is_robot_indexable": true, "report_reasons": null, "author": "Fickle-Diet-3060", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0qw22/how_are_data_engineer_salaries_in_new_zealand/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0qw22/how_are_data_engineer_salaries_in_new_zealand/", "subreddit_subscribers": 80570, "created_utc": 1669009601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dfcot0zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Made a post about weird hybrid titles coming out. Who can guess the job duties without reading the full posting (which can be Easily found on LinkedIN for anyone interested)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 66, "top_awarded_type": null, "hide_score": false, "name": "t3_z0wzb0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/wI0LS_wQTFa0N8oq6Rx5xiXYpYQ4CIknIAYOvv4yEA0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669031156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9lc0okovka1a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9lc0okovka1a1.png?auto=webp&amp;s=9150a5487c6a0fe1c4b10435678b9361caafa34b", "width": 908, "height": 432}, "resolutions": [{"url": "https://preview.redd.it/9lc0okovka1a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=db074733d799d3e20c43e18f43584ecf97e7195b", "width": 108, "height": 51}, {"url": "https://preview.redd.it/9lc0okovka1a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=339386dd677d010b9cb024f9bfe9d3b6445ab396", "width": 216, "height": 102}, {"url": "https://preview.redd.it/9lc0okovka1a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=74a5b4b4a9956b1501d41bc683afafe712073550", "width": 320, "height": 152}, {"url": "https://preview.redd.it/9lc0okovka1a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=61fabfcfab0682c4f5cca3744d64fdef0600f5d2", "width": 640, "height": 304}], "variants": {}, "id": "1nk3YUgpBJpISmiABbzlD9Co0XakzXnBCry9eCzOy-A"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z0wzb0", "is_robot_indexable": true, "report_reasons": null, "author": "DrRedmondNYC", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0wzb0/made_a_post_about_weird_hybrid_titles_coming_out/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9lc0okovka1a1.png", "subreddit_subscribers": 80570, "created_utc": 1669031156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, I've been working as a data analyst but doing ETL work for 1.5 years. I want to pursue the DE path and want to make sure I'm learning the right skills to set me up for potentially joining FAANG+ in a few years. \n\nI currently use Python, SQL server, Azure, Tableau, SSIS, and Autosys as a scheduler. New job moves me from 83k salary to 100k salary (MCOL) and uses Scala, Spark, Kafka, Hive/Hadoop, and SQL. \n\nIs it a bad idea to accept this job to use scala exclusively if I don't have a ton of interest in learning it? I love python and don't know if giving it up for a while to learn Scala will be a benefit or a detriment. I have a degree in economics so I was wondering if learning functional programming would be a good Idea. Or if I should look for a job with a different tech stack. Thanks in advance", "author_fullname": "t2_10m7s5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Worth taking job in Scala if passion is with Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0joea", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668990069.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668988952.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;ve been working as a data analyst but doing ETL work for 1.5 years. I want to pursue the DE path and want to make sure I&amp;#39;m learning the right skills to set me up for potentially joining FAANG+ in a few years. &lt;/p&gt;\n\n&lt;p&gt;I currently use Python, SQL server, Azure, Tableau, SSIS, and Autosys as a scheduler. New job moves me from 83k salary to 100k salary (MCOL) and uses Scala, Spark, Kafka, Hive/Hadoop, and SQL. &lt;/p&gt;\n\n&lt;p&gt;Is it a bad idea to accept this job to use scala exclusively if I don&amp;#39;t have a ton of interest in learning it? I love python and don&amp;#39;t know if giving it up for a while to learn Scala will be a benefit or a detriment. I have a degree in economics so I was wondering if learning functional programming would be a good Idea. Or if I should look for a job with a different tech stack. Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z0joea", "is_robot_indexable": true, "report_reasons": null, "author": "Nolstr", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0joea/worth_taking_job_in_scala_if_passion_is_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0joea/worth_taking_job_in_scala_if_passion_is_with/", "subreddit_subscribers": 80570, "created_utc": 1668988952.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi readers, I made this post to share my journey as a Junior Data Engineer in a tech startup that initially has no data infrastructure. How it all began, what I've learnt, where I've failed, and where I am now.\n\nBefore I begin telling you about my journey, let me explain a bit about my background, ie: education and how I applied for this job post-covid. I graduated with a Bachelor of Computer Science in Data Science, and with that, it would have been natural for me to apply for a job as a Data Scientist. I applied for that position for around a month and sadly, did not get any response. This ofcourse mace me feel depressed and frustrated thinking that I would never be hired as a Data Scientist. Then, I decided to shift my strategy to another method, in which I approached a professional recruiter to assist me in finding a suitable company who would value learning and taking a chance with me. Thankfully, within a week, I got a response from a company which eventually became the workplace I am currently in.\n\nSo let\u2019s talk about the interview process. I was interviewed for a Data Scientist position, that consists of 3 seperate sessions. In the first session, I was given an assignment to solve \"The Birthday\" problem along with a probability problem (which I kind of forgot the actual name of it). To be completely honest, I had no idea what \"The Birthday\" problem even is, so I did what everyone who was given an interview assignment does and searched for it online. I then copied and pasted the code/solution for the assignment. It only took about 30 minutes to understand the code and prepare it for the second session. As the second session was about to get started, I bluntly told the interviewer that I just sort've cheated and pasted the online code/solution. Regardless, the interviewer brushed it off, and began another interview process. A few questions of the solution I copied were thrown at me, and I was able to answer a few of them with relative confidence. Though after this session, I immediately lost hope due to the factors in how I did the assignment and answered the questions, which again I remind you thar I literally copied and pasted the code to make it run. To my surprise, about a couple weeks later, I was asked to attend a third interview! I of course went in expecting some feedback on what I've done so far(maybe some \"DO NOT CHEAT!\" advice), but instead they congratulated me and told me I got the post. I was left shocked and confused but ultimately happy at this turn of events. \n\nFast forward to my first day in office, it was also during a Program Increment event, and I had no idea what anyone was doing. I kept hearing my manager go on about SAFe (Scaled Agile Framework) and Scrum events. To be frank, I was so overloaded with information and was pretty shocked(scared more like) when my manager told me that I was to be split in half, figuratively of course, to be a member of both the Data and the Integration teams. This was due to the data team being new and small, only consisting of myself and my manager(whom by now I should mention, was the one who interviewed me). So during the first few days, I was trying to understand what the company is trying to acheive as well as my job scope within these two teams. I spent almost a month doing practically nothing and instead kept going around and bugging my colleages(specifically the Software Testing Lead, in my defense, they told me to ask him anything) to show me the ropes and understand what the system architecture is and so on and so on. \n\nIn the first few months in the company, I was temporarily assigned as a software tester in the Integration team and an analyst in the Data team.\u00a0 For the testing scope, most of the work is clear and easy to perform as it was just testing API callbacks, validating the documents' structure, and checking the acceptance of the API. I was a bit more relaxed in the Integration team but when it came to the Data side of things, I felt overwhelmed with stress and confusion from time to time. \n\nIn the data team, I was experimenting with a lot of stuff, such as analysing the company communication platform, coming up with word cloud and scraping information online. I spent most of my time cleaning the data and I felt it was pretty much wasted as it never reached production. While I was studyinh in University, the process of making an ML/AI model is straightforward, as the data has been cleaned and ready to use, but in real life, we need a data infrastructure which provides clean data for the downstream user to do analytics or ML/AI model training. That\u2019s when I began to learn about data engineering, this led to me realised that to create a fancy ML/AI, first we need to have a good dataset or else it will just be garbage in and garbage out. \n\nWith this newfound knowledge, I consulted with my manager and volunteered myself to explore the data engineering field and him being himself gave me the green light to do so. When I was exploring I discovered a lot of terms like data warehousing, data lake, ETL, ELT, reserver ETL and etc. I was so confused and overwhelmed by it so I searched how to start in DE(Data Engineering), and came to find that different people share different DE roadmaps with the similarities being the fundamentals, the usage of Python and SQL. At first, I never understood why we even need SQL in DE, so for the next few months, I kept following The Seattle Data Guys aka Ben Rogojan. I learn a lot of stuff from him such as explaining the different tools from the website.\n\nAfter a while, my company FINALLY appointed someone else as a Data Lead(this person is not the manager who interviewed me because he was busy being the Release Train Engineer for SAFe). This is good because most of the time I have no one else to consult to about data matters and I end-up YOLO-ing the job, leading to me messing up or it being so bad that it doesn\u2019t get the pushed to the production stage. At last, I finally have a team but it still only consists of two people hahaha. So then, we start performing proper work like researching ETL tools, data warehouse and data visualisation tools. \n\nI spent the following month trying out those ETL tools and Data Warehousing. I learnt a lot of stuff like the best practices, ETL vs ELT the use cases and so on. A website I'm fond of, [g2.com](https://g2.com), is a good place to compare different tools. I was tempted at first to try out a famous ETL tool, Fivetran. However,it isn't that beginner-friendly when compared to HevoData(which my company currently uses). That is way more user-friendly, where I can straight away query the data from HevoData without needing to query the data from the destination GUI, ie: AWS Redshift. I also learnt about DBT labs, which I still struggle with because I need to learn how to host them on GitHub and link it to the destination. \n\nSo, where did I fail? I failed in quite a few things such as understanding the use and importance of SQL, especially for the transformation part. I also made some mistakes such as I need to make a schema of raw data and a schema of transforming data in the data warehouse/ data lakehouse because I use the ELT method. I've also messed up by learning theorical concepts without an inkling of real-world use cases, as I'm afraid to try it out at first because in reality, depending on a companys' data maturity, there will have different ways to carry out building the data infrastructure. I still have other stuff to learn like database denormalisation, advanced SQL query, and etc. \n\nMy take is if you truly want to survive in the DE field, don\u2019t be afraid to fail, learn from the it and improve upon it. That\u2019s how I was able to build the companys' first data pipeline and Data warehouse/data lakehouse. I'm still way beyond from being expert let alone covering everything in this field but I will continue to keep on learning as I proceed. Thanks for reading this and P.S: This is my first time writing a blog, so if you have any suggestions for me feel free to tell me in the comments. Please excuse my English, even with I had someone to proofread for me and he isn't that great in English either.", "author_fullname": "t2_k7g9mhou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Journey as a Junior Data Engineer in a tech startup that initially has no data infrastructure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0u75v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669021348.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi readers, I made this post to share my journey as a Junior Data Engineer in a tech startup that initially has no data infrastructure. How it all began, what I&amp;#39;ve learnt, where I&amp;#39;ve failed, and where I am now.&lt;/p&gt;\n\n&lt;p&gt;Before I begin telling you about my journey, let me explain a bit about my background, ie: education and how I applied for this job post-covid. I graduated with a Bachelor of Computer Science in Data Science, and with that, it would have been natural for me to apply for a job as a Data Scientist. I applied for that position for around a month and sadly, did not get any response. This ofcourse mace me feel depressed and frustrated thinking that I would never be hired as a Data Scientist. Then, I decided to shift my strategy to another method, in which I approached a professional recruiter to assist me in finding a suitable company who would value learning and taking a chance with me. Thankfully, within a week, I got a response from a company which eventually became the workplace I am currently in.&lt;/p&gt;\n\n&lt;p&gt;So let\u2019s talk about the interview process. I was interviewed for a Data Scientist position, that consists of 3 seperate sessions. In the first session, I was given an assignment to solve &amp;quot;The Birthday&amp;quot; problem along with a probability problem (which I kind of forgot the actual name of it). To be completely honest, I had no idea what &amp;quot;The Birthday&amp;quot; problem even is, so I did what everyone who was given an interview assignment does and searched for it online. I then copied and pasted the code/solution for the assignment. It only took about 30 minutes to understand the code and prepare it for the second session. As the second session was about to get started, I bluntly told the interviewer that I just sort&amp;#39;ve cheated and pasted the online code/solution. Regardless, the interviewer brushed it off, and began another interview process. A few questions of the solution I copied were thrown at me, and I was able to answer a few of them with relative confidence. Though after this session, I immediately lost hope due to the factors in how I did the assignment and answered the questions, which again I remind you thar I literally copied and pasted the code to make it run. To my surprise, about a couple weeks later, I was asked to attend a third interview! I of course went in expecting some feedback on what I&amp;#39;ve done so far(maybe some &amp;quot;DO NOT CHEAT!&amp;quot; advice), but instead they congratulated me and told me I got the post. I was left shocked and confused but ultimately happy at this turn of events. &lt;/p&gt;\n\n&lt;p&gt;Fast forward to my first day in office, it was also during a Program Increment event, and I had no idea what anyone was doing. I kept hearing my manager go on about SAFe (Scaled Agile Framework) and Scrum events. To be frank, I was so overloaded with information and was pretty shocked(scared more like) when my manager told me that I was to be split in half, figuratively of course, to be a member of both the Data and the Integration teams. This was due to the data team being new and small, only consisting of myself and my manager(whom by now I should mention, was the one who interviewed me). So during the first few days, I was trying to understand what the company is trying to acheive as well as my job scope within these two teams. I spent almost a month doing practically nothing and instead kept going around and bugging my colleages(specifically the Software Testing Lead, in my defense, they told me to ask him anything) to show me the ropes and understand what the system architecture is and so on and so on. &lt;/p&gt;\n\n&lt;p&gt;In the first few months in the company, I was temporarily assigned as a software tester in the Integration team and an analyst in the Data team.\u00a0 For the testing scope, most of the work is clear and easy to perform as it was just testing API callbacks, validating the documents&amp;#39; structure, and checking the acceptance of the API. I was a bit more relaxed in the Integration team but when it came to the Data side of things, I felt overwhelmed with stress and confusion from time to time. &lt;/p&gt;\n\n&lt;p&gt;In the data team, I was experimenting with a lot of stuff, such as analysing the company communication platform, coming up with word cloud and scraping information online. I spent most of my time cleaning the data and I felt it was pretty much wasted as it never reached production. While I was studyinh in University, the process of making an ML/AI model is straightforward, as the data has been cleaned and ready to use, but in real life, we need a data infrastructure which provides clean data for the downstream user to do analytics or ML/AI model training. That\u2019s when I began to learn about data engineering, this led to me realised that to create a fancy ML/AI, first we need to have a good dataset or else it will just be garbage in and garbage out. &lt;/p&gt;\n\n&lt;p&gt;With this newfound knowledge, I consulted with my manager and volunteered myself to explore the data engineering field and him being himself gave me the green light to do so. When I was exploring I discovered a lot of terms like data warehousing, data lake, ETL, ELT, reserver ETL and etc. I was so confused and overwhelmed by it so I searched how to start in DE(Data Engineering), and came to find that different people share different DE roadmaps with the similarities being the fundamentals, the usage of Python and SQL. At first, I never understood why we even need SQL in DE, so for the next few months, I kept following The Seattle Data Guys aka Ben Rogojan. I learn a lot of stuff from him such as explaining the different tools from the website.&lt;/p&gt;\n\n&lt;p&gt;After a while, my company FINALLY appointed someone else as a Data Lead(this person is not the manager who interviewed me because he was busy being the Release Train Engineer for SAFe). This is good because most of the time I have no one else to consult to about data matters and I end-up YOLO-ing the job, leading to me messing up or it being so bad that it doesn\u2019t get the pushed to the production stage. At last, I finally have a team but it still only consists of two people hahaha. So then, we start performing proper work like researching ETL tools, data warehouse and data visualisation tools. &lt;/p&gt;\n\n&lt;p&gt;I spent the following month trying out those ETL tools and Data Warehousing. I learnt a lot of stuff like the best practices, ETL vs ELT the use cases and so on. A website I&amp;#39;m fond of, &lt;a href=\"https://g2.com\"&gt;g2.com&lt;/a&gt;, is a good place to compare different tools. I was tempted at first to try out a famous ETL tool, Fivetran. However,it isn&amp;#39;t that beginner-friendly when compared to HevoData(which my company currently uses). That is way more user-friendly, where I can straight away query the data from HevoData without needing to query the data from the destination GUI, ie: AWS Redshift. I also learnt about DBT labs, which I still struggle with because I need to learn how to host them on GitHub and link it to the destination. &lt;/p&gt;\n\n&lt;p&gt;So, where did I fail? I failed in quite a few things such as understanding the use and importance of SQL, especially for the transformation part. I also made some mistakes such as I need to make a schema of raw data and a schema of transforming data in the data warehouse/ data lakehouse because I use the ELT method. I&amp;#39;ve also messed up by learning theorical concepts without an inkling of real-world use cases, as I&amp;#39;m afraid to try it out at first because in reality, depending on a companys&amp;#39; data maturity, there will have different ways to carry out building the data infrastructure. I still have other stuff to learn like database denormalisation, advanced SQL query, and etc. &lt;/p&gt;\n\n&lt;p&gt;My take is if you truly want to survive in the DE field, don\u2019t be afraid to fail, learn from the it and improve upon it. That\u2019s how I was able to build the companys&amp;#39; first data pipeline and Data warehouse/data lakehouse. I&amp;#39;m still way beyond from being expert let alone covering everything in this field but I will continue to keep on learning as I proceed. Thanks for reading this and P.S: This is my first time writing a blog, so if you have any suggestions for me feel free to tell me in the comments. Please excuse my English, even with I had someone to proofread for me and he isn&amp;#39;t that great in English either.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z0u75v", "is_robot_indexable": true, "report_reasons": null, "author": "FlorexOng_a1", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0u75v/journey_as_a_junior_data_engineer_in_a_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0u75v/journey_as_a_junior_data_engineer_in_a_tech/", "subreddit_subscribers": 80570, "created_utc": 1669021348.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working in the data engineering space for about 2.5 years, with 1 YOE as a BI dev and another 1.5 as a DE.  Current and previous experience has pretty much been SSIS/Informatica for E and L, with all of the T done in-database with SQL.  I would say my skill usage distribution is about 60% SQL, 20% GUI tools, and 20% python. The work I do with python isn't really ETL or large scale application work, more like DE adjacent tasks such as monitoring, alerting, and automation.\n\nI'm looking to move into more of a SWE type of DE where I'm primarily working with code, preferably python/scala/spark or similar.  I find that I'm a much better python developer, and that I really hate working with bloated PL/SQL codebases.  I would prefer to work in an environment with more mature SWE practices like version control, CI/CD, proper testing/development pipelines, etc. I've been leading the charge within my team to transition to maintaining our python projects and SQL codebase in our GitHub repos, and building out Docker containers for local development environments.\n\nMy research is pointing me to seek out positions labeled SWE - Big Data, SWE - Data Platform, Backend, or just DE that fits the job description above. I'm getting ready to start job searching again going into Q1 2023.  I've been preparing by studying DSA/Leetcode Blind 75 and building a personal project in my desired stack (spark cluster running locally in a set of Docker containers). Is this enough to switch into this type of engineering job?\n\nBonus question: Are the code-focused jobs I'm looking for falling out of favor for the more modern version of my current stack? Fivetran/Snowflake/DBT seems to be the modern data stack, but I really want to escape pure SQL hell.\n\nAny advice would be appreciated. Thanks!", "author_fullname": "t2_4rifsjav", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving from SQL/GUI stack to python/scala/big data/SWE stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0ab99", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668966412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working in the data engineering space for about 2.5 years, with 1 YOE as a BI dev and another 1.5 as a DE.  Current and previous experience has pretty much been SSIS/Informatica for E and L, with all of the T done in-database with SQL.  I would say my skill usage distribution is about 60% SQL, 20% GUI tools, and 20% python. The work I do with python isn&amp;#39;t really ETL or large scale application work, more like DE adjacent tasks such as monitoring, alerting, and automation.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to move into more of a SWE type of DE where I&amp;#39;m primarily working with code, preferably python/scala/spark or similar.  I find that I&amp;#39;m a much better python developer, and that I really hate working with bloated PL/SQL codebases.  I would prefer to work in an environment with more mature SWE practices like version control, CI/CD, proper testing/development pipelines, etc. I&amp;#39;ve been leading the charge within my team to transition to maintaining our python projects and SQL codebase in our GitHub repos, and building out Docker containers for local development environments.&lt;/p&gt;\n\n&lt;p&gt;My research is pointing me to seek out positions labeled SWE - Big Data, SWE - Data Platform, Backend, or just DE that fits the job description above. I&amp;#39;m getting ready to start job searching again going into Q1 2023.  I&amp;#39;ve been preparing by studying DSA/Leetcode Blind 75 and building a personal project in my desired stack (spark cluster running locally in a set of Docker containers). Is this enough to switch into this type of engineering job?&lt;/p&gt;\n\n&lt;p&gt;Bonus question: Are the code-focused jobs I&amp;#39;m looking for falling out of favor for the more modern version of my current stack? Fivetran/Snowflake/DBT seems to be the modern data stack, but I really want to escape pure SQL hell.&lt;/p&gt;\n\n&lt;p&gt;Any advice would be appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z0ab99", "is_robot_indexable": true, "report_reasons": null, "author": "Techthrowaway2222888", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0ab99/moving_from_sqlgui_stack_to_pythonscalabig/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0ab99/moving_from_sqlgui_stack_to_pythonscalabig/", "subreddit_subscribers": 80570, "created_utc": 1668966412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Those who are working or have worked on spark and hive tables with AWS EMR, how do you automate the deployment of Hive DDL queries to production instead of manually executing the commands. Is there any way to automate that via jenkins if we push the ddl statements in Git", "author_fullname": "t2_3xh5j7zr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automating DDL Hive table deployment process to AWS.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0m3be", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1668998103.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668995489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Those who are working or have worked on spark and hive tables with AWS EMR, how do you automate the deployment of Hive DDL queries to production instead of manually executing the commands. Is there any way to automate that via jenkins if we push the ddl statements in Git&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z0m3be", "is_robot_indexable": true, "report_reasons": null, "author": "thedatumgirl", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0m3be/automating_ddl_hive_table_deployment_process_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0m3be/automating_ddl_hive_table_deployment_process_to/", "subreddit_subscribers": 80570, "created_utc": 1668995489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My current client is offering to add me to the 5 day Azure Data Engineer certification program classes they are organizing for their employees.\n\nI grew into Data Engineering from a Data Analyst position with a Business Administration background many years ago and I have basic experience with the relevant tools. Will the 5 day training be enough to pass the exam? And if not, what else should I do?", "author_fullname": "t2_u2p974i5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Engineer DP-203 5 day training enough", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0g25s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668980168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My current client is offering to add me to the 5 day Azure Data Engineer certification program classes they are organizing for their employees.&lt;/p&gt;\n\n&lt;p&gt;I grew into Data Engineering from a Data Analyst position with a Business Administration background many years ago and I have basic experience with the relevant tools. Will the 5 day training be enough to pass the exam? And if not, what else should I do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z0g25s", "is_robot_indexable": true, "report_reasons": null, "author": "MarcScripts", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0g25s/azure_data_engineer_dp203_5_day_training_enough/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0g25s/azure_data_engineer_dp203_5_day_training_enough/", "subreddit_subscribers": 80570, "created_utc": 1668980168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, \n\nPlease check the library I developed:\n\n[https://jaume-jci.github.io/ds-blocks/](https://jaume-jci.github.io/ds-blocks/)\n\nI use it everyday at work and would be happy to receive comments about it if you have the chance to try it out. I am in the process of increasing its documentation, but if you have a specific thing you would like documented, please let me know by creating an issue in github. \n\nThanks!", "author_fullname": "t2_sgitsmr4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DS Blocks: write modular, compact, and decoupled data science pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0ctfh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1668972404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, &lt;/p&gt;\n\n&lt;p&gt;Please check the library I developed:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://jaume-jci.github.io/ds-blocks/\"&gt;https://jaume-jci.github.io/ds-blocks/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I use it everyday at work and would be happy to receive comments about it if you have the chance to try it out. I am in the process of increasing its documentation, but if you have a specific thing you would like documented, please let me know by creating an issue in github. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "z0ctfh", "is_robot_indexable": true, "report_reasons": null, "author": "Hot_Elk345", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0ctfh/ds_blocks_write_modular_compact_and_decoupled/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0ctfh/ds_blocks_write_modular_compact_and_decoupled/", "subreddit_subscribers": 80570, "created_utc": 1668972404.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello folks!  \nI'm going to prepare the **Elastic Certified Engineer Exam** and apart from the paid Elastic Engineer course ([https://www.elastic.co/training/elasticsearch-engineer](https://www.elastic.co/training/elasticsearch-engineer)) I've not found other resources.  \nFor those who prepared the exam:\n\n1. Is the Elastic Engineer course worth the price?\n2. What were the resources you found useful for preparing the certification? Are they for free?\n3. How much time did the study take?\n4. Is the certification more theoretical or practical?\n\nAny suggestions are well accepted! Thanks everybody for your help.  \nWish you a great start to the week!", "author_fullname": "t2_bju57m13", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Elastic Certified Engineer Exam", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0ucxf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669021953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks!&lt;br/&gt;\nI&amp;#39;m going to prepare the &lt;strong&gt;Elastic Certified Engineer Exam&lt;/strong&gt; and apart from the paid Elastic Engineer course (&lt;a href=\"https://www.elastic.co/training/elasticsearch-engineer\"&gt;https://www.elastic.co/training/elasticsearch-engineer&lt;/a&gt;) I&amp;#39;ve not found other resources.&lt;br/&gt;\nFor those who prepared the exam:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is the Elastic Engineer course worth the price?&lt;/li&gt;\n&lt;li&gt;What were the resources you found useful for preparing the certification? Are they for free?&lt;/li&gt;\n&lt;li&gt;How much time did the study take?&lt;/li&gt;\n&lt;li&gt;Is the certification more theoretical or practical?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any suggestions are well accepted! Thanks everybody for your help.&lt;br/&gt;\nWish you a great start to the week!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TY1YULW0Hq_Gtl2aSW8toSCSdK7Po7SGTXiWL5AGvdc.jpg?auto=webp&amp;s=0c3d8962b0cc2bbc33ba2924d03e01adbc52906b", "width": 1600, "height": 837}, "resolutions": [{"url": "https://external-preview.redd.it/TY1YULW0Hq_Gtl2aSW8toSCSdK7Po7SGTXiWL5AGvdc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c54b3074b2b90776d500097d82463dcf3d183884", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/TY1YULW0Hq_Gtl2aSW8toSCSdK7Po7SGTXiWL5AGvdc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=865f8c33a5229448f9ac03c8658d6903f51907e9", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/TY1YULW0Hq_Gtl2aSW8toSCSdK7Po7SGTXiWL5AGvdc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0634edd925cd046af1d703280f9ab23df413c9ed", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/TY1YULW0Hq_Gtl2aSW8toSCSdK7Po7SGTXiWL5AGvdc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6db5f51ca7369c4938b17b8dfc295ab2fffea06b", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/TY1YULW0Hq_Gtl2aSW8toSCSdK7Po7SGTXiWL5AGvdc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e8c321e8ca6e4f9500ac1f2dfe5aed7776afae4", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/TY1YULW0Hq_Gtl2aSW8toSCSdK7Po7SGTXiWL5AGvdc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f201ec22036ad66c2a7e05f4095b4c1ced2f7e4", "width": 1080, "height": 564}], "variants": {}, "id": "p624g08QT7D7F0oaVe1DXFX6hZsxhkBTgP30g5FM7kI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z0ucxf", "is_robot_indexable": true, "report_reasons": null, "author": "woland_96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0ucxf/elastic_certified_engineer_exam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0ucxf/elastic_certified_engineer_exam/", "subreddit_subscribers": 80570, "created_utc": 1669021953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5tz7z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Dodgy State of Stream Processing Delivery Guarantees", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_z12dql", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QmpBOCvY8mY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"The Dodgy State of Delivery Guarantees\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "The Dodgy State of Delivery Guarantees", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QmpBOCvY8mY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"The Dodgy State of Delivery Guarantees\"&gt;&lt;/iframe&gt;", "author_name": "Jeffail", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/QmpBOCvY8mY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Jeffail"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QmpBOCvY8mY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"The Dodgy State of Delivery Guarantees\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/z12dql", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/YRR3b8_BlHgeTA8P57ZTrl_GN3dmlqmby5c8IFL-yC0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669046090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=QmpBOCvY8mY", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rzwLrNWEOrQIfIOHmANPpQFl9N0oxXqSQD3swEEr2NI.jpg?auto=webp&amp;s=0305706187a2011f35c7c669268ee1e0888ea9ca", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/rzwLrNWEOrQIfIOHmANPpQFl9N0oxXqSQD3swEEr2NI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=92e39c5703310577ac56647bf0b7d49900e1d2e4", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/rzwLrNWEOrQIfIOHmANPpQFl9N0oxXqSQD3swEEr2NI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1031f8aae1c55d2db6e6ba587fd8ba729d31c03a", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/rzwLrNWEOrQIfIOHmANPpQFl9N0oxXqSQD3swEEr2NI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8b710bb5a738c4841e95e5a3507aca5e02a1be7", "width": 320, "height": 240}], "variants": {}, "id": "YBOTc6rsHRbN6AAvcd7IQ1EdOwNs2tixrFQmCC3PtM0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z12dql", "is_robot_indexable": true, "report_reasons": null, "author": "mihaitodor", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z12dql/the_dodgy_state_of_stream_processing_delivery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=QmpBOCvY8mY", "subreddit_subscribers": 80570, "created_utc": 1669046090.0, "num_crossposts": 1, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "The Dodgy State of Delivery Guarantees", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QmpBOCvY8mY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"The Dodgy State of Delivery Guarantees\"&gt;&lt;/iframe&gt;", "author_name": "Jeffail", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/QmpBOCvY8mY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Jeffail"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Given recent debates on the necessity of certain tooling with small datasets, it would be interesting to know the distribution of workloads for users on this subreddit. Volume is obviously one of many potential proxies for the complexity of a workload, but it should suffice for a high level view.\n\n[View Poll](https://www.reddit.com/poll/z11xd8)", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much data do you process via your pipelines in a given day?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z11xd8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669044987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given recent debates on the necessity of certain tooling with small datasets, it would be interesting to know the distribution of workloads for users on this subreddit. Volume is obviously one of many potential proxies for the complexity of a workload, but it should suffice for a high level view.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/z11xd8\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z11xd8", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1669304187618, "options": [{"text": "Under 1GB", "id": "19944314"}, {"text": "1GB to 10GB", "id": "19944315"}, {"text": "10GB to 100GB", "id": "19944316"}, {"text": "100GB to 1TB", "id": "19944317"}, {"text": "1 TB+", "id": "19944318"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 50, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/z11xd8/how_much_data_do_you_process_via_your_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/z11xd8/how_much_data_do_you_process_via_your_pipelines/", "subreddit_subscribers": 80570, "created_utc": 1669044987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you there is a use case for it? From a DE standpoint to implement ETL?\n\nRight now Im on a project which the biggest table is 10M and we are using Databricks for a simple ETL process to load a dimensional model.\n\nLifting a cluster to deal with this volume sounds silly. This could be easily achieved with raw python or a DB.", "author_fullname": "t2_3174m8nl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks for small datasets? Worth it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z11ew5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669043711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you there is a use case for it? From a DE standpoint to implement ETL?&lt;/p&gt;\n\n&lt;p&gt;Right now Im on a project which the biggest table is 10M and we are using Databricks for a simple ETL process to load a dimensional model.&lt;/p&gt;\n\n&lt;p&gt;Lifting a cluster to deal with this volume sounds silly. This could be easily achieved with raw python or a DB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z11ew5", "is_robot_indexable": true, "report_reasons": null, "author": "PaleBass", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z11ew5/databricks_for_small_datasets_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z11ew5/databricks_for_small_datasets_worth_it/", "subreddit_subscribers": 80570, "created_utc": 1669043711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy,\n\nConsidering the architecture for implementing something to ingest a stream from Twitter API, I'd like it to be distributable.\n\nSo far I've considered:\n\nCreating ingestion via Python (tweepy probably), containerised and deployed into K8s with scalable replicas. \n\nSeparate pod for Postgres DB for data persistence. \n\n Without overcomplicating it is there an easier way to avoid dealing with duplicates from the stream? (I thought about maybe using Kafka or some message bus)", "author_fullname": "t2_11ua04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parallelised twitter stream - how to avoid duplicates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0uiky", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669022523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy,&lt;/p&gt;\n\n&lt;p&gt;Considering the architecture for implementing something to ingest a stream from Twitter API, I&amp;#39;d like it to be distributable.&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve considered:&lt;/p&gt;\n\n&lt;p&gt;Creating ingestion via Python (tweepy probably), containerised and deployed into K8s with scalable replicas. &lt;/p&gt;\n\n&lt;p&gt;Separate pod for Postgres DB for data persistence. &lt;/p&gt;\n\n&lt;p&gt;Without overcomplicating it is there an easier way to avoid dealing with duplicates from the stream? (I thought about maybe using Kafka or some message bus)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z0uiky", "is_robot_indexable": true, "report_reasons": null, "author": "7007001", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0uiky/parallelised_twitter_stream_how_to_avoid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0uiky/parallelised_twitter_stream_how_to_avoid/", "subreddit_subscribers": 80570, "created_utc": 1669022523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, I am just starting on my way to learn DE. I have CS and engineering degree and I am now working as a BI engineer and would like someone to give me some mentoring and tell me if I am going on the right path.", "author_fullname": "t2_ozdnflqp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice and mentoring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0u41u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669021058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, I am just starting on my way to learn DE. I have CS and engineering degree and I am now working as a BI engineer and would like someone to give me some mentoring and tell me if I am going on the right path.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z0u41u", "is_robot_indexable": true, "report_reasons": null, "author": "Various_Bandicoot977", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0u41u/advice_and_mentoring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0u41u/advice_and_mentoring/", "subreddit_subscribers": 80570, "created_utc": 1669021058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to know if I can use apache flume to stream postgres log to a pyspark program. If yes can you tell me how, thanks", "author_fullname": "t2_cb7rpz4k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use apache flume to stream postgres log file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0sx8t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669016624.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to know if I can use apache flume to stream postgres log to a pyspark program. If yes can you tell me how, thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z0sx8t", "is_robot_indexable": true, "report_reasons": null, "author": "Mr_marshmellow19", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0sx8t/use_apache_flume_to_stream_postgres_log_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0sx8t/use_apache_flume_to_stream_postgres_log_file/", "subreddit_subscribers": 80570, "created_utc": 1669016624.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Free online course\n\nIntroduction to Power BI\n\nIn this course, you\u2019ll go from zero to hero as you discover how to use this popular business intelligence platform through hands-on exercises.\n\n[https://formationgratuite.net/Introduction-to-Power-BI/](https://formationgratuite.net/Introduction-to-Power-BI/)", "author_fullname": "t2_ca2mv3e7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introduction to Power BI - Free online course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z13dx8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669048588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Free online course&lt;/p&gt;\n\n&lt;p&gt;Introduction to Power BI&lt;/p&gt;\n\n&lt;p&gt;In this course, you\u2019ll go from zero to hero as you discover how to use this popular business intelligence platform through hands-on exercises.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://formationgratuite.net/Introduction-to-Power-BI/\"&gt;https://formationgratuite.net/Introduction-to-Power-BI/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gWillDPtz8S9fqu3z_2CJtKwn7K-4JGGc80WRivqakE.jpg?auto=webp&amp;s=fc5e8a60e405087ef02fc609376980b91e65677b", "width": 940, "height": 788}, "resolutions": [{"url": "https://external-preview.redd.it/gWillDPtz8S9fqu3z_2CJtKwn7K-4JGGc80WRivqakE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49d0755ed96a8f3a1d6c80e04df15501011f291a", "width": 108, "height": 90}, {"url": "https://external-preview.redd.it/gWillDPtz8S9fqu3z_2CJtKwn7K-4JGGc80WRivqakE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a28fecd117ae38bd5a6da1c47617f90631815313", "width": 216, "height": 181}, {"url": "https://external-preview.redd.it/gWillDPtz8S9fqu3z_2CJtKwn7K-4JGGc80WRivqakE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=80ad1e1f472ffb2f5c94147a6c1d06fd076822d8", "width": 320, "height": 268}, {"url": "https://external-preview.redd.it/gWillDPtz8S9fqu3z_2CJtKwn7K-4JGGc80WRivqakE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=44e0a26db070b4ef5a13a22521a0956c8c13e942", "width": 640, "height": 536}], "variants": {}, "id": "wHqW370wf9afWH-dOw8frn8SzXpisyKcQLd9kwOoMK4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "z13dx8", "is_robot_indexable": true, "report_reasons": null, "author": "MDLearning", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z13dx8/introduction_to_power_bi_free_online_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z13dx8/introduction_to_power_bi_free_online_course/", "subreddit_subscribers": 80570, "created_utc": 1669048588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m using FastAPI for Machine Learning prediction model deployment (classification case), In fact, I have 2 type of models that works the following way:\n\n1. if model got all features (**primary and secondary**) =&gt; apply **model\\_type1**\n2. if model got only **primary features** (missing secondary) =&gt; apply **model\\_type2**\n3. if model got missing primary features =&gt; display an error (invalid for prediction, we can't use any of the two models due to insufficient data)\n\nPS. model\\_type1 has features of model\\_type2 (called primary) + other features (called secondary), while model\\_type2 must have only primary features, if none of them is an option an error should be displayed.\n\nTo achieve this, I have the inputs of type \"array of JSON\" in the form below:\n\n    [ { \n    \"Id\":\"value\",\n    \"feature_primary1\":\"value\", \n    \"feature_primary2\":\"value\", \n    \"feature_primary3\":\"value\", \n    \"feature_secondary1\":\"value\", \n    \"feature_secondary2\":\"value\" \n    }, \n    { \n    \"Id\":\"value\", \n    \"feature_primary1\":\"value\", \n    \"feature_primary2\":\"value\", \n    \"feature_primary3\":\"value\", \n    \"feature_secondary1\":\"value\", \n    \"feature_secondary2\":\"value\" \n    }, \n    { \n    \"Id\":\"value\", \n    \"feature_primary1\":\"value\", \n    \"feature_primary2\":\"value\", \n    \"feature_primary3\":\"value\", \n    \"feature_secondary1\":\"value\", \n    \"feature_secondary2\":\"value\" \n    } \n    ... \n    ] \n\nAnd I need to filter the inputs before feeding them to the predictive model. Meaning, to filter each JSON  \n of the array.\n\nSo, I want to create 3 lists where:\n\n1. if inputs match **model\\_type1** append to list1\n2. if inputs doesn't match **model\\_type1** and match **model\\_type2** append to list2\n3. if inputs doesn't match **model\\_type1** nor **model\\_type2** append error detail to list3\n\nAnd then assign each list to its proper model and display the prediction results (Outputs). So if we send the following inputs:\n\n    [ \n    { \n    \"Id\": 1,\n    \"feature_primary1\": \"David\", \n    \"feature_primary2\": 15670.87, \n    \"feature_primary3\": \"Male\", \n    \"feature_secondary1\": \"Yes\", \n    \"feature_secondary2\": 45 \n    }, \n    { \n    \"Id\": 2, \n    \"feature_primary1\": \"Alice\", \n    \"feature_primary2\": 78995.65, \n    \"feature_primary3\": \"Female\", \n    \"feature_secondary1\": NaN, \n    \"feature_secondary2\": NaN   \n    }, \n    { \n    \"Id\": 3, \n    \"feature_primary1\": \"John\", \n    \"feature_primary2\": NaN, \n    \"feature_primary3\": NaN, \n    \"feature_secondary1\": NaN, \n    \"feature_secondary2\": 79 \n    } \n    ... \n    ] \n\nThe output (result of prediction) should look like this :\n\n    [ \n    { \n    \"Id\": 1, \n    \"type_model\": \"model_type1\", \n    \"prediction\": \"class1\" \n    }, \n    { \n    \"Id\": 2, \n    \"type_model\": \"model_type2\", \n    \"prediction\": \"class2\" \n    }, \n    { \"Id\": 3, \n    \"error_description\": \"missing values for feature_primary2,                          feature_primary3 and feature_secondary1\" \n    } \n    ... \n    ] \n\nHow can I achieve the filter process within the 3 lists, in an optimized way with FastAPI in Python?", "author_fullname": "t2_baqjslqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Filter inputs of FastAPI and assign each type to a specific list", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z1312n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669047692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m using FastAPI for Machine Learning prediction model deployment (classification case), In fact, I have 2 type of models that works the following way:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;if model got all features (&lt;strong&gt;primary and secondary&lt;/strong&gt;) =&amp;gt; apply &lt;strong&gt;model_type1&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;if model got only &lt;strong&gt;primary features&lt;/strong&gt; (missing secondary) =&amp;gt; apply &lt;strong&gt;model_type2&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;if model got missing primary features =&amp;gt; display an error (invalid for prediction, we can&amp;#39;t use any of the two models due to insufficient data)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;PS. model_type1 has features of model_type2 (called primary) + other features (called secondary), while model_type2 must have only primary features, if none of them is an option an error should be displayed.&lt;/p&gt;\n\n&lt;p&gt;To achieve this, I have the inputs of type &amp;quot;array of JSON&amp;quot; in the form below:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[ { \n&amp;quot;Id&amp;quot;:&amp;quot;value&amp;quot;,\n&amp;quot;feature_primary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary3&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary2&amp;quot;:&amp;quot;value&amp;quot; \n}, \n{ \n&amp;quot;Id&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary3&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary2&amp;quot;:&amp;quot;value&amp;quot; \n}, \n{ \n&amp;quot;Id&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_primary3&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;:&amp;quot;value&amp;quot;, \n&amp;quot;feature_secondary2&amp;quot;:&amp;quot;value&amp;quot; \n} \n... \n] \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And I need to filter the inputs before feeding them to the predictive model. Meaning, to filter each JSON&lt;br/&gt;\n of the array.&lt;/p&gt;\n\n&lt;p&gt;So, I want to create 3 lists where:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;if inputs match &lt;strong&gt;model_type1&lt;/strong&gt; append to list1&lt;/li&gt;\n&lt;li&gt;if inputs doesn&amp;#39;t match &lt;strong&gt;model_type1&lt;/strong&gt; and match &lt;strong&gt;model_type2&lt;/strong&gt; append to list2&lt;/li&gt;\n&lt;li&gt;if inputs doesn&amp;#39;t match &lt;strong&gt;model_type1&lt;/strong&gt; nor &lt;strong&gt;model_type2&lt;/strong&gt; append error detail to list3&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;And then assign each list to its proper model and display the prediction results (Outputs). So if we send the following inputs:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[ \n{ \n&amp;quot;Id&amp;quot;: 1,\n&amp;quot;feature_primary1&amp;quot;: &amp;quot;David&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;: 15670.87, \n&amp;quot;feature_primary3&amp;quot;: &amp;quot;Male&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;: &amp;quot;Yes&amp;quot;, \n&amp;quot;feature_secondary2&amp;quot;: 45 \n}, \n{ \n&amp;quot;Id&amp;quot;: 2, \n&amp;quot;feature_primary1&amp;quot;: &amp;quot;Alice&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;: 78995.65, \n&amp;quot;feature_primary3&amp;quot;: &amp;quot;Female&amp;quot;, \n&amp;quot;feature_secondary1&amp;quot;: NaN, \n&amp;quot;feature_secondary2&amp;quot;: NaN   \n}, \n{ \n&amp;quot;Id&amp;quot;: 3, \n&amp;quot;feature_primary1&amp;quot;: &amp;quot;John&amp;quot;, \n&amp;quot;feature_primary2&amp;quot;: NaN, \n&amp;quot;feature_primary3&amp;quot;: NaN, \n&amp;quot;feature_secondary1&amp;quot;: NaN, \n&amp;quot;feature_secondary2&amp;quot;: 79 \n} \n... \n] \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The output (result of prediction) should look like this :&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[ \n{ \n&amp;quot;Id&amp;quot;: 1, \n&amp;quot;type_model&amp;quot;: &amp;quot;model_type1&amp;quot;, \n&amp;quot;prediction&amp;quot;: &amp;quot;class1&amp;quot; \n}, \n{ \n&amp;quot;Id&amp;quot;: 2, \n&amp;quot;type_model&amp;quot;: &amp;quot;model_type2&amp;quot;, \n&amp;quot;prediction&amp;quot;: &amp;quot;class2&amp;quot; \n}, \n{ &amp;quot;Id&amp;quot;: 3, \n&amp;quot;error_description&amp;quot;: &amp;quot;missing values for feature_primary2,                          feature_primary3 and feature_secondary1&amp;quot; \n} \n... \n] \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;How can I achieve the filter process within the 3 lists, in an optimized way with FastAPI in Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z1312n", "is_robot_indexable": true, "report_reasons": null, "author": "According-Promise-23", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z1312n/filter_inputs_of_fastapi_and_assign_each_type_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z1312n/filter_inputs_of_fastapi_and_assign_each_type_to/", "subreddit_subscribers": 80570, "created_utc": 1669047692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I stumbled across [this](https://informationscience.unt.edu/ms-information-systems) from the University of North Texas as I was searching for graduate level data modeling courses (I work as an analytics engineer with mainly dbt and GBQ, hardly any pipelines yet since we use stitch for now).. and it has me pretty interested due to the coursework.\n\nBy contrast and because of the popularity of computer science, I've been focusing on completing prerequisites for [Georgia Tech's OMSCS](https://omscs.gatech.edu/specialization-computing-systems), particularly in computing systems, through an online junior/community college.\n\nFor computing systems, I'm planning on learning about operating systems, networks, security, and computer architecture. Most of the prereqs have been in C and or C++, which is tangential to data engineering at best. There's numerous posts about SQL + Python being enough for most jobs (not considering jobs needing scala/java/BE knowledge). This all seems like a lot of preparation and knowledge acquisition for topics that I won't directly use in AE/DE. Have also read posts (from here and r/OMSCS) that say a masters isn't the best way to acquire knowledge needed to do one's job.\n\nMy impression is that most of DE is OJT and/or learned from personal projects and not really covered in academia at all. That being said, it seems the information science side of things worries about storage, organization, quality, etc. of the data. Isn't that primarily what we do, beyond moving the data?\n\nDoes it make more sense to stick with computing systems, or should I entertain something like the other masters, or something completely different? My end goal is to essentially stay in analytics or data engineering for the long term and make as much money as possible. No aspirations beyond that right now.", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why isn't information science a popular pathway into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z11r7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669044586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I stumbled across &lt;a href=\"https://informationscience.unt.edu/ms-information-systems\"&gt;this&lt;/a&gt; from the University of North Texas as I was searching for graduate level data modeling courses (I work as an analytics engineer with mainly dbt and GBQ, hardly any pipelines yet since we use stitch for now).. and it has me pretty interested due to the coursework.&lt;/p&gt;\n\n&lt;p&gt;By contrast and because of the popularity of computer science, I&amp;#39;ve been focusing on completing prerequisites for &lt;a href=\"https://omscs.gatech.edu/specialization-computing-systems\"&gt;Georgia Tech&amp;#39;s OMSCS&lt;/a&gt;, particularly in computing systems, through an online junior/community college.&lt;/p&gt;\n\n&lt;p&gt;For computing systems, I&amp;#39;m planning on learning about operating systems, networks, security, and computer architecture. Most of the prereqs have been in C and or C++, which is tangential to data engineering at best. There&amp;#39;s numerous posts about SQL + Python being enough for most jobs (not considering jobs needing scala/java/BE knowledge). This all seems like a lot of preparation and knowledge acquisition for topics that I won&amp;#39;t directly use in AE/DE. Have also read posts (from here and &lt;a href=\"/r/OMSCS\"&gt;r/OMSCS&lt;/a&gt;) that say a masters isn&amp;#39;t the best way to acquire knowledge needed to do one&amp;#39;s job.&lt;/p&gt;\n\n&lt;p&gt;My impression is that most of DE is OJT and/or learned from personal projects and not really covered in academia at all. That being said, it seems the information science side of things worries about storage, organization, quality, etc. of the data. Isn&amp;#39;t that primarily what we do, beyond moving the data?&lt;/p&gt;\n\n&lt;p&gt;Does it make more sense to stick with computing systems, or should I entertain something like the other masters, or something completely different? My end goal is to essentially stay in analytics or data engineering for the long term and make as much money as possible. No aspirations beyond that right now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z11r7y", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z11r7y/why_isnt_information_science_a_popular_pathway/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z11r7y/why_isnt_information_science_a_popular_pathway/", "subreddit_subscribers": 80570, "created_utc": 1669044586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working with a large undocumented Oracle database that has over 1900 tables and 900 views. I know approximately which are the main tables and how the system works, but I'm wondering if there are any visualization or other type of tools that you use in your daily life to navigate such large complex databases and do exploratory work (see relationships between tables, column comments where such exist, etc.).\n\nPerhaps you have any other tips and tricks on what to do when approaching such a problem?\n\nThe goal is to extract some recurring data once a day, but first I have to unfortunately find it.", "author_fullname": "t2_elso2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tools that help to explore and navigate database tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z10fxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669041207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working with a large undocumented Oracle database that has over 1900 tables and 900 views. I know approximately which are the main tables and how the system works, but I&amp;#39;m wondering if there are any visualization or other type of tools that you use in your daily life to navigate such large complex databases and do exploratory work (see relationships between tables, column comments where such exist, etc.).&lt;/p&gt;\n\n&lt;p&gt;Perhaps you have any other tips and tricks on what to do when approaching such a problem?&lt;/p&gt;\n\n&lt;p&gt;The goal is to extract some recurring data once a day, but first I have to unfortunately find it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z10fxk", "is_robot_indexable": true, "report_reasons": null, "author": "Kardinals", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z10fxk/tools_that_help_to_explore_and_navigate_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z10fxk/tools_that_help_to_explore_and_navigate_database/", "subreddit_subscribers": 80570, "created_utc": 1669041207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Simple question, curious about experiences and how much of an operational pain it is / isn't. I intend to experiment with it when I get some time but, in the meantime, I thought I would ask the hivemind.", "author_fullname": "t2_dxegl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone using Prefect Orion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0zfhb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669038505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Simple question, curious about experiences and how much of an operational pain it is / isn&amp;#39;t. I intend to experiment with it when I get some time but, in the meantime, I thought I would ask the hivemind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "z0zfhb", "is_robot_indexable": true, "report_reasons": null, "author": "nutso_muzz", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0zfhb/anyone_using_prefect_orion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0zfhb/anyone_using_prefect_orion/", "subreddit_subscribers": 80570, "created_utc": 1669038505.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI'm on my way to creating a new pipeline on our Azure stack that is supposed to read Excel sheets located at Google Drive partitions.\n\nWe're using Data Factory to read those files, but they don't have a connection directly with Google Drive.\n\nSo, I'm assuming I have basically two options\n\n1. Transfer all those files to SharePoint manually or ask clients to use it instead of Gdrive (not nice)\n2. Create an automation or pipeline at GCP that extract files from GDrive and sends to Google Cloud Storage (data factory has built-in connection with it)\n\nHow would you guys handle this scenario? There are more options available?\n\nThanks in advance!\n\nRegards,\n\nDouglas.", "author_fullname": "t2_30mkkbap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory reading flat files from Google Drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0yqpc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669036536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on my way to creating a new pipeline on our Azure stack that is supposed to read Excel sheets located at Google Drive partitions.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re using Data Factory to read those files, but they don&amp;#39;t have a connection directly with Google Drive.&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m assuming I have basically two options&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Transfer all those files to SharePoint manually or ask clients to use it instead of Gdrive (not nice)&lt;/li&gt;\n&lt;li&gt;Create an automation or pipeline at GCP that extract files from GDrive and sends to Google Cloud Storage (data factory has built-in connection with it)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;How would you guys handle this scenario? There are more options available?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;Regards,&lt;/p&gt;\n\n&lt;p&gt;Douglas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "z0yqpc", "is_robot_indexable": true, "report_reasons": null, "author": "ddddddkkk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/z0yqpc/azure_data_factory_reading_flat_files_from_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0yqpc/azure_data_factory_reading_flat_files_from_google/", "subreddit_subscribers": 80570, "created_utc": 1669036536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://medium.com/dev-genius/data-processing-and-analysis-using-spark-spark-project-1-3f52516272a7](https://medium.com/dev-genius/data-processing-and-analysis-using-spark-spark-project-1-3f52516272a7)", "author_fullname": "t2_7oampu1h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Processing and Analysis using Spark | Spark Project-1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0wzng", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669031188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/dev-genius/data-processing-and-analysis-using-spark-spark-project-1-3f52516272a7\"&gt;https://medium.com/dev-genius/data-processing-and-analysis-using-spark-spark-project-1-3f52516272a7&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?auto=webp&amp;s=20f5c598da0f726c0dc984526647eae6fe4c97f2", "width": 1200, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8f4cbfbb3100edaa4d8de9fbbfd2c55e88c4ddd", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac3e01858cd638a208b4ba634dcbf44b20f34e0c", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e13905cadd8cdc93192b539f75027342f4fec511", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea93eebc2223084e23b32d12900b165ba5a28bbc", "width": 640, "height": 346}, {"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e4018604129d965e0fb3564c52cce3b4b60273c3", "width": 960, "height": 520}, {"url": "https://external-preview.redd.it/V1jJYPkvoTLYZw16aUkhD3ctMFSV-cbGU7cuvHKsFSc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1141bfabf3a6010c851f306ca5d51776b9a03293", "width": 1080, "height": 585}], "variants": {}, "id": "G745Y6CwlB7JbdlpVSGhxXc4G8tAb-IPbzGzjIM7fiY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "z0wzng", "is_robot_indexable": true, "report_reasons": null, "author": "Sidharth_r", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/z0wzng/data_processing_and_analysis_using_spark_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/z0wzng/data_processing_and_analysis_using_spark_spark/", "subreddit_subscribers": 80570, "created_utc": 1669031188.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}