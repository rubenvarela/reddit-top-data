{"kind": "Listing", "data": {"after": null, "dist": 10, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I observed a few company where one of the person on the hiring panel (often a fellow data scientist and not a person of managerial position) rapidly ask technical questions. I am not sure if it is pre-mediated to stress test a candidate, but the experience left a sour taste in my mouth. What do you think about this interview strategy?\n\nUnrelated, I got a little revenge when I get to rapidly fire the panel for their take on the negative reviews I gathered on glassdoor.", "author_fullname": "t2_35uzbd5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "rapid fire technical questioning during an interview a red flag?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z11itd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 107, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 107, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1669044476.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669043997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I observed a few company where one of the person on the hiring panel (often a fellow data scientist and not a person of managerial position) rapidly ask technical questions. I am not sure if it is pre-mediated to stress test a candidate, but the experience left a sour taste in my mouth. What do you think about this interview strategy?&lt;/p&gt;\n\n&lt;p&gt;Unrelated, I got a little revenge when I get to rapidly fire the panel for their take on the negative reviews I gathered on glassdoor.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "z11itd", "is_robot_indexable": true, "report_reasons": null, "author": "fanhui3", "discussion_type": null, "num_comments": 69, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z11itd/rapid_fire_technical_questioning_during_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z11itd/rapid_fire_technical_questioning_during_an/", "subreddit_subscribers": 820896, "created_utc": 1669043997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Weekly Entering &amp; Transitioning - Thread 21 Nov, 2022 - 28 Nov, 2022", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0q0yl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669006869.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;\n&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;\n&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;\n&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;\n&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=\"https://www.reddit.com/r/datascience/wiki/frequently-asked-questions\"&gt;FAQ&lt;/a&gt; and Resources pages on our wiki. You can also search for answers in &lt;a href=\"https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new\"&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "new", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z0q0yl", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 16, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z0q0yl/weekly_entering_transitioning_thread_21_nov_2022/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/datascience/comments/z0q0yl/weekly_entering_transitioning_thread_21_nov_2022/", "subreddit_subscribers": 820896, "created_utc": 1669006869.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_26vasc8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just wanted to share the best typo I'll ever come across in an academic paper. Right in the acknowledgments, too.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 63, "top_awarded_type": null, "hide_score": true, "name": "t3_z1bb5z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GHKkiK09k74d1wdHYGuEwqVcqUW0kI6afx9FzJrp_zA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1669067190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/5qabzetyjd1a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/5qabzetyjd1a1.png?auto=webp&amp;s=cde95a96bd10392a89a2ddb5aea562bdc98b1282", "width": 2065, "height": 939}, "resolutions": [{"url": "https://preview.redd.it/5qabzetyjd1a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e6ce10f75c0c9f5befe5f9cd6e2974a8bbcf9fc", "width": 108, "height": 49}, {"url": "https://preview.redd.it/5qabzetyjd1a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ee7155fc04f2a2a37cd5ddbbad57d53d5d49e44", "width": 216, "height": 98}, {"url": "https://preview.redd.it/5qabzetyjd1a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=06dfb3ef8fbea0a6aca16b71f61e7475d1a8e73f", "width": 320, "height": 145}, {"url": "https://preview.redd.it/5qabzetyjd1a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=23f6ec6e50c1a78f006b151713b04a87412034db", "width": 640, "height": 291}, {"url": "https://preview.redd.it/5qabzetyjd1a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=95f6d06150d3dd1f374073a040f0d24fc25dadbb", "width": 960, "height": 436}, {"url": "https://preview.redd.it/5qabzetyjd1a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2cb713a0420f6cafcc0e45648aca50c0ee3feba9", "width": 1080, "height": 491}], "variants": {}, "id": "cvCS17NPuAJL2zmo-4XCZpZ4NZ3e2t5CicU0qlqbTYw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "z1bb5z", "is_robot_indexable": true, "report_reasons": null, "author": "icanelectoo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z1bb5z/just_wanted_to_share_the_best_typo_ill_ever_come/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/5qabzetyjd1a1.png", "subreddit_subscribers": 820896, "created_utc": 1669067190.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have an engineering background. I am exploring a career move into data science. Was offered a senior data scientist role at a large IT firm. My core responsibilities include \"data exploration and providing structured data for analysis\", which to me sounds like a data analyst role and not data scientist.\n\nNeed some suggestions if I should proceed. From what I read online, data scientists do spend a lot of time exploring right data sources and structuring documents for optimum training and visualizations. How much time do you typically spend in these activities (in terms of % of work per week)? I want to see how much time I will get for learning \"cool\" stuff like training models and playing with algorithms. \n\nThanks!", "author_fullname": "t2_r0jobtqp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much time do you typically spend in exploring and structuring data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0q4jp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669007186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an engineering background. I am exploring a career move into data science. Was offered a senior data scientist role at a large IT firm. My core responsibilities include &amp;quot;data exploration and providing structured data for analysis&amp;quot;, which to me sounds like a data analyst role and not data scientist.&lt;/p&gt;\n\n&lt;p&gt;Need some suggestions if I should proceed. From what I read online, data scientists do spend a lot of time exploring right data sources and structuring documents for optimum training and visualizations. How much time do you typically spend in these activities (in terms of % of work per week)? I want to see how much time I will get for learning &amp;quot;cool&amp;quot; stuff like training models and playing with algorithms. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z0q4jp", "is_robot_indexable": true, "report_reasons": null, "author": "Professor1441", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z0q4jp/how_much_time_do_you_typically_spend_in_exploring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z0q4jp/how_much_time_do_you_typically_spend_in_exploring/", "subreddit_subscribers": 820896, "created_utc": 1669007186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Currently working for a large engineering company, usually based in the European HQ office (where all the Data &amp; AI/Engineering team sits), but now based out of sales office in South America on a 6 month secondment.\n\nI am essentially the link between the local sales offices (in 8 countries across the Americas) and HQ, where all the digital product development is done. I document the current data collection processes and attempt to drive traction for various digital offerings that are in development. I am in constant communication with multiple internal teams and external customers.\n\nI have a masters in Data Science &amp; Analytics, but also 3 years of sales experience - if that puts my background in perspective.\n\nAnyway, would like to tidy up my CV and am struggling what job title this would constitute. My direct report says I am an \"Evangelist\" but not a fan of that classification.\n\nAny suggestions are welcome!", "author_fullname": "t2_fmj4170y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is my job title?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z1buws", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669068437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently working for a large engineering company, usually based in the European HQ office (where all the Data &amp;amp; AI/Engineering team sits), but now based out of sales office in South America on a 6 month secondment.&lt;/p&gt;\n\n&lt;p&gt;I am essentially the link between the local sales offices (in 8 countries across the Americas) and HQ, where all the digital product development is done. I document the current data collection processes and attempt to drive traction for various digital offerings that are in development. I am in constant communication with multiple internal teams and external customers.&lt;/p&gt;\n\n&lt;p&gt;I have a masters in Data Science &amp;amp; Analytics, but also 3 years of sales experience - if that puts my background in perspective.&lt;/p&gt;\n\n&lt;p&gt;Anyway, would like to tidy up my CV and am struggling what job title this would constitute. My direct report says I am an &amp;quot;Evangelist&amp;quot; but not a fan of that classification.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions are welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z1buws", "is_robot_indexable": true, "report_reasons": null, "author": "shadowgroover", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z1buws/what_is_my_job_title/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z1buws/what_is_my_job_title/", "subreddit_subscribers": 820896, "created_utc": 1669068437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This [paper](https://igi-web.tugraz.at/PDF/kdd08.pdf), as well as many others (e.g. these [paper](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf) [paper](https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009_BPC_BigChaos.pdf) from the winners of the Netflix Prize) that came out of the Netflix Prize contest recommend that you remove global effects one by one as a preprocessing step. However, when I try this, it only seems to increase my RMSE.\n\nFor example, if I train a model with the following optimization problem:  \n\nhttps://preview.redd.it/u6nkqddknd1a1.png?width=429&amp;format=png&amp;auto=webp&amp;s=b628883edb2c1729f47568b903449fa3a6ea26d0\n\nand then use those residuals to train a latent factor model with this optimization problem: \n\nhttps://preview.redd.it/wlyjab0mnd1a1.png?width=495&amp;format=png&amp;auto=webp&amp;s=1031b65d6d18661fb7b89a88116c73f0567ed8c2\n\nI get no improvement from the first model. Is this the wrong optimization problem to train on residuals with? I use hyper-parameters very similar to the ones delineated in the papers I linked, and the number of latent factors I choose has relatively little impact on my RMSE.\n\nAdditionally, I have tried combining the user/items biases with the pq interaction into one LFM, and it is worse than the one with only biases. Even adding implicit bias (SVD++) only increases RMSE, so I am sort of confused about why this is happening and how to fix it. My best RMSE I can get is \\~1.0011 and is from the most basic bias-only model. However, the papers I linked get RMSE well below 1. I know I am not factoring in time, but my RMSE should still be much lower with out those things.\n\nAlso note, I am using the [Yelp dataset](https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset) from kaggle instead of the Netflix dataset.", "author_fullname": "t2_2pqawytk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to remove global effects for latent factor recommender?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 13, "top_awarded_type": null, "hide_score": true, "media_metadata": {"u6nkqddknd1a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 10, "x": 108, "u": "https://preview.redd.it/u6nkqddknd1a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=792ee36dd7b6ccf2e25ee8439990e3946879f500"}, {"y": 21, "x": 216, "u": "https://preview.redd.it/u6nkqddknd1a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e89f0d429a43f96de439c45d4610b78b3662e23"}, {"y": 31, "x": 320, "u": "https://preview.redd.it/u6nkqddknd1a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8f06d505c2b16789369eb6aca317e18832ac3ca"}], "s": {"y": 42, "x": 429, "u": "https://preview.redd.it/u6nkqddknd1a1.png?width=429&amp;format=png&amp;auto=webp&amp;s=b628883edb2c1729f47568b903449fa3a6ea26d0"}, "id": "u6nkqddknd1a1"}, "wlyjab0mnd1a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 9, "x": 108, "u": "https://preview.redd.it/wlyjab0mnd1a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=74bcbc80b9a88cc7b9b76c0e66e97b348be2a896"}, {"y": 18, "x": 216, "u": "https://preview.redd.it/wlyjab0mnd1a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fcdbb0952d42cefa92e73f370c7c4a3dbfbf54e0"}, {"y": 27, "x": 320, "u": "https://preview.redd.it/wlyjab0mnd1a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=70ae4a234b2cfdb1b042a262dc2b7bd99d8b5f0a"}], "s": {"y": 42, "x": 495, "u": "https://preview.redd.it/wlyjab0mnd1a1.png?width=495&amp;format=png&amp;auto=webp&amp;s=1031b65d6d18661fb7b89a88116c73f0567ed8c2"}, "id": "wlyjab0mnd1a1"}}, "name": "t3_z1bhzc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/eB4HBKlyGHh-E2WTHgbnJZfntH3zOKVxx20RmOqSQqM.jpg", "edited": 1669068356.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669067634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This &lt;a href=\"https://igi-web.tugraz.at/PDF/kdd08.pdf\"&gt;paper&lt;/a&gt;, as well as many others (e.g. these &lt;a href=\"https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf\"&gt;paper&lt;/a&gt; &lt;a href=\"https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009_BPC_BigChaos.pdf\"&gt;paper&lt;/a&gt; from the winners of the Netflix Prize) that came out of the Netflix Prize contest recommend that you remove global effects one by one as a preprocessing step. However, when I try this, it only seems to increase my RMSE.&lt;/p&gt;\n\n&lt;p&gt;For example, if I train a model with the following optimization problem:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u6nkqddknd1a1.png?width=429&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b628883edb2c1729f47568b903449fa3a6ea26d0\"&gt;https://preview.redd.it/u6nkqddknd1a1.png?width=429&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b628883edb2c1729f47568b903449fa3a6ea26d0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;and then use those residuals to train a latent factor model with this optimization problem: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wlyjab0mnd1a1.png?width=495&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1031b65d6d18661fb7b89a88116c73f0567ed8c2\"&gt;https://preview.redd.it/wlyjab0mnd1a1.png?width=495&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1031b65d6d18661fb7b89a88116c73f0567ed8c2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I get no improvement from the first model. Is this the wrong optimization problem to train on residuals with? I use hyper-parameters very similar to the ones delineated in the papers I linked, and the number of latent factors I choose has relatively little impact on my RMSE.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I have tried combining the user/items biases with the pq interaction into one LFM, and it is worse than the one with only biases. Even adding implicit bias (SVD++) only increases RMSE, so I am sort of confused about why this is happening and how to fix it. My best RMSE I can get is ~1.0011 and is from the most basic bias-only model. However, the papers I linked get RMSE well below 1. I know I am not factoring in time, but my RMSE should still be much lower with out those things.&lt;/p&gt;\n\n&lt;p&gt;Also note, I am using the &lt;a href=\"https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset\"&gt;Yelp dataset&lt;/a&gt; from kaggle instead of the Netflix dataset.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z1bhzc", "is_robot_indexable": true, "report_reasons": null, "author": "Sentientlog", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z1bhzc/how_to_remove_global_effects_for_latent_factor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z1bhzc/how_to_remove_global_effects_for_latent_factor/", "subreddit_subscribers": 820896, "created_utc": 1669067634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Google Sheets have useful ImportXML() and ImportHTML() functions which allow to extract data from external websites and put them to sheet cells. If you never used it, try it now! Put this in Google Sheets cell: =IMPORTXML(\"  \n[https://en.wikipedia.org/wiki/Moon\\_landing](https://en.wikipedia.org/wiki/Moon_landing%60%60)\", \"//a/@href\")\n\nIt is possible to do a lot of things with ImportXML, but there are a lot of cons as well:\n\n* If the target website data requires some cleanup post-processing, it's getting very complicated since you are now \"programming with excel formulas\", rather painful process compared to regular code writing in conventional programming languages\n* There is no proper launch &amp; cache control so the function can be triggered occasionally and if the HTTP request fails, cells will be populated with ERR! values\n* The approach only works with most basic websites (no SPAs rendered in browsers can be scraped this way, any basic web scraping protection or connectivity issue breaks the process, no control over HTTP request geo location, or number of retries)\n\nWhen ImportXML() fails, the second approach to web scraping in Google Sheets is usually to write some custom [Google Apps Script](https://www.google.com/script/start/). This approach is much more flexible, just write Javascript code and deploy it as Google Sheets addon, but it takes a lot of time, and is not too easy to debug and iterate over - definitely not low code.\n\nSo, I have recently discovered an interesting approach of web scraping to Google Sheets using Make.com/Zapier alternative called Pipedream.com, and this approach is pretty flexible, reliable AND low code. It requires two external web services and Google Sheets is basically used as data store only, all the logic is done in Pipedream, which is a low code automation platform for developers, and all the web scraping heavy lifting (data retrieval, extracting data from HTML, data post processing) is done in ScrapeNinja.net low code sandbox. The best part of the Pipedream is that this approach is infinitely more flexible and reliable than plain Google Sheets, as it's basically a direct Zapier competitor (but with better free plan, and low-code more than no-code, so some Javascript knowledge might be required).\n\nHere is the video of using Pipedream and [ScrapeNinja.net](https://scrapeninja.net/) to extract HackerNews titles to Google Sheets every hour:\n\n[https://youtu.be/uBC752CWTew](https://youtu.be/uBC752CWTew)\n\nHow do you do web scraping in Google Sheets?", "author_fullname": "t2_13hqmc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web scraping in Google Sheets: a low-code alternative to ImportXML() approach", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z0tums", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669020047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google Sheets have useful ImportXML() and ImportHTML() functions which allow to extract data from external websites and put them to sheet cells. If you never used it, try it now! Put this in Google Sheets cell: =IMPORTXML(&amp;quot;&lt;br/&gt;\n&lt;a href=\"https://en.wikipedia.org/wiki/Moon_landing%60%60\"&gt;https://en.wikipedia.org/wiki/Moon_landing&lt;/a&gt;&amp;quot;, &amp;quot;//a/@href&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;It is possible to do a lot of things with ImportXML, but there are a lot of cons as well:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If the target website data requires some cleanup post-processing, it&amp;#39;s getting very complicated since you are now &amp;quot;programming with excel formulas&amp;quot;, rather painful process compared to regular code writing in conventional programming languages&lt;/li&gt;\n&lt;li&gt;There is no proper launch &amp;amp; cache control so the function can be triggered occasionally and if the HTTP request fails, cells will be populated with ERR! values&lt;/li&gt;\n&lt;li&gt;The approach only works with most basic websites (no SPAs rendered in browsers can be scraped this way, any basic web scraping protection or connectivity issue breaks the process, no control over HTTP request geo location, or number of retries)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;When ImportXML() fails, the second approach to web scraping in Google Sheets is usually to write some custom &lt;a href=\"https://www.google.com/script/start/\"&gt;Google Apps Script&lt;/a&gt;. This approach is much more flexible, just write Javascript code and deploy it as Google Sheets addon, but it takes a lot of time, and is not too easy to debug and iterate over - definitely not low code.&lt;/p&gt;\n\n&lt;p&gt;So, I have recently discovered an interesting approach of web scraping to Google Sheets using Make.com/Zapier alternative called Pipedream.com, and this approach is pretty flexible, reliable AND low code. It requires two external web services and Google Sheets is basically used as data store only, all the logic is done in Pipedream, which is a low code automation platform for developers, and all the web scraping heavy lifting (data retrieval, extracting data from HTML, data post processing) is done in ScrapeNinja.net low code sandbox. The best part of the Pipedream is that this approach is infinitely more flexible and reliable than plain Google Sheets, as it&amp;#39;s basically a direct Zapier competitor (but with better free plan, and low-code more than no-code, so some Javascript knowledge might be required).&lt;/p&gt;\n\n&lt;p&gt;Here is the video of using Pipedream and &lt;a href=\"https://scrapeninja.net/\"&gt;ScrapeNinja.net&lt;/a&gt; to extract HackerNews titles to Google Sheets every hour:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/uBC752CWTew\"&gt;https://youtu.be/uBC752CWTew&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;How do you do web scraping in Google Sheets?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z0tums", "is_robot_indexable": true, "report_reasons": null, "author": "superjet1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z0tums/web_scraping_in_google_sheets_a_lowcode/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z0tums/web_scraping_in_google_sheets_a_lowcode/", "subreddit_subscribers": 820896, "created_utc": 1669020047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hey everyone,\n\nI apologise in advance and this is not meant to be spam or anything but an exploration for those interested in helping mitigate space debris.\n\nWe're a small startup based in New Zealand reaching out to all Data Scientists and Space Debris Analysts who are interested in participating in a proof-of-concept Data Science Tournament. The goal of the tournament is to determine spacecraft collision risk. There will be prizes for winners of the competition. And if you don't win, that's okay too because everyone participating in the tournament will get something special.\n\nWhy are we doing this? We're trying a way to tackle the space debris problem and the ongoing crowding of orbits.\n\nWho can participate? Anyone really. You can be a student interested in data science, a data science professional, a citizen scientist, a space debris analyst, a company, an agency, etc. There are no gates here except the curiosity of learning to do it.\n\nDo you need to know orbital mechanics? Nope. All descriptive information will be provided prior &amp; during the tournament.\n\nWhat's the purpose of the tournament? Long term, to create reliable collision risk scores &amp; thresholds for possible collision events by crowd sourcing the data science as a way to tackle the blind men and the elephant problem in this domain.\n\nWhere can you signup? [https://dora-tournaments.carrd.co/](https://dora-tournaments.carrd.co/)\n\nPlease share this with your friends and colleagues who may be interested in participating in one of the most important data science tournaments for humanity.\n\nIf you're interested in what we're trying work on, you're welcome to check out at the links below:\n\n\\- Watchtower: [https://watchtower.world/](https://watchtower.world/)", "author_fullname": "t2_l2hs1w90", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[P] Data Science Tournament to determine Spacecraft Collision Risk", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_z1cb3l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1669069477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I apologise in advance and this is not meant to be spam or anything but an exploration for those interested in helping mitigate space debris.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re a small startup based in New Zealand reaching out to all Data Scientists and Space Debris Analysts who are interested in participating in a proof-of-concept Data Science Tournament. The goal of the tournament is to determine spacecraft collision risk. There will be prizes for winners of the competition. And if you don&amp;#39;t win, that&amp;#39;s okay too because everyone participating in the tournament will get something special.&lt;/p&gt;\n\n&lt;p&gt;Why are we doing this? We&amp;#39;re trying a way to tackle the space debris problem and the ongoing crowding of orbits.&lt;/p&gt;\n\n&lt;p&gt;Who can participate? Anyone really. You can be a student interested in data science, a data science professional, a citizen scientist, a space debris analyst, a company, an agency, etc. There are no gates here except the curiosity of learning to do it.&lt;/p&gt;\n\n&lt;p&gt;Do you need to know orbital mechanics? Nope. All descriptive information will be provided prior &amp;amp; during the tournament.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the purpose of the tournament? Long term, to create reliable collision risk scores &amp;amp; thresholds for possible collision events by crowd sourcing the data science as a way to tackle the blind men and the elephant problem in this domain.&lt;/p&gt;\n\n&lt;p&gt;Where can you signup? &lt;a href=\"https://dora-tournaments.carrd.co/\"&gt;https://dora-tournaments.carrd.co/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please share this with your friends and colleagues who may be interested in participating in one of the most important data science tournaments for humanity.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re interested in what we&amp;#39;re trying work on, you&amp;#39;re welcome to check out at the links below:&lt;/p&gt;\n\n&lt;p&gt;- Watchtower: &lt;a href=\"https://watchtower.world/\"&gt;https://watchtower.world/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/d69gHmqHly7u9zhFMaolxanbFEKbyXbBTbIC5vFGcnY.jpg?auto=webp&amp;s=e76fbfe47163cbab8392abd33518dd45bc747891", "width": 1280, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/d69gHmqHly7u9zhFMaolxanbFEKbyXbBTbIC5vFGcnY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ffb26d8e20b06fc693750ce2f458012359a13bb2", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/d69gHmqHly7u9zhFMaolxanbFEKbyXbBTbIC5vFGcnY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=69227e6ec91ad43a0b0249221e7844f6e63400f2", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/d69gHmqHly7u9zhFMaolxanbFEKbyXbBTbIC5vFGcnY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c745a007cb41b52634257b3af70b2311c0c69999", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/d69gHmqHly7u9zhFMaolxanbFEKbyXbBTbIC5vFGcnY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=62d703cec86c20cda94e4b3d036db4e9337beedb", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/d69gHmqHly7u9zhFMaolxanbFEKbyXbBTbIC5vFGcnY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=77a0a7c7b6e4b39906f2462d99c5bf799383ce37", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/d69gHmqHly7u9zhFMaolxanbFEKbyXbBTbIC5vFGcnY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=066425171b2c33d2ff982fe294c2a4c1911ffea7", "width": 1080, "height": 675}], "variants": {}, "id": "HuKDPbL5BZcs2C3Xx_NSiTAtghQKTwaxYmQxFqDGMEU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z1cb3l", "is_robot_indexable": true, "report_reasons": null, "author": "gritty_69", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z1cb3l/p_data_science_tournament_to_determine_spacecraft/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z1cb3l/p_data_science_tournament_to_determine_spacecraft/", "subreddit_subscribers": 820896, "created_utc": 1669069477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Tldr; How to convince stakeholders that your product is working as intended and the can trust the output of it?\n\nBackground: I have build this rather simple data pipeline which transforms the input data, applies some simple aggregations (not even the harmonic mean), some simple filters and produces plots of the data. Now, the stakeholders which are the sales team and my boss don't trust the output of the whole thing (anymore).\n\nMeasures I took to build trust:\n\n* included unit tests with test cases in order to verify aggregation and construction of metrics\n* separate aggregation and plotting into different steps of the pipeline\n* made a diagram with data flow and all definitions\n* rely mostly on well tested third party libraries (pandas and matplotlib)\n* Versioning and documenting the code and the input to have reproducible pipeline runs\n\nReasons why I think they don't trust it:\n* I am the only person developing it, so there is no review process and I could test my own logical errors with the unit tests\n* Communication between the sales team and me was suboptimal, i.e. the meaning of some metrics was not clear to them and I did not always get what the expected of some metrics were\n* The underling input data quality was poor and changing often but was out of my control. In the end it was blamed on the product even it was not it's fault\n* Development in production due to changing requirements, tight deadlines and feature request, which lead to minor bugs\n\nI am interested in your thoughts how I can improve the situation and how I can avoid similar problems in the future.", "author_fullname": "t2_c1bt3c4m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get stakeholders to trust your product", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z16efm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669055810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tldr; How to convince stakeholders that your product is working as intended and the can trust the output of it?&lt;/p&gt;\n\n&lt;p&gt;Background: I have build this rather simple data pipeline which transforms the input data, applies some simple aggregations (not even the harmonic mean), some simple filters and produces plots of the data. Now, the stakeholders which are the sales team and my boss don&amp;#39;t trust the output of the whole thing (anymore).&lt;/p&gt;\n\n&lt;p&gt;Measures I took to build trust:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;included unit tests with test cases in order to verify aggregation and construction of metrics&lt;/li&gt;\n&lt;li&gt;separate aggregation and plotting into different steps of the pipeline&lt;/li&gt;\n&lt;li&gt;made a diagram with data flow and all definitions&lt;/li&gt;\n&lt;li&gt;rely mostly on well tested third party libraries (pandas and matplotlib)&lt;/li&gt;\n&lt;li&gt;Versioning and documenting the code and the input to have reproducible pipeline runs&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Reasons why I think they don&amp;#39;t trust it:\n* I am the only person developing it, so there is no review process and I could test my own logical errors with the unit tests\n* Communication between the sales team and me was suboptimal, i.e. the meaning of some metrics was not clear to them and I did not always get what the expected of some metrics were\n* The underling input data quality was poor and changing often but was out of my control. In the end it was blamed on the product even it was not it&amp;#39;s fault\n* Development in production due to changing requirements, tight deadlines and feature request, which lead to minor bugs&lt;/p&gt;\n\n&lt;p&gt;I am interested in your thoughts how I can improve the situation and how I can avoid similar problems in the future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z16efm", "is_robot_indexable": true, "report_reasons": null, "author": "Life_Ad_6195", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z16efm/how_to_get_stakeholders_to_trust_your_product/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z16efm/how_to_get_stakeholders_to_trust_your_product/", "subreddit_subscribers": 820896, "created_utc": 1669055810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "[https://www.reddit.com/r/quantum\\_Mach\\_Learning?utm\\_medium=android\\_app&amp;utm\\_source=share](https://www.reddit.com/r/quantum_Mach_Learning?utm_medium=android_app&amp;utm_source=share)\n\n&amp;#x200B;\n\nHi all,\n\nSo this is an invite to a quantum machine learning community which as the name says is aimed for people who are interested in this promising field to ineract and network with one another....\n\nMay you be an undergrad or a grad student or a professional working in another field....you are welcome to join.\n\nHope you do join and make it a thriving and lively place...\n\nSee you all great people there....", "author_fullname": "t2_ucr73atc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "COME ONE COME ALL..... Invite to a quantum machine learing community...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_z127u8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.11, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Networking", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1669045704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/quantum_Mach_Learning?utm_medium=android_app&amp;amp;utm_source=share\"&gt;https://www.reddit.com/r/quantum_Mach_Learning?utm_medium=android_app&amp;amp;utm_source=share&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;So this is an invite to a quantum machine learning community which as the name says is aimed for people who are interested in this promising field to ineract and network with one another....&lt;/p&gt;\n\n&lt;p&gt;May you be an undergrad or a grad student or a professional working in another field....you are welcome to join.&lt;/p&gt;\n\n&lt;p&gt;Hope you do join and make it a thriving and lively place...&lt;/p&gt;\n\n&lt;p&gt;See you all great people there....&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "z127u8", "is_robot_indexable": true, "report_reasons": null, "author": "thejay147", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/z127u8/come_one_come_all_invite_to_a_quantum_machine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/z127u8/come_one_come_all_invite_to_a_quantum_machine/", "subreddit_subscribers": 820896, "created_utc": 1669045704.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}