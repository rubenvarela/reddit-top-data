{"kind": "Listing", "data": {"after": "t3_zghslh", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "r/dataengineering wrapped", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zgpcsx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": "#46d160", "ups": 122, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 122, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/wLlymWwl9eaAfSQZjuMwbaOW4JGgAzxz075QTC52Xo8.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670568048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/wjtxidai0v4a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/wjtxidai0v4a1.jpg?auto=webp&amp;s=2a561f30687e02c450631655922ba11edd826201", "width": 1080, "height": 1920}, "resolutions": [{"url": "https://preview.redd.it/wjtxidai0v4a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=770909f78c9872d2f502348b9a412ab739e263bf", "width": 108, "height": 192}, {"url": "https://preview.redd.it/wjtxidai0v4a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=497471eacf95be0d0c3c9523087648970cac3bb3", "width": 216, "height": 384}, {"url": "https://preview.redd.it/wjtxidai0v4a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f6d7faebc4cb0a3cda11ac0ce99aa067ac07d57", "width": 320, "height": 568}, {"url": "https://preview.redd.it/wjtxidai0v4a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=363afbeff41a35f07e6a6a688bfccb90805a5fed", "width": 640, "height": 1137}, {"url": "https://preview.redd.it/wjtxidai0v4a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=93c8858b820203780874918db4e315dca44b1098", "width": 960, "height": 1706}, {"url": "https://preview.redd.it/wjtxidai0v4a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=13ea126ab7a94f38585c114bdeb6f1a97dc8a76a", "width": 1080, "height": 1920}], "variants": {}, "id": "6NRogcjC4XiYOQhU1pjkrZuPyCrvBBP8GeLQ9t2y6MY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5d8a87e8-a952-11eb-9a8a-0e3979f03641", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Sr. Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "zgpcsx", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/zgpcsx/rdataengineering_wrapped/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/wjtxidai0v4a1.jpg", "subreddit_subscribers": 82374, "created_utc": 1670568048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/s8llls4you4a1.png?width=460&amp;format=png&amp;auto=webp&amp;s=0e7cccdde22e2da0be5d14dff0e0f7f02d664628\n\nI love the irony of this :D \n\n&amp;#x200B;\n\n(and probably also the meta-paradox of being a jerk by posting this thus violating the very rule I'm citing \ud83d\ude09 )", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dates are hard\u2014we can relate to that, can't we r/dataengineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"s8llls4you4a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 109, "x": 108, "u": "https://preview.redd.it/s8llls4you4a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3f756ff21ba91600cd4d8c7c74f1d322c80d4937"}, {"y": 219, "x": 216, "u": "https://preview.redd.it/s8llls4you4a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=36c3599e24a661f896517cb1bb61cb7490a02724"}, {"y": 324, "x": 320, "u": "https://preview.redd.it/s8llls4you4a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=da1061070f16f5a33564ac4cd46d6d066cff4802"}], "s": {"y": 467, "x": 460, "u": "https://preview.redd.it/s8llls4you4a1.png?width=460&amp;format=png&amp;auto=webp&amp;s=0e7cccdde22e2da0be5d14dff0e0f7f02d664628"}, "id": "s8llls4you4a1"}}, "name": "t3_zgtrnk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 56, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 56, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/lRoOsAy7xdttw5-id-v1nfKCGtkz5CUP62h43Pfxseo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670582282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s8llls4you4a1.png?width=460&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e7cccdde22e2da0be5d14dff0e0f7f02d664628\"&gt;https://preview.redd.it/s8llls4you4a1.png?width=460&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e7cccdde22e2da0be5d14dff0e0f7f02d664628&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I love the irony of this :D &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(and probably also the meta-paradox of being a jerk by posting this thus violating the very rule I&amp;#39;m citing \ud83d\ude09 )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "zgtrnk", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgtrnk/dates_are_hardwe_can_relate_to_that_cant_we/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgtrnk/dates_are_hardwe_can_relate_to_that_cant_we/", "subreddit_subscribers": 82374, "created_utc": 1670582282.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for advice on some airflow pipelines and if I\u2019m about to over engineer a problem.\n\nCurrent solution:\nLoading incremental files from s3 to append or upsert to a table, but the current dags work by finding the execution date either in the file name or the file loaded date\n\nAlternative:\nI\u2019m looking at something that compares all files in the bucket to what has been loaded or is existing in the table and only loading missing ones, so removing the dependency on looking for files matching the exec date.\n\nIt feels a little too fragile to me relying on using todays date to find files, but not sure how common a solution like that is with incremental data loading.", "author_fullname": "t2_8u9r0t6s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental data ingestion solutions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zgj16n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670549748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for advice on some airflow pipelines and if I\u2019m about to over engineer a problem.&lt;/p&gt;\n\n&lt;p&gt;Current solution:\nLoading incremental files from s3 to append or upsert to a table, but the current dags work by finding the execution date either in the file name or the file loaded date&lt;/p&gt;\n\n&lt;p&gt;Alternative:\nI\u2019m looking at something that compares all files in the bucket to what has been loaded or is existing in the table and only loading missing ones, so removing the dependency on looking for files matching the exec date.&lt;/p&gt;\n\n&lt;p&gt;It feels a little too fragile to me relying on using todays date to find files, but not sure how common a solution like that is with incremental data loading.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zgj16n", "is_robot_indexable": true, "report_reasons": null, "author": "rubemtogether", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgj16n/incremental_data_ingestion_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgj16n/incremental_data_ingestion_solutions/", "subreddit_subscribers": 82374, "created_utc": 1670549748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want a bit more flexibility in my career, currently I'm mid-senior depending on who you ask. Python, SQL, AWS, some ml.\n\n I am based in Europe.\n\n Is it any different to work as a data engineer on contract? I mean, is it hard to find work or are the expectations very different?", "author_fullname": "t2_t6g1hmt1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is the crontactor life?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zgc27z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670534473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want a bit more flexibility in my career, currently I&amp;#39;m mid-senior depending on who you ask. Python, SQL, AWS, some ml.&lt;/p&gt;\n\n&lt;p&gt;I am based in Europe.&lt;/p&gt;\n\n&lt;p&gt;Is it any different to work as a data engineer on contract? I mean, is it hard to find work or are the expectations very different?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zgc27z", "is_robot_indexable": true, "report_reasons": null, "author": "Low_Tourist_4053", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgc27z/how_is_the_crontactor_life/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgc27z/how_is_the_crontactor_life/", "subreddit_subscribers": 82374, "created_utc": 1670534473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So far I've created a Python script to merge the critical data from each excel into a JSON file so every file it's an object, it takes approximately half a second per excel file so I'm going to parse 7 thousand at a time so I doesn't take the whole day. \nThe thing is that the data isn't clean, so I don't think JSON it's the best file type for this scenario. \nHave you done something similar? Any advice?\n\nEdit: Sorry, Forgot to mention, although the data is stored in excels it is not in tabular. \nEach file it's basically a QA test register, consist of a unique ID, multiple parameters containing part number, date, qty and stuff like that, then it contains the amount of features measured and each feature measure can have multiple samples", "author_fullname": "t2_5ho9t8h4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have over 70,000 excel files that I need to analyze.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zh0asy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670604366.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670599985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So far I&amp;#39;ve created a Python script to merge the critical data from each excel into a JSON file so every file it&amp;#39;s an object, it takes approximately half a second per excel file so I&amp;#39;m going to parse 7 thousand at a time so I doesn&amp;#39;t take the whole day. \nThe thing is that the data isn&amp;#39;t clean, so I don&amp;#39;t think JSON it&amp;#39;s the best file type for this scenario. \nHave you done something similar? Any advice?&lt;/p&gt;\n\n&lt;p&gt;Edit: Sorry, Forgot to mention, although the data is stored in excels it is not in tabular. \nEach file it&amp;#39;s basically a QA test register, consist of a unique ID, multiple parameters containing part number, date, qty and stuff like that, then it contains the amount of features measured and each feature measure can have multiple samples&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zh0asy", "is_robot_indexable": true, "report_reasons": null, "author": "Slayne_S", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh0asy/i_have_over_70000_excel_files_that_i_need_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh0asy/i_have_over_70000_excel_files_that_i_need_to/", "subreddit_subscribers": 82374, "created_utc": 1670599985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE's,\n\nI'm pretty new at this... Trying to build a data engineering pipeline stack on my local machine and it all feels very overwhelming at the moment. \n\nI've put together the following of what I'm trying to build:\n\nhttps://preview.redd.it/fks6cfwb2s4a1.png?width=829&amp;format=png&amp;auto=webp&amp;s=5a1452c2e7ed863b617bb98d4d63703bd634aa2c\n\nI'm clear on what to build. I'm having difficulties putting this all together. My question (and I know this is very general) is how would go about building this? \n\nI could create a new pipenv / conda environment, pip install Airflow, Airbyte, dbt, Metabase and also install Postgres, or I guess I could do it inside a Docker container, right?  \n\n\nI've Googled and have come across some great posts and Github repos where something similar has been done, but it would be good if we could have a chat about it here. \n\nNot looking at using cloud alternatives, and would prefer to stick to open source as a way to learn.", "author_fullname": "t2_o72dwjd5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help on building a data engineering pipeline stack locally (Airflow, Airbyte, dbt, Metabase)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 65, "top_awarded_type": null, "hide_score": false, "media_metadata": {"fks6cfwb2s4a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 50, "x": 108, "u": "https://preview.redd.it/fks6cfwb2s4a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f93722266250c4dc3b53627ea83e648d7afedef6"}, {"y": 101, "x": 216, "u": "https://preview.redd.it/fks6cfwb2s4a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=99828ad6971342aa7676d81f217bc5698448022c"}, {"y": 150, "x": 320, "u": "https://preview.redd.it/fks6cfwb2s4a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9cfb80ca34a9b82eedd4763263b5d85b50bbf67a"}, {"y": 300, "x": 640, "u": "https://preview.redd.it/fks6cfwb2s4a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=71fd2646ffdef6c6def6192d3990c770d204c0ad"}], "s": {"y": 389, "x": 829, "u": "https://preview.redd.it/fks6cfwb2s4a1.png?width=829&amp;format=png&amp;auto=webp&amp;s=5a1452c2e7ed863b617bb98d4d63703bd634aa2c"}, "id": "fks6cfwb2s4a1"}}, "name": "t3_zgjje4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WQwsKUmo9CXJRa5jCtdj5d0tz8tjWlqpyKVXBq9bD1M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670551034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE&amp;#39;s,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m pretty new at this... Trying to build a data engineering pipeline stack on my local machine and it all feels very overwhelming at the moment. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve put together the following of what I&amp;#39;m trying to build:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fks6cfwb2s4a1.png?width=829&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a1452c2e7ed863b617bb98d4d63703bd634aa2c\"&gt;https://preview.redd.it/fks6cfwb2s4a1.png?width=829&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a1452c2e7ed863b617bb98d4d63703bd634aa2c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m clear on what to build. I&amp;#39;m having difficulties putting this all together. My question (and I know this is very general) is how would go about building this? &lt;/p&gt;\n\n&lt;p&gt;I could create a new pipenv / conda environment, pip install Airflow, Airbyte, dbt, Metabase and also install Postgres, or I guess I could do it inside a Docker container, right?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve Googled and have come across some great posts and Github repos where something similar has been done, but it would be good if we could have a chat about it here. &lt;/p&gt;\n\n&lt;p&gt;Not looking at using cloud alternatives, and would prefer to stick to open source as a way to learn.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zgjje4", "is_robot_indexable": true, "report_reasons": null, "author": "Active-Proposal-932", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgjje4/help_on_building_a_data_engineering_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgjje4/help_on_building_a_data_engineering_pipeline/", "subreddit_subscribers": 82374, "created_utc": 1670551034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What advice do you have for aspiring data engineers who are just starting out in the field?", "author_fullname": "t2_3p620rl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice For Aspiring DE's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zgpues", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670569674.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What advice do you have for aspiring data engineers who are just starting out in the field?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "DE @ Amazon/Lyft/Author of Ace DE Interview", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zgpues", "is_robot_indexable": true, "report_reasons": null, "author": "chrisgarzon19", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/zgpues/advice_for_aspiring_des/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgpues/advice_for_aspiring_des/", "subreddit_subscribers": 82374, "created_utc": 1670569674.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w2y7e99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacing Looker with Cube and Metabase", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_zgvvsz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/o3_3qur-Hl0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Replacing Looker with Cube and Metabase\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Replacing Looker with Cube and Metabase", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/o3_3qur-Hl0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Replacing Looker with Cube and Metabase\"&gt;&lt;/iframe&gt;", "author_name": "Cube", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/o3_3qur-Hl0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@cube8910"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/o3_3qur-Hl0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Replacing Looker with Cube and Metabase\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/zgvvsz", "height": 200}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YmpGAtrzB9E7h1Vk3tkK6lZ1zlaRbu2MYLbvtljxYPI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670588424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=o3_3qur-Hl0", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hWtyNNl6WciiPeYyQJqCtdWwoHLpx_jAV7nAVzn8GDA.jpg?auto=webp&amp;s=d5aac53f42371f81fbeef23c370b1cbb18895cd9", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/hWtyNNl6WciiPeYyQJqCtdWwoHLpx_jAV7nAVzn8GDA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ccc476f4ed901ef702f24ac7a7aafbaec7313f22", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/hWtyNNl6WciiPeYyQJqCtdWwoHLpx_jAV7nAVzn8GDA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bd4958722e896d207507f1105a5ee64f59292e6c", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/hWtyNNl6WciiPeYyQJqCtdWwoHLpx_jAV7nAVzn8GDA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2db1858a5ef4affb38c8c04bea4d9fdceab50014", "width": 320, "height": 240}], "variants": {}, "id": "CJFTP2Zny9qZBWmH_kAC0Z7hc80p1yvrqY10mRzaE4Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "zgvvsz", "is_robot_indexable": true, "report_reasons": null, "author": "whichalps", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgvvsz/replacing_looker_with_cube_and_metabase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=o3_3qur-Hl0", "subreddit_subscribers": 82374, "created_utc": 1670588424.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Replacing Looker with Cube and Metabase", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/o3_3qur-Hl0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Replacing Looker with Cube and Metabase\"&gt;&lt;/iframe&gt;", "author_name": "Cube", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/o3_3qur-Hl0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@cube8910"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Medium Post: [https://medium.com/@stefentaime\\_10958/end-to-end-data-engineering-project-with-spark-mongodb-minio-postgres-and-metabase-2c400672b50d](https://medium.com/@stefentaime_10958/end-to-end-data-engineering-project-with-spark-mongodb-minio-postgres-and-metabase-2c400672b50d)\n\n[ End to end data engineering project with Spark, Mongodb, Minio, postgres and Metabase](https://preview.redd.it/fc7jjjvxcr4a1.png?width=1354&amp;format=png&amp;auto=webp&amp;s=d38cb2c05ee94cbdadc358fa3062ea83d6ec354f)", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "End to end data engineering project with Spark, Mongodb, Minio, postgres and Metabase", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "media_metadata": {"fc7jjjvxcr4a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/fc7jjjvxcr4a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7eda1b162716f2e2cc7b52a47b060e2b890f0b85"}, {"y": 111, "x": 216, "u": "https://preview.redd.it/fc7jjjvxcr4a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=44d2e9b9d2d3ce13f972e985eb0b7e34cf18420c"}, {"y": 165, "x": 320, "u": "https://preview.redd.it/fc7jjjvxcr4a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1770f25b781bab5fc4e762bbe254c97fa45662e7"}, {"y": 331, "x": 640, "u": "https://preview.redd.it/fc7jjjvxcr4a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b553d0bfa65c7d4a80998d105c3ca5284213db26"}, {"y": 497, "x": 960, "u": "https://preview.redd.it/fc7jjjvxcr4a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a058a1541ef421928d622419156f0b2043e32e3"}, {"y": 559, "x": 1080, "u": "https://preview.redd.it/fc7jjjvxcr4a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=240db4c3240a0d648900445c8721b13a64c61800"}], "s": {"y": 702, "x": 1354, "u": "https://preview.redd.it/fc7jjjvxcr4a1.png?width=1354&amp;format=png&amp;auto=webp&amp;s=d38cb2c05ee94cbdadc358fa3062ea83d6ec354f"}, "id": "fc7jjjvxcr4a1"}}, "name": "t3_zgflf1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b4WAIteOoZ2VPNfa70i029ymHcgE34jBjmEAA1NaxwY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1670541869.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Medium Post: &lt;a href=\"https://medium.com/@stefentaime_10958/end-to-end-data-engineering-project-with-spark-mongodb-minio-postgres-and-metabase-2c400672b50d\"&gt;https://medium.com/@stefentaime_10958/end-to-end-data-engineering-project-with-spark-mongodb-minio-postgres-and-metabase-2c400672b50d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fc7jjjvxcr4a1.png?width=1354&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d38cb2c05ee94cbdadc358fa3062ea83d6ec354f\"&gt; End to end data engineering project with Spark, Mongodb, Minio, postgres and Metabase&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8-nAVru3wmiKduC_X1oqHyYrSgGaK1AN5BJbo0JartM.jpg?auto=webp&amp;s=559de009a158bee9df75354f6281d8d4992c63d1", "width": 1200, "height": 622}, "resolutions": [{"url": "https://external-preview.redd.it/8-nAVru3wmiKduC_X1oqHyYrSgGaK1AN5BJbo0JartM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5150c4d2eca6a483cc84906137e180d652040fd3", "width": 108, "height": 55}, {"url": "https://external-preview.redd.it/8-nAVru3wmiKduC_X1oqHyYrSgGaK1AN5BJbo0JartM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a85206de435398467c049a178c18f27718f7fd7", "width": 216, "height": 111}, {"url": "https://external-preview.redd.it/8-nAVru3wmiKduC_X1oqHyYrSgGaK1AN5BJbo0JartM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d622b7cf8c88b94548b66665464dfa73da805d7d", "width": 320, "height": 165}, {"url": "https://external-preview.redd.it/8-nAVru3wmiKduC_X1oqHyYrSgGaK1AN5BJbo0JartM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=18fbf184aa6058c99374f33ab06a685c0b765f67", "width": 640, "height": 331}, {"url": "https://external-preview.redd.it/8-nAVru3wmiKduC_X1oqHyYrSgGaK1AN5BJbo0JartM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=845d40eae146b7bb6575c6b04c7445e7e7469345", "width": 960, "height": 497}, {"url": "https://external-preview.redd.it/8-nAVru3wmiKduC_X1oqHyYrSgGaK1AN5BJbo0JartM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1547ca5ca3559102274eeec839f31aa0bd05d16", "width": 1080, "height": 559}], "variants": {}, "id": "JG5ckGJZs2WQMJZbAAuE8wExW-DvFGE9Q20-FRNZuX0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zgflf1", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgflf1/end_to_end_data_engineering_project_with_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgflf1/end_to_end_data_engineering_project_with_spark/", "subreddit_subscribers": 82374, "created_utc": 1670541869.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This one seems confusing to me as if you don't run create if not exists every time then technically your batch job is not idempotent. On the other hand, it seems inelegant to have complicated table definitions etc in your batch job.\n\nFor context I'm working with pyspark jobs on databricks.", "author_fullname": "t2_18xb5x05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you have create table jobs/pipelines seperate from regularly run update pipelines? Or do you just run create if not exists every time you run your batch jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zh0sb5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670601212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This one seems confusing to me as if you don&amp;#39;t run create if not exists every time then technically your batch job is not idempotent. On the other hand, it seems inelegant to have complicated table definitions etc in your batch job.&lt;/p&gt;\n\n&lt;p&gt;For context I&amp;#39;m working with pyspark jobs on databricks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zh0sb5", "is_robot_indexable": true, "report_reasons": null, "author": "the-data-scientist", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh0sb5/do_you_have_create_table_jobspipelines_seperate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh0sb5/do_you_have_create_table_jobspipelines_seperate/", "subreddit_subscribers": 82374, "created_utc": 1670601212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know if its possible to use Synapse Spark Pool with DBT? If so can you share your experience with that set up? I see documentation for using Synapse dedicated and seeverless SQL pool with DBT but not the Spark Pool. All the DBT Spark documentation is for databricks and AWS EMR. Any insight would help", "author_fullname": "t2_l15do6pb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT + Spark (synapse)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zh0whm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670601488.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know if its possible to use Synapse Spark Pool with DBT? If so can you share your experience with that set up? I see documentation for using Synapse dedicated and seeverless SQL pool with DBT but not the Spark Pool. All the DBT Spark documentation is for databricks and AWS EMR. Any insight would help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zh0whm", "is_robot_indexable": true, "report_reasons": null, "author": "Fearless_Strategy_19", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh0whm/dbt_spark_synapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh0whm/dbt_spark_synapse/", "subreddit_subscribers": 82374, "created_utc": 1670601488.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I am in somewhat a tough situation. I have been offered a job offer and contract with all benefits and salary mentioned, but it's contingent on certain criteria, all which I'm fine with except for a reference from my current manager. I don't mind giving reference of my manager, but I feel like there's a risk that if I ask reference from my manager, and give notice thereafter, and if the job offer is rescinded from the new place for some reason, I will be left in a difficult situation where my manager will not trust me thereafter.\n\nHas anyone else experienced such a dilemma? HR from new place says that I can give my notice safely and just ask HR/Manager from old place for reference since all other checks (background, drugs etc.) have been done.", "author_fullname": "t2_l35gwhuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Offer in Writing, but Contingent on Reference From Current Manager (UK)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zh0i52", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670600485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I am in somewhat a tough situation. I have been offered a job offer and contract with all benefits and salary mentioned, but it&amp;#39;s contingent on certain criteria, all which I&amp;#39;m fine with except for a reference from my current manager. I don&amp;#39;t mind giving reference of my manager, but I feel like there&amp;#39;s a risk that if I ask reference from my manager, and give notice thereafter, and if the job offer is rescinded from the new place for some reason, I will be left in a difficult situation where my manager will not trust me thereafter.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else experienced such a dilemma? HR from new place says that I can give my notice safely and just ask HR/Manager from old place for reference since all other checks (background, drugs etc.) have been done.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "zh0i52", "is_robot_indexable": true, "report_reasons": null, "author": "Senior_Anteater4688", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh0i52/job_offer_in_writing_but_contingent_on_reference/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh0i52/job_offer_in_writing_but_contingent_on_reference/", "subreddit_subscribers": 82374, "created_utc": 1670600485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am coming across this term called Shard when I was reading some Docs on Pub/Sub in GCP and wanted to know what exactly is this? I have read some articles which I found on google. But can some one explain this to me in easy terms.   \nThanks in Advance.", "author_fullname": "t2_76h7jr47", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a Shard?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zh4uqs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670610718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am coming across this term called Shard when I was reading some Docs on Pub/Sub in GCP and wanted to know what exactly is this? I have read some articles which I found on google. But can some one explain this to me in easy terms.&lt;br/&gt;\nThanks in Advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zh4uqs", "is_robot_indexable": true, "report_reasons": null, "author": "s1va1209", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh4uqs/what_is_a_shard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh4uqs/what_is_a_shard/", "subreddit_subscribers": 82374, "created_utc": 1670610718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "even more so for cdc type of data/streaming data without affecting its real time latency\n\nwhere/how does it fit in your data architecture", "author_fullname": "t2_epjy5nh4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how do you do anonymisation in your organisation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zh4gho", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670610021.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670609819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;even more so for cdc type of data/streaming data without affecting its real time latency&lt;/p&gt;\n\n&lt;p&gt;where/how does it fit in your data architecture&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zh4gho", "is_robot_indexable": true, "report_reasons": null, "author": "Spare-Youth-6874", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh4gho/how_do_you_do_anonymisation_in_your_organisation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh4gho/how_do_you_do_anonymisation_in_your_organisation/", "subreddit_subscribers": 82374, "created_utc": 1670609819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The new way of creating Sensors in Apache Airflow! \ud83e\udef6", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_zh096u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/QYZEqGGgvyRtgAsk1QABGdNW7bECXjuSXvyf1O8eBR0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670599872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/posts/marclamberti_airflow-apacheairflow-dataengineering-activity-7006984438110670848-d-p7?utm_source=share&amp;utm_medium=member_desktop", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oJknx6cTFRscfGdJIsTMbfqf06Fv5fmQhXj_yp80qOo.jpg?auto=webp&amp;s=5b233c2c1455e8ff462eb90e113da7221296f40f", "width": 1042, "height": 544}, "resolutions": [{"url": "https://external-preview.redd.it/oJknx6cTFRscfGdJIsTMbfqf06Fv5fmQhXj_yp80qOo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a5f9d38fd913415726afe77428ba64ab0708533", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oJknx6cTFRscfGdJIsTMbfqf06Fv5fmQhXj_yp80qOo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=34271b22c2e3dd27275f68a40bcf46b05a5b2b15", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/oJknx6cTFRscfGdJIsTMbfqf06Fv5fmQhXj_yp80qOo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dd385f8fe0654c50c40bf397b220dcb4485c712b", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/oJknx6cTFRscfGdJIsTMbfqf06Fv5fmQhXj_yp80qOo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3abe88e0d983064e21d9abbe70d8a59f1dfbe316", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/oJknx6cTFRscfGdJIsTMbfqf06Fv5fmQhXj_yp80qOo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cdcad97c501f073d584fbf374b6b7220fc593173", "width": 960, "height": 501}], "variants": {}, "id": "INVLTnqVQs_P7yaCQEoPAvOsj4Ol3PZKsrTyuWMcxSk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zh096u", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh096u/the_new_way_of_creating_sensors_in_apache_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/posts/marclamberti_airflow-apacheairflow-dataengineering-activity-7006984438110670848-d-p7?utm_source=share&amp;utm_medium=member_desktop", "subreddit_subscribers": 82374, "created_utc": 1670599872.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the general consensus on where a production **database** for **Airflow** should be?\n\nI usually start an Airflow Docker container. Airflow rightfully suggests to not use SQLite, but instead switch to Postgres or Mysql. \n\nNow, from an architecture and resilience point of view:\n\n1. Should I install the RDBMS (I choose Postgres) in the same Docker container as Airflow?\n2. or should I put the database in an extra container?", "author_fullname": "t2_136crg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow DB: In the same container or a separate one?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zgy0w1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670594275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the general consensus on where a production &lt;strong&gt;database&lt;/strong&gt; for &lt;strong&gt;Airflow&lt;/strong&gt; should be?&lt;/p&gt;\n\n&lt;p&gt;I usually start an Airflow Docker container. Airflow rightfully suggests to not use SQLite, but instead switch to Postgres or Mysql. &lt;/p&gt;\n\n&lt;p&gt;Now, from an architecture and resilience point of view:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Should I install the RDBMS (I choose Postgres) in the same Docker container as Airflow?&lt;/li&gt;\n&lt;li&gt;or should I put the database in an extra container?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zgy0w1", "is_robot_indexable": true, "report_reasons": null, "author": "Boruroku", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgy0w1/airflow_db_in_the_same_container_or_a_separate_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgy0w1/airflow_db_in_the_same_container_or_a_separate_one/", "subreddit_subscribers": 82374, "created_utc": 1670594275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just started my first DE job and am the only DE here. I'm trying to work my way through DE courses to architect something up which I'm currently building as a demo. I'm working with a pretty much non-existent senior DE who doesn't have much technical or programming knowledge but rather is a manager. \n\nI'm here for 3 months now and I'm quite unhappy as I thought I'd have better mentorship and leadership however, it really is a one man band here. I'm unsure as to whether to find a new job or not now. What should I do?", "author_fullname": "t2_tgwctzuo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm a junior/grad, is it normal for me to be the ONLY data engineer in a company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zgvc4r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670586909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just started my first DE job and am the only DE here. I&amp;#39;m trying to work my way through DE courses to architect something up which I&amp;#39;m currently building as a demo. I&amp;#39;m working with a pretty much non-existent senior DE who doesn&amp;#39;t have much technical or programming knowledge but rather is a manager. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m here for 3 months now and I&amp;#39;m quite unhappy as I thought I&amp;#39;d have better mentorship and leadership however, it really is a one man band here. I&amp;#39;m unsure as to whether to find a new job or not now. What should I do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zgvc4r", "is_robot_indexable": true, "report_reasons": null, "author": "Icy_Requirement_3635", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgvc4r/im_a_juniorgrad_is_it_normal_for_me_to_be_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgvc4r/im_a_juniorgrad_is_it_normal_for_me_to_be_the/", "subreddit_subscribers": 82374, "created_utc": 1670586909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_22rg4hgk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Ingestion In Plain Java", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 56, "top_awarded_type": null, "hide_score": false, "name": "t3_zgeqc3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TPdZuEWZiBIH1lY840UcfpSbylVle1ymNuFBBre0_WI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670540068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "asyncq.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://asyncq.com/data-ingestion-in-plain-java", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XDy3rxwH6UMJebJpulxW0kU_9yk-nhL5j0YFhOZoqe0.jpg?auto=webp&amp;s=ba584c84c12ce627813a4b2dac0d84792062502f", "width": 1600, "height": 647}, "resolutions": [{"url": "https://external-preview.redd.it/XDy3rxwH6UMJebJpulxW0kU_9yk-nhL5j0YFhOZoqe0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=953e5e6ca67f2ea7437a56975bfb82b59716a475", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/XDy3rxwH6UMJebJpulxW0kU_9yk-nhL5j0YFhOZoqe0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7bee635ac0aaea11ed542d76b263d300db2fd510", "width": 216, "height": 87}, {"url": "https://external-preview.redd.it/XDy3rxwH6UMJebJpulxW0kU_9yk-nhL5j0YFhOZoqe0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7f6abfe8b63cf6b336cb15828feb442ba1677120", "width": 320, "height": 129}, {"url": "https://external-preview.redd.it/XDy3rxwH6UMJebJpulxW0kU_9yk-nhL5j0YFhOZoqe0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc012869911cc18a32bfcd8ae57d6de63599ce92", "width": 640, "height": 258}, {"url": "https://external-preview.redd.it/XDy3rxwH6UMJebJpulxW0kU_9yk-nhL5j0YFhOZoqe0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=523e7bb7bffabca25005283c8cbcf41d1741c6de", "width": 960, "height": 388}, {"url": "https://external-preview.redd.it/XDy3rxwH6UMJebJpulxW0kU_9yk-nhL5j0YFhOZoqe0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1d62cc095b4f5ead421a04f41d26ded786152697", "width": 1080, "height": 436}], "variants": {}, "id": "PYyTZxyfeYEbnBAatFEToVFPlayu-p8AaYeE5x2Fw_k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zgeqc3", "is_robot_indexable": true, "report_reasons": null, "author": "suraj-mishra15", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgeqc3/data_ingestion_in_plain_java/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://asyncq.com/data-ingestion-in-plain-java", "subreddit_subscribers": 82374, "created_utc": 1670540068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What if I tell you that there is a no-code portal where you sign up, turn a few knobs and you can start dropping excel and csvs and they are queryable or your existing dashboard is already populated?  \nNothing fancy, drag and drop csvs, populate data and refresh reports. Would your small to medium business buy this?", "author_fullname": "t2_ya36aq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A no/low code data warehouse SaaS - would your org use it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zh8n3m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670619744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What if I tell you that there is a no-code portal where you sign up, turn a few knobs and you can start dropping excel and csvs and they are queryable or your existing dashboard is already populated?&lt;br/&gt;\nNothing fancy, drag and drop csvs, populate data and refresh reports. Would your small to medium business buy this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zh8n3m", "is_robot_indexable": true, "report_reasons": null, "author": "droaak", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh8n3m/a_nolow_code_data_warehouse_saas_would_your_org/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh8n3m/a_nolow_code_data_warehouse_saas_would_your_org/", "subreddit_subscribers": 82374, "created_utc": 1670619744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ever wonder what BI tools other teams are using? We've gathered data from over 250 data teams and outlined the most popular BI tools used by Secoda customers in 2022: https://www.secoda.co/blog/top-5-business-intelligence-tools", "author_fullname": "t2_aiinah9q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Top 5 business intelligence tools (based on over 250 data teams)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zh768a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670616241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever wonder what BI tools other teams are using? We&amp;#39;ve gathered data from over 250 data teams and outlined the most popular BI tools used by Secoda customers in 2022: &lt;a href=\"https://www.secoda.co/blog/top-5-business-intelligence-tools\"&gt;https://www.secoda.co/blog/top-5-business-intelligence-tools&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ObfKZ_8zvF5Srb8sORcNGuFAE3Pm1xV-29YAl3ZpLU0.jpg?auto=webp&amp;s=4a7be7b670f7bcda1ada014d20855437305db957", "width": 767, "height": 448}, "resolutions": [{"url": "https://external-preview.redd.it/ObfKZ_8zvF5Srb8sORcNGuFAE3Pm1xV-29YAl3ZpLU0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d26e8292fce31a830851ce27b2ec0ce70efc60c", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/ObfKZ_8zvF5Srb8sORcNGuFAE3Pm1xV-29YAl3ZpLU0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c026311ccad9c2794d7fedd61ef5fe6f7070e499", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/ObfKZ_8zvF5Srb8sORcNGuFAE3Pm1xV-29YAl3ZpLU0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4cbbf6b73c3a26e61840a55fda9ed4a4eb09eda3", "width": 320, "height": 186}, {"url": "https://external-preview.redd.it/ObfKZ_8zvF5Srb8sORcNGuFAE3Pm1xV-29YAl3ZpLU0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=21032ca66176918321a8b632697179c12a1b9849", "width": 640, "height": 373}], "variants": {}, "id": "3xy2BqEaM_UDTGtVbw-A3eDKj-kygQF7HPVI6AuGkz4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zh768a", "is_robot_indexable": true, "report_reasons": null, "author": "secodaHQ", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh768a/top_5_business_intelligence_tools_based_on_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh768a/top_5_business_intelligence_tools_based_on_over/", "subreddit_subscribers": 82374, "created_utc": 1670616241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to understand what\u2019s the best way to troubleshoot and debug data inconsistency in prod or downstream applications. \n\nDefinitely live debugging and tinkering around prod data is not an option, although I\u2019m guilty of having done that on few occasions. \n\nWhat do you folks do? What does the process look like? Reproducing is not exactly possible bcz the state of data keeps changing and you never know which data caused the error. \n\nOne rogue way of dealing with it is blindly rerunning that pipeline hoping and praying it writes clean data this time. \n\nHow do you troubleshoot data issues in prod?", "author_fullname": "t2_8r6amwln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debugging data errors or inconsistencies in downstream dashboards", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zgxm83", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670593177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to understand what\u2019s the best way to troubleshoot and debug data inconsistency in prod or downstream applications. &lt;/p&gt;\n\n&lt;p&gt;Definitely live debugging and tinkering around prod data is not an option, although I\u2019m guilty of having done that on few occasions. &lt;/p&gt;\n\n&lt;p&gt;What do you folks do? What does the process look like? Reproducing is not exactly possible bcz the state of data keeps changing and you never know which data caused the error. &lt;/p&gt;\n\n&lt;p&gt;One rogue way of dealing with it is blindly rerunning that pipeline hoping and praying it writes clean data this time. &lt;/p&gt;\n\n&lt;p&gt;How do you troubleshoot data issues in prod?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zgxm83", "is_robot_indexable": true, "report_reasons": null, "author": "money_noob_007", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgxm83/debugging_data_errors_or_inconsistencies_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgxm83/debugging_data_errors_or_inconsistencies_in/", "subreddit_subscribers": 82374, "created_utc": 1670593177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tutorial: Multi-Table Transactions on the Lakehouse \u2013 Enabled by Dremio Arctic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_zgv89y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/q5-aSKAzjU-CqI2KRGxhX1sFkncEcNbrqUmQ7DhIgR8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670586601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dremio.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dremio.com/blog/multi-table-transactions-on-the-lakehouse-enabled-by-dremio-arctic/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?auto=webp&amp;s=677cfd2acf5f5314a32e08944e0951a1f33653f6", "width": 1201, "height": 629}, "resolutions": [{"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=44822d4e1e6d4168f34ab6113de12db8eb8237fa", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8ca6804f857e3ac1eeb4cdfdefda2979d98a202c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac4453013c6e922f576686e26eaf4f02abc504ae", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fba584a4d0519c1a5d41c1cf6ab8d567abea9fac", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2b91903c24fc515615dda409ebf8c60adcbf5c4", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28a6061daf8a4c19c2cbe3e8970f1b8bdc882434", "width": 1080, "height": 565}], "variants": {}, "id": "LBpbW9OLTwQyp-VamoofyQF1Ff8aV6ZdpWiaa-KyRig"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zgv89y", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgv89y/tutorial_multitable_transactions_on_the_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dremio.com/blog/multi-table-transactions-on-the-lakehouse-enabled-by-dremio-arctic/", "subreddit_subscribers": 82374, "created_utc": 1670586601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long Story Short: Company sent laptop early before joining - Later, I clearly mentioned I am not joining and asked them how to return it. Both on call and mail, but no response.\n\nIt's been almost 2 years, can I start using the laptop? Is there anyway they track it? \n\nPS: It's Windows laptop       \n\n[View Poll](https://www.reddit.com/poll/zgv4a8)", "author_fullname": "t2_8h8kwca0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I Start using Startup Laptop? It's been 2 years and they did not collet it!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zgv4a8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670586289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long Story Short: Company sent laptop early before joining - Later, I clearly mentioned I am not joining and asked them how to return it. Both on call and mail, but no response.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been almost 2 years, can I start using the laptop? Is there anyway they track it? &lt;/p&gt;\n\n&lt;p&gt;PS: It&amp;#39;s Windows laptop       &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/zgv4a8\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zgv4a8", "is_robot_indexable": true, "report_reasons": null, "author": "Rude-Veterinarian-45", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1670845489290, "options": [{"text": "Start Using, its been 2 years", "id": "20279087"}, {"text": "Don't Use it, they can track", "id": "20279088"}, {"text": "Other, please comment", "id": "20279089"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 78, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgv4a8/can_i_start_using_startup_laptop_its_been_2_years/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/zgv4a8/can_i_start_using_startup_laptop_its_been_2_years/", "subreddit_subscribers": 82374, "created_utc": 1670586289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have mostly been a full-stack developer with experience in cloud technology as well. This is my first big work involving data pipeline.\n\nProblem - currently data scientists run ad-hoc sqoop jobs to query data periodically. We need to schedule the same using airflow. We need to store the query results in s3 bucket as it is used for ML mod training.\n\nConstraint- we have to run these airflow jobs to trigger queries with kubeOperator, which I don\u2019t think can handle millions of records that are generated everyday. Much worse if it has to be done monthly. \n\nHow should we go about it? Use the airflow job to remotely run sqoop job? Or use Kafka connect ( have no clue about it) or something else?", "author_fullname": "t2_yn202", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving from ad-hoc sqoop job to Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zgtata", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670580827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have mostly been a full-stack developer with experience in cloud technology as well. This is my first big work involving data pipeline.&lt;/p&gt;\n\n&lt;p&gt;Problem - currently data scientists run ad-hoc sqoop jobs to query data periodically. We need to schedule the same using airflow. We need to store the query results in s3 bucket as it is used for ML mod training.&lt;/p&gt;\n\n&lt;p&gt;Constraint- we have to run these airflow jobs to trigger queries with kubeOperator, which I don\u2019t think can handle millions of records that are generated everyday. Much worse if it has to be done monthly. &lt;/p&gt;\n\n&lt;p&gt;How should we go about it? Use the airflow job to remotely run sqoop job? Or use Kafka connect ( have no clue about it) or something else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zgtata", "is_robot_indexable": true, "report_reasons": null, "author": "litti_wala", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgtata/moving_from_adhoc_sqoop_job_to_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgtata/moving_from_adhoc_sqoop_job_to_airflow/", "subreddit_subscribers": 82374, "created_utc": 1670580827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was wondering if anyone has found tools like GitHub Copilot/Replit Ghostwriter/ChatGPT/etc useful for increasing DE productivity. It seems like it should be here Real Soon Now but I'm not sure if it can help much with things like Airflow DAGs, SQL queries, BI dashboards (let alone data modeling).", "author_fullname": "t2_ycsml", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Usage of Tools Like Copilot?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zghslh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670546785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if anyone has found tools like GitHub Copilot/Replit Ghostwriter/ChatGPT/etc useful for increasing DE productivity. It seems like it should be here Real Soon Now but I&amp;#39;m not sure if it can help much with things like Airflow DAGs, SQL queries, BI dashboards (let alone data modeling).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zghslh", "is_robot_indexable": true, "report_reasons": null, "author": "kevinpostlewaite", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zghslh/de_usage_of_tools_like_copilot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zghslh/de_usage_of_tools_like_copilot/", "subreddit_subscribers": 82374, "created_utc": 1670546785.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}