{"kind": "Listing", "data": {"after": "t3_zavlak", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "HELP!! I lost 4Pib of data after storing it on floppy disks, i had about 390,937,467 (+-1) disks and after 1 got corrupted i lost all my data. What can I do? I didn't try anything at all, I'm using windows 10 debloated that barely works. \n\nThanks for doing the work for me!\n\nEdit 1: someone said i should try using a neodinium magnet on the floppy disk, my metal chair flew across the room but the floppy disk is still dead\n\nEdit 2: someone told me to check how the drive sounds when it reads the disk, sounds like vrrrrrrrrrr, skrrrrrrr, krrrrrr then vrrrr again\n\nEdit 3: someone told me to use cloud to backup my data, i don't know how to reach that cloud whilst carrying all my disks\n\nEdit 4: someone said i should use DSDD, i don't like DS games, no thanks!\n\nEdit 5: got told that diskettes sometimes get dirty and i should try licking it, tastes like TV static, 1/10 never trying again.", "author_fullname": "t2_iuagqudg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "R/DataHoarder shitpost be like", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zazo3z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 778, "total_awards_received": 3, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 778, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670030253.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670019444.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;HELP!! I lost 4Pib of data after storing it on floppy disks, i had about 390,937,467 (+-1) disks and after 1 got corrupted i lost all my data. What can I do? I didn&amp;#39;t try anything at all, I&amp;#39;m using windows 10 debloated that barely works. &lt;/p&gt;\n\n&lt;p&gt;Thanks for doing the work for me!&lt;/p&gt;\n\n&lt;p&gt;Edit 1: someone said i should try using a neodinium magnet on the floppy disk, my metal chair flew across the room but the floppy disk is still dead&lt;/p&gt;\n\n&lt;p&gt;Edit 2: someone told me to check how the drive sounds when it reads the disk, sounds like vrrrrrrrrrr, skrrrrrrr, krrrrrr then vrrrr again&lt;/p&gt;\n\n&lt;p&gt;Edit 3: someone told me to use cloud to backup my data, i don&amp;#39;t know how to reach that cloud whilst carrying all my disks&lt;/p&gt;\n\n&lt;p&gt;Edit 4: someone said i should use DSDD, i don&amp;#39;t like DS games, no thanks!&lt;/p&gt;\n\n&lt;p&gt;Edit 5: got told that diskettes sometimes get dirty and i should try licking it, tastes like TV static, 1/10 never trying again.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 150, "id": "award_f44611f1-b89e-46dc-97fe-892280b13b82", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Thank you stranger. Shows the award.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Helpful", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png"}, {"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 250, "id": "award_435a5692-f508-4b31-8083-ddc576f26ad3", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/hehehe_b_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/hehehe_b_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/hehehe_b_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/hehehe_b_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/hehehe_b_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/hehehe_b_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "That's a little funny", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "hehehehe", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/zenxchy18kd61_hehehehe.png?width=16&amp;height=16&amp;auto=webp&amp;s=c24daed27d55b27095d94d1b5ca9f944ea1dc087", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/zenxchy18kd61_hehehehe.png?width=32&amp;height=32&amp;auto=webp&amp;s=f0791d2973305c3591b628a5a9748f74cac827ec", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/zenxchy18kd61_hehehehe.png?width=48&amp;height=48&amp;auto=webp&amp;s=b71aeaa9514327b9fe3a2f19b21466dd3d2f4dff", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/zenxchy18kd61_hehehehe.png?width=64&amp;height=64&amp;auto=webp&amp;s=41bad6195c6ef5c2855d2e81da2fa12d357bc015", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/zenxchy18kd61_hehehehe.png?width=128&amp;height=128&amp;auto=webp&amp;s=d7ab516eb8e614f60ac6c80a71cc0db5ffd9b0b5", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 512, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/zenxchy18kd61_hehehehe.png"}, {"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 75, "id": "award_9663243a-e77f-44cf-abc6-850ead2cd18d", "penny_donate": null, "award_sub_type": "PREMIUM", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/SnooClappingPremium_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "For an especially amazing showing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Bravo Grande!", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=16&amp;height=16&amp;auto=webp&amp;s=3459bdf1d1777821a831c5bf9834f4365263fcff", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=32&amp;height=32&amp;auto=webp&amp;s=9181d68065ccfccf2b1074e499cd7c1103aa2ce8", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=48&amp;height=48&amp;auto=webp&amp;s=339b368d395219120abc50d54fb3e2cdcad8ca4f", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=64&amp;height=64&amp;auto=webp&amp;s=de4ebbe92f9019de05aaa77f88810d44adbe1e50", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png?width=128&amp;height=128&amp;auto=webp&amp;s=ba6c1add5204ea43e5af010bd9622392a42140e3", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 512, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_q0gj4/59e02tmkl4451_BravoGrande-Static.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zazo3z", "is_robot_indexable": true, "report_reasons": null, "author": "GamerKingHD", "discussion_type": null, "num_comments": 86, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zazo3z/rdatahoarder_shitpost_be_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zazo3z/rdatahoarder_shitpost_be_like/", "subreddit_subscribers": 657316, "created_utc": 1670019444.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Fellow datahoarders! I need your help.\n\nWhat programs do you use to sort out photos/videos/media? I\u2019ve been doing a multi-year project where I\u2019m gathering all of my family photos, old and new. I follow the 3-2-1 backup plan to make sure nothing happens in the task of gathering and digitizing old photos, but now I have thousands of pieces of media that I need to comprehensively organize that will make them easy to find and view. I\u2019ve looked online but a lot of the programs don\u2019t seem to be what I specifically need, lacking some feature. \n\nThe key features I want out of the organizing software: \n\n- Tagging system (e.g. being able to search for a photo/group of photos by date/person/event/place/source at the same time) \n\n- A timeline view (not really needed but would definitely be awesome.)\n\n- Ability to mass rename media in an organized way (eg. making a group of photos \u20182021_03_08-baseball-game-pics-cannon_rebel\u2019 \n\n- Lossless conversion (might just need another program for this one) / lossless viewing after import \n\n- Supports ranges of file formats (PNG, JPEG, HEIC, GIF, etc.)\n\n\n\nThe program doesn\u2019t need cloud options/storage available or built-in photo editing, but if it happens to have it then that\u2019s an added bonus. \n\nIn total when I\u2019m completely done with the project, I\u2019m expecting probably about 200-300gb of media. \n\nI\u2019m open for both free and paid programs, just let me know a good one, thanks!\n\nEdit: Currently using Windows 10", "author_fullname": "t2_259g0f9y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on a good photo/video/digital asset organizer!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zatuzg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670019382.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670006511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Fellow datahoarders! I need your help.&lt;/p&gt;\n\n&lt;p&gt;What programs do you use to sort out photos/videos/media? I\u2019ve been doing a multi-year project where I\u2019m gathering all of my family photos, old and new. I follow the 3-2-1 backup plan to make sure nothing happens in the task of gathering and digitizing old photos, but now I have thousands of pieces of media that I need to comprehensively organize that will make them easy to find and view. I\u2019ve looked online but a lot of the programs don\u2019t seem to be what I specifically need, lacking some feature. &lt;/p&gt;\n\n&lt;p&gt;The key features I want out of the organizing software: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Tagging system (e.g. being able to search for a photo/group of photos by date/person/event/place/source at the same time) &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A timeline view (not really needed but would definitely be awesome.)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ability to mass rename media in an organized way (eg. making a group of photos \u20182021_03_08-baseball-game-pics-cannon_rebel\u2019 &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Lossless conversion (might just need another program for this one) / lossless viewing after import &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Supports ranges of file formats (PNG, JPEG, HEIC, GIF, etc.)&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The program doesn\u2019t need cloud options/storage available or built-in photo editing, but if it happens to have it then that\u2019s an added bonus. &lt;/p&gt;\n\n&lt;p&gt;In total when I\u2019m completely done with the project, I\u2019m expecting probably about 200-300gb of media. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m open for both free and paid programs, just let me know a good one, thanks!&lt;/p&gt;\n\n&lt;p&gt;Edit: Currently using Windows 10&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zatuzg", "is_robot_indexable": true, "report_reasons": null, "author": "Acharvix", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zatuzg/need_advice_on_a_good_photovideodigital_asset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zatuzg/need_advice_on_a_good_photovideodigital_asset/", "subreddit_subscribers": 657316, "created_utc": 1670006511.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nI bought one of these at the $109.99 price (about $118 with tax, delivered) to add as an 8th drive on a DS1821+. After Black Friday it is now back up to $119.99:\n\n[https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y](https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y)\n\nAll other slots are populated with a mix of helium and non-helium 8TB drives, all MyBook and EasyStore shucks. I could not find any information on the Internet regarding whether this drive would honor the disable head parking command. It does.\n\nThe warranty was advertised as 2 years. WD warranty checker lists the warranty as 3 years.\n\nThis WD80EAZZ arrived with idle3-tools reporting head parking at 80 (8 seconds). This drive honors the idle3-tools command to disable head parking. I ran idle3-tools via direct sata connection from an Ubuntu 20.04 server.\n\nTo do this from Ubuntu:\n\nsudo apt update\n\nsudo apt install idle3-tools\n\nsudo fdisk -l (to list what disk label your WD80EAZZ is set at)\n\nsudo idle3ctl -g /dev/sdX (replace \"X\" with the correct drive label)\n\n(verify that drive respond to the idle3 command, and lists the parking time as 80)\n\nsudo idle3ctl -d /dev/sdX (replace \"X\" with the correct drive label)\n\n(you should get a \"head parking disabled\" message)\n\nShutoff the computer. Fully poweroff the drive. Reboot and run\n\nsudo idle3ctl -g /dev/sdX (replace \"X\" with the correct drive label)\n\n...to verify that the head parking disabled setting survived poweroff.\n\nThis drive appears to be mechanically identical to the white label non-helium EasyStore/MyBook drives. But, it does arrive with head parking set to 80.\n\nThis drive seems like a steal at this price. I had 2 4TB Reds fail within the warranty period. Due to the failed Reds, I gave up on Reds and started buying white label 8TB drives. I have had zero white label drives fail so far. I am sure one white will fail tomorrow.", "author_fullname": "t2_46ha7mck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Blue 8TB Hard Drive WD80EAZZ 5640RPM CMR $109.99", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb178f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670022957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought one of these at the $109.99 price (about $118 with tax, delivered) to add as an 8th drive on a DS1821+. After Black Friday it is now back up to $119.99:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y\"&gt;https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;All other slots are populated with a mix of helium and non-helium 8TB drives, all MyBook and EasyStore shucks. I could not find any information on the Internet regarding whether this drive would honor the disable head parking command. It does.&lt;/p&gt;\n\n&lt;p&gt;The warranty was advertised as 2 years. WD warranty checker lists the warranty as 3 years.&lt;/p&gt;\n\n&lt;p&gt;This WD80EAZZ arrived with idle3-tools reporting head parking at 80 (8 seconds). This drive honors the idle3-tools command to disable head parking. I ran idle3-tools via direct sata connection from an Ubuntu 20.04 server.&lt;/p&gt;\n\n&lt;p&gt;To do this from Ubuntu:&lt;/p&gt;\n\n&lt;p&gt;sudo apt update&lt;/p&gt;\n\n&lt;p&gt;sudo apt install idle3-tools&lt;/p&gt;\n\n&lt;p&gt;sudo fdisk -l (to list what disk label your WD80EAZZ is set at)&lt;/p&gt;\n\n&lt;p&gt;sudo idle3ctl -g /dev/sdX (replace &amp;quot;X&amp;quot; with the correct drive label)&lt;/p&gt;\n\n&lt;p&gt;(verify that drive respond to the idle3 command, and lists the parking time as 80)&lt;/p&gt;\n\n&lt;p&gt;sudo idle3ctl -d /dev/sdX (replace &amp;quot;X&amp;quot; with the correct drive label)&lt;/p&gt;\n\n&lt;p&gt;(you should get a &amp;quot;head parking disabled&amp;quot; message)&lt;/p&gt;\n\n&lt;p&gt;Shutoff the computer. Fully poweroff the drive. Reboot and run&lt;/p&gt;\n\n&lt;p&gt;sudo idle3ctl -g /dev/sdX (replace &amp;quot;X&amp;quot; with the correct drive label)&lt;/p&gt;\n\n&lt;p&gt;...to verify that the head parking disabled setting survived poweroff.&lt;/p&gt;\n\n&lt;p&gt;This drive appears to be mechanically identical to the white label non-helium EasyStore/MyBook drives. But, it does arrive with head parking set to 80.&lt;/p&gt;\n\n&lt;p&gt;This drive seems like a steal at this price. I had 2 4TB Reds fail within the warranty period. Due to the failed Reds, I gave up on Reds and started buying white label 8TB drives. I have had zero white label drives fail so far. I am sure one white will fail tomorrow.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb178f", "is_robot_indexable": true, "report_reasons": null, "author": "CottonBambino", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb178f/wd_blue_8tb_hard_drive_wd80eazz_5640rpm_cmr_10999/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb178f/wd_blue_8tb_hard_drive_wd80eazz_5640rpm_cmr_10999/", "subreddit_subscribers": 657316, "created_utc": 1670022957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have around 5TB worth of data that I want to consolidate into one place and want to know how would I go around and do such a thing.   \nShort backstory, I had a system with 2 drives, a 500GB SSD and 2TB HDD. Bought a 1TB HDD during start of the pandemic for recording location of recorded meetings. Year 1 into Pandemic, 2TB HDD has an error and on a \"Caution\" status from CrystalDiskInfo. I was also planning to build a new rig that year so I bought one 1TB NVME SSD(not part of the situation), a 2TB Ironwolf HDD and 4TB Barracuda.  \nI didn't want to use my new drives on the PC so I used two 1TB External HDDs to store the data. Then a year later the 1TB I bought at the start of the pandemic was in \"Bad\" condition, so I gave in and plugged my 4TB HDD and transferred my files. For some reason I also plugged in the 2TB HDD. Now all of them are in use.  \n\n\nSo I did a disk analyze using WizTree and found out that I have half of my total storage full, not to mention earlier in the day I got an error that there was not enough space in my boot drive when I was saving stuff into there.  \n\n\nNow how would I go around completely copying over these files into a whole separate drive/drives, eliminating duplicates and make sure everything is copied properly, so that I can completely wipe my current system and basically start from scratch.  \n I would also like to ask on what kind of storage I should use to store this on. I think it will be just a cold storage, with the occasional rummage through, but it might also be another dump place I can continue adding to. I don't want to delete any of the files, more of its a hassle rather than sentimental stuff, but there are stuff that I definitely want to keep there.  I would also say that I don't have much money to put towards storing these. So I guess a solution where I can add on over time would be best.", "author_fullname": "t2_mu7c9au", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would I go about copying around 5TB worth of data, from multiple drives to a singular drive/drives (Shared Pools/Raid)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbhmth", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670077996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have around 5TB worth of data that I want to consolidate into one place and want to know how would I go around and do such a thing.&lt;br/&gt;\nShort backstory, I had a system with 2 drives, a 500GB SSD and 2TB HDD. Bought a 1TB HDD during start of the pandemic for recording location of recorded meetings. Year 1 into Pandemic, 2TB HDD has an error and on a &amp;quot;Caution&amp;quot; status from CrystalDiskInfo. I was also planning to build a new rig that year so I bought one 1TB NVME SSD(not part of the situation), a 2TB Ironwolf HDD and 4TB Barracuda.&lt;br/&gt;\nI didn&amp;#39;t want to use my new drives on the PC so I used two 1TB External HDDs to store the data. Then a year later the 1TB I bought at the start of the pandemic was in &amp;quot;Bad&amp;quot; condition, so I gave in and plugged my 4TB HDD and transferred my files. For some reason I also plugged in the 2TB HDD. Now all of them are in use.  &lt;/p&gt;\n\n&lt;p&gt;So I did a disk analyze using WizTree and found out that I have half of my total storage full, not to mention earlier in the day I got an error that there was not enough space in my boot drive when I was saving stuff into there.  &lt;/p&gt;\n\n&lt;p&gt;Now how would I go around completely copying over these files into a whole separate drive/drives, eliminating duplicates and make sure everything is copied properly, so that I can completely wipe my current system and basically start from scratch.&lt;br/&gt;\n I would also like to ask on what kind of storage I should use to store this on. I think it will be just a cold storage, with the occasional rummage through, but it might also be another dump place I can continue adding to. I don&amp;#39;t want to delete any of the files, more of its a hassle rather than sentimental stuff, but there are stuff that I definitely want to keep there.  I would also say that I don&amp;#39;t have much money to put towards storing these. So I guess a solution where I can add on over time would be best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbhmth", "is_robot_indexable": true, "report_reasons": null, "author": "InfraDelta", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbhmth/how_would_i_go_about_copying_around_5tb_worth_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbhmth/how_would_i_go_about_copying_around_5tb_worth_of/", "subreddit_subscribers": 657316, "created_utc": 1670077996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have an existing computer computer which currently runs unraid.  I got a free to me 2 bay QNAP a while back and have since used that as my NAS for a while.  Now that the existing unraid computer is just serving 5-10 docker containers for putzing around and not on storage duty, I'd like to dabble into ZFS and actually make it a nicer NAS and use the QNAP as a backup.  I snagged three 20TB Ironwolf Pros and am going to rebuild the machine tomorrow.\n\nAfter reading lots of guides and watching videos, I think I have a plan:   \n   \nThe computer\n------------   \n* i5-7600\n* [motherboard](https://www.gigabyte.com/Motherboard/GA-Z270X-Gaming-K7-rev-10#kf)\n* 32 GB DDR4\n* 256GB Samsung 830 SSD\n* 2 Hodgepodge-y 4TB rust (probably won't re-use)\n* 3x of the aforementioned Ironwolf Pro 20TB (ST20000NEZ00)   \n\nInstall   \n-------   \n* Ubuntu Server (22.04?) or Debian Bullseye, install on SSD as \"normal\"\n* Install zfs as per [this guide](https://openzfs.github.io/openzfs-docs/Getting%20Started/index.html)\n* set up a raidz1 pool, and other settings as per the guide on the wiki here     \n   \nQuestions   \n---------   \n* One of the guides says to enable compression... is that really a good idea?   \n* Do I really need to create datasets?  Seems logical maybe?  I figured the main use of this NAS will be media/movies, then a small portion for a main backup, with my older QNAP NAS being the real \"backup\".  any pros/cons to creating datasets vs just using directories?   \n* I've seen some folks say to scrub once a week, others say once a month, does it matter?   \n* are snapshots really worth doing?  maybe weekly or monthly or something?   \n* any particular tools/monitoring tools you like using?   \n* any particular BIOS or other hardware-y type settings that matter?", "author_fullname": "t2_j4g0k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building/Rebuilding server, need ZFS advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb8o3a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670045361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an existing computer computer which currently runs unraid.  I got a free to me 2 bay QNAP a while back and have since used that as my NAS for a while.  Now that the existing unraid computer is just serving 5-10 docker containers for putzing around and not on storage duty, I&amp;#39;d like to dabble into ZFS and actually make it a nicer NAS and use the QNAP as a backup.  I snagged three 20TB Ironwolf Pros and am going to rebuild the machine tomorrow.&lt;/p&gt;\n\n&lt;p&gt;After reading lots of guides and watching videos, I think I have a plan:   &lt;/p&gt;\n\n&lt;h2&gt;The computer&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;i5-7600&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.gigabyte.com/Motherboard/GA-Z270X-Gaming-K7-rev-10#kf\"&gt;motherboard&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;32 GB DDR4&lt;/li&gt;\n&lt;li&gt;256GB Samsung 830 SSD&lt;/li&gt;\n&lt;li&gt;2 Hodgepodge-y 4TB rust (probably won&amp;#39;t re-use)&lt;/li&gt;\n&lt;li&gt;3x of the aforementioned Ironwolf Pro 20TB (ST20000NEZ00)&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Install   &lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ubuntu Server (22.04?) or Debian Bullseye, install on SSD as &amp;quot;normal&amp;quot;&lt;/li&gt;\n&lt;li&gt;Install zfs as per &lt;a href=\"https://openzfs.github.io/openzfs-docs/Getting%20Started/index.html\"&gt;this guide&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;set up a raidz1 pool, and other settings as per the guide on the wiki here&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Questions   &lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;One of the guides says to enable compression... is that really a good idea?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Do I really need to create datasets?  Seems logical maybe?  I figured the main use of this NAS will be media/movies, then a small portion for a main backup, with my older QNAP NAS being the real &amp;quot;backup&amp;quot;.  any pros/cons to creating datasets vs just using directories?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve seen some folks say to scrub once a week, others say once a month, does it matter?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;are snapshots really worth doing?  maybe weekly or monthly or something?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;any particular tools/monitoring tools you like using?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;any particular BIOS or other hardware-y type settings that matter?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gd-XgGWoQSWb_PunVE-qNEM_8rmzskSKHgZ9r16z6wA.jpg?auto=webp&amp;s=22225accca5c684e6f3124aba9f86b0a35bacfa1", "width": 1000, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/gd-XgGWoQSWb_PunVE-qNEM_8rmzskSKHgZ9r16z6wA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ca3feae19de36209c0aece3b0bfad340a6c7052f", "width": 108, "height": 70}, {"url": "https://external-preview.redd.it/gd-XgGWoQSWb_PunVE-qNEM_8rmzskSKHgZ9r16z6wA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af81478e17619dd20cf414bdfff16b212669cb74", "width": 216, "height": 140}, {"url": "https://external-preview.redd.it/gd-XgGWoQSWb_PunVE-qNEM_8rmzskSKHgZ9r16z6wA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f89406265402dd5bf7acec72908d4dda51c4ee40", "width": 320, "height": 208}, {"url": "https://external-preview.redd.it/gd-XgGWoQSWb_PunVE-qNEM_8rmzskSKHgZ9r16z6wA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=437a77915a882912bff8e7643289c5d5c322179d", "width": 640, "height": 416}, {"url": "https://external-preview.redd.it/gd-XgGWoQSWb_PunVE-qNEM_8rmzskSKHgZ9r16z6wA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e547b710de8f29f274fe59e3de552b9c7fb14e89", "width": 960, "height": 624}], "variants": {}, "id": "YLt_FlzniWHuyqed9xHhZUKiPsbpH2MBs9VFfr3-NTc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb8o3a", "is_robot_indexable": true, "report_reasons": null, "author": "impala454", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb8o3a/buildingrebuilding_server_need_zfs_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb8o3a/buildingrebuilding_server_need_zfs_advice/", "subreddit_subscribers": 657316, "created_utc": 1670045361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I use Kodi for my HTPC, and I like having 4K photos from around the world randomly cycling as a screensaver... but it'd be nice if I could set it up to automatically grab new images.\n\nBest source I know of for good 4K photos from around the world is the Bing daily wallpaper. And there are sites like [this](https://bingwallpaper.anerg.com/) that have them archived. Three questions:\n\n1) How could I scrape a site like that to just grab the 4K images, and also, name the files with each photo's caption;\n\n2) What other sources of good 4K images of the real world are there; and\n\n3) How can I automate the downloading? It'd be nice if, every day, my system would just grab new images for me.", "author_fullname": "t2_fdefm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to scrape Bing and other wallpapers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb7037", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670039880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use Kodi for my HTPC, and I like having 4K photos from around the world randomly cycling as a screensaver... but it&amp;#39;d be nice if I could set it up to automatically grab new images.&lt;/p&gt;\n\n&lt;p&gt;Best source I know of for good 4K photos from around the world is the Bing daily wallpaper. And there are sites like &lt;a href=\"https://bingwallpaper.anerg.com/\"&gt;this&lt;/a&gt; that have them archived. Three questions:&lt;/p&gt;\n\n&lt;p&gt;1) How could I scrape a site like that to just grab the 4K images, and also, name the files with each photo&amp;#39;s caption;&lt;/p&gt;\n\n&lt;p&gt;2) What other sources of good 4K images of the real world are there; and&lt;/p&gt;\n\n&lt;p&gt;3) How can I automate the downloading? It&amp;#39;d be nice if, every day, my system would just grab new images for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb7037", "is_robot_indexable": true, "report_reasons": null, "author": "Slackroyd", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb7037/how_to_scrape_bing_and_other_wallpapers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb7037/how_to_scrape_bing_and_other_wallpapers/", "subreddit_subscribers": 657316, "created_utc": 1670039880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "In the beginning of the year I've bought a cheap SSD SomnAmbulist 120GB. After a couple of benchmarks, I've noted some strange throughput measurements. Briefly:\n\n- **Initial Benchmark** | [image](https://imgur.com/FCsKFt0)  \n  Just after format, the throughput is according to the advertised specifications at 510MB/s read and 440MB/s write.  \n  It starts to drop at 35% capacity written and stabilizes at 200~300MB/s read and 40MB/s write.  \n  Although this drop is not mentioned by the seller's ads, it is indeed an expected behavior and should return to normal after a couple of minutes in idle (flush the cache into the permanent area).\n\n- **Benchmark after 1 hour in idle** | [image](https://imgur.com/mHliXkO)  \n  Unfortunately, the throughput has not returned to normal (yet?).  \n  315MB/s read and 440MB/s write, and the speed drop started earlier at 10% capacity written.\n\n- **Benchmark after 12 hours in idle** | [image](https://imgur.com/KVLn0Fy)  \n  The throughput still has not returned to normal.\n\n\n**Questions**\n\n1. Is this the expected behavior for such cheap SSD or indeed there is something strange?\n2. What might be going wrong here?\n3. Any idea how to fix this throughput?\n\n----------------------------------------------\n\n**Debug Info**\n\n- SSD/TRIM Supported\n- fstrim enabled\n- SSD APM (Advanced Power Management) not supported  \n- f3 (Fight Flash Fraud) OK  \n\n```\n## SSD/TRIM SUPPORTED\nroot@debian:~# hdparm -I /dev/sda | grep -i \"trim\\|power\"\n       *\tPower Management feature set\n       *\tData Set Management TRIM supported (limit 8 blocks)\n       *\tDeterministic read data after TRIM\n\n\n## SSD/TRIM AVAILABLE (DISC-MAX NON ZERO)\nroot@debian:~# lsblk -D /dev/sda\nNAME DISC-ALN DISC-GRAN DISC-MAX DISC-ZERO\nsda         0      512B       2G         0\n\n\n## ADVANCED POWER MANAGEMENT NOT SUPPORTED\nroot@debian:~# hdparm -B /dev/sda\n\n/dev/sda:\n APM_level\t= not supported\n\n\n\n## FSTRIM ENABLE\n\nroot@debian:~# systemctl status fstrim.timer\n\u25cf fstrim.timer - Discard unused blocks once a week\n     Loaded: loaded (/lib/systemd/system/fstrim.timer; enabled; vendor preset: enabled)\n     Active: active (waiting) since Sat 2022-12-03 03:36:10 UTC; 35min ago\n    Trigger: Mon 2022-12-05 00:51:25 UTC; 1 day 20h left\n   Triggers: \u25cf fstrim.service\n       Docs: man:fstrim\n\nDec 03 03:36:10 debian systemd[1]: Started Discard unused blocks once a week.\n\n\nroot@debian:~# systemctl status fstrim\n\u25cf fstrim.service - Discard unused blocks on filesystems from /etc/fstab\n     Loaded: loaded (/lib/systemd/system/fstrim.service; static)\n     Active: inactive (dead)\nTriggeredBy: \u25cf fstrim.timer\n       Docs: man:fstrim(8)\n\n\nroot@debian:~# fstrim -av\n```", "author_fullname": "t2_j0e8md80", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheap SSD: Slow Speed Issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb6dz0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670044296.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670037915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the beginning of the year I&amp;#39;ve bought a cheap SSD SomnAmbulist 120GB. After a couple of benchmarks, I&amp;#39;ve noted some strange throughput measurements. Briefly:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Initial Benchmark&lt;/strong&gt; | &lt;a href=\"https://imgur.com/FCsKFt0\"&gt;image&lt;/a&gt;&lt;br/&gt;\nJust after format, the throughput is according to the advertised specifications at 510MB/s read and 440MB/s write.&lt;br/&gt;\nIt starts to drop at 35% capacity written and stabilizes at 200~300MB/s read and 40MB/s write.&lt;br/&gt;\nAlthough this drop is not mentioned by the seller&amp;#39;s ads, it is indeed an expected behavior and should return to normal after a couple of minutes in idle (flush the cache into the permanent area).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Benchmark after 1 hour in idle&lt;/strong&gt; | &lt;a href=\"https://imgur.com/mHliXkO\"&gt;image&lt;/a&gt;&lt;br/&gt;\nUnfortunately, the throughput has not returned to normal (yet?).&lt;br/&gt;\n315MB/s read and 440MB/s write, and the speed drop started earlier at 10% capacity written.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Benchmark after 12 hours in idle&lt;/strong&gt; | &lt;a href=\"https://imgur.com/KVLn0Fy\"&gt;image&lt;/a&gt;&lt;br/&gt;\nThe throughput still has not returned to normal.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is this the expected behavior for such cheap SSD or indeed there is something strange?&lt;/li&gt;\n&lt;li&gt;What might be going wrong here?&lt;/li&gt;\n&lt;li&gt;Any idea how to fix this throughput?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;&lt;strong&gt;Debug Info&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SSD/TRIM Supported&lt;/li&gt;\n&lt;li&gt;fstrim enabled&lt;/li&gt;\n&lt;li&gt;SSD APM (Advanced Power Management) not supported&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;f3 (Fight Flash Fraud) OK&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;h2&gt;SSD/TRIM SUPPORTED&lt;/h2&gt;\n\n&lt;p&gt;root@debian:~# hdparm -I /dev/sda | grep -i &amp;quot;trim|power&amp;quot;\n       *    Power Management feature set\n       *    Data Set Management TRIM supported (limit 8 blocks)\n       *    Deterministic read data after TRIM&lt;/p&gt;\n\n&lt;h2&gt;SSD/TRIM AVAILABLE (DISC-MAX NON ZERO)&lt;/h2&gt;\n\n&lt;p&gt;root@debian:~# lsblk -D /dev/sda\nNAME DISC-ALN DISC-GRAN DISC-MAX DISC-ZERO\nsda         0      512B       2G         0&lt;/p&gt;\n\n&lt;h2&gt;ADVANCED POWER MANAGEMENT NOT SUPPORTED&lt;/h2&gt;\n\n&lt;p&gt;root@debian:~# hdparm -B /dev/sda&lt;/p&gt;\n\n&lt;p&gt;/dev/sda:\n APM_level  = not supported&lt;/p&gt;\n\n&lt;h2&gt;FSTRIM ENABLE&lt;/h2&gt;\n\n&lt;p&gt;root@debian:~# systemctl status fstrim.timer\n\u25cf fstrim.timer - Discard unused blocks once a week\n     Loaded: loaded (/lib/systemd/system/fstrim.timer; enabled; vendor preset: enabled)\n     Active: active (waiting) since Sat 2022-12-03 03:36:10 UTC; 35min ago\n    Trigger: Mon 2022-12-05 00:51:25 UTC; 1 day 20h left\n   Triggers: \u25cf fstrim.service\n       Docs: man:fstrim&lt;/p&gt;\n\n&lt;p&gt;Dec 03 03:36:10 debian systemd[1]: Started Discard unused blocks once a week.&lt;/p&gt;\n\n&lt;p&gt;root@debian:~# systemctl status fstrim\n\u25cf fstrim.service - Discard unused blocks on filesystems from /etc/fstab\n     Loaded: loaded (/lib/systemd/system/fstrim.service; static)\n     Active: inactive (dead)\nTriggeredBy: \u25cf fstrim.timer\n       Docs: man:fstrim(8)&lt;/p&gt;\n\n&lt;p&gt;root@debian:~# fstrim -av\n```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tFolgDkT-6MRwNGve2X4AOxAKXc-RKbzrve2F7d6U1U.jpg?auto=webp&amp;s=5c78586ef611c3b5f59b3af867140d6a58de8ee9", "width": 600, "height": 315}, "resolutions": [{"url": "https://external-preview.redd.it/tFolgDkT-6MRwNGve2X4AOxAKXc-RKbzrve2F7d6U1U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c6284fb44ae80e97a08025e961c9186f605cb408", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/tFolgDkT-6MRwNGve2X4AOxAKXc-RKbzrve2F7d6U1U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a43d2dd8f89e5cc09823f0a673acaf383d095f7a", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/tFolgDkT-6MRwNGve2X4AOxAKXc-RKbzrve2F7d6U1U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=92d21491b7e1d3e1bf7750dff0c8bb53e2b1e3fe", "width": 320, "height": 168}], "variants": {}, "id": "YqPoNI41Sv8sjvPeZxiZ1d45Dcng8Xoabu_EPPX9NsE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zb6dz0", "is_robot_indexable": true, "report_reasons": null, "author": "Jeron_Baffom", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb6dz0/cheap_ssd_slow_speed_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb6dz0/cheap_ssd_slow_speed_issue/", "subreddit_subscribers": 657316, "created_utc": 1670037915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey Guys,I've recently had a weird failure mode on my unraid box and thought I would consult the hive-mind for ideas &amp; thoughts.\n\n**TLDR:**\n\nI think that somehow port/channel 3 on the first port of my LSI 9240-8i is bad. Swapping breakout cable results in the same sata port being bad.\n\nHas anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?\n\n**Key system info:**\n\n* It's been up and happy for over a year.\n* I'm using a LSI 9240-8i in IT mode as an HBA\n* I'm using 3 x 14TB WD drives all attached to a single SAS SFF-8087 to 4x SATA Cable\n* 1 x Parity disk and 2 x data disks\n\n**What happened:**\n\n1. Woke up in the morning to a scheduled parity check having failed. Looking at the logs. It failed not long after it started. The second data disk was marked by unRaid as \"Device disabled, contents emulated\".\n2. Ok, no biggie - the disk has failed. Stopped the array, unassigned the failed disk and shut down. Ordered a replacement.\n3. While waiting for the replacement, I pulled the failed disk and had a look at it with CrystalDiskInfo. Good SMART health and zero errors of any kind. Hmmm.\n4. New disk arrives. Slap it into the machine (same cable as the 'failed' disk) and unraid offers to rebuild the array to it. Click go.\n5. Almost immediately, read errors are reported, rebuild fails and the array is stuck in some kind of read-check paused status. Shit.\n6. Hmm. Maybe it's a bad cable? Shut down and swap the SAS SFF-8087 to 4x SATA Cable for a spare. Plug in both the new drive as well as the 'failed' drive (I suspect the drive itself is actually fine, so I may add it as a second parity later). New drive is on port 3 of the breakout cable (where the failed drive had been) and the 'failed' drive is now on port 4.\n7. Boot up unraid and assign the original 'failed' drive to the array as a new disk. Start the array and unraid starts to rebuild the array with no issues!\n8. Go to my unassigned disks and I can see the new drive. Spin it up and run a SMART test. SMART test fails. Twice. Errors in the log:\n\n&amp;#x200B;\n\n    Dec  3 12:11:06 tower  emhttpd: spinning up /dev/sdd\n    Dec  3 12:11:17 tower  emhttpd: read SMART /dev/sdd\n    Dec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronizing SCSI cache\n    Dec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronize Cache(10) failed: Result: hostbyte=0x01 driverbyte=DRIVER_OK\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 27344764928 512-byte logical blocks: (14.0 TB/12.7 TiB)\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 4096-byte physical blocks\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write Protect is off\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Mode Sense: 7f 00 10 08\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write cache: enabled, read cache: enabled, supports DPO and FUA\n    Dec  3 12:12:53 tower kernel: sdd: sdd1\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Attached SCSI disk\n    Dec  3 12:12:54 tower unassigned.devices: Disk with ID 'TOSHIBA_HDWG21E_Z1A0A0CCFP7G (sdd)' is not set to auto mount.\n\n**Conclusion:**\n\nI think the cable was actually fine and somehow port/channel 3 on the first port of my LSI 9240-8i is somehow bad.\n\nHas anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?", "author_fullname": "t2_ckt3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debugging LSI 9240-8i in IT mode", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbfyvu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670073114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Guys,I&amp;#39;ve recently had a weird failure mode on my unraid box and thought I would consult the hive-mind for ideas &amp;amp; thoughts.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I think that somehow port/channel 3 on the first port of my LSI 9240-8i is bad. Swapping breakout cable results in the same sata port being bad.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key system info:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It&amp;#39;s been up and happy for over a year.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m using a LSI 9240-8i in IT mode as an HBA&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m using 3 x 14TB WD drives all attached to a single SAS SFF-8087 to 4x SATA Cable&lt;/li&gt;\n&lt;li&gt;1 x Parity disk and 2 x data disks&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What happened:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Woke up in the morning to a scheduled parity check having failed. Looking at the logs. It failed not long after it started. The second data disk was marked by unRaid as &amp;quot;Device disabled, contents emulated&amp;quot;.&lt;/li&gt;\n&lt;li&gt;Ok, no biggie - the disk has failed. Stopped the array, unassigned the failed disk and shut down. Ordered a replacement.&lt;/li&gt;\n&lt;li&gt;While waiting for the replacement, I pulled the failed disk and had a look at it with CrystalDiskInfo. Good SMART health and zero errors of any kind. Hmmm.&lt;/li&gt;\n&lt;li&gt;New disk arrives. Slap it into the machine (same cable as the &amp;#39;failed&amp;#39; disk) and unraid offers to rebuild the array to it. Click go.&lt;/li&gt;\n&lt;li&gt;Almost immediately, read errors are reported, rebuild fails and the array is stuck in some kind of read-check paused status. Shit.&lt;/li&gt;\n&lt;li&gt;Hmm. Maybe it&amp;#39;s a bad cable? Shut down and swap the SAS SFF-8087 to 4x SATA Cable for a spare. Plug in both the new drive as well as the &amp;#39;failed&amp;#39; drive (I suspect the drive itself is actually fine, so I may add it as a second parity later). New drive is on port 3 of the breakout cable (where the failed drive had been) and the &amp;#39;failed&amp;#39; drive is now on port 4.&lt;/li&gt;\n&lt;li&gt;Boot up unraid and assign the original &amp;#39;failed&amp;#39; drive to the array as a new disk. Start the array and unraid starts to rebuild the array with no issues!&lt;/li&gt;\n&lt;li&gt;Go to my unassigned disks and I can see the new drive. Spin it up and run a SMART test. SMART test fails. Twice. Errors in the log:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Dec  3 12:11:06 tower  emhttpd: spinning up /dev/sdd\nDec  3 12:11:17 tower  emhttpd: read SMART /dev/sdd\nDec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronizing SCSI cache\nDec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronize Cache(10) failed: Result: hostbyte=0x01 driverbyte=DRIVER_OK\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 27344764928 512-byte logical blocks: (14.0 TB/12.7 TiB)\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 4096-byte physical blocks\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write Protect is off\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Mode Sense: 7f 00 10 08\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write cache: enabled, read cache: enabled, supports DPO and FUA\nDec  3 12:12:53 tower kernel: sdd: sdd1\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Attached SCSI disk\nDec  3 12:12:54 tower unassigned.devices: Disk with ID &amp;#39;TOSHIBA_HDWG21E_Z1A0A0CCFP7G (sdd)&amp;#39; is not set to auto mount.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I think the cable was actually fine and somehow port/channel 3 on the first port of my LSI 9240-8i is somehow bad.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbfyvu", "is_robot_indexable": true, "report_reasons": null, "author": "kabadisha", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbfyvu/debugging_lsi_92408i_in_it_mode/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbfyvu/debugging_lsi_92408i_in_it_mode/", "subreddit_subscribers": 657316, "created_utc": 1670073114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There's this one Flash Game (Recess Special Operations) that i've wanted to replay for years, but it doesn't exist anywhere\n\n* not on Wayback Machine\n* not even on Flashpoint, which had almost every other game I could think of\n\nonly 2 levels have ever been recovered out of 7\n\nI'm willing to pay a reward if anyone is able to recover it, I just don't know if such a service is provided (or if im allowed to request that here)\n\nthe details about it are here if anyone is curious [https://lostmediawiki.com/Recess\\_Special\\_Operations\\_(partially\\_found\\_Flash\\_game\\_of\\_Disney\\_Channel\\_animated\\_series;\\_early\\_2000s)](https://lostmediawiki.com/Recess_Special_Operations_(partially_found_Flash_game_of_Disney_Channel_animated_series;_early_2000s))\n\nOriginal Link to the Game [https://web.archive.org/web/20041126051651/http://psc.disney.go.com/abcnetworks/toondisney/games/recess\\_special\\_ops/index.html](https://web.archive.org/web/20041126051651/http://psc.disney.go.com/abcnetworks/toondisney/games/recess_special_ops/index.html)\n\nCurrent Archived Level 2 and Level 5, which lack the background music\n\n[https://www.gamesflow.com/jeux.php?id=234718](https://www.gamesflow.com/jeux.php?id=234718)\n\n[https://www.gamesflow.com/jeux.php?id=234719](https://www.gamesflow.com/jeux.php?id=234719)", "author_fullname": "t2_13oll9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any Paid Reward Services to recover lost media?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb8roa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670045908.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670045705.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s this one Flash Game (Recess Special Operations) that i&amp;#39;ve wanted to replay for years, but it doesn&amp;#39;t exist anywhere&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;not on Wayback Machine&lt;/li&gt;\n&lt;li&gt;not even on Flashpoint, which had almost every other game I could think of&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;only 2 levels have ever been recovered out of 7&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m willing to pay a reward if anyone is able to recover it, I just don&amp;#39;t know if such a service is provided (or if im allowed to request that here)&lt;/p&gt;\n\n&lt;p&gt;the details about it are here if anyone is curious &lt;a href=\"https://lostmediawiki.com/Recess_Special_Operations_(partially_found_Flash_game_of_Disney_Channel_animated_series;_early_2000s\"&gt;https://lostmediawiki.com/Recess_Special_Operations_(partially_found_Flash_game_of_Disney_Channel_animated_series;_early_2000s)&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Original Link to the Game &lt;a href=\"https://web.archive.org/web/20041126051651/http://psc.disney.go.com/abcnetworks/toondisney/games/recess_special_ops/index.html\"&gt;https://web.archive.org/web/20041126051651/http://psc.disney.go.com/abcnetworks/toondisney/games/recess_special_ops/index.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Current Archived Level 2 and Level 5, which lack the background music&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.gamesflow.com/jeux.php?id=234718\"&gt;https://www.gamesflow.com/jeux.php?id=234718&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.gamesflow.com/jeux.php?id=234719\"&gt;https://www.gamesflow.com/jeux.php?id=234719&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb8roa", "is_robot_indexable": true, "report_reasons": null, "author": "Falconflyer75", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb8roa/are_there_any_paid_reward_services_to_recover/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb8roa/are_there_any_paid_reward_services_to_recover/", "subreddit_subscribers": 657316, "created_utc": 1670045705.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I want to finally upgrade from my current system (Z87-G45 motherboard, Intel 4770k, 32GB DDR3) and build a new system.  \n\n\nI also want to stop paying for Dropbox and have my own NAS that replicates that functionality as well as be a media server to stream my owned content.  \n\n\nSo, I was thinking of just reusing my current system for the NAS, but here are the drawbacks from my understanding: 1 GB Ethernet on motherboard, no built-in WIFI, and no NVME slots (does have MSATA though).  \n\n\nI could buy a 10G LAN card (100 dollars) as well as an adapter card (15?) to be able to use an NVME drive for caching.  Or, I could sell the motherboard, CPU, RAM, and AIO combo and use that money to build the cheapest newish system using a motherboard that has at least a 2.5G LAN port.  \n\n\nI have never built a server nor have had any kind of network storage before, so this is all a new experience for me.   \n\n\nWhat would you do or recommend me do?\n\nAny advice would be appreciated, including which OS/software system to use!!!", "author_fullname": "t2_2h4tdnqt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reusing Old PC vs Selling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb6f5a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670038024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I want to finally upgrade from my current system (Z87-G45 motherboard, Intel 4770k, 32GB DDR3) and build a new system.  &lt;/p&gt;\n\n&lt;p&gt;I also want to stop paying for Dropbox and have my own NAS that replicates that functionality as well as be a media server to stream my owned content.  &lt;/p&gt;\n\n&lt;p&gt;So, I was thinking of just reusing my current system for the NAS, but here are the drawbacks from my understanding: 1 GB Ethernet on motherboard, no built-in WIFI, and no NVME slots (does have MSATA though).  &lt;/p&gt;\n\n&lt;p&gt;I could buy a 10G LAN card (100 dollars) as well as an adapter card (15?) to be able to use an NVME drive for caching.  Or, I could sell the motherboard, CPU, RAM, and AIO combo and use that money to build the cheapest newish system using a motherboard that has at least a 2.5G LAN port.  &lt;/p&gt;\n\n&lt;p&gt;I have never built a server nor have had any kind of network storage before, so this is all a new experience for me.   &lt;/p&gt;\n\n&lt;p&gt;What would you do or recommend me do?&lt;/p&gt;\n\n&lt;p&gt;Any advice would be appreciated, including which OS/software system to use!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb6f5a", "is_robot_indexable": true, "report_reasons": null, "author": "Junglist4RLife", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb6f5a/reusing_old_pc_vs_selling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb6f5a/reusing_old_pc_vs_selling/", "subreddit_subscribers": 657316, "created_utc": 1670038024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Talk about general topics in our Discussion Thread!\n\n* Try out new software that you liked/hated? \n* Tell us about that $40 2TB MicroSD card from Amazon that's totally not a scam\n* Come show us how much data you lost since you didn't have backups!\n\nTotally not an attempt to build community rapport.", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DataHoarder Discussion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zaybxv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Bi-Weekly Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670016610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Talk about general topics in our Discussion Thread!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Try out new software that you liked/hated? &lt;/li&gt;\n&lt;li&gt;Tell us about that $40 2TB MicroSD card from Amazon that&amp;#39;s totally not a scam&lt;/li&gt;\n&lt;li&gt;Come show us how much data you lost since you didn&amp;#39;t have backups!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Totally not an attempt to build community rapport.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zaybxv", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zaybxv/datahoarder_discussion/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/zaybxv/datahoarder_discussion/", "subreddit_subscribers": 657316, "created_utc": 1670016610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m a code noob.\n\nAside from a few simple concepts I can\u2019t script or code. \n\nI\u2019m wondering if you think it\u2019s feasible to get consistent results with AI generated code.\n\nIt would make my life so much easier. \n\nThis is an example, wdyt?\n\n[Here is an example of a Python script that checks for changes at a URL, and if there are changes, takes a screenshot and saves it to a specific folder:](https://i.imgur.com/fMR00wz.jpg)", "author_fullname": "t2_akf0w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to USE AI to write scripts for data hoarding?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zbm58l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670090171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a code noob.&lt;/p&gt;\n\n&lt;p&gt;Aside from a few simple concepts I can\u2019t script or code. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m wondering if you think it\u2019s feasible to get consistent results with AI generated code.&lt;/p&gt;\n\n&lt;p&gt;It would make my life so much easier. &lt;/p&gt;\n\n&lt;p&gt;This is an example, wdyt?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/fMR00wz.jpg\"&gt;Here is an example of a Python script that checks for changes at a URL, and if there are changes, takes a screenshot and saves it to a specific folder:&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?auto=webp&amp;s=bf755b85c339bc6079c1b3a1ae9cc0ce1a5ccc79", "width": 1048, "height": 1710}, "resolutions": [{"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=360ecb919246bc3e0207c69df707e99d0e0246d1", "width": 108, "height": 176}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ea811c86a0739ff9c729ea1ff0cab77f68c7bcf", "width": 216, "height": 352}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=caa8de9baa73d9cbfdb530833192eab9d6014d73", "width": 320, "height": 522}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f120a979a4bc351aa022af6f4215c667b2a73567", "width": 640, "height": 1044}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bdefd051dc61ac6e920b8aa5a20a2b2f6a33fd19", "width": 960, "height": 1566}], "variants": {}, "id": "D56t9ZeJJiAWGiIrwM0_68X5POaYTVxrgStV49UCEIk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbm58l", "is_robot_indexable": true, "report_reasons": null, "author": "badatmathdave", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbm58l/is_it_possible_to_use_ai_to_write_scripts_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbm58l/is_it_possible_to_use_ai_to_write_scripts_for/", "subreddit_subscribers": 657316, "created_utc": 1670090171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all,\n\nI am in the process of backing up a complete music library from a website (+- 10000 albums).  \nHere are always 2 options to download a flac and mp3 version. When the files are being downloaded individually they have always the same name whithout knowing if you have downloaded the flac or mp3 version.\n\nFilename.rar (contains 1 mp3 album)  \nFilename.rar (contains 1 flac album)\n\nby using jdownloader 2 i always download both and append a number\n\nFilename.rar  \nFilename\\_2.rar\n\nBut i still do not know if the archive contains mp3 or flac.\n\nIs there any piece of software that can open the archive, see what extensions the songs have and append this to the filename?\n\nFilename.rar -&gt; Filename\\_flac.rar  \nFilename\\_2.rar -&gt; Filename\\_2\\_mp3.rar\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_85rup3n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Append to archive name based on extension of contents.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zbkp39", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670086453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I am in the process of backing up a complete music library from a website (+- 10000 albums).&lt;br/&gt;\nHere are always 2 options to download a flac and mp3 version. When the files are being downloaded individually they have always the same name whithout knowing if you have downloaded the flac or mp3 version.&lt;/p&gt;\n\n&lt;p&gt;Filename.rar (contains 1 mp3 album)&lt;br/&gt;\nFilename.rar (contains 1 flac album)&lt;/p&gt;\n\n&lt;p&gt;by using jdownloader 2 i always download both and append a number&lt;/p&gt;\n\n&lt;p&gt;Filename.rar&lt;br/&gt;\nFilename_2.rar&lt;/p&gt;\n\n&lt;p&gt;But i still do not know if the archive contains mp3 or flac.&lt;/p&gt;\n\n&lt;p&gt;Is there any piece of software that can open the archive, see what extensions the songs have and append this to the filename?&lt;/p&gt;\n\n&lt;p&gt;Filename.rar -&amp;gt; Filename_flac.rar&lt;br/&gt;\nFilename_2.rar -&amp;gt; Filename_2_mp3.rar&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbkp39", "is_robot_indexable": true, "report_reasons": null, "author": "carval444", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbkp39/append_to_archive_name_based_on_extension_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbkp39/append_to_archive_name_based_on_extension_of/", "subreddit_subscribers": 657316, "created_utc": 1670086453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought HP LTO-5 Ultrium 3280 FC drive from eBay and have to upgrade firmware.\nBut this file is locked to download and I don't have permission to download it.\nThe URL is; https://support.hpe.com/connect/s/softwaredetails?softwareId=co_154832_1\nDoes anyone help me? Thanks in advance!", "author_fullname": "t2_jnn18zst", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download locked file on HPE support center?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zbk72l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3Hv7ABYSZFObeFGJR-JMYdzL_xKFatrJnNldPXy8jzw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670085046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought HP LTO-5 Ultrium 3280 FC drive from eBay and have to upgrade firmware.\nBut this file is locked to download and I don&amp;#39;t have permission to download it.\nThe URL is; &lt;a href=\"https://support.hpe.com/connect/s/softwaredetails?softwareId=co_154832_1\"&gt;https://support.hpe.com/connect/s/softwaredetails?softwareId=co_154832_1&lt;/a&gt;\nDoes anyone help me? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/6qfgwoea4r3a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?auto=webp&amp;s=56ffe1538c200c8efe74db845f208432839f4867", "width": 1440, "height": 2862}, "resolutions": [{"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=36eba71941689b950e430c7a47313759d96cd4fb", "width": 108, "height": 214}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=01d491c9f50728ff74aaf9a41190c2ef49ff26bd", "width": 216, "height": 429}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=98610143610cf4095d700a321143e58803ac6143", "width": 320, "height": 636}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e2fcc45c42d717f4fdc84e09a7c88d3c17d898e", "width": 640, "height": 1272}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8a83b58f39ac6f150fd45247b554e4e611a59b2", "width": 960, "height": 1908}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fb16bce58540c7884d17bdf07670b1391b04b42", "width": 1080, "height": 2146}], "variants": {}, "id": "mNKFwNb_IuvqdSRmN5qqHuJNjmhyCAGfJ7ysIVhwghU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbk72l", "is_robot_indexable": true, "report_reasons": null, "author": "AMDRadeonHD6950", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbk72l/how_to_download_locked_file_on_hpe_support_center/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/6qfgwoea4r3a1.png", "subreddit_subscribers": 657316, "created_utc": 1670085046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all! I recently filled up my 40tb box that I keep in my local DC (1Gb connection from house to DC) and don\u2019t have room for more drives so I ended up buying another box with the exact same hardware for another 40tb but isn\u2019t in the same rack.\n\nDoes anybody know how to go about making 1 80tb volume? I\u2019d rather not pass the drives directly to the 1st host due to wanting to utilize the second host for processing power along with storage. \n\nI\u2019m using Windows Server Datacenter 2019 for my host OS (yes yes I know, it\u2019s just so easy to join them to my ADDC and create permissions extremely easy).\n\nMy initial thought was to use iscsi to pass the drives over a dedicated 1Gb link (no connection to the host would be more than 1Gb anyway) and just add them to the pool that way. Unless you guys have a better solution? Was thinking maybe clustering but it seems that you only get mirroring and not striping.", "author_fullname": "t2_xkr04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good way to pool storage across 2 physical boxes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbjweh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670084210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all! I recently filled up my 40tb box that I keep in my local DC (1Gb connection from house to DC) and don\u2019t have room for more drives so I ended up buying another box with the exact same hardware for another 40tb but isn\u2019t in the same rack.&lt;/p&gt;\n\n&lt;p&gt;Does anybody know how to go about making 1 80tb volume? I\u2019d rather not pass the drives directly to the 1st host due to wanting to utilize the second host for processing power along with storage. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m using Windows Server Datacenter 2019 for my host OS (yes yes I know, it\u2019s just so easy to join them to my ADDC and create permissions extremely easy).&lt;/p&gt;\n\n&lt;p&gt;My initial thought was to use iscsi to pass the drives over a dedicated 1Gb link (no connection to the host would be more than 1Gb anyway) and just add them to the pool that way. Unless you guys have a better solution? Was thinking maybe clustering but it seems that you only get mirroring and not striping.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbjweh", "is_robot_indexable": true, "report_reasons": null, "author": "Kawaiisampler", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zbjweh/any_good_way_to_pool_storage_across_2_physical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbjweh/any_good_way_to_pool_storage_across_2_physical/", "subreddit_subscribers": 657316, "created_utc": 1670084210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello guys,\n\nI purchased a Storinator Q30 case and are looking to buy components for it ( CPU, RAM and MB)  \nI have 2 \\* lsi 16i cards and an 10gbe card that will be put into the computer aswell.\n\nStorage i have 5\\*10tb and 5\\*18tb with 20 more slots in the case empty, so a build that can handle all the pcie lanes ( im counting 20 lanes) \n\nWas thinking to get an ASUS PRIME Z690-P D4 (DDR4) and Intel Core i9-12900 together with 128gb ram but not sure thats the best path to take? read that QSV is favored because of plex and HW transcoding\n\nThe OS im going to be using for this build is going to be Unraid as im going to run some stuff besides plex. nothing that cpu heavy. ( pihole, \\*arr\u00b4s, grafana. etc) and were wondering if you guys could give me suggestions of what path would be the best to take. cost wise, not really any limits.\n\n&amp;#x200B;\n\nthanks in advice!", "author_fullname": "t2_14mlze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a new server for my data hoarding needs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbcosn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670060747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;/p&gt;\n\n&lt;p&gt;I purchased a Storinator Q30 case and are looking to buy components for it ( CPU, RAM and MB)&lt;br/&gt;\nI have 2 * lsi 16i cards and an 10gbe card that will be put into the computer aswell.&lt;/p&gt;\n\n&lt;p&gt;Storage i have 5*10tb and 5*18tb with 20 more slots in the case empty, so a build that can handle all the pcie lanes ( im counting 20 lanes) &lt;/p&gt;\n\n&lt;p&gt;Was thinking to get an ASUS PRIME Z690-P D4 (DDR4) and Intel Core i9-12900 together with 128gb ram but not sure thats the best path to take? read that QSV is favored because of plex and HW transcoding&lt;/p&gt;\n\n&lt;p&gt;The OS im going to be using for this build is going to be Unraid as im going to run some stuff besides plex. nothing that cpu heavy. ( pihole, *arr\u00b4s, grafana. etc) and were wondering if you guys could give me suggestions of what path would be the best to take. cost wise, not really any limits.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;thanks in advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbcosn", "is_robot_indexable": true, "report_reasons": null, "author": "yompe", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbcosn/building_a_new_server_for_my_data_hoarding_needs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbcosn/building_a_new_server_for_my_data_hoarding_needs/", "subreddit_subscribers": 657316, "created_utc": 1670060747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I've did some research, but rather ask here outright.\n\nThe worry is the availability of a reader 10 years down the line in order to transfer the data onto something newer.\n\nThe issue is I'm on budget and I just can't buy a NAS with data redundancy RAID and stuff right now. About half if not more of my data isn't changing so it might work if I go with write once disks.\n\nIf I do choose this option ... I have lots of HDDs already, I'd like to know some more details about Blu-Ray in terms of reliability, like DVDs would randomly get bad sectors no matter how well you cared about them (or maybe I just had cheap disks?) I would get 1 broken disk per 10 disks in a brand new fresh DVD pack.", "author_fullname": "t2_2hsxrobx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blu-Ray Disk as Data Archival?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbchcf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670059873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;ve did some research, but rather ask here outright.&lt;/p&gt;\n\n&lt;p&gt;The worry is the availability of a reader 10 years down the line in order to transfer the data onto something newer.&lt;/p&gt;\n\n&lt;p&gt;The issue is I&amp;#39;m on budget and I just can&amp;#39;t buy a NAS with data redundancy RAID and stuff right now. About half if not more of my data isn&amp;#39;t changing so it might work if I go with write once disks.&lt;/p&gt;\n\n&lt;p&gt;If I do choose this option ... I have lots of HDDs already, I&amp;#39;d like to know some more details about Blu-Ray in terms of reliability, like DVDs would randomly get bad sectors no matter how well you cared about them (or maybe I just had cheap disks?) I would get 1 broken disk per 10 disks in a brand new fresh DVD pack.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbchcf", "is_robot_indexable": true, "report_reasons": null, "author": "Sloperon", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbchcf/bluray_disk_as_data_archival/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbchcf/bluray_disk_as_data_archival/", "subreddit_subscribers": 657316, "created_utc": 1670059873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For example cheap second hand SATA drives, maybe lots of small drives, mismatched drives.\n\n\n\n\n\n\n\nJust in general, does anyone use unreliable drives and how/what for? I have weak internet so using cheap drives for 'disposable' data somehow would be cool.", "author_fullname": "t2_ijcwy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I use unreliable drives for unimportant storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb77cz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670040503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For example cheap second hand SATA drives, maybe lots of small drives, mismatched drives.&lt;/p&gt;\n\n&lt;p&gt;Just in general, does anyone use unreliable drives and how/what for? I have weak internet so using cheap drives for &amp;#39;disposable&amp;#39; data somehow would be cool.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb77cz", "is_robot_indexable": true, "report_reasons": null, "author": "Pyroven", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb77cz/how_can_i_use_unreliable_drives_for_unimportant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb77cz/how_can_i_use_unreliable_drives_for_unimportant/", "subreddit_subscribers": 657316, "created_utc": 1670040503.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for suggestions on how to backup approx 20TB.\nZero knowledge would be preferable. I'm looking at online as I don't really have the space for any more drives.\nI'm currently using Backblaze personal but I have Veracrypt containers over 500GB which would trip up the restore. B2 would cost a bit for this much data. If I'm backing up to a zero knowledge service then the containers wouldn't be required.\nAny suggestions please?", "author_fullname": "t2_93hxx3dy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to backup 20TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb85fv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670043615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for suggestions on how to backup approx 20TB.\nZero knowledge would be preferable. I&amp;#39;m looking at online as I don&amp;#39;t really have the space for any more drives.\nI&amp;#39;m currently using Backblaze personal but I have Veracrypt containers over 500GB which would trip up the restore. B2 would cost a bit for this much data. If I&amp;#39;m backing up to a zero knowledge service then the containers wouldn&amp;#39;t be required.\nAny suggestions please?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb85fv", "is_robot_indexable": true, "report_reasons": null, "author": "paulschofield_76", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb85fv/how_to_backup_20tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb85fv/how_to_backup_20tb/", "subreddit_subscribers": 657316, "created_utc": 1670043615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Can anyone tell me what the difference is between these 2 WD Hard Drives 0F48155 and WUH722222ALE6L4?", "author_fullname": "t2_fka1auzj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can anyone tell me what the difference is between these 2 WD Hard Drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zazg73", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670018948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anyone tell me what the difference is between these 2 WD Hard Drives 0F48155 and WUH722222ALE6L4?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zazg73", "is_robot_indexable": true, "report_reasons": null, "author": "DependentCapable4820", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zazg73/can_anyone_tell_me_what_the_difference_is_between/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zazg73/can_anyone_tell_me_what_the_difference_is_between/", "subreddit_subscribers": 657316, "created_utc": 1670018948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Background\n\nI'm scheduled to move next year to another continent (11hr plane flight away) and would like to keep a backup of my data at a family members house. This is to store my personal data/linux ISOs etc and is purely for my benefit. Unfortunately, there's no one who can maintain the system in my abscence &amp; I dont have a concrete plan on how often I will be back in my home country.\n\nSo, the question is, how much maintenance does a 6disk array Truenas system need with new drives?\n\nI realise this question is subjective as everyones 'milage may vary' and no guarantee our experiences would be the same.\n\nMy estimates are:\n\n* consumer nas drives last around 3 years from  new\n* sometimes more, sometimes less\n* system would be running z2 so if a drive fails then Id shut down the system and consider my options.\n* Ideally, if scrubbing has issues, SMART detects issues then I'd want to replace the 'bad' drive\n\nQuestion1.  Is it a bad idea to consider having data off-site without anyone to maintain it? Does more drives=more maintenance? \n\nQu2: i.e I could 3 larger drives in Z1 or 6smaller drives in Z2, does one option (typically) require more involvement in replacing faulty drives on average? \n\nI would of course be able to access the drive via ssh/web interface to maintain the nas remotely.\n\nQu3: Finally, in the case that one drive has bad sectors in truenas - is there an easy way to work out which drive has the fault? In the case I had to relay this to someone on site - how do we work out which bad drive out of the 6 is the bad one? \n\nThanks for your advice.", "author_fullname": "t2_d2ja9fnm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much maintenance over the years required for a typical NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zaygnk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670016878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m scheduled to move next year to another continent (11hr plane flight away) and would like to keep a backup of my data at a family members house. This is to store my personal data/linux ISOs etc and is purely for my benefit. Unfortunately, there&amp;#39;s no one who can maintain the system in my abscence &amp;amp; I dont have a concrete plan on how often I will be back in my home country.&lt;/p&gt;\n\n&lt;p&gt;So, the question is, how much maintenance does a 6disk array Truenas system need with new drives?&lt;/p&gt;\n\n&lt;p&gt;I realise this question is subjective as everyones &amp;#39;milage may vary&amp;#39; and no guarantee our experiences would be the same.&lt;/p&gt;\n\n&lt;p&gt;My estimates are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;consumer nas drives last around 3 years from  new&lt;/li&gt;\n&lt;li&gt;sometimes more, sometimes less&lt;/li&gt;\n&lt;li&gt;system would be running z2 so if a drive fails then Id shut down the system and consider my options.&lt;/li&gt;\n&lt;li&gt;Ideally, if scrubbing has issues, SMART detects issues then I&amp;#39;d want to replace the &amp;#39;bad&amp;#39; drive&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Question1.  Is it a bad idea to consider having data off-site without anyone to maintain it? Does more drives=more maintenance? &lt;/p&gt;\n\n&lt;p&gt;Qu2: i.e I could 3 larger drives in Z1 or 6smaller drives in Z2, does one option (typically) require more involvement in replacing faulty drives on average? &lt;/p&gt;\n\n&lt;p&gt;I would of course be able to access the drive via ssh/web interface to maintain the nas remotely.&lt;/p&gt;\n\n&lt;p&gt;Qu3: Finally, in the case that one drive has bad sectors in truenas - is there an easy way to work out which drive has the fault? In the case I had to relay this to someone on site - how do we work out which bad drive out of the 6 is the bad one? &lt;/p&gt;\n\n&lt;p&gt;Thanks for your advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zaygnk", "is_robot_indexable": true, "report_reasons": null, "author": "Maximum-Warning-4186", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zaygnk/how_much_maintenance_over_the_years_required_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zaygnk/how_much_maintenance_over_the_years_required_for/", "subreddit_subscribers": 657316, "created_utc": 1670016878.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI would appreciate any guidance on best practices for cold storage of content. I generate content for projects which is stored on External WD drives. One drive per project with each project consuming 4-8TB of space. Roughly 2-3 projects per quarter. Consequently, I'm starting to get overwhelmed with the number of external drives living in a closet.\n\nThere was a great video posted recently showing a migration to LTO which caught my interest. The size, weight and longevity of the LTO cartridges is very appealing but, wow, the LTO drive is pricey!\n\nIs the move to LTO worth the apparent pain and initial cost?", "author_fullname": "t2_wvopt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question: LTO for Cold Storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zatvnk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670006556.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I would appreciate any guidance on best practices for cold storage of content. I generate content for projects which is stored on External WD drives. One drive per project with each project consuming 4-8TB of space. Roughly 2-3 projects per quarter. Consequently, I&amp;#39;m starting to get overwhelmed with the number of external drives living in a closet.&lt;/p&gt;\n\n&lt;p&gt;There was a great video posted recently showing a migration to LTO which caught my interest. The size, weight and longevity of the LTO cartridges is very appealing but, wow, the LTO drive is pricey!&lt;/p&gt;\n\n&lt;p&gt;Is the move to LTO worth the apparent pain and initial cost?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zatvnk", "is_robot_indexable": true, "report_reasons": null, "author": "Obsidian28", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zatvnk/question_lto_for_cold_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zatvnk/question_lto_for_cold_storage/", "subreddit_subscribers": 657316, "created_utc": 1670006556.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys,\n\nI'm currently using a bunch of old HDDs and a USB dock to do my backups, but things are starting to get out of control. The stuff I'm backing up keeps growing, and my options have become use bigger (and more expensive) HDDs to try and keep the total number of HDDs down, or use lots of smaller HDDs, which are becoming a bit of a pain to store since there's so many of them. I'm looking for a better solution.\n\nI've heard about LTO tapes and noticed I can pick up bare LTO drives for pretty cheap used. The tapes are also very cheap, even brand new! So it seems like LTO would hit a pretty good balance between price and space.\n\nI've been reading up on LTO drives, but there's still a few things I'm unsure of:\n\n1. Do you think LTOs would be a good solution for me?\n2. If they have a SAS connector, you can connect them using any SAS PCIe card, right?\n3. How the hell do you power a bare drive? Can that even be done, or is it REQUIRED that the drive be put into some sort of enclosure with extra electronics? I was hoping to pick up a bare drive and connect it to my server via the SAS port, but I haven't been able to find any information about powering them in this state. I'm not seeing any standard power connectors on any of the drives I've seen, save for the MOLEX plugs on really old LTO drives like LTO3s.\n4. Is using the \"compressed\" option to get maximum space worth it? Or is it agonizingly slow or something?\n5. From what I can see, you can upgrade a \"complete\" LTO setup by swapping out the drive, but is there limitations on that? i.e Newer LTO drives need a connector that the old enclosure doesn't have or maybe the hardware inside it isn't powerful enough for the new drive?\n6. Why they hell are so many drives missing their faceplate? They can be used without it can't they?", "author_fullname": "t2_pry45az", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to switch to LTO tape for backups, would r/DataHoarder recommend that? How do I even power these things?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb5qha", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670036244.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670035904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using a bunch of old HDDs and a USB dock to do my backups, but things are starting to get out of control. The stuff I&amp;#39;m backing up keeps growing, and my options have become use bigger (and more expensive) HDDs to try and keep the total number of HDDs down, or use lots of smaller HDDs, which are becoming a bit of a pain to store since there&amp;#39;s so many of them. I&amp;#39;m looking for a better solution.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve heard about LTO tapes and noticed I can pick up bare LTO drives for pretty cheap used. The tapes are also very cheap, even brand new! So it seems like LTO would hit a pretty good balance between price and space.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been reading up on LTO drives, but there&amp;#39;s still a few things I&amp;#39;m unsure of:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Do you think LTOs would be a good solution for me?&lt;/li&gt;\n&lt;li&gt;If they have a SAS connector, you can connect them using any SAS PCIe card, right?&lt;/li&gt;\n&lt;li&gt;How the hell do you power a bare drive? Can that even be done, or is it REQUIRED that the drive be put into some sort of enclosure with extra electronics? I was hoping to pick up a bare drive and connect it to my server via the SAS port, but I haven&amp;#39;t been able to find any information about powering them in this state. I&amp;#39;m not seeing any standard power connectors on any of the drives I&amp;#39;ve seen, save for the MOLEX plugs on really old LTO drives like LTO3s.&lt;/li&gt;\n&lt;li&gt;Is using the &amp;quot;compressed&amp;quot; option to get maximum space worth it? Or is it agonizingly slow or something?&lt;/li&gt;\n&lt;li&gt;From what I can see, you can upgrade a &amp;quot;complete&amp;quot; LTO setup by swapping out the drive, but is there limitations on that? i.e Newer LTO drives need a connector that the old enclosure doesn&amp;#39;t have or maybe the hardware inside it isn&amp;#39;t powerful enough for the new drive?&lt;/li&gt;\n&lt;li&gt;Why they hell are so many drives missing their faceplate? They can be used without it can&amp;#39;t they?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb5qha", "is_robot_indexable": true, "report_reasons": null, "author": "fireaza", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb5qha/looking_to_switch_to_lto_tape_for_backups_would/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb5qha/looking_to_switch_to_lto_tape_for_backups_would/", "subreddit_subscribers": 657316, "created_utc": 1670035904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI've rethought my design for my next nas (Truenas scale) and am contemplating using 6 x4TB drives via Raid-Z2. I'd like to re-use a PSU I already have (I need to go inspect it otherwise I'd share with you now) and am wondering the following questions:\n\n1. How much PSU wattage do I need?\n2. I heard somewhere that molex converters to Sata power is bad. Is this true?\n3. Is there a 'gold standard' of how to power 6 drives using a typical consumer PSU?\n\nThanks all for your input.", "author_fullname": "t2_d2ja9fnm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSU choice for running 6 nas drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zaxygo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670015848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve rethought my design for my next nas (Truenas scale) and am contemplating using 6 x4TB drives via Raid-Z2. I&amp;#39;d like to re-use a PSU I already have (I need to go inspect it otherwise I&amp;#39;d share with you now) and am wondering the following questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How much PSU wattage do I need?&lt;/li&gt;\n&lt;li&gt;I heard somewhere that molex converters to Sata power is bad. Is this true?&lt;/li&gt;\n&lt;li&gt;Is there a &amp;#39;gold standard&amp;#39; of how to power 6 drives using a typical consumer PSU?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks all for your input.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zaxygo", "is_robot_indexable": true, "report_reasons": null, "author": "Maximum-Warning-4186", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zaxygo/psu_choice_for_running_6_nas_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zaxygo/psu_choice_for_running_6_nas_drives/", "subreddit_subscribers": 657316, "created_utc": 1670015848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I regularly back up my main PC drive to an internal HDD using Macrium Reflect. I just image the full drive and send that image to that HDD. \n\nHowever, I\u2019m paranoid about catastrophes such as fires and such, so I\u2019ve been looking for a way to at least save one image on an offshore cloud. \n\nI tried Macrium Reflect with Microsoft Azure, but my 20mb/s upload speed against a 400GB image file made uploading basically impossible. \n\nI\u2019m not sure what to do. My other alternative is to occasionally back up a single image to an external HDD, and hide that in a fireproof safe or something, but even that\u2019s not foolproof. \n\nIdeas? Ideally not expensive ideas lol.", "author_fullname": "t2_55o33vla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How To Backup Drive to Cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zavlak", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670010544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I regularly back up my main PC drive to an internal HDD using Macrium Reflect. I just image the full drive and send that image to that HDD. &lt;/p&gt;\n\n&lt;p&gt;However, I\u2019m paranoid about catastrophes such as fires and such, so I\u2019ve been looking for a way to at least save one image on an offshore cloud. &lt;/p&gt;\n\n&lt;p&gt;I tried Macrium Reflect with Microsoft Azure, but my 20mb/s upload speed against a 400GB image file made uploading basically impossible. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m not sure what to do. My other alternative is to occasionally back up a single image to an external HDD, and hide that in a fireproof safe or something, but even that\u2019s not foolproof. &lt;/p&gt;\n\n&lt;p&gt;Ideas? Ideally not expensive ideas lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zavlak", "is_robot_indexable": true, "report_reasons": null, "author": "TheBiggestHorseCock", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zavlak/how_to_backup_drive_to_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zavlak/how_to_backup_drive_to_cloud/", "subreddit_subscribers": 657316, "created_utc": 1670010544.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}