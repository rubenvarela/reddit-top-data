{"kind": "Listing", "data": {"after": "t3_zbm58l", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have around 5TB worth of data that I want to consolidate into one place and want to know how would I go around and do such a thing.   \nShort backstory, I had a system with 2 drives, a 500GB SSD and 2TB HDD. Bought a 1TB HDD during start of the pandemic for recording location of recorded meetings. Year 1 into Pandemic, 2TB HDD has an error and on a \"Caution\" status from CrystalDiskInfo. I was also planning to build a new rig that year so I bought one 1TB NVME SSD(not part of the situation), a 2TB Ironwolf HDD and 4TB Barracuda.  \nI didn't want to use my new drives on the PC so I used two 1TB External HDDs to store the data. Then a year later the 1TB I bought at the start of the pandemic was in \"Bad\" condition, so I gave in and plugged my 4TB HDD and transferred my files. For some reason I also plugged in the 2TB HDD. Now all of them are in use.  \n\n\nSo I did a disk analyze using WizTree and found out that I have half of my total storage full, not to mention earlier in the day I got an error that there was not enough space in my boot drive when I was saving stuff into there.  \n\n\nNow how would I go around completely copying over these files into a whole separate drive/drives, eliminating duplicates and make sure everything is copied properly, so that I can completely wipe my current system and basically start from scratch.  \n I would also like to ask on what kind of storage I should use to store this on. I think it will be just a cold storage, with the occasional rummage through, but it might also be another dump place I can continue adding to. I don't want to delete any of the files, more of its a hassle rather than sentimental stuff, but there are stuff that I definitely want to keep there.  I would also say that I don't have much money to put towards storing these. So I guess a solution where I can add on over time would be best.", "author_fullname": "t2_mu7c9au", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would I go about copying around 5TB worth of data, from multiple drives to a singular drive/drives (Shared Pools/Raid)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbhmth", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670077996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have around 5TB worth of data that I want to consolidate into one place and want to know how would I go around and do such a thing.&lt;br/&gt;\nShort backstory, I had a system with 2 drives, a 500GB SSD and 2TB HDD. Bought a 1TB HDD during start of the pandemic for recording location of recorded meetings. Year 1 into Pandemic, 2TB HDD has an error and on a &amp;quot;Caution&amp;quot; status from CrystalDiskInfo. I was also planning to build a new rig that year so I bought one 1TB NVME SSD(not part of the situation), a 2TB Ironwolf HDD and 4TB Barracuda.&lt;br/&gt;\nI didn&amp;#39;t want to use my new drives on the PC so I used two 1TB External HDDs to store the data. Then a year later the 1TB I bought at the start of the pandemic was in &amp;quot;Bad&amp;quot; condition, so I gave in and plugged my 4TB HDD and transferred my files. For some reason I also plugged in the 2TB HDD. Now all of them are in use.  &lt;/p&gt;\n\n&lt;p&gt;So I did a disk analyze using WizTree and found out that I have half of my total storage full, not to mention earlier in the day I got an error that there was not enough space in my boot drive when I was saving stuff into there.  &lt;/p&gt;\n\n&lt;p&gt;Now how would I go around completely copying over these files into a whole separate drive/drives, eliminating duplicates and make sure everything is copied properly, so that I can completely wipe my current system and basically start from scratch.&lt;br/&gt;\n I would also like to ask on what kind of storage I should use to store this on. I think it will be just a cold storage, with the occasional rummage through, but it might also be another dump place I can continue adding to. I don&amp;#39;t want to delete any of the files, more of its a hassle rather than sentimental stuff, but there are stuff that I definitely want to keep there.  I would also say that I don&amp;#39;t have much money to put towards storing these. So I guess a solution where I can add on over time would be best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbhmth", "is_robot_indexable": true, "report_reasons": null, "author": "InfraDelta", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbhmth/how_would_i_go_about_copying_around_5tb_worth_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbhmth/how_would_i_go_about_copying_around_5tb_worth_of/", "subreddit_subscribers": 657349, "created_utc": 1670077996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I\u2019m trying to download a lot of YouTube videos in huge playlist. I have a really fast internet (5gbit/s), but the softwares that I tried (4K video downloaded and Open Video Downloader) are slow, like 3 MB/s for 4k video download and 1MB/s for Oen video downloader. I founded some online websites with a lot of stupid ads, like https://x2download.app/ , that download at a really fast speed, but they aren\u2019t good for download more than few videos at once. What do you use? I have both windows, Linux and Mac.", "author_fullname": "t2_b80xl4j2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best software for download YouTube videos and playlist in mass", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbn1ke", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670092438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019m trying to download a lot of YouTube videos in huge playlist. I have a really fast internet (5gbit/s), but the softwares that I tried (4K video downloaded and Open Video Downloader) are slow, like 3 MB/s for 4k video download and 1MB/s for Oen video downloader. I founded some online websites with a lot of stupid ads, like &lt;a href=\"https://x2download.app/\"&gt;https://x2download.app/&lt;/a&gt; , that download at a really fast speed, but they aren\u2019t good for download more than few videos at once. What do you use? I have both windows, Linux and Mac.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bBZtnOZdA4YFrPl8m5wpdkUIGtT2JyV2kLHwlKLfkI8.jpg?auto=webp&amp;s=2ed9d33e203acca30d6a9235bb3f536609a105df", "width": 496, "height": 251}, "resolutions": [{"url": "https://external-preview.redd.it/bBZtnOZdA4YFrPl8m5wpdkUIGtT2JyV2kLHwlKLfkI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b73398f420d1ea825f5cb7c61acd8e4b921825d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/bBZtnOZdA4YFrPl8m5wpdkUIGtT2JyV2kLHwlKLfkI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e505c0a39a37c1deb12ef8cc438f9e5e1ce6d39b", "width": 216, "height": 109}, {"url": "https://external-preview.redd.it/bBZtnOZdA4YFrPl8m5wpdkUIGtT2JyV2kLHwlKLfkI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b0758c1f43eae0dd0e9bd3287520e17df2bd82b", "width": 320, "height": 161}], "variants": {}, "id": "RzHaQHhYQQZo_4fJo6oQXmSdOd-gIK8_CoINdfSoBAA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbn1ke", "is_robot_indexable": true, "report_reasons": null, "author": "StrengthLocal2543", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbn1ke/best_software_for_download_youtube_videos_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbn1ke/best_software_for_download_youtube_videos_and/", "subreddit_subscribers": 657349, "created_utc": 1670092438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nI bought one of these at the $109.99 price (about $118 with tax, delivered) to add as an 8th drive on a DS1821+. After Black Friday it is now back up to $119.99:\n\n[https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y](https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y)\n\nAll other slots are populated with a mix of helium and non-helium 8TB drives, all MyBook and EasyStore shucks. I could not find any information on the Internet regarding whether this drive would honor the disable head parking command. It does.\n\nThe warranty was advertised as 2 years. WD warranty checker lists the warranty as 3 years.\n\nThis WD80EAZZ arrived with idle3-tools reporting head parking at 80 (8 seconds). This drive honors the idle3-tools command to disable head parking. I ran idle3-tools via direct sata connection from an Ubuntu 20.04 server.\n\nTo do this from Ubuntu:\n\nsudo apt update\n\nsudo apt install idle3-tools\n\nsudo fdisk -l (to list what disk label your WD80EAZZ is set at)\n\nsudo idle3ctl -g /dev/sdX (replace \"X\" with the correct drive label)\n\n(verify that drive respond to the idle3 command, and lists the parking time as 80)\n\nsudo idle3ctl -d /dev/sdX (replace \"X\" with the correct drive label)\n\n(you should get a \"head parking disabled\" message)\n\nShutoff the computer. Fully poweroff the drive. Reboot and run\n\nsudo idle3ctl -g /dev/sdX (replace \"X\" with the correct drive label)\n\n...to verify that the head parking disabled setting survived poweroff.\n\nThis drive appears to be mechanically identical to the white label non-helium EasyStore/MyBook drives. But, it does arrive with head parking set to 80.\n\nThis drive seems like a steal at this price. I had 2 4TB Reds fail within the warranty period. Due to the failed Reds, I gave up on Reds and started buying white label 8TB drives. I have had zero white label drives fail so far. I am sure one white will fail tomorrow.", "author_fullname": "t2_46ha7mck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Blue 8TB Hard Drive WD80EAZZ 5640RPM CMR $109.99", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb178f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670022957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought one of these at the $109.99 price (about $118 with tax, delivered) to add as an 8th drive on a DS1821+. After Black Friday it is now back up to $119.99:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y\"&gt;https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;All other slots are populated with a mix of helium and non-helium 8TB drives, all MyBook and EasyStore shucks. I could not find any information on the Internet regarding whether this drive would honor the disable head parking command. It does.&lt;/p&gt;\n\n&lt;p&gt;The warranty was advertised as 2 years. WD warranty checker lists the warranty as 3 years.&lt;/p&gt;\n\n&lt;p&gt;This WD80EAZZ arrived with idle3-tools reporting head parking at 80 (8 seconds). This drive honors the idle3-tools command to disable head parking. I ran idle3-tools via direct sata connection from an Ubuntu 20.04 server.&lt;/p&gt;\n\n&lt;p&gt;To do this from Ubuntu:&lt;/p&gt;\n\n&lt;p&gt;sudo apt update&lt;/p&gt;\n\n&lt;p&gt;sudo apt install idle3-tools&lt;/p&gt;\n\n&lt;p&gt;sudo fdisk -l (to list what disk label your WD80EAZZ is set at)&lt;/p&gt;\n\n&lt;p&gt;sudo idle3ctl -g /dev/sdX (replace &amp;quot;X&amp;quot; with the correct drive label)&lt;/p&gt;\n\n&lt;p&gt;(verify that drive respond to the idle3 command, and lists the parking time as 80)&lt;/p&gt;\n\n&lt;p&gt;sudo idle3ctl -d /dev/sdX (replace &amp;quot;X&amp;quot; with the correct drive label)&lt;/p&gt;\n\n&lt;p&gt;(you should get a &amp;quot;head parking disabled&amp;quot; message)&lt;/p&gt;\n\n&lt;p&gt;Shutoff the computer. Fully poweroff the drive. Reboot and run&lt;/p&gt;\n\n&lt;p&gt;sudo idle3ctl -g /dev/sdX (replace &amp;quot;X&amp;quot; with the correct drive label)&lt;/p&gt;\n\n&lt;p&gt;...to verify that the head parking disabled setting survived poweroff.&lt;/p&gt;\n\n&lt;p&gt;This drive appears to be mechanically identical to the white label non-helium EasyStore/MyBook drives. But, it does arrive with head parking set to 80.&lt;/p&gt;\n\n&lt;p&gt;This drive seems like a steal at this price. I had 2 4TB Reds fail within the warranty period. Due to the failed Reds, I gave up on Reds and started buying white label 8TB drives. I have had zero white label drives fail so far. I am sure one white will fail tomorrow.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb178f", "is_robot_indexable": true, "report_reasons": null, "author": "CottonBambino", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb178f/wd_blue_8tb_hard_drive_wd80eazz_5640rpm_cmr_10999/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb178f/wd_blue_8tb_hard_drive_wd80eazz_5640rpm_cmr_10999/", "subreddit_subscribers": 657349, "created_utc": 1670022957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reminder to backup up your data! 5 month old ADATA SSD Failure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": true, "name": "t3_zbsc58", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_odtvi", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/c6ZFKQ7JVOCF0jg0AO52CeWpTuzSk6hq8KQ7B4KP-WI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "pcmasterrace", "selftext": "", "author_fullname": "t2_odtvi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Reminder to backup up your data! 5 month old ADATA SSD Failure", "link_flair_richtext": [{"e": "text", "t": "NSFMR"}], "subreddit_name_prefixed": "r/pcmasterrace", "hidden": false, "pwls": 6, "link_flair_css_class": "red", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": true, "media_metadata": {"8gh0drxx8r3a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 68, "x": 108, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=467c6b9aeca1853aae4afedede89671e468ed6d9"}, {"y": 136, "x": 216, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de01e9c1b36b463b62456e3f6571c69151ed6d86"}, {"y": 201, "x": 320, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f1b4050501da4df186d3012704b967e4e62dd3af"}, {"y": 403, "x": 640, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d652b57fb88f89a47060aa6eba4ca090446bae86"}, {"y": 605, "x": 960, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7e154adf575548c1e8c7519e94c81a299ce197a"}, {"y": 680, "x": 1080, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=23ca023ae7e4d2b5db23023fbfef46af21ca4596"}], "s": {"y": 2199, "x": 3489, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=3489&amp;format=pjpg&amp;auto=webp&amp;s=c664512290e4e874b038cee82d88d4f9dcfd0230"}, "id": "8gh0drxx8r3a1"}, "8li9yryx8r3a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0be58dc406ebfc3060a3b9950f96e04919d0f5f9"}, {"y": 152, "x": 216, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be2d802ea06dd13e01a688d864087906c7384695"}, {"y": 225, "x": 320, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ca8f5f1e5338faf6c176691cae54983c121e3f1"}, {"y": 451, "x": 640, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=013bd21e2dfd6141f53c776893291f2a47810d75"}, {"y": 676, "x": 960, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ed35fba989543c53543f759f175d6809095e48e5"}, {"y": 761, "x": 1080, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=901c4629a266928d0c3fd2d9432723ce29d7b28a"}], "s": {"y": 2816, "x": 3994, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=3994&amp;format=pjpg&amp;auto=webp&amp;s=e6ffe2ff213f2a61014d949bed019d450321bc8b"}, "id": "8li9yryx8r3a1"}, "f2j1efzx8r3a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8fafd849d4e87690af25a3769c71b1f72def8ef2"}, {"y": 125, "x": 216, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c251b4eefd6ea98fe5354c45efabde285db131f"}, {"y": 185, "x": 320, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=04806382b74b637dfead07d632ab90bb75838866"}, {"y": 371, "x": 640, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=957edbc397f6e587bf129efa4c7ee879e5706c89"}, {"y": 557, "x": 960, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=96053e9b4b138e52ae0c2580172abe216ff7d017"}, {"y": 626, "x": 1080, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6966b64aee329d93f347e4d7dd93d4878c1ed8dc"}], "s": {"y": 1998, "x": 3442, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=3442&amp;format=pjpg&amp;auto=webp&amp;s=61b590496d3fec4ba0778b279e24bce32710051d"}, "id": "f2j1efzx8r3a1"}, "lak43zxx8r3a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 64, "x": 108, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f2d1e9403f49050ecca445266abe5ac94904c2c"}, {"y": 129, "x": 216, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a701d655f6550ade97f26713e9fc9044c5e31448"}, {"y": 192, "x": 320, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=be4d9aa835c8c9524ae1a0abb2d0b98bf37996e0"}, {"y": 384, "x": 640, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4cb99076471feb6426c41481707e01db5d707dc4"}, {"y": 576, "x": 960, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=52607c9a610d5afd13f5c7beebb3315abc09fa2a"}, {"y": 648, "x": 1080, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8dc7600018490b28d77cf29fd89d18cd3182a625"}], "s": {"y": 2056, "x": 3425, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=3425&amp;format=pjpg&amp;auto=webp&amp;s=58ec36c8ad0a08a882e379bae5ad68e1c9d88060"}, "id": "lak43zxx8r3a1"}}, "name": "t3_zbs22g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "That's a lot of bad blocks", "media_id": "8gh0drxx8r3a1", "id": 215627376}, {"media_id": "8li9yryx8r3a1", "id": 215627377}, {"caption": "Even the log file is corrupt!", "media_id": "lak43zxx8r3a1", "id": 215627378}, {"media_id": "f2j1efzx8r3a1", "id": 215627379}]}, "link_flair_text": "NSFMR", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/c6ZFKQ7JVOCF0jg0AO52CeWpTuzSk6hq8KQ7B4KP-WI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670104896.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zbs22g", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4c6e9074-3133-11e4-b380-12313b12e896", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sgp1", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff0000", "id": "zbs22g", "is_robot_indexable": true, "report_reasons": null, "author": "hboyd2003", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/pcmasterrace/comments/zbs22g/reminder_to_backup_up_your_data_5_month_old_adata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zbs22g", "subreddit_subscribers": 6898317, "created_utc": 1670104896.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1670105590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zbs22g", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbsc58", "is_robot_indexable": true, "report_reasons": null, "author": "hboyd2003", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_zbs22g", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbsc58/reminder_to_backup_up_your_data_5_month_old_adata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zbs22g", "subreddit_subscribers": 657349, "created_utc": 1670105590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI'm planning a 6 drive (storage) truenas build + 1 ssd for the OS drive.\n\nMy mobo has 6 sata ports and so consequently, I'm one short. \n\nTo utilise my 6 sata ports for the 6 data drives. I understand there is the option to boot the OS from USB (converted to Sata). Alternatively, I could look to get an HBA card (pci express --&gt; sata) but hear there are compatibility issues via this route. If you know of a good HBA card thats not too expensive, I'd love to hear.\n\nAny guidance here would be appreciated!", "author_fullname": "t2_d2ja9fnm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "6 drive nas - HDA card question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbowdf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670097168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning a 6 drive (storage) truenas build + 1 ssd for the OS drive.&lt;/p&gt;\n\n&lt;p&gt;My mobo has 6 sata ports and so consequently, I&amp;#39;m one short. &lt;/p&gt;\n\n&lt;p&gt;To utilise my 6 sata ports for the 6 data drives. I understand there is the option to boot the OS from USB (converted to Sata). Alternatively, I could look to get an HBA card (pci express --&amp;gt; sata) but hear there are compatibility issues via this route. If you know of a good HBA card thats not too expensive, I&amp;#39;d love to hear.&lt;/p&gt;\n\n&lt;p&gt;Any guidance here would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbowdf", "is_robot_indexable": true, "report_reasons": null, "author": "Maximum-Warning-4186", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbowdf/6_drive_nas_hda_card_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbowdf/6_drive_nas_hda_card_question/", "subreddit_subscribers": 657349, "created_utc": 1670097168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought HP LTO-5 Ultrium 3280 FC drive from eBay and have to upgrade firmware.\nBut this file is locked to download and I don't have permission to download it.\nThe URL is; https://support.hpe.com/connect/s/softwaredetails?softwareId=co_154832_1\nDoes anyone help me? Thanks in advance!", "author_fullname": "t2_jnn18zst", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download locked file on HPE support center?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zbk72l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3Hv7ABYSZFObeFGJR-JMYdzL_xKFatrJnNldPXy8jzw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670085046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought HP LTO-5 Ultrium 3280 FC drive from eBay and have to upgrade firmware.\nBut this file is locked to download and I don&amp;#39;t have permission to download it.\nThe URL is; &lt;a href=\"https://support.hpe.com/connect/s/softwaredetails?softwareId=co_154832_1\"&gt;https://support.hpe.com/connect/s/softwaredetails?softwareId=co_154832_1&lt;/a&gt;\nDoes anyone help me? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/6qfgwoea4r3a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?auto=webp&amp;s=56ffe1538c200c8efe74db845f208432839f4867", "width": 1440, "height": 2862}, "resolutions": [{"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=36eba71941689b950e430c7a47313759d96cd4fb", "width": 108, "height": 214}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=01d491c9f50728ff74aaf9a41190c2ef49ff26bd", "width": 216, "height": 429}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=98610143610cf4095d700a321143e58803ac6143", "width": 320, "height": 636}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e2fcc45c42d717f4fdc84e09a7c88d3c17d898e", "width": 640, "height": 1272}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8a83b58f39ac6f150fd45247b554e4e611a59b2", "width": 960, "height": 1908}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fb16bce58540c7884d17bdf07670b1391b04b42", "width": 1080, "height": 2146}], "variants": {}, "id": "mNKFwNb_IuvqdSRmN5qqHuJNjmhyCAGfJ7ysIVhwghU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbk72l", "is_robot_indexable": true, "report_reasons": null, "author": "AMDRadeonHD6950", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbk72l/how_to_download_locked_file_on_hpe_support_center/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/6qfgwoea4r3a1.png", "subreddit_subscribers": 657349, "created_utc": 1670085046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I've did some research, but rather ask here outright.\n\nThe worry is the availability of a reader 10 years down the line in order to transfer the data onto something newer.\n\nThe issue is I'm on budget and I just can't buy a NAS with data redundancy RAID and stuff right now. About half if not more of my data isn't changing so it might work if I go with write once disks.\n\nIf I do choose this option ... I have lots of HDDs already, I'd like to know some more details about Blu-Ray in terms of reliability, like DVDs would randomly get bad sectors no matter how well you cared about them (or maybe I just had cheap disks?) I would get 1 broken disk per 10 disks in a brand new fresh DVD pack.", "author_fullname": "t2_2hsxrobx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blu-Ray Disk as Data Archival?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbchcf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670059873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;ve did some research, but rather ask here outright.&lt;/p&gt;\n\n&lt;p&gt;The worry is the availability of a reader 10 years down the line in order to transfer the data onto something newer.&lt;/p&gt;\n\n&lt;p&gt;The issue is I&amp;#39;m on budget and I just can&amp;#39;t buy a NAS with data redundancy RAID and stuff right now. About half if not more of my data isn&amp;#39;t changing so it might work if I go with write once disks.&lt;/p&gt;\n\n&lt;p&gt;If I do choose this option ... I have lots of HDDs already, I&amp;#39;d like to know some more details about Blu-Ray in terms of reliability, like DVDs would randomly get bad sectors no matter how well you cared about them (or maybe I just had cheap disks?) I would get 1 broken disk per 10 disks in a brand new fresh DVD pack.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbchcf", "is_robot_indexable": true, "report_reasons": null, "author": "Sloperon", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbchcf/bluray_disk_as_data_archival/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbchcf/bluray_disk_as_data_archival/", "subreddit_subscribers": 657349, "created_utc": 1670059873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I use Kodi for my HTPC, and I like having 4K photos from around the world randomly cycling as a screensaver... but it'd be nice if I could set it up to automatically grab new images.\n\nBest source I know of for good 4K photos from around the world is the Bing daily wallpaper. And there are sites like [this](https://bingwallpaper.anerg.com/) that have them archived. Three questions:\n\n1) How could I scrape a site like that to just grab the 4K images, and also, name the files with each photo's caption;\n\n2) What other sources of good 4K images of the real world are there; and\n\n3) How can I automate the downloading? It'd be nice if, every day, my system would just grab new images for me.", "author_fullname": "t2_fdefm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to scrape Bing and other wallpapers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb7037", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670039880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use Kodi for my HTPC, and I like having 4K photos from around the world randomly cycling as a screensaver... but it&amp;#39;d be nice if I could set it up to automatically grab new images.&lt;/p&gt;\n\n&lt;p&gt;Best source I know of for good 4K photos from around the world is the Bing daily wallpaper. And there are sites like &lt;a href=\"https://bingwallpaper.anerg.com/\"&gt;this&lt;/a&gt; that have them archived. Three questions:&lt;/p&gt;\n\n&lt;p&gt;1) How could I scrape a site like that to just grab the 4K images, and also, name the files with each photo&amp;#39;s caption;&lt;/p&gt;\n\n&lt;p&gt;2) What other sources of good 4K images of the real world are there; and&lt;/p&gt;\n\n&lt;p&gt;3) How can I automate the downloading? It&amp;#39;d be nice if, every day, my system would just grab new images for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb7037", "is_robot_indexable": true, "report_reasons": null, "author": "Slackroyd", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb7037/how_to_scrape_bing_and_other_wallpapers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb7037/how_to_scrape_bing_and_other_wallpapers/", "subreddit_subscribers": 657349, "created_utc": 1670039880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I want to finally upgrade from my current system (Z87-G45 motherboard, Intel 4770k, 32GB DDR3) and build a new system.  \n\n\nI also want to stop paying for Dropbox and have my own NAS that replicates that functionality as well as be a media server to stream my owned content.  \n\n\nSo, I was thinking of just reusing my current system for the NAS, but here are the drawbacks from my understanding: 1 GB Ethernet on motherboard, no built-in WIFI, and no NVME slots (does have MSATA though).  \n\n\nI could buy a 10G LAN card (100 dollars) as well as an adapter card (15?) to be able to use an NVME drive for caching.  Or, I could sell the motherboard, CPU, RAM, and AIO combo and use that money to build the cheapest newish system using a motherboard that has at least a 2.5G LAN port.  \n\n\nI have never built a server nor have had any kind of network storage before, so this is all a new experience for me.   \n\n\nWhat would you do or recommend me do?\n\nAny advice would be appreciated, including which OS/software system to use!!!", "author_fullname": "t2_2h4tdnqt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reusing Old PC vs Selling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb6f5a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670038024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I want to finally upgrade from my current system (Z87-G45 motherboard, Intel 4770k, 32GB DDR3) and build a new system.  &lt;/p&gt;\n\n&lt;p&gt;I also want to stop paying for Dropbox and have my own NAS that replicates that functionality as well as be a media server to stream my owned content.  &lt;/p&gt;\n\n&lt;p&gt;So, I was thinking of just reusing my current system for the NAS, but here are the drawbacks from my understanding: 1 GB Ethernet on motherboard, no built-in WIFI, and no NVME slots (does have MSATA though).  &lt;/p&gt;\n\n&lt;p&gt;I could buy a 10G LAN card (100 dollars) as well as an adapter card (15?) to be able to use an NVME drive for caching.  Or, I could sell the motherboard, CPU, RAM, and AIO combo and use that money to build the cheapest newish system using a motherboard that has at least a 2.5G LAN port.  &lt;/p&gt;\n\n&lt;p&gt;I have never built a server nor have had any kind of network storage before, so this is all a new experience for me.   &lt;/p&gt;\n\n&lt;p&gt;What would you do or recommend me do?&lt;/p&gt;\n\n&lt;p&gt;Any advice would be appreciated, including which OS/software system to use!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb6f5a", "is_robot_indexable": true, "report_reasons": null, "author": "Junglist4RLife", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb6f5a/reusing_old_pc_vs_selling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb6f5a/reusing_old_pc_vs_selling/", "subreddit_subscribers": 657349, "created_utc": 1670038024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a complete beginner when it comes to technology, and I really need your help because I genuinely don't understand specs.\n\nI'm getting an external hard drive for a laptop that doesn't have room for an internal hard drive. I'm trying to pick the most reliable and best of them.\n\n1. [Basic Portable Drive 2 TB](https://www.seagate.com/gb/en/products/external-hard-drives/basic-external-hard-drive/)\n2. [Toshiba Canvio Ready](https://www.toshiba-storage.com/products/toshiba-portable-hard-drives-canvio-ready/)\n3. [Toshiba Canvio Basics](https://storage.toshiba.com/consumer-hdd/external/canvio-basics)\n\nFinally, I have one more option - buying this [docking station](https://gembird.com/item.aspx?id=9536) to connect one of the internal HDDs (links below) to my laptop - I'm not sure if this is a good alternative or if it's reliable at all. \n\n1. [Toshiba P300 Desktop PC](https://www.toshiba-storage.com/products/toshiba-internal-hard-drives-p300/)\n2. [WD Blue PC Desktop Hard Drive](https://www.westerndigital.com/products/internal-drives/wd-blue-desktop-sata-hdd#WD20EZBX)\n\n**I apologize if this is a stupid and bothersome question**, but as a girl (who knows very little about tech and) is looking for a good photo and video storage solution, I hope you won't mind helping me with this.", "author_fullname": "t2_kh69bbbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which one of these is the best photo storage solution? Thank you in advance.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbqeu6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670100896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a complete beginner when it comes to technology, and I really need your help because I genuinely don&amp;#39;t understand specs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m getting an external hard drive for a laptop that doesn&amp;#39;t have room for an internal hard drive. I&amp;#39;m trying to pick the most reliable and best of them.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.seagate.com/gb/en/products/external-hard-drives/basic-external-hard-drive/\"&gt;Basic Portable Drive 2 TB&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.toshiba-storage.com/products/toshiba-portable-hard-drives-canvio-ready/\"&gt;Toshiba Canvio Ready&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://storage.toshiba.com/consumer-hdd/external/canvio-basics\"&gt;Toshiba Canvio Basics&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Finally, I have one more option - buying this &lt;a href=\"https://gembird.com/item.aspx?id=9536\"&gt;docking station&lt;/a&gt; to connect one of the internal HDDs (links below) to my laptop - I&amp;#39;m not sure if this is a good alternative or if it&amp;#39;s reliable at all. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.toshiba-storage.com/products/toshiba-internal-hard-drives-p300/\"&gt;Toshiba P300 Desktop PC&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.westerndigital.com/products/internal-drives/wd-blue-desktop-sata-hdd#WD20EZBX\"&gt;WD Blue PC Desktop Hard Drive&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;I apologize if this is a stupid and bothersome question&lt;/strong&gt;, but as a girl (who knows very little about tech and) is looking for a good photo and video storage solution, I hope you won&amp;#39;t mind helping me with this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbqeu6", "is_robot_indexable": true, "report_reasons": null, "author": "the-emotional-emu", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbqeu6/which_one_of_these_is_the_best_photo_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbqeu6/which_one_of_these_is_the_best_photo_storage/", "subreddit_subscribers": 657349, "created_utc": 1670100896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have an existing computer computer which currently runs unraid.  I got a free to me 2 bay QNAP a while back and have since used that as my NAS for a while.  Now that the existing unraid computer is just serving 5-10 docker containers for putzing around and not on storage duty, I'd like to dabble into ZFS and actually make it a nicer NAS and use the QNAP as a backup.  I snagged three 20TB Ironwolf Pros and am going to rebuild the machine tomorrow.\n\nAfter reading lots of guides and watching videos, I think I have a plan:   \n   \nThe computer\n------------   \n* i5-7600\n* [motherboard](https://www.gigabyte.com/Motherboard/GA-Z270X-Gaming-K7-rev-10#kf)\n* 32 GB DDR4\n* 256GB Samsung 830 SSD\n* 2 Hodgepodge-y 4TB rust (probably won't re-use)\n* 3x of the aforementioned Ironwolf Pro 20TB (ST20000NEZ00)   \n\nInstall   \n-------   \n* Ubuntu Server (22.04?) or Debian Bullseye, install on SSD as \"normal\"\n* Install zfs as per [this guide](https://openzfs.github.io/openzfs-docs/Getting%20Started/index.html)\n* set up a raidz1 pool, and other settings as per the guide on the wiki here     \n   \nQuestions   \n---------   \n* One of the guides says to enable compression... is that really a good idea?   \n* Do I really need to create datasets?  Seems logical maybe?  I figured the main use of this NAS will be media/movies, then a small portion for a main backup, with my older QNAP NAS being the real \"backup\".  any pros/cons to creating datasets vs just using directories?   \n* I've seen some folks say to scrub once a week, others say once a month, does it matter?   \n* are snapshots really worth doing?  maybe weekly or monthly or something?   \n* any particular tools/monitoring tools you like using?   \n* any particular BIOS or other hardware-y type settings that matter?", "author_fullname": "t2_j4g0k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building/Rebuilding server, need ZFS advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb8o3a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670045361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an existing computer computer which currently runs unraid.  I got a free to me 2 bay QNAP a while back and have since used that as my NAS for a while.  Now that the existing unraid computer is just serving 5-10 docker containers for putzing around and not on storage duty, I&amp;#39;d like to dabble into ZFS and actually make it a nicer NAS and use the QNAP as a backup.  I snagged three 20TB Ironwolf Pros and am going to rebuild the machine tomorrow.&lt;/p&gt;\n\n&lt;p&gt;After reading lots of guides and watching videos, I think I have a plan:   &lt;/p&gt;\n\n&lt;h2&gt;The computer&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;i5-7600&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.gigabyte.com/Motherboard/GA-Z270X-Gaming-K7-rev-10#kf\"&gt;motherboard&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;32 GB DDR4&lt;/li&gt;\n&lt;li&gt;256GB Samsung 830 SSD&lt;/li&gt;\n&lt;li&gt;2 Hodgepodge-y 4TB rust (probably won&amp;#39;t re-use)&lt;/li&gt;\n&lt;li&gt;3x of the aforementioned Ironwolf Pro 20TB (ST20000NEZ00)&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Install   &lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ubuntu Server (22.04?) or Debian Bullseye, install on SSD as &amp;quot;normal&amp;quot;&lt;/li&gt;\n&lt;li&gt;Install zfs as per &lt;a href=\"https://openzfs.github.io/openzfs-docs/Getting%20Started/index.html\"&gt;this guide&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;set up a raidz1 pool, and other settings as per the guide on the wiki here&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Questions   &lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;One of the guides says to enable compression... is that really a good idea?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Do I really need to create datasets?  Seems logical maybe?  I figured the main use of this NAS will be media/movies, then a small portion for a main backup, with my older QNAP NAS being the real &amp;quot;backup&amp;quot;.  any pros/cons to creating datasets vs just using directories?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve seen some folks say to scrub once a week, others say once a month, does it matter?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;are snapshots really worth doing?  maybe weekly or monthly or something?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;any particular tools/monitoring tools you like using?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;any particular BIOS or other hardware-y type settings that matter?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gd-XgGWoQSWb_PunVE-qNEM_8rmzskSKHgZ9r16z6wA.jpg?auto=webp&amp;s=22225accca5c684e6f3124aba9f86b0a35bacfa1", "width": 1000, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/gd-XgGWoQSWb_PunVE-qNEM_8rmzskSKHgZ9r16z6wA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ca3feae19de36209c0aece3b0bfad340a6c7052f", "width": 108, "height": 70}, {"url": "https://external-preview.redd.it/gd-XgGWoQSWb_PunVE-qNEM_8rmzskSKHgZ9r16z6wA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af81478e17619dd20cf414bdfff16b212669cb74", "width": 216, "height": 140}, {"url": "https://external-preview.redd.it/gd-XgGWoQSWb_PunVE-qNEM_8rmzskSKHgZ9r16z6wA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f89406265402dd5bf7acec72908d4dda51c4ee40", "width": 320, "height": 208}, {"url": "https://external-preview.redd.it/gd-XgGWoQSWb_PunVE-qNEM_8rmzskSKHgZ9r16z6wA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=437a77915a882912bff8e7643289c5d5c322179d", "width": 640, "height": 416}, {"url": "https://external-preview.redd.it/gd-XgGWoQSWb_PunVE-qNEM_8rmzskSKHgZ9r16z6wA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e547b710de8f29f274fe59e3de552b9c7fb14e89", "width": 960, "height": 624}], "variants": {}, "id": "YLt_FlzniWHuyqed9xHhZUKiPsbpH2MBs9VFfr3-NTc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb8o3a", "is_robot_indexable": true, "report_reasons": null, "author": "impala454", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb8o3a/buildingrebuilding_server_need_zfs_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb8o3a/buildingrebuilding_server_need_zfs_advice/", "subreddit_subscribers": 657349, "created_utc": 1670045361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "In the beginning of the year I've bought a cheap SSD SomnAmbulist 120GB. After a couple of benchmarks, I've noted some strange throughput measurements. Briefly:\n\n- **Initial Benchmark** | [image](https://imgur.com/FCsKFt0)  \n  Just after format, the throughput is according to the advertised specifications at 510MB/s read and 440MB/s write.  \n  It starts to drop at 35% capacity written and stabilizes at 200~300MB/s read and 40MB/s write.  \n  Although this drop is not mentioned by the seller's ads, it is indeed an expected behavior and should return to normal after a couple of minutes in idle (flush the cache into the permanent area).\n\n- **Benchmark after 1 hour in idle** | [image](https://imgur.com/mHliXkO)  \n  Unfortunately, the throughput has not returned to normal (yet?).  \n  315MB/s read and 440MB/s write, and the speed drop started earlier at 10% capacity written.\n\n- **Benchmark after 12 hours in idle** | [image](https://imgur.com/KVLn0Fy)  \n  The throughput still has not returned to normal.\n\n\n**Questions**\n\n1. Is this the expected behavior for such cheap SSD or indeed there is something strange?\n2. What might be going wrong here?\n3. Any idea how to fix this throughput?\n\n----------------------------------------------\n\n**Debug Info**\n\n- SSD/TRIM Supported\n- fstrim enabled\n- SSD APM (Advanced Power Management) not supported  \n- f3 (Fight Flash Fraud) OK  \n\n```\n## SSD/TRIM SUPPORTED\nroot@debian:~# hdparm -I /dev/sda | grep -i \"trim\\|power\"\n       *\tPower Management feature set\n       *\tData Set Management TRIM supported (limit 8 blocks)\n       *\tDeterministic read data after TRIM\n\n\n## SSD/TRIM AVAILABLE (DISC-MAX NON ZERO)\nroot@debian:~# lsblk -D /dev/sda\nNAME DISC-ALN DISC-GRAN DISC-MAX DISC-ZERO\nsda         0      512B       2G         0\n\n\n## ADVANCED POWER MANAGEMENT NOT SUPPORTED\nroot@debian:~# hdparm -B /dev/sda\n\n/dev/sda:\n APM_level\t= not supported\n\n\n\n## FSTRIM ENABLE\n\nroot@debian:~# systemctl status fstrim.timer\n\u25cf fstrim.timer - Discard unused blocks once a week\n     Loaded: loaded (/lib/systemd/system/fstrim.timer; enabled; vendor preset: enabled)\n     Active: active (waiting) since Sat 2022-12-03 03:36:10 UTC; 35min ago\n    Trigger: Mon 2022-12-05 00:51:25 UTC; 1 day 20h left\n   Triggers: \u25cf fstrim.service\n       Docs: man:fstrim\n\nDec 03 03:36:10 debian systemd[1]: Started Discard unused blocks once a week.\n\n\nroot@debian:~# systemctl status fstrim\n\u25cf fstrim.service - Discard unused blocks on filesystems from /etc/fstab\n     Loaded: loaded (/lib/systemd/system/fstrim.service; static)\n     Active: inactive (dead)\nTriggeredBy: \u25cf fstrim.timer\n       Docs: man:fstrim(8)\n\n\nroot@debian:~# fstrim -av\n```", "author_fullname": "t2_j0e8md80", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheap SSD: Slow Speed Issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb6dz0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670044296.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670037915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the beginning of the year I&amp;#39;ve bought a cheap SSD SomnAmbulist 120GB. After a couple of benchmarks, I&amp;#39;ve noted some strange throughput measurements. Briefly:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Initial Benchmark&lt;/strong&gt; | &lt;a href=\"https://imgur.com/FCsKFt0\"&gt;image&lt;/a&gt;&lt;br/&gt;\nJust after format, the throughput is according to the advertised specifications at 510MB/s read and 440MB/s write.&lt;br/&gt;\nIt starts to drop at 35% capacity written and stabilizes at 200~300MB/s read and 40MB/s write.&lt;br/&gt;\nAlthough this drop is not mentioned by the seller&amp;#39;s ads, it is indeed an expected behavior and should return to normal after a couple of minutes in idle (flush the cache into the permanent area).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Benchmark after 1 hour in idle&lt;/strong&gt; | &lt;a href=\"https://imgur.com/mHliXkO\"&gt;image&lt;/a&gt;&lt;br/&gt;\nUnfortunately, the throughput has not returned to normal (yet?).&lt;br/&gt;\n315MB/s read and 440MB/s write, and the speed drop started earlier at 10% capacity written.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Benchmark after 12 hours in idle&lt;/strong&gt; | &lt;a href=\"https://imgur.com/KVLn0Fy\"&gt;image&lt;/a&gt;&lt;br/&gt;\nThe throughput still has not returned to normal.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is this the expected behavior for such cheap SSD or indeed there is something strange?&lt;/li&gt;\n&lt;li&gt;What might be going wrong here?&lt;/li&gt;\n&lt;li&gt;Any idea how to fix this throughput?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;&lt;strong&gt;Debug Info&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SSD/TRIM Supported&lt;/li&gt;\n&lt;li&gt;fstrim enabled&lt;/li&gt;\n&lt;li&gt;SSD APM (Advanced Power Management) not supported&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;f3 (Fight Flash Fraud) OK&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;h2&gt;SSD/TRIM SUPPORTED&lt;/h2&gt;\n\n&lt;p&gt;root@debian:~# hdparm -I /dev/sda | grep -i &amp;quot;trim|power&amp;quot;\n       *    Power Management feature set\n       *    Data Set Management TRIM supported (limit 8 blocks)\n       *    Deterministic read data after TRIM&lt;/p&gt;\n\n&lt;h2&gt;SSD/TRIM AVAILABLE (DISC-MAX NON ZERO)&lt;/h2&gt;\n\n&lt;p&gt;root@debian:~# lsblk -D /dev/sda\nNAME DISC-ALN DISC-GRAN DISC-MAX DISC-ZERO\nsda         0      512B       2G         0&lt;/p&gt;\n\n&lt;h2&gt;ADVANCED POWER MANAGEMENT NOT SUPPORTED&lt;/h2&gt;\n\n&lt;p&gt;root@debian:~# hdparm -B /dev/sda&lt;/p&gt;\n\n&lt;p&gt;/dev/sda:\n APM_level  = not supported&lt;/p&gt;\n\n&lt;h2&gt;FSTRIM ENABLE&lt;/h2&gt;\n\n&lt;p&gt;root@debian:~# systemctl status fstrim.timer\n\u25cf fstrim.timer - Discard unused blocks once a week\n     Loaded: loaded (/lib/systemd/system/fstrim.timer; enabled; vendor preset: enabled)\n     Active: active (waiting) since Sat 2022-12-03 03:36:10 UTC; 35min ago\n    Trigger: Mon 2022-12-05 00:51:25 UTC; 1 day 20h left\n   Triggers: \u25cf fstrim.service\n       Docs: man:fstrim&lt;/p&gt;\n\n&lt;p&gt;Dec 03 03:36:10 debian systemd[1]: Started Discard unused blocks once a week.&lt;/p&gt;\n\n&lt;p&gt;root@debian:~# systemctl status fstrim\n\u25cf fstrim.service - Discard unused blocks on filesystems from /etc/fstab\n     Loaded: loaded (/lib/systemd/system/fstrim.service; static)\n     Active: inactive (dead)\nTriggeredBy: \u25cf fstrim.timer\n       Docs: man:fstrim(8)&lt;/p&gt;\n\n&lt;p&gt;root@debian:~# fstrim -av\n```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tFolgDkT-6MRwNGve2X4AOxAKXc-RKbzrve2F7d6U1U.jpg?auto=webp&amp;s=5c78586ef611c3b5f59b3af867140d6a58de8ee9", "width": 600, "height": 315}, "resolutions": [{"url": "https://external-preview.redd.it/tFolgDkT-6MRwNGve2X4AOxAKXc-RKbzrve2F7d6U1U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c6284fb44ae80e97a08025e961c9186f605cb408", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/tFolgDkT-6MRwNGve2X4AOxAKXc-RKbzrve2F7d6U1U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a43d2dd8f89e5cc09823f0a673acaf383d095f7a", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/tFolgDkT-6MRwNGve2X4AOxAKXc-RKbzrve2F7d6U1U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=92d21491b7e1d3e1bf7750dff0c8bb53e2b1e3fe", "width": 320, "height": 168}], "variants": {}, "id": "YqPoNI41Sv8sjvPeZxiZ1d45Dcng8Xoabu_EPPX9NsE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zb6dz0", "is_robot_indexable": true, "report_reasons": null, "author": "Jeron_Baffom", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb6dz0/cheap_ssd_slow_speed_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb6dz0/cheap_ssd_slow_speed_issue/", "subreddit_subscribers": 657349, "created_utc": 1670037915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was planning on making a raid 5 array from 2 12 tb hdds and and 1 raid 0 array made from 3 4 tb hdds. The 12tb are 7200 rpm while the 3 \\* 4 hdds are 5400 rpm. Is this possible and more so not stupid? I know raid 10 is basically raid 1 + 0 but I'm not sure if it has to be implemented in a certain way to work. Using mdadm from linux kernel if your wondering. Google wasn't answering my question.", "author_fullname": "t2_1v9ndvba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "can a raid array be made from another raid array?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbny9t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "RAID configuration", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670094748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was planning on making a raid 5 array from 2 12 tb hdds and and 1 raid 0 array made from 3 4 tb hdds. The 12tb are 7200 rpm while the 3 * 4 hdds are 5400 rpm. Is this possible and more so not stupid? I know raid 10 is basically raid 1 + 0 but I&amp;#39;m not sure if it has to be implemented in a certain way to work. Using mdadm from linux kernel if your wondering. Google wasn&amp;#39;t answering my question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zbny9t", "is_robot_indexable": true, "report_reasons": null, "author": "quantumechanicalhose", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbny9t/can_a_raid_array_be_made_from_another_raid_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbny9t/can_a_raid_array_be_made_from_another_raid_array/", "subreddit_subscribers": 657349, "created_utc": 1670094748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all,\n\nI am in the process of backing up a complete music library from a website (+- 10000 albums).  \nHere are always 2 options to download a flac and mp3 version. When the files are being downloaded individually they have always the same name whithout knowing if you have downloaded the flac or mp3 version.\n\nFilename.rar (contains 1 mp3 album)  \nFilename.rar (contains 1 flac album)\n\nby using jdownloader 2 i always download both and append a number\n\nFilename.rar  \nFilename\\_2.rar\n\nBut i still do not know if the archive contains mp3 or flac.\n\nIs there any piece of software that can open the archive, see what extensions the songs have and append this to the filename?\n\nFilename.rar -&gt; Filename\\_flac.rar  \nFilename\\_2.rar -&gt; Filename\\_2\\_mp3.rar\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_85rup3n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Append to archive name based on extension of contents.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbkp39", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670086453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I am in the process of backing up a complete music library from a website (+- 10000 albums).&lt;br/&gt;\nHere are always 2 options to download a flac and mp3 version. When the files are being downloaded individually they have always the same name whithout knowing if you have downloaded the flac or mp3 version.&lt;/p&gt;\n\n&lt;p&gt;Filename.rar (contains 1 mp3 album)&lt;br/&gt;\nFilename.rar (contains 1 flac album)&lt;/p&gt;\n\n&lt;p&gt;by using jdownloader 2 i always download both and append a number&lt;/p&gt;\n\n&lt;p&gt;Filename.rar&lt;br/&gt;\nFilename_2.rar&lt;/p&gt;\n\n&lt;p&gt;But i still do not know if the archive contains mp3 or flac.&lt;/p&gt;\n\n&lt;p&gt;Is there any piece of software that can open the archive, see what extensions the songs have and append this to the filename?&lt;/p&gt;\n\n&lt;p&gt;Filename.rar -&amp;gt; Filename_flac.rar&lt;br/&gt;\nFilename_2.rar -&amp;gt; Filename_2_mp3.rar&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbkp39", "is_robot_indexable": true, "report_reasons": null, "author": "carval444", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbkp39/append_to_archive_name_based_on_extension_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbkp39/append_to_archive_name_based_on_extension_of/", "subreddit_subscribers": 657349, "created_utc": 1670086453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello there!\n\nFirst of all, sorry for my bad englisch german native here :) \n\nI have a question about ZFS-Replication, perhaps someone have some infos.\n\nI have a Main-Storage System with ZFS and ECC RAM, the Second-Storage-System is running ZFS without ECC.\n\nIn my opinion it's ok to run the second storage system without ECC, because the Main System have.\n\nThe Second System is only for storing zfs-snapshot-replication as Backup\n\nSince i'm safe against bit root on my main storage after the data are writen to disk there is no chance to transfer corrupted data on my second storage system even without ecc on the second system.\n\nIf i have a bit-root in ram on the second system while replication, zfs would check the different parity and resend it.\n\nDo I have a flaw in my thinking?", "author_fullname": "t2_8pe5532a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ZFS Snapshot Replication without ECC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbjjh1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670083236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there!&lt;/p&gt;\n\n&lt;p&gt;First of all, sorry for my bad englisch german native here :) &lt;/p&gt;\n\n&lt;p&gt;I have a question about ZFS-Replication, perhaps someone have some infos.&lt;/p&gt;\n\n&lt;p&gt;I have a Main-Storage System with ZFS and ECC RAM, the Second-Storage-System is running ZFS without ECC.&lt;/p&gt;\n\n&lt;p&gt;In my opinion it&amp;#39;s ok to run the second storage system without ECC, because the Main System have.&lt;/p&gt;\n\n&lt;p&gt;The Second System is only for storing zfs-snapshot-replication as Backup&lt;/p&gt;\n\n&lt;p&gt;Since i&amp;#39;m safe against bit root on my main storage after the data are writen to disk there is no chance to transfer corrupted data on my second storage system even without ecc on the second system.&lt;/p&gt;\n\n&lt;p&gt;If i have a bit-root in ram on the second system while replication, zfs would check the different parity and resend it.&lt;/p&gt;\n\n&lt;p&gt;Do I have a flaw in my thinking?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbjjh1", "is_robot_indexable": true, "report_reasons": null, "author": "Vertax1337", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbjjh1/zfs_snapshot_replication_without_ecc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbjjh1/zfs_snapshot_replication_without_ecc/", "subreddit_subscribers": 657349, "created_utc": 1670083236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\nI've already searched the threads but it looks like the provided links are now out of date.  \nI need to download a couple books off Scribd, not documents that are user-uploaded, but ebooks Scribd provides. They are too large to download one page at a time without taking too much time. If you know of any ebook rippers for Scribd I would be grateful.\n\nThank you!", "author_fullname": "t2_69i9rgfi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working downloader for Scribd books in late 2022 (not documents)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbhd7s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670077226.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve already searched the threads but it looks like the provided links are now out of date.&lt;br/&gt;\nI need to download a couple books off Scribd, not documents that are user-uploaded, but ebooks Scribd provides. They are too large to download one page at a time without taking too much time. If you know of any ebook rippers for Scribd I would be grateful.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbhd7s", "is_robot_indexable": true, "report_reasons": null, "author": "throw_away_bay_bay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbhd7s/working_downloader_for_scribd_books_in_late_2022/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbhd7s/working_downloader_for_scribd_books_in_late_2022/", "subreddit_subscribers": 657349, "created_utc": 1670077226.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey Guys,I've recently had a weird failure mode on my unraid box and thought I would consult the hive-mind for ideas &amp; thoughts.\n\n**TLDR:**\n\nI think that somehow port/channel 3 on the first port of my LSI 9240-8i is bad. Swapping breakout cable results in the same sata port being bad.\n\nHas anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?\n\n**Key system info:**\n\n* It's been up and happy for over a year.\n* I'm using a LSI 9240-8i in IT mode as an HBA\n* I'm using 3 x 14TB WD drives all attached to a single SAS SFF-8087 to 4x SATA Cable\n* 1 x Parity disk and 2 x data disks\n\n**What happened:**\n\n1. Woke up in the morning to a scheduled parity check having failed. Looking at the logs. It failed not long after it started. The second data disk was marked by unRaid as \"Device disabled, contents emulated\".\n2. Ok, no biggie - the disk has failed. Stopped the array, unassigned the failed disk and shut down. Ordered a replacement.\n3. While waiting for the replacement, I pulled the failed disk and had a look at it with CrystalDiskInfo. Good SMART health and zero errors of any kind. Hmmm.\n4. New disk arrives. Slap it into the machine (same cable as the 'failed' disk) and unraid offers to rebuild the array to it. Click go.\n5. Almost immediately, read errors are reported, rebuild fails and the array is stuck in some kind of read-check paused status. Shit.\n6. Hmm. Maybe it's a bad cable? Shut down and swap the SAS SFF-8087 to 4x SATA Cable for a spare. Plug in both the new drive as well as the 'failed' drive (I suspect the drive itself is actually fine, so I may add it as a second parity later). New drive is on port 3 of the breakout cable (where the failed drive had been) and the 'failed' drive is now on port 4.\n7. Boot up unraid and assign the original 'failed' drive to the array as a new disk. Start the array and unraid starts to rebuild the array with no issues!\n8. Go to my unassigned disks and I can see the new drive. Spin it up and run a SMART test. SMART test fails. Twice. Errors in the log:\n\n&amp;#x200B;\n\n    Dec  3 12:11:06 tower  emhttpd: spinning up /dev/sdd\n    Dec  3 12:11:17 tower  emhttpd: read SMART /dev/sdd\n    Dec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronizing SCSI cache\n    Dec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronize Cache(10) failed: Result: hostbyte=0x01 driverbyte=DRIVER_OK\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 27344764928 512-byte logical blocks: (14.0 TB/12.7 TiB)\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 4096-byte physical blocks\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write Protect is off\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Mode Sense: 7f 00 10 08\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write cache: enabled, read cache: enabled, supports DPO and FUA\n    Dec  3 12:12:53 tower kernel: sdd: sdd1\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Attached SCSI disk\n    Dec  3 12:12:54 tower unassigned.devices: Disk with ID 'TOSHIBA_HDWG21E_Z1A0A0CCFP7G (sdd)' is not set to auto mount.\n\n**Conclusion:**\n\nI think the cable was actually fine and somehow port/channel 3 on the first port of my LSI 9240-8i is somehow bad.\n\nHas anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?", "author_fullname": "t2_ckt3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debugging LSI 9240-8i in IT mode", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbfyvu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670073114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Guys,I&amp;#39;ve recently had a weird failure mode on my unraid box and thought I would consult the hive-mind for ideas &amp;amp; thoughts.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I think that somehow port/channel 3 on the first port of my LSI 9240-8i is bad. Swapping breakout cable results in the same sata port being bad.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key system info:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It&amp;#39;s been up and happy for over a year.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m using a LSI 9240-8i in IT mode as an HBA&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m using 3 x 14TB WD drives all attached to a single SAS SFF-8087 to 4x SATA Cable&lt;/li&gt;\n&lt;li&gt;1 x Parity disk and 2 x data disks&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What happened:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Woke up in the morning to a scheduled parity check having failed. Looking at the logs. It failed not long after it started. The second data disk was marked by unRaid as &amp;quot;Device disabled, contents emulated&amp;quot;.&lt;/li&gt;\n&lt;li&gt;Ok, no biggie - the disk has failed. Stopped the array, unassigned the failed disk and shut down. Ordered a replacement.&lt;/li&gt;\n&lt;li&gt;While waiting for the replacement, I pulled the failed disk and had a look at it with CrystalDiskInfo. Good SMART health and zero errors of any kind. Hmmm.&lt;/li&gt;\n&lt;li&gt;New disk arrives. Slap it into the machine (same cable as the &amp;#39;failed&amp;#39; disk) and unraid offers to rebuild the array to it. Click go.&lt;/li&gt;\n&lt;li&gt;Almost immediately, read errors are reported, rebuild fails and the array is stuck in some kind of read-check paused status. Shit.&lt;/li&gt;\n&lt;li&gt;Hmm. Maybe it&amp;#39;s a bad cable? Shut down and swap the SAS SFF-8087 to 4x SATA Cable for a spare. Plug in both the new drive as well as the &amp;#39;failed&amp;#39; drive (I suspect the drive itself is actually fine, so I may add it as a second parity later). New drive is on port 3 of the breakout cable (where the failed drive had been) and the &amp;#39;failed&amp;#39; drive is now on port 4.&lt;/li&gt;\n&lt;li&gt;Boot up unraid and assign the original &amp;#39;failed&amp;#39; drive to the array as a new disk. Start the array and unraid starts to rebuild the array with no issues!&lt;/li&gt;\n&lt;li&gt;Go to my unassigned disks and I can see the new drive. Spin it up and run a SMART test. SMART test fails. Twice. Errors in the log:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Dec  3 12:11:06 tower  emhttpd: spinning up /dev/sdd\nDec  3 12:11:17 tower  emhttpd: read SMART /dev/sdd\nDec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronizing SCSI cache\nDec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronize Cache(10) failed: Result: hostbyte=0x01 driverbyte=DRIVER_OK\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 27344764928 512-byte logical blocks: (14.0 TB/12.7 TiB)\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 4096-byte physical blocks\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write Protect is off\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Mode Sense: 7f 00 10 08\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write cache: enabled, read cache: enabled, supports DPO and FUA\nDec  3 12:12:53 tower kernel: sdd: sdd1\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Attached SCSI disk\nDec  3 12:12:54 tower unassigned.devices: Disk with ID &amp;#39;TOSHIBA_HDWG21E_Z1A0A0CCFP7G (sdd)&amp;#39; is not set to auto mount.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I think the cable was actually fine and somehow port/channel 3 on the first port of my LSI 9240-8i is somehow bad.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbfyvu", "is_robot_indexable": true, "report_reasons": null, "author": "kabadisha", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbfyvu/debugging_lsi_92408i_in_it_mode/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbfyvu/debugging_lsi_92408i_in_it_mode/", "subreddit_subscribers": 657349, "created_utc": 1670073114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello guys,\n\nI purchased a Storinator Q30 case and are looking to buy components for it ( CPU, RAM and MB)  \nI have 2 \\* lsi 16i cards and an 10gbe card that will be put into the computer aswell.\n\nStorage i have 5\\*10tb and 5\\*18tb with 20 more slots in the case empty, so a build that can handle all the pcie lanes ( im counting 20 lanes) \n\nWas thinking to get an ASUS PRIME Z690-P D4 (DDR4) and Intel Core i9-12900 together with 128gb ram but not sure thats the best path to take? read that QSV is favored because of plex and HW transcoding\n\nThe OS im going to be using for this build is going to be Unraid as im going to run some stuff besides plex. nothing that cpu heavy. ( pihole, \\*arr\u00b4s, grafana. etc) and were wondering if you guys could give me suggestions of what path would be the best to take. cost wise, not really any limits.\n\n&amp;#x200B;\n\nthanks in advice!", "author_fullname": "t2_14mlze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a new server for my data hoarding needs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbcosn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670060747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;/p&gt;\n\n&lt;p&gt;I purchased a Storinator Q30 case and are looking to buy components for it ( CPU, RAM and MB)&lt;br/&gt;\nI have 2 * lsi 16i cards and an 10gbe card that will be put into the computer aswell.&lt;/p&gt;\n\n&lt;p&gt;Storage i have 5*10tb and 5*18tb with 20 more slots in the case empty, so a build that can handle all the pcie lanes ( im counting 20 lanes) &lt;/p&gt;\n\n&lt;p&gt;Was thinking to get an ASUS PRIME Z690-P D4 (DDR4) and Intel Core i9-12900 together with 128gb ram but not sure thats the best path to take? read that QSV is favored because of plex and HW transcoding&lt;/p&gt;\n\n&lt;p&gt;The OS im going to be using for this build is going to be Unraid as im going to run some stuff besides plex. nothing that cpu heavy. ( pihole, *arr\u00b4s, grafana. etc) and were wondering if you guys could give me suggestions of what path would be the best to take. cost wise, not really any limits.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;thanks in advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbcosn", "is_robot_indexable": true, "report_reasons": null, "author": "yompe", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbcosn/building_a_new_server_for_my_data_hoarding_needs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbcosn/building_a_new_server_for_my_data_hoarding_needs/", "subreddit_subscribers": 657349, "created_utc": 1670060747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There's this one Flash Game (Recess Special Operations) that i've wanted to replay for years, but it doesn't exist anywhere\n\n* not on Wayback Machine\n* not even on Flashpoint, which had almost every other game I could think of\n\nonly 2 levels have ever been recovered out of 7\n\nI'm willing to pay a reward if anyone is able to recover it, I just don't know if such a service is provided (or if im allowed to request that here)\n\nthe details about it are here if anyone is curious [https://lostmediawiki.com/Recess\\_Special\\_Operations\\_(partially\\_found\\_Flash\\_game\\_of\\_Disney\\_Channel\\_animated\\_series;\\_early\\_2000s)](https://lostmediawiki.com/Recess_Special_Operations_(partially_found_Flash_game_of_Disney_Channel_animated_series;_early_2000s))\n\nOriginal Link to the Game [https://web.archive.org/web/20041126051651/http://psc.disney.go.com/abcnetworks/toondisney/games/recess\\_special\\_ops/index.html](https://web.archive.org/web/20041126051651/http://psc.disney.go.com/abcnetworks/toondisney/games/recess_special_ops/index.html)\n\nCurrent Archived Level 2 and Level 5, which lack the background music\n\n[https://www.gamesflow.com/jeux.php?id=234718](https://www.gamesflow.com/jeux.php?id=234718)\n\n[https://www.gamesflow.com/jeux.php?id=234719](https://www.gamesflow.com/jeux.php?id=234719)", "author_fullname": "t2_13oll9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any Paid Reward Services to recover lost media?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb8roa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670045908.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670045705.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s this one Flash Game (Recess Special Operations) that i&amp;#39;ve wanted to replay for years, but it doesn&amp;#39;t exist anywhere&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;not on Wayback Machine&lt;/li&gt;\n&lt;li&gt;not even on Flashpoint, which had almost every other game I could think of&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;only 2 levels have ever been recovered out of 7&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m willing to pay a reward if anyone is able to recover it, I just don&amp;#39;t know if such a service is provided (or if im allowed to request that here)&lt;/p&gt;\n\n&lt;p&gt;the details about it are here if anyone is curious &lt;a href=\"https://lostmediawiki.com/Recess_Special_Operations_(partially_found_Flash_game_of_Disney_Channel_animated_series;_early_2000s\"&gt;https://lostmediawiki.com/Recess_Special_Operations_(partially_found_Flash_game_of_Disney_Channel_animated_series;_early_2000s)&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Original Link to the Game &lt;a href=\"https://web.archive.org/web/20041126051651/http://psc.disney.go.com/abcnetworks/toondisney/games/recess_special_ops/index.html\"&gt;https://web.archive.org/web/20041126051651/http://psc.disney.go.com/abcnetworks/toondisney/games/recess_special_ops/index.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Current Archived Level 2 and Level 5, which lack the background music&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.gamesflow.com/jeux.php?id=234718\"&gt;https://www.gamesflow.com/jeux.php?id=234718&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.gamesflow.com/jeux.php?id=234719\"&gt;https://www.gamesflow.com/jeux.php?id=234719&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb8roa", "is_robot_indexable": true, "report_reasons": null, "author": "Falconflyer75", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb8roa/are_there_any_paid_reward_services_to_recover/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb8roa/are_there_any_paid_reward_services_to_recover/", "subreddit_subscribers": 657349, "created_utc": 1670045705.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "~My problems are listed at the bottom~\nSpecs: Windows 11, Intel 10th Gen, Rtx 2070, 16GB ram, various ssds and Hdds. \nPreface: (50k photos down to 12k):(2wks labor) \nCurrent protocols: \n- Duplicate Cleaner Pro (Favorite) \n- Dupeguru (similar to czkawka) ~Do not like the reference file feature and lack of sorting. \n- Czkawka (lack of gui, ease and usefulness in my case) ~ only used for strict dupes\n- Antidupl (One of my favs, incredible how it can determine blurriness and highlight differences, no matter how subtle (side by side) ~ sucks how it forces you to delete the file rather than move it. NEVER DELETE DUPES OR OVERWRITE NAMES FOR THAT MATTER. \n-Bad Peggy (Good for finding bad recoveries in a master folder) \n-Acdsee Ultimate (Good for cataloging media but lacks automation and options) \n-ON1 ( lots of bugs, similar to most DAM, better at identify similar photos then acdsee) \n\n\nWhat is my problem??? Let me explain what happened\n\n1st - I have recovered a lot of old jumper drives that may have been forgotten or abandon due to corruption, these contain deleted files from &lt; high school-college. \n\n2nd - Combined recovered photos and videos with my two back up protocols Amazon photos and Google photos. Reasoning is both services had similar libraries but several time periods missing from each. \n3rd - Also combined my current hdd backups (physically active) that may or may not have been backed up on the cloud. (Ex.different phones + backups, unsupported cloud formats) \n************************************\nIssues I have not been able to address via current solutions:(Photos first priority for now)\n1. I have a lot of burst shots. \n- need a tool that will allow me to use multiple monitors (have 3 - 27\u201d monitors, but it\u2019s fine if it supports 2) to compare say 6-10 similar images span across monitors to make them as large as possible making it easier to pick the ones I want to keep. \n2. Nonsense images, ie: screenshots, album art, porn, icon packs.\n- no matter how many filters I use nothing quite narrows it down enough to find or categorize them all. Need auto categorization based on image recognition. \n\nLastly all files are NTFS, JPEG, MP4 or similar shot from various smart phones. Snap chat and photo editors robbed me of most exif data.", "author_fullname": "t2_74lo7d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Similar image or picture finder, that lets you compare groups of similar images; 3 or more at a time on multi monitors.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb7hnw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670041435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;~My problems are listed at the bottom~\nSpecs: Windows 11, Intel 10th Gen, Rtx 2070, 16GB ram, various ssds and Hdds. \nPreface: (50k photos down to 12k):(2wks labor) \nCurrent protocols: \n- Duplicate Cleaner Pro (Favorite) \n- Dupeguru (similar to czkawka) ~Do not like the reference file feature and lack of sorting. \n- Czkawka (lack of gui, ease and usefulness in my case) ~ only used for strict dupes\n- Antidupl (One of my favs, incredible how it can determine blurriness and highlight differences, no matter how subtle (side by side) ~ sucks how it forces you to delete the file rather than move it. NEVER DELETE DUPES OR OVERWRITE NAMES FOR THAT MATTER. \n-Bad Peggy (Good for finding bad recoveries in a master folder) \n-Acdsee Ultimate (Good for cataloging media but lacks automation and options) \n-ON1 ( lots of bugs, similar to most DAM, better at identify similar photos then acdsee) &lt;/p&gt;\n\n&lt;p&gt;What is my problem??? Let me explain what happened&lt;/p&gt;\n\n&lt;p&gt;1st - I have recovered a lot of old jumper drives that may have been forgotten or abandon due to corruption, these contain deleted files from &amp;lt; high school-college. &lt;/p&gt;\n\n&lt;p&gt;2nd - Combined recovered photos and videos with my two back up protocols Amazon photos and Google photos. Reasoning is both services had similar libraries but several time periods missing from each. \n3rd - Also combined my current hdd backups (physically active) that may or may not have been backed up on the cloud. (Ex.different phones + backups, unsupported cloud formats) &lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Issues I have not been able to address via current solutions:(Photos first priority for now)\n1. I have a lot of burst shots. \n- need a tool that will allow me to use multiple monitors (have 3 - 27\u201d monitors, but it\u2019s fine if it supports 2) to compare say 6-10 similar images span across monitors to make them as large as possible making it easier to pick the ones I want to keep. \n2. Nonsense images, ie: screenshots, album art, porn, icon packs.\n- no matter how many filters I use nothing quite narrows it down enough to find or categorize them all. Need auto categorization based on image recognition. &lt;/p&gt;\n\n&lt;p&gt;Lastly all files are NTFS, JPEG, MP4 or similar shot from various smart phones. Snap chat and photo editors robbed me of most exif data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb7hnw", "is_robot_indexable": true, "report_reasons": null, "author": "xironskier05x", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb7hnw/similar_image_or_picture_finder_that_lets_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb7hnw/similar_image_or_picture_finder_that_lets_you/", "subreddit_subscribers": 657349, "created_utc": 1670041435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyone have a process they use to sort 1000s of unsorted photos? Cosplay rip downloaded from a site is like 4000 photos all in one folder. I'd like to get all the photos organized into the character\\\\shoot\\\\etc. Other than manually going through and moving to folder anyone have a program, app, script or something they are aware of which can determine similar photos (using AI maybe) and sort them?\n\nThanks", "author_fullname": "t2_1f6ljebz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Program\\Process to sort a bunch of unsorted photos.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbomv1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670096483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have a process they use to sort 1000s of unsorted photos? Cosplay rip downloaded from a site is like 4000 photos all in one folder. I&amp;#39;d like to get all the photos organized into the character\\shoot\\etc. Other than manually going through and moving to folder anyone have a program, app, script or something they are aware of which can determine similar photos (using AI maybe) and sort them?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbomv1", "is_robot_indexable": true, "report_reasons": null, "author": "basicallyahurricane", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbomv1/programprocess_to_sort_a_bunch_of_unsorted_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbomv1/programprocess_to_sort_a_bunch_of_unsorted_photos/", "subreddit_subscribers": 657349, "created_utc": 1670096483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all! I recently filled up my 40tb box that I keep in my local DC (1Gb connection from house to DC) and don\u2019t have room for more drives so I ended up buying another box with the exact same hardware for another 40tb but isn\u2019t in the same rack.\n\nDoes anybody know how to go about making 1 80tb volume? I\u2019d rather not pass the drives directly to the 1st host due to wanting to utilize the second host for processing power along with storage. \n\nI\u2019m using Windows Server Datacenter 2019 for my host OS (yes yes I know, it\u2019s just so easy to join them to my ADDC and create permissions extremely easy).\n\nMy initial thought was to use iscsi to pass the drives over a dedicated 1Gb link (no connection to the host would be more than 1Gb anyway) and just add them to the pool that way. Unless you guys have a better solution? Was thinking maybe clustering but it seems that you only get mirroring and not striping.", "author_fullname": "t2_xkr04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good way to pool storage across 2 physical boxes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbjweh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670084210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all! I recently filled up my 40tb box that I keep in my local DC (1Gb connection from house to DC) and don\u2019t have room for more drives so I ended up buying another box with the exact same hardware for another 40tb but isn\u2019t in the same rack.&lt;/p&gt;\n\n&lt;p&gt;Does anybody know how to go about making 1 80tb volume? I\u2019d rather not pass the drives directly to the 1st host due to wanting to utilize the second host for processing power along with storage. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m using Windows Server Datacenter 2019 for my host OS (yes yes I know, it\u2019s just so easy to join them to my ADDC and create permissions extremely easy).&lt;/p&gt;\n\n&lt;p&gt;My initial thought was to use iscsi to pass the drives over a dedicated 1Gb link (no connection to the host would be more than 1Gb anyway) and just add them to the pool that way. Unless you guys have a better solution? Was thinking maybe clustering but it seems that you only get mirroring and not striping.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbjweh", "is_robot_indexable": true, "report_reasons": null, "author": "Kawaiisampler", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zbjweh/any_good_way_to_pool_storage_across_2_physical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbjweh/any_good_way_to_pool_storage_across_2_physical/", "subreddit_subscribers": 657349, "created_utc": 1670084210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for suggestions on how to backup approx 20TB.\nZero knowledge would be preferable. I'm looking at online as I don't really have the space for any more drives.\nI'm currently using Backblaze personal but I have Veracrypt containers over 500GB which would trip up the restore. B2 would cost a bit for this much data. If I'm backing up to a zero knowledge service then the containers wouldn't be required.\nAny suggestions please?", "author_fullname": "t2_93hxx3dy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to backup 20TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb85fv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670043615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for suggestions on how to backup approx 20TB.\nZero knowledge would be preferable. I&amp;#39;m looking at online as I don&amp;#39;t really have the space for any more drives.\nI&amp;#39;m currently using Backblaze personal but I have Veracrypt containers over 500GB which would trip up the restore. B2 would cost a bit for this much data. If I&amp;#39;m backing up to a zero knowledge service then the containers wouldn&amp;#39;t be required.\nAny suggestions please?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb85fv", "is_robot_indexable": true, "report_reasons": null, "author": "paulschofield_76", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb85fv/how_to_backup_20tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb85fv/how_to_backup_20tb/", "subreddit_subscribers": 657349, "created_utc": 1670043615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For example cheap second hand SATA drives, maybe lots of small drives, mismatched drives.\n\n\n\n\n\n\n\nJust in general, does anyone use unreliable drives and how/what for? I have weak internet so using cheap drives for 'disposable' data somehow would be cool.", "author_fullname": "t2_ijcwy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I use unreliable drives for unimportant storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zb77cz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670040503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For example cheap second hand SATA drives, maybe lots of small drives, mismatched drives.&lt;/p&gt;\n\n&lt;p&gt;Just in general, does anyone use unreliable drives and how/what for? I have weak internet so using cheap drives for &amp;#39;disposable&amp;#39; data somehow would be cool.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zb77cz", "is_robot_indexable": true, "report_reasons": null, "author": "Pyroven", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zb77cz/how_can_i_use_unreliable_drives_for_unimportant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zb77cz/how_can_i_use_unreliable_drives_for_unimportant/", "subreddit_subscribers": 657349, "created_utc": 1670040503.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m a code noob.\n\nAside from a few simple concepts I can\u2019t script or code. \n\nI\u2019m wondering if you think it\u2019s feasible to get consistent results with AI generated code.\n\nIt would make my life so much easier. \n\nThis is an example, wdyt?\n\n[Here is an example of a Python script that checks for changes at a URL, and if there are changes, takes a screenshot and saves it to a specific folder:](https://i.imgur.com/fMR00wz.jpg)", "author_fullname": "t2_akf0w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to USE AI to write scripts for data hoarding?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbm58l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670090171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a code noob.&lt;/p&gt;\n\n&lt;p&gt;Aside from a few simple concepts I can\u2019t script or code. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m wondering if you think it\u2019s feasible to get consistent results with AI generated code.&lt;/p&gt;\n\n&lt;p&gt;It would make my life so much easier. &lt;/p&gt;\n\n&lt;p&gt;This is an example, wdyt?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/fMR00wz.jpg\"&gt;Here is an example of a Python script that checks for changes at a URL, and if there are changes, takes a screenshot and saves it to a specific folder:&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?auto=webp&amp;s=bf755b85c339bc6079c1b3a1ae9cc0ce1a5ccc79", "width": 1048, "height": 1710}, "resolutions": [{"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=360ecb919246bc3e0207c69df707e99d0e0246d1", "width": 108, "height": 176}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ea811c86a0739ff9c729ea1ff0cab77f68c7bcf", "width": 216, "height": 352}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=caa8de9baa73d9cbfdb530833192eab9d6014d73", "width": 320, "height": 522}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f120a979a4bc351aa022af6f4215c667b2a73567", "width": 640, "height": 1044}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bdefd051dc61ac6e920b8aa5a20a2b2f6a33fd19", "width": 960, "height": 1566}], "variants": {}, "id": "D56t9ZeJJiAWGiIrwM0_68X5POaYTVxrgStV49UCEIk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbm58l", "is_robot_indexable": true, "report_reasons": null, "author": "badatmathdave", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbm58l/is_it_possible_to_use_ai_to_write_scripts_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbm58l/is_it_possible_to_use_ai_to_write_scripts_for/", "subreddit_subscribers": 657349, "created_utc": 1670090171.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}