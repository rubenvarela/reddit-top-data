{"kind": "Listing", "data": {"after": null, "dist": 12, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have started as the first data scientist in an e-commerce start-up some months ago. Although my tasks are currently more on the data engineering side, it\u2019s a very exciting time. Now there will be another data scientist joining soon and I\u2019m trying to figure out what some of the most important practices are that could leverage our teamwork. Whether it\u2019s about communication or about specific code practices (unit testing, etc.), what would you consider the most important points for newly assembled DS-teams?\n\nThanks already!", "author_fullname": "t2_qrf0fsnc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what would you consider to be the most important practices for a newly assembled DS team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zuvov7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671964426.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have started as the first data scientist in an e-commerce start-up some months ago. Although my tasks are currently more on the data engineering side, it\u2019s a very exciting time. Now there will be another data scientist joining soon and I\u2019m trying to figure out what some of the most important practices are that could leverage our teamwork. Whether it\u2019s about communication or about specific code practices (unit testing, etc.), what would you consider the most important points for newly assembled DS-teams?&lt;/p&gt;\n\n&lt;p&gt;Thanks already!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zuvov7", "is_robot_indexable": true, "report_reasons": null, "author": "Odd-Concert-4591", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zuvov7/what_would_you_consider_to_be_the_most_important/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zuvov7/what_would_you_consider_to_be_the_most_important/", "subreddit_subscribers": 830228, "created_utc": 1671964426.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "https://www.ocf.berkeley.edu/~johnlab/pdfs/2008chapter.pdf describes 18,000 words or phrases related to personality that were somehow clustered in multi factor analysis to form the big 5 personality traits.\n\n- How was this done? \n- Any code online reproducing what exactly the researchers did? Possibly with a different dataset.", "author_fullname": "t2_kn6yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a rigorous description and dataset describing how the big 5 personality traits were constructed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zuj7b4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671917218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.ocf.berkeley.edu/%7Ejohnlab/pdfs/2008chapter.pdf\"&gt;https://www.ocf.berkeley.edu/~johnlab/pdfs/2008chapter.pdf&lt;/a&gt; describes 18,000 words or phrases related to personality that were somehow clustered in multi factor analysis to form the big 5 personality traits.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How was this done? &lt;/li&gt;\n&lt;li&gt;Any code online reproducing what exactly the researchers did? Possibly with a different dataset.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zuj7b4", "is_robot_indexable": true, "report_reasons": null, "author": "extremeaxe5", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zuj7b4/is_there_a_rigorous_description_and_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zuj7b4/is_there_a_rigorous_description_and_dataset/", "subreddit_subscribers": 830228, "created_utc": 1671917218.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "... In the form of F1-scores in binary outcomes.\n\nPythagoras grinning in his grave rn.", "author_fullname": "t2_ggt3e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I unironically didn't get a perfect position as data scientist because i didn't know the harmonic mean", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zuysnq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671977417.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;... In the form of F1-scores in binary outcomes.&lt;/p&gt;\n\n&lt;p&gt;Pythagoras grinning in his grave rn.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zuysnq", "is_robot_indexable": true, "report_reasons": null, "author": "Garlicguy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zuysnq/i_unironically_didnt_get_a_perfect_position_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zuysnq/i_unironically_didnt_get_a_perfect_position_as/", "subreddit_subscribers": 830228, "created_utc": 1671977417.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_ui3b85iz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "beginner in data science, looking for a community to join", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zv2jwr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671989787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zv2jwr", "is_robot_indexable": true, "report_reasons": null, "author": "kwame_Alph", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zv2jwr/beginner_in_data_science_looking_for_a_community/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zv2jwr/beginner_in_data_science_looking_for_a_community/", "subreddit_subscribers": 830228, "created_utc": 1671989787.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "If you have a calculated column that has undefined values, how can you use that column in an analysis (specifically regression or random forest) when the fact that a value is undefined could be important?\n\nThere's a few options I know of, but each have pitfalls:\n\n1. Use the columns that create the calculated column.     Downside: often these values are colinear\n2. Replace undefined with 0s.     Downside: this can skew the data toward 0 which reduces accuracy\n3. Remove rows with undefined values.    Downside: Smaller datasets don't have this option, and even in large ones, undefined values could be a strong indicator\n\nI've considered the option of converting undefined into 0s and adding a binary column that marks true for previously undefined values, but I can't find resources on that so I'm not sure it would be legit.\n\nHow have you dealt with undefined data? At some point do you just need to pick the best option for your data even though none are perfect?\n\nEDIT: By undefined, I am specifically talking about x/0 errors. For example, percentage columns in a marketing report where 0 leads converted from a campaign, but there were also 0 leads acquired.", "author_fullname": "t2_4em4ozqw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you treat undefined values?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zuhm3e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671913750.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671912387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you have a calculated column that has undefined values, how can you use that column in an analysis (specifically regression or random forest) when the fact that a value is undefined could be important?&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a few options I know of, but each have pitfalls:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Use the columns that create the calculated column.     Downside: often these values are colinear&lt;/li&gt;\n&lt;li&gt;Replace undefined with 0s.     Downside: this can skew the data toward 0 which reduces accuracy&lt;/li&gt;\n&lt;li&gt;Remove rows with undefined values.    Downside: Smaller datasets don&amp;#39;t have this option, and even in large ones, undefined values could be a strong indicator&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ve considered the option of converting undefined into 0s and adding a binary column that marks true for previously undefined values, but I can&amp;#39;t find resources on that so I&amp;#39;m not sure it would be legit.&lt;/p&gt;\n\n&lt;p&gt;How have you dealt with undefined data? At some point do you just need to pick the best option for your data even though none are perfect?&lt;/p&gt;\n\n&lt;p&gt;EDIT: By undefined, I am specifically talking about x/0 errors. For example, percentage columns in a marketing report where 0 leads converted from a campaign, but there were also 0 leads acquired.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zuhm3e", "is_robot_indexable": true, "report_reasons": null, "author": "Lintaar", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zuhm3e/how_do_you_treat_undefined_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zuhm3e/how_do_you_treat_undefined_values/", "subreddit_subscribers": 830228, "created_utc": 1671912387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Does anyone have experience in deploying a similar large scale forecasting system? (Assuming enough data is available) How did the final model / system looked like? What ML algorithm was deployed in production?\n\nDid one ML model fit on all the data to forecast accurately for all products? Or multiple models were trained specifically for every product/store? What loss function/metrics were used? \n\nIt will be great to hear your experiences.", "author_fullname": "t2_8wsx4nmm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Productionizing large scale ML model that can forecast sales for hundred-thousands of products for multiple stores (SKU/store)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zujsw7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671919052.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have experience in deploying a similar large scale forecasting system? (Assuming enough data is available) How did the final model / system looked like? What ML algorithm was deployed in production?&lt;/p&gt;\n\n&lt;p&gt;Did one ML model fit on all the data to forecast accurately for all products? Or multiple models were trained specifically for every product/store? What loss function/metrics were used? &lt;/p&gt;\n\n&lt;p&gt;It will be great to hear your experiences.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zujsw7", "is_robot_indexable": true, "report_reasons": null, "author": "k-deeplearning99", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zujsw7/productionizing_large_scale_ml_model_that_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zujsw7/productionizing_large_scale_ml_model_that_can/", "subreddit_subscribers": 830228, "created_utc": 1671919052.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Been an industry data scientist for 6 years in fintech and gaming.   \nIn fintech, I sensed a need for interpretability and robustness. Also, I was not working with a lot of data(\\~500k observations to train models). Consequently, I got into the habit of building tree-based models by default, specifically xgboost. Used explainability techniques such as shap to explain models. \n\nAfter moving to online gaming, the scrutiny is less and the scale is far more. I now have the freedom to use deep learning. I need to be able to demonstrate the effectiveness using experiments, but beyond that, do not need explainability at a granular level. Advantages I see with using deep learning-\n\n1. Custom loss functions - basically any differentiable loss function can be trained on. This has huge advantages when the business goal is not aligned with the loss functions out of the box\n2. Learning Embeddings - The ability to condense features into dense, latent representations which can be used for any number of use cases\n3. Multiple outputs per model - tweaking the architecture\n\nSee all this, Deep learning seems to offer a lot of advantages, even if the performance might be similar to tree-based methods. What do you guys think?", "author_fullname": "t2_tetcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The case for deep learning for tabular data.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zuuab4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671958136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been an industry data scientist for 6 years in fintech and gaming.&lt;br/&gt;\nIn fintech, I sensed a need for interpretability and robustness. Also, I was not working with a lot of data(~500k observations to train models). Consequently, I got into the habit of building tree-based models by default, specifically xgboost. Used explainability techniques such as shap to explain models. &lt;/p&gt;\n\n&lt;p&gt;After moving to online gaming, the scrutiny is less and the scale is far more. I now have the freedom to use deep learning. I need to be able to demonstrate the effectiveness using experiments, but beyond that, do not need explainability at a granular level. Advantages I see with using deep learning-&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Custom loss functions - basically any differentiable loss function can be trained on. This has huge advantages when the business goal is not aligned with the loss functions out of the box&lt;/li&gt;\n&lt;li&gt;Learning Embeddings - The ability to condense features into dense, latent representations which can be used for any number of use cases&lt;/li&gt;\n&lt;li&gt;Multiple outputs per model - tweaking the architecture&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;See all this, Deep learning seems to offer a lot of advantages, even if the performance might be similar to tree-based methods. What do you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zuuab4", "is_robot_indexable": true, "report_reasons": null, "author": "dhruvnigam93", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zuuab4/the_case_for_deep_learning_for_tabular_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zuuab4/the_case_for_deep_learning_for_tabular_data/", "subreddit_subscribers": 830228, "created_utc": 1671958136.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Ugh. So, I went through all the lectures and examples provided for Central Michigan's ML and AI boot camp  which, although much more expensive than a Udemy class, are not as high quality. The classes are hosted on Ed2go. Now, for a capstone project, to pass the class, I need to design models to take video input of from driver POV and overlay on an output video the current speed of the vehicle, boxes around the signs with sign classification, and road edges / centerline lines. I am starting with the speed detection using a a dataset I found by commaai on GitHub. Thing is, there was never any discussion about how to preprocess video to get it into an RNN. We discussed how RNN's work but not much preprocessing. Are there any keras preprocessing layers anyone would suggest? Speed detection is not really an image classification problem. I have at least split the video into an array of frame data using openCV already. This capstone seems very advanced compared to the instruction given. \n\nP.s. I only took the class because the state offered to pay for it and it sounded interesting. After it started they informed me that I I don't pass the final I will have to pay it all back. So, let's just say there is strong monetary motivation to figure this out.", "author_fullname": "t2_15b0k000", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bootcamp isn't great", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zuh1de", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671910665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ugh. So, I went through all the lectures and examples provided for Central Michigan&amp;#39;s ML and AI boot camp  which, although much more expensive than a Udemy class, are not as high quality. The classes are hosted on Ed2go. Now, for a capstone project, to pass the class, I need to design models to take video input of from driver POV and overlay on an output video the current speed of the vehicle, boxes around the signs with sign classification, and road edges / centerline lines. I am starting with the speed detection using a a dataset I found by commaai on GitHub. Thing is, there was never any discussion about how to preprocess video to get it into an RNN. We discussed how RNN&amp;#39;s work but not much preprocessing. Are there any keras preprocessing layers anyone would suggest? Speed detection is not really an image classification problem. I have at least split the video into an array of frame data using openCV already. This capstone seems very advanced compared to the instruction given. &lt;/p&gt;\n\n&lt;p&gt;P.s. I only took the class because the state offered to pay for it and it sounded interesting. After it started they informed me that I I don&amp;#39;t pass the final I will have to pay it all back. So, let&amp;#39;s just say there is strong monetary motivation to figure this out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zuh1de", "is_robot_indexable": true, "report_reasons": null, "author": "smothry", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zuh1de/bootcamp_isnt_great/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zuh1de/bootcamp_isnt_great/", "subreddit_subscribers": 830228, "created_utc": 1671910665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi y'all, first off, I'm an absolute beginner regarding statistics, therefore my probably trivial question.\n\nI recently did an online survey, where people could list their personal problems regarding a specific topic and then \"weigh\" them individually, using a contingent of 10 points. (If they had only listed one problem, they had to use up their 10 points, thus weighting their only problem with 10 points, while other participants possibly had to distribute their contingent)\n\nThe goal of the survey is to find out the \"importance\" of each problem and then publish it. (each participant is equally or at least similarly experienced and can be considered an expert)\n\nThe problem is, that some people have listed completely different problems than others, in many cases, there is no overlap at all.\n\nFor example, the results look like this (\"-\" indicating that the person has not listed the problem)\n\n&amp;#x200B;\n\n||Person 1|Person 2|Person 3|Person 4|\n|:-|:-|:-|:-|:-|\n|Problem A|10|4|\\-|\\-|\n|Problem B|\\-|1|3|\\-|\n|Problem C|\\-|2|3|\\-|\n|Problem D|\\-|3|4|\\-|\n|Problem E|\\-|\\-|\\-|5|\n|Problem F|\\-|\\-|\\-|5|\n\n&amp;#x200B;\n\nSimply taking the average of each weight for each problem feels wrong to me. People who have listed 3 problems, which may all be equally important, could have given Problem XY 3 points, while another participant who only voted on Problem XY  would have given it 10 points - thus \"skewing\" the average. (If I am wrong, please correct me here)\n\nBut how would I calculate it correctly?\n\nLet's say we want to calculate the importance of Problem A, which 2 people (Person 1 &amp; Person 2) have mentioned,  four people participating in the survey. 6 Problems were submitted in total.\n\nPerson 1 has only voted on Problem A, thus rating it with 10 points. Person 2 has slightly more experience and thus voted on 4 problems in total, rating Problem A with 4 points.\n\nMy approach would be the following (e.g. for Problem A)\n\n1. Check who has voted on Problem A\n2. Of all those People, what person listed the most problems? (Person 2, 4 Problems)\n3. How many problems did this person vote on? -&gt; It's 4!\n4. Go through each person (who has mentioned Problem A) and calculate this:( Problem A Score ) \\* ( Number of Probs. Person voted on / 4 ), thus receiving a \"normalized\" Score\n5. Add the result of each person up and then divide it by the number of persons who have voted on Problem A (2)\n\nIn this case, the \"importance\" of Problem A would be: (see table above)  \n(10 \\* (1/4) + 4 \\*(4/4)) / 2 = **3,25**\n\nProblem B: (1 \\* (4/4) + 3 \\* (3/4)) / 2 = **1,625**  \nProblem C: (2 \\* (4/4) + 3 \\* (3/4)) / 2 = **2,125**  \nProblem D: (3 \\* (4/4) + 4 \\* (3/4)) / 2 = **3**  \nProblem E: (5\\*(2/2)) / 1 = **5**  \nProblem F: (5\\*(2/2)) / 1 = **5**\n\nCan I do this? Is this the best way of doing this? Thank you so much!", "author_fullname": "t2_ikyqvyr9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I correctly evaluate the results of this survey? (added my Approach, please criticize)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zv1tcw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671987555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi y&amp;#39;all, first off, I&amp;#39;m an absolute beginner regarding statistics, therefore my probably trivial question.&lt;/p&gt;\n\n&lt;p&gt;I recently did an online survey, where people could list their personal problems regarding a specific topic and then &amp;quot;weigh&amp;quot; them individually, using a contingent of 10 points. (If they had only listed one problem, they had to use up their 10 points, thus weighting their only problem with 10 points, while other participants possibly had to distribute their contingent)&lt;/p&gt;\n\n&lt;p&gt;The goal of the survey is to find out the &amp;quot;importance&amp;quot; of each problem and then publish it. (each participant is equally or at least similarly experienced and can be considered an expert)&lt;/p&gt;\n\n&lt;p&gt;The problem is, that some people have listed completely different problems than others, in many cases, there is no overlap at all.&lt;/p&gt;\n\n&lt;p&gt;For example, the results look like this (&amp;quot;-&amp;quot; indicating that the person has not listed the problem)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;Person 1&lt;/th&gt;\n&lt;th align=\"left\"&gt;Person 2&lt;/th&gt;\n&lt;th align=\"left\"&gt;Person 3&lt;/th&gt;\n&lt;th align=\"left\"&gt;Person 4&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Problem A&lt;/td&gt;\n&lt;td align=\"left\"&gt;10&lt;/td&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Problem B&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Problem C&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Problem D&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Problem E&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Problem F&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;td align=\"left\"&gt;-&lt;/td&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Simply taking the average of each weight for each problem feels wrong to me. People who have listed 3 problems, which may all be equally important, could have given Problem XY 3 points, while another participant who only voted on Problem XY  would have given it 10 points - thus &amp;quot;skewing&amp;quot; the average. (If I am wrong, please correct me here)&lt;/p&gt;\n\n&lt;p&gt;But how would I calculate it correctly?&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say we want to calculate the importance of Problem A, which 2 people (Person 1 &amp;amp; Person 2) have mentioned,  four people participating in the survey. 6 Problems were submitted in total.&lt;/p&gt;\n\n&lt;p&gt;Person 1 has only voted on Problem A, thus rating it with 10 points. Person 2 has slightly more experience and thus voted on 4 problems in total, rating Problem A with 4 points.&lt;/p&gt;\n\n&lt;p&gt;My approach would be the following (e.g. for Problem A)&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Check who has voted on Problem A&lt;/li&gt;\n&lt;li&gt;Of all those People, what person listed the most problems? (Person 2, 4 Problems)&lt;/li&gt;\n&lt;li&gt;How many problems did this person vote on? -&amp;gt; It&amp;#39;s 4!&lt;/li&gt;\n&lt;li&gt;Go through each person (who has mentioned Problem A) and calculate this:( Problem A Score ) * ( Number of Probs. Person voted on / 4 ), thus receiving a &amp;quot;normalized&amp;quot; Score&lt;/li&gt;\n&lt;li&gt;Add the result of each person up and then divide it by the number of persons who have voted on Problem A (2)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;In this case, the &amp;quot;importance&amp;quot; of Problem A would be: (see table above)&lt;br/&gt;\n(10 * (1/4) + 4 *(4/4)) / 2 = &lt;strong&gt;3,25&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Problem B: (1 * (4/4) + 3 * (3/4)) / 2 = &lt;strong&gt;1,625&lt;/strong&gt;&lt;br/&gt;\nProblem C: (2 * (4/4) + 3 * (3/4)) / 2 = &lt;strong&gt;2,125&lt;/strong&gt;&lt;br/&gt;\nProblem D: (3 * (4/4) + 4 * (3/4)) / 2 = &lt;strong&gt;3&lt;/strong&gt;&lt;br/&gt;\nProblem E: (5*(2/2)) / 1 = &lt;strong&gt;5&lt;/strong&gt;&lt;br/&gt;\nProblem F: (5*(2/2)) / 1 = &lt;strong&gt;5&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Can I do this? Is this the best way of doing this? Thank you so much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zv1tcw", "is_robot_indexable": true, "report_reasons": null, "author": "qrztkxnxzkfrmkos", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zv1tcw/how_can_i_correctly_evaluate_the_results_of_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zv1tcw/how_can_i_correctly_evaluate_the_results_of_this/", "subreddit_subscribers": 830228, "created_utc": 1671987555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_aic6s0ug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Average similarity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zv2dt5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671989281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zv2dt5", "is_robot_indexable": true, "report_reasons": null, "author": "EnthusiasmEntire1492", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zv2dt5/average_similarity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zv2dt5/average_similarity/", "subreddit_subscribers": 830228, "created_utc": 1671989281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_903k40ag", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How did you land your first DS job after school?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zuqz7o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671944233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zuqz7o", "is_robot_indexable": true, "report_reasons": null, "author": "Impossible-Cry-495", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zuqz7o/how_did_you_land_your_first_ds_job_after_school/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zuqz7o/how_did_you_land_your_first_ds_job_after_school/", "subreddit_subscribers": 830228, "created_utc": 1671944233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I don't see any flair for \"Please do my job for me\"", "author_fullname": "t2_937iap2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone else feel that they should be able to charge a large fee to answer these questions ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zuko96", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.19, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671921734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t see any flair for &amp;quot;Please do my job for me&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zuko96", "is_robot_indexable": true, "report_reasons": null, "author": "AmongstYou666", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zuko96/does_anyone_else_feel_that_they_should_be_able_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zuko96/does_anyone_else_feel_that_they_should_be_able_to/", "subreddit_subscribers": 830228, "created_utc": 1671921734.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}