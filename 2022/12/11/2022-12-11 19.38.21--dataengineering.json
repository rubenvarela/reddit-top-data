{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know what the \u201cmarket share\u201d is for these two languages in the field of data engineering? I\u2019m a Pythonista but am curious if Scala is worth learning for future success as a data engineer.", "author_fullname": "t2_127pnl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scala vs Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zifzcy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670735356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know what the \u201cmarket share\u201d is for these two languages in the field of data engineering? I\u2019m a Pythonista but am curious if Scala is worth learning for future success as a data engineer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zifzcy", "is_robot_indexable": true, "report_reasons": null, "author": "skydog92", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zifzcy/scala_vs_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zifzcy/scala_vs_python/", "subreddit_subscribers": 82586, "created_utc": 1670735356.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_nrfxa5al", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Future of Big Data Systems by Spark creator Matei Zaharia", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_zi7hrj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 42, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/aRk6Lk6L5gA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Future of Big Data Systems by Spark creator Matei Zaharia\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Future of Big Data Systems by Spark creator Matei Zaharia", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/aRk6Lk6L5gA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Future of Big Data Systems by Spark creator Matei Zaharia\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/aRk6Lk6L5gA/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/aRk6Lk6L5gA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Future of Big Data Systems by Spark creator Matei Zaharia\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/zi7hrj", "height": 200}, "link_flair_text": "Interview", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Nx2_TecTpkre5K9-IDjOsTlKMzcKzvVON0bM5eadaIc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670715844.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/aRk6Lk6L5gA", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NjXpxJvHD561QkDdsfd6VaIRpafEWOIr5FB58g_Osj8.jpg?auto=webp&amp;s=6163850426c519392d194763812b65ead23f9969", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/NjXpxJvHD561QkDdsfd6VaIRpafEWOIr5FB58g_Osj8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d8bad56c4220db5d34b06edb5fc83ab3b279376", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/NjXpxJvHD561QkDdsfd6VaIRpafEWOIr5FB58g_Osj8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ea11c283bdaba19fb02f718aa5a1685707505e93", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/NjXpxJvHD561QkDdsfd6VaIRpafEWOIr5FB58g_Osj8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5df9a64eff9702d07b15ce6ccfef15fcf1e88212", "width": 320, "height": 240}], "variants": {}, "id": "VrUmpf7gHYb0jVrTvpkyd98lRn3v5vdJxtDeGa9WdNc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "zi7hrj", "is_robot_indexable": true, "report_reasons": null, "author": "CatanNicollo", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zi7hrj/future_of_big_data_systems_by_spark_creator_matei/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/aRk6Lk6L5gA", "subreddit_subscribers": 82586, "created_utc": 1670715844.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Future of Big Data Systems by Spark creator Matei Zaharia", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/aRk6Lk6L5gA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Future of Big Data Systems by Spark creator Matei Zaharia\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/aRk6Lk6L5gA/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I get a lot of vague questions at work, for example \"create a dataset so that so-and-so team can do this-and-that\" with little to no context. It took me a while to navigate this kind of ambiguity, and I'm not sure whether I'm there yet, but there are a few things that I do\n\n1. **Ask clarifying questions**. Many times stakeholders have no idea what they're asking for and how they want the result to look like. It's worth getting into conversations with them because there's a chance that you can jointly agree on an end result that might look nothing like what they initially imagined.\n2. **Start small, scale later**. I have analysts requesting data going back 5 years, but I know from experience that more recent data (going back 12 months) is actually better for answering business questions. Our product has evolved so much over the last years that the data collected pre-COVID looks nothing like the data we collect today, so analysts should be able to work with recent data, which I can readily provide. After I have done that and the analysts have worked with it, we jointly evaluate whether it's still necessary to use historical data. And no sooner than that do I actually look at that data far back.\n3. **EDIT: Look at what's been done before**. I find that many DEs are more eager to reinvent the wheel and build something from scratch, rather than deal legacy codes and systems. However, I've found that it helps to at least pause and consider existing tech before deciding whether a new solution is really required. Oftentimes I've found that legacy stuff isn't actually all that bad, and even if I can incorporate even just 10% of it in the new solution, therefore saving myself some time and effort, I consider that a win.\n\nHow about you, how do you navigate ambiguity in your work as a data engineer?", "author_fullname": "t2_7m2ues2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineers, how do you approach ambiguity in your work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zixrj6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670776118.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670774865.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I get a lot of vague questions at work, for example &amp;quot;create a dataset so that so-and-so team can do this-and-that&amp;quot; with little to no context. It took me a while to navigate this kind of ambiguity, and I&amp;#39;m not sure whether I&amp;#39;m there yet, but there are a few things that I do&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Ask clarifying questions&lt;/strong&gt;. Many times stakeholders have no idea what they&amp;#39;re asking for and how they want the result to look like. It&amp;#39;s worth getting into conversations with them because there&amp;#39;s a chance that you can jointly agree on an end result that might look nothing like what they initially imagined.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start small, scale later&lt;/strong&gt;. I have analysts requesting data going back 5 years, but I know from experience that more recent data (going back 12 months) is actually better for answering business questions. Our product has evolved so much over the last years that the data collected pre-COVID looks nothing like the data we collect today, so analysts should be able to work with recent data, which I can readily provide. After I have done that and the analysts have worked with it, we jointly evaluate whether it&amp;#39;s still necessary to use historical data. And no sooner than that do I actually look at that data far back.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;EDIT: Look at what&amp;#39;s been done before&lt;/strong&gt;. I find that many DEs are more eager to reinvent the wheel and build something from scratch, rather than deal legacy codes and systems. However, I&amp;#39;ve found that it helps to at least pause and consider existing tech before deciding whether a new solution is really required. Oftentimes I&amp;#39;ve found that legacy stuff isn&amp;#39;t actually all that bad, and even if I can incorporate even just 10% of it in the new solution, therefore saving myself some time and effort, I consider that a win.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;How about you, how do you navigate ambiguity in your work as a data engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zixrj6", "is_robot_indexable": true, "report_reasons": null, "author": "DataScienceIsScience", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zixrj6/data_engineers_how_do_you_approach_ambiguity_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zixrj6/data_engineers_how_do_you_approach_ambiguity_in/", "subreddit_subscribers": 82586, "created_utc": 1670774865.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im currently a uni student and I'm interested in the data field. I thought about going into analytics first and then carrying that exp. over to data engineering. I understand tools change a lot and that I have to stay up to date with tech. \n\nBut I wanted to know how these roles might evolve in the future and what skills I can expect to learn moving forward. Like what are things trending towards in the data ecosystem? Are things headed to low-code? Should I look into ML? \n\nAlso will our skillsets carry over to whatever comes next? \n\nThank You.", "author_fullname": "t2_76fvluuq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Future Data Roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zigxic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670737406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im currently a uni student and I&amp;#39;m interested in the data field. I thought about going into analytics first and then carrying that exp. over to data engineering. I understand tools change a lot and that I have to stay up to date with tech. &lt;/p&gt;\n\n&lt;p&gt;But I wanted to know how these roles might evolve in the future and what skills I can expect to learn moving forward. Like what are things trending towards in the data ecosystem? Are things headed to low-code? Should I look into ML? &lt;/p&gt;\n\n&lt;p&gt;Also will our skillsets carry over to whatever comes next? &lt;/p&gt;\n\n&lt;p&gt;Thank You.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zigxic", "is_robot_indexable": true, "report_reasons": null, "author": "notGaruda1", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zigxic/future_data_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zigxic/future_data_roles/", "subreddit_subscribers": 82586, "created_utc": 1670737406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI have a service that will push data each second to some data storage. The data will then be used to populate results in a web app, where users will have access to certain data based on their permission level but they won't have to write any data.\n\n&amp;#x200B;\n\n[High-level schema](https://preview.redd.it/1kqdnsxj265a1.png?width=960&amp;format=png&amp;auto=webp&amp;s=50fbb0fce1c1f80c29abceb520c21639d1ea4a23)\n\nWhat would be the best data solution for this system?\n\nI used Firebase in the past with was quite good to handle real time updates, but I suspect there are better solutions out there.\n\nEDIT: you can imagine this as a web app where users, based on access level, will monitor in real-time data coming from different sensors that has been processed by my service.", "author_fullname": "t2_3fyu9j5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best data storage solution for high-frequency (near real-time) updates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"1kqdnsxj265a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/1kqdnsxj265a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f36314f882772bbf093842a51cba56884114ef8"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/1kqdnsxj265a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c8ae5ea07cd6fcd4f5edeba5b2a8e502a2be1d7"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/1kqdnsxj265a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8892dd81e25e6ba2c0be325eb17cb18cb03c67fb"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/1kqdnsxj265a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d8bf390fd7596af6a377c162f6148654c0b484dc"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/1kqdnsxj265a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=997099a2767cd04bcb0c079a10a3487ea5f24307"}], "s": {"y": 720, "x": 960, "u": "https://preview.redd.it/1kqdnsxj265a1.png?width=960&amp;format=png&amp;auto=webp&amp;s=50fbb0fce1c1f80c29abceb520c21639d1ea4a23"}, "id": "1kqdnsxj265a1"}}, "name": "t3_zibr6p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/dO_pp5zdtLnGxAdYYt2IDI2n1BbsIXWUvMKtkCtOLg8.jpg", "edited": 1670768413.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670726105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I have a service that will push data each second to some data storage. The data will then be used to populate results in a web app, where users will have access to certain data based on their permission level but they won&amp;#39;t have to write any data.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1kqdnsxj265a1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50fbb0fce1c1f80c29abceb520c21639d1ea4a23\"&gt;High-level schema&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What would be the best data solution for this system?&lt;/p&gt;\n\n&lt;p&gt;I used Firebase in the past with was quite good to handle real time updates, but I suspect there are better solutions out there.&lt;/p&gt;\n\n&lt;p&gt;EDIT: you can imagine this as a web app where users, based on access level, will monitor in real-time data coming from different sensors that has been processed by my service.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zibr6p", "is_robot_indexable": true, "report_reasons": null, "author": "EntropyRX", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zibr6p/what_is_the_best_data_storage_solution_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zibr6p/what_is_the_best_data_storage_solution_for/", "subreddit_subscribers": 82586, "created_utc": 1670726105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a POC of my product, I would need to deploy a camera at a certain location and be able to stream the footage with an image detection algorithm running on it in the cloud. The end user would be able to view the live footage with the ML detections together with some other stats. I'm still unclear on what tech/service/approach to use.  \n\n\n[https://medium.com/nerd-for-tech/live-streaming-using-opencv-c0ef28a5e497](https://medium.com/nerd-for-tech/live-streaming-using-opencv-c0ef28a5e497)  \nThis article explains it by using socket programming. But it looks oversimplified and I'm not sure if the same approach can be used if I'm moving to the next development stage (for production).", "author_fullname": "t2_2nkat5ah", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to send live footage to the cloud and client interface?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zihub9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670739421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a POC of my product, I would need to deploy a camera at a certain location and be able to stream the footage with an image detection algorithm running on it in the cloud. The end user would be able to view the live footage with the ML detections together with some other stats. I&amp;#39;m still unclear on what tech/service/approach to use.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/nerd-for-tech/live-streaming-using-opencv-c0ef28a5e497\"&gt;https://medium.com/nerd-for-tech/live-streaming-using-opencv-c0ef28a5e497&lt;/a&gt;&lt;br/&gt;\nThis article explains it by using socket programming. But it looks oversimplified and I&amp;#39;m not sure if the same approach can be used if I&amp;#39;m moving to the next development stage (for production).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HJEwBkka6bG2mhJsRuqAWaSPMVI-4g8S6g1YxJjf4bk.jpg?auto=webp&amp;s=2b0027e96a69583fbb5278b1e1536dfdb10f9037", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/HJEwBkka6bG2mhJsRuqAWaSPMVI-4g8S6g1YxJjf4bk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b2baef623c0eaf8740e11fc173e384da9f51169", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/HJEwBkka6bG2mhJsRuqAWaSPMVI-4g8S6g1YxJjf4bk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a35bbf8b80497dba2e1c46a31509f141e9800b4", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/HJEwBkka6bG2mhJsRuqAWaSPMVI-4g8S6g1YxJjf4bk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=69e3e28a4338d1a624ccc50946b5a6949071cdac", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/HJEwBkka6bG2mhJsRuqAWaSPMVI-4g8S6g1YxJjf4bk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=18a0d0d098f22e5e980e9ea6cbc5231f7aa46bae", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/HJEwBkka6bG2mhJsRuqAWaSPMVI-4g8S6g1YxJjf4bk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d666b6818efe7ade8c33a8825757d545b39afb8f", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/HJEwBkka6bG2mhJsRuqAWaSPMVI-4g8S6g1YxJjf4bk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=65242d071fc42eb4438292d360e0ca3f35045435", "width": 1080, "height": 607}], "variants": {}, "id": "x5r4Y48-UNE2uT5Bm4KR8XWWoQF32xdpiJ0JAg9Jtdc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zihub9", "is_robot_indexable": true, "report_reasons": null, "author": "kurkurzz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zihub9/how_to_send_live_footage_to_the_cloud_and_client/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zihub9/how_to_send_live_footage_to_the_cloud_and_client/", "subreddit_subscribers": 82586, "created_utc": 1670739421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_22rg4hgk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL with Dataflow &amp; BigQuery - Async Queue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 69, "top_awarded_type": null, "hide_score": false, "name": "t3_zigjvp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tdDUn7eiy8un_aLv8J58N-tHJTrIB5TTaC-Ru7s8mtQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670736561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "asyncq.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://asyncq.com/etl-with-dataflow-bigquery", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1DNYWNGw3IQgaS7FG_Ki3Xiw24Ol8WhGJS6giyNUr80.jpg?auto=webp&amp;s=34c5fab18a2f4f8785ece7ab7004993a106ad48d", "width": 1356, "height": 672}, "resolutions": [{"url": "https://external-preview.redd.it/1DNYWNGw3IQgaS7FG_Ki3Xiw24Ol8WhGJS6giyNUr80.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=010e481d06ce7a7ba9aee7afe029b05e3652f6be", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/1DNYWNGw3IQgaS7FG_Ki3Xiw24Ol8WhGJS6giyNUr80.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f47b618a6fbe18c474b9dc061a82c2ea9eb547d0", "width": 216, "height": 107}, {"url": "https://external-preview.redd.it/1DNYWNGw3IQgaS7FG_Ki3Xiw24Ol8WhGJS6giyNUr80.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=81bf0b9263f52eaea6aac024ac14b0f06c35169d", "width": 320, "height": 158}, {"url": "https://external-preview.redd.it/1DNYWNGw3IQgaS7FG_Ki3Xiw24Ol8WhGJS6giyNUr80.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=87e8771e19e6f9d596c836f9160724a9f6842a81", "width": 640, "height": 317}, {"url": "https://external-preview.redd.it/1DNYWNGw3IQgaS7FG_Ki3Xiw24Ol8WhGJS6giyNUr80.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=09e5a262ffd174cd563b9d5b511875d8db302805", "width": 960, "height": 475}, {"url": "https://external-preview.redd.it/1DNYWNGw3IQgaS7FG_Ki3Xiw24Ol8WhGJS6giyNUr80.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0212b1db86968d52ddb9ee04d8f6968930ffe851", "width": 1080, "height": 535}], "variants": {}, "id": "O99Frg8vKItnbYctwPxu4rDi2lrTF_8jh5GbYyEf-5A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zigjvp", "is_robot_indexable": true, "report_reasons": null, "author": "suraj-mishra15", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zigjvp/etl_with_dataflow_bigquery_async_queue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://asyncq.com/etl-with-dataflow-bigquery", "subreddit_subscribers": 82586, "created_utc": 1670736561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to apply for a job and it requires me to know recommendation systems, streaming and big data. \n\nI have all the other ML amd infrastructure skills mentioned in the description but I want to add a project for this. \n\nWhat are some easy project ideas that can help me get an interview call?", "author_fullname": "t2_c2zeqd7i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick project ideas pyspark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zid408", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670729075.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to apply for a job and it requires me to know recommendation systems, streaming and big data. &lt;/p&gt;\n\n&lt;p&gt;I have all the other ML amd infrastructure skills mentioned in the description but I want to add a project for this. &lt;/p&gt;\n\n&lt;p&gt;What are some easy project ideas that can help me get an interview call?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zid408", "is_robot_indexable": true, "report_reasons": null, "author": "LetsJustGrowUp", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zid408/quick_project_ideas_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zid408/quick_project_ideas_pyspark/", "subreddit_subscribers": 82586, "created_utc": 1670729075.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_88bdez9q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Designing and Planning an Event Store System", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_zic6ms", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9MU7sxu7HTEYzMsquTaJnEs1cc3we45tebm46SuSKps.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670727001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "coraspe-ramses.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://coraspe-ramses.medium.com/designing-and-planning-an-event-store-system-be4df7519442", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?auto=webp&amp;s=3eaddf9e51bf4c5cefeaad71031e28b6a84f1f7c", "width": 1200, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e573daa31d80943c4285481fcb704c15e16ce29", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c973763e9fdcb0b33da6f550d516c3aa252af87", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6a0f62eba8a22f321c369c70f67b39363c9e247", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=948994eb15b1ecc7835f5b341dd7279175b3e417", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=221b994d8c4285c1683ae509a2c468e3713023f2", "width": 960, "height": 576}, {"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=abfd065ae2bda0e79e763b13db228d67aecfa990", "width": 1080, "height": 648}], "variants": {}, "id": "R8w61k1rvY7F-US8sPnunbp6rR3lonn0z685fjPeP9k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zic6ms", "is_robot_indexable": true, "report_reasons": null, "author": "ramses-coraspe", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/zic6ms/designing_and_planning_an_event_store_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://coraspe-ramses.medium.com/designing-and-planning-an-event-store-system-be4df7519442", "subreddit_subscribers": 82586, "created_utc": 1670727001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recently i been playing with the idea of work work as a mid level DE in either China or Singapore. \n\nIf it matters, i am asian and I speak and write fluent mandarin.\n\nJust wondering if anyone worked in either of these markets and mind to share their experiences? Like how transfer is the stack and WLB.", "author_fullname": "t2_c3yqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Job Market in China or Singapore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zj0el1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670779272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently i been playing with the idea of work work as a mid level DE in either China or Singapore. &lt;/p&gt;\n\n&lt;p&gt;If it matters, i am asian and I speak and write fluent mandarin.&lt;/p&gt;\n\n&lt;p&gt;Just wondering if anyone worked in either of these markets and mind to share their experiences? Like how transfer is the stack and WLB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zj0el1", "is_robot_indexable": true, "report_reasons": null, "author": "543254447", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zj0el1/de_job_market_in_china_or_singapore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zj0el1/de_job_market_in_china_or_singapore/", "subreddit_subscribers": 82586, "created_utc": 1670779272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This may be a too broad question but recently I was wondering how those subsecond-claimed queries happen when trying to retrieve GBs of data, 365+ days of timeseries data in user friendly response times. Is that an utopia? Or how is that problem approached?\n\nI was recently playing around incrementing data in a MongoDB instance, putting indexes in place and such... And the more data I've got, slower the queries would be to the level of being +1 minute long... And if those were real queries to be fetched from a dashboard it wouldn't be usable at all.\n\nSo basically, from a DE perspective, how are those use cases handled? Is a DB always the answer or a datalake full of Avro files for example is better?", "author_fullname": "t2_paxcw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is fast data retrieval achieved when long periods are requested?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zizrtv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670778255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This may be a too broad question but recently I was wondering how those subsecond-claimed queries happen when trying to retrieve GBs of data, 365+ days of timeseries data in user friendly response times. Is that an utopia? Or how is that problem approached?&lt;/p&gt;\n\n&lt;p&gt;I was recently playing around incrementing data in a MongoDB instance, putting indexes in place and such... And the more data I&amp;#39;ve got, slower the queries would be to the level of being +1 minute long... And if those were real queries to be fetched from a dashboard it wouldn&amp;#39;t be usable at all.&lt;/p&gt;\n\n&lt;p&gt;So basically, from a DE perspective, how are those use cases handled? Is a DB always the answer or a datalake full of Avro files for example is better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zizrtv", "is_robot_indexable": true, "report_reasons": null, "author": "edgraq", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zizrtv/how_is_fast_data_retrieval_achieved_when_long/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zizrtv/how_is_fast_data_retrieval_achieved_when_long/", "subreddit_subscribers": 82586, "created_utc": 1670778255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_udyamnod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get Rid of Your Old Database Migrations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 54, "top_awarded_type": null, "hide_score": false, "name": "t3_zin0rg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6WTNiuFS5_JAfJ3a5K4R5No7m9jfDNYEfMfLIlaUBuc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670753621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "andrealeopardi.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://andrealeopardi.com/posts/get-rid-of-your-old-database-migrations/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_mvhnTabaR409r5EZ4n_GiheUGnzq8j_Bnuo10nd95E.jpg?auto=webp&amp;s=946659e0ef28b907169f42c28169f58258c0815c", "width": 1024, "height": 398}, "resolutions": [{"url": "https://external-preview.redd.it/_mvhnTabaR409r5EZ4n_GiheUGnzq8j_Bnuo10nd95E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5299803c58da172948d2090a99f6bb7725c1e556", "width": 108, "height": 41}, {"url": "https://external-preview.redd.it/_mvhnTabaR409r5EZ4n_GiheUGnzq8j_Bnuo10nd95E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe26d58b9a0cb7d51d4a87083f6130812f157f90", "width": 216, "height": 83}, {"url": "https://external-preview.redd.it/_mvhnTabaR409r5EZ4n_GiheUGnzq8j_Bnuo10nd95E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab02ec6d7a13dedeca80b16f9659011f25dae53d", "width": 320, "height": 124}, {"url": "https://external-preview.redd.it/_mvhnTabaR409r5EZ4n_GiheUGnzq8j_Bnuo10nd95E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f1f81a02a0225d75c68a36cb58007e9f85a04a19", "width": 640, "height": 248}, {"url": "https://external-preview.redd.it/_mvhnTabaR409r5EZ4n_GiheUGnzq8j_Bnuo10nd95E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f275b0b8d32277795c43c0a26271976d17d8c021", "width": 960, "height": 373}], "variants": {}, "id": "lzrV3g2Z9Be66nVVuja802hhB2knXzU0e3Sdwsnuh5g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zin0rg", "is_robot_indexable": true, "report_reasons": null, "author": "usustoe", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zin0rg/get_rid_of_your_old_database_migrations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://andrealeopardi.com/posts/get-rid-of-your-old-database-migrations/", "subreddit_subscribers": 82586, "created_utc": 1670753621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on a personal project for learning new skills. I have a large usage dataset that has member Id, usage qty, and epoch_timestamp. Each member usage is sent every 10 minutes unless there is an issue with the source.  For a given member they could have several records that end up being corrected later or a new historical record comes in when the source may come back online. Think store sales data that may get delayed.\n\nFor example \n\nMember 123 has 3 records Dec 1 for the 1pm hour. I pull that in a batch.\n\nOn Dec 2 I  pull the next days data which also includes a record from November that had been delayed.\n\nUltimately I need to roll these up to the hour for each member but this is a large (billions of rows) set that is increasing every day. Therefore, I'd like to avoid processing all records and just deal with changed or new and how to do that at the roll-up level.\n\nIs there an example somewhere on how to go about handling this efficiently?", "author_fullname": "t2_11qvgaee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental Time Series Usage Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zi3u6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670724153.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670707062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a personal project for learning new skills. I have a large usage dataset that has member Id, usage qty, and epoch_timestamp. Each member usage is sent every 10 minutes unless there is an issue with the source.  For a given member they could have several records that end up being corrected later or a new historical record comes in when the source may come back online. Think store sales data that may get delayed.&lt;/p&gt;\n\n&lt;p&gt;For example &lt;/p&gt;\n\n&lt;p&gt;Member 123 has 3 records Dec 1 for the 1pm hour. I pull that in a batch.&lt;/p&gt;\n\n&lt;p&gt;On Dec 2 I  pull the next days data which also includes a record from November that had been delayed.&lt;/p&gt;\n\n&lt;p&gt;Ultimately I need to roll these up to the hour for each member but this is a large (billions of rows) set that is increasing every day. Therefore, I&amp;#39;d like to avoid processing all records and just deal with changed or new and how to do that at the roll-up level.&lt;/p&gt;\n\n&lt;p&gt;Is there an example somewhere on how to go about handling this efficiently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zi3u6t", "is_robot_indexable": true, "report_reasons": null, "author": "Yankee1423", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zi3u6t/incremental_time_series_usage_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zi3u6t/incremental_time_series_usage_data/", "subreddit_subscribers": 82586, "created_utc": 1670707062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen this kind of posts multiple times on Linkedin and I want to try it out. \n\nBut I'm yet to come up with any data engineering idea that could work with this. \n\nHave you seen these posts before? What kind of ways can a DE \"add relevant value\" ? \\\\\n\nI'm open to all ideas.\n\n\"People say December is the worst time to job search.  \n\n\nThat\u2019s simply not true.  \n\n\nIt\u2019s actually the best time to get ahead of your competition.  \n\n\nHere are 3 things you can do between now and January 2nd to put yourself ahead of the pack:  \n\n\n**1 Make a list of target companies.**  \n\n\nNot just companies you\u2019ll submit an app to, but companies you\u2019d be psyched to work for!  \n\n\nAim for 15.\n\nYou want \\~3 of them to be \"stretch\" dream companies.\n\nThen you want 8-10 of them to be \"stepping stone\" companies that meet your criteria and allow you to build experience while getting paid what you're worth.\n\nFinally, you want \\~3 of them to be \"sandbox\" companies where you can test strategies in a low stakes environment.  \n\n\n**2 Do deep research on those companies.**  \n\n\nListen to earnings calls, read articles, find interviews and podcasts with executives.  \n\n\nWhat are their goals, challenges, and initiatives for the next 6-12 months?  \n\n\nHow can you add relevant value?\n\n**3 Make a list of contacts at those companies.**  \n\n\nFind 10-15 people at each company who could influence your ability to get hired for the role you want.  \n\n\nFind their email and save them in a spreadsheet.  \n\n\n10-15 at each company = \\~150 total people.\n\n**4 Schedule your emails for the new year.**  \n\n\nOne of the biggest complaints I hear from job seekers is that they don\u2019t have enough time.  \n\n\nBoth Gmail and Outlook have schedule send features.  \n\n\nDraft up notes to every person on your contact list and schedule them to send in January.  \n\n\nAfter that?  \n\n\nKick back, relax, and enjoy the end of this crazy year!\"", "author_fullname": "t2_jvuhrmb0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any idea on how to \"add relevant value\" as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zii77p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670740208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen this kind of posts multiple times on Linkedin and I want to try it out. &lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m yet to come up with any data engineering idea that could work with this. &lt;/p&gt;\n\n&lt;p&gt;Have you seen these posts before? What kind of ways can a DE &amp;quot;add relevant value&amp;quot; ? \\&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m open to all ideas.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;People say December is the worst time to job search.  &lt;/p&gt;\n\n&lt;p&gt;That\u2019s simply not true.  &lt;/p&gt;\n\n&lt;p&gt;It\u2019s actually the best time to get ahead of your competition.  &lt;/p&gt;\n\n&lt;p&gt;Here are 3 things you can do between now and January 2nd to put yourself ahead of the pack:  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1 Make a list of target companies.&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;Not just companies you\u2019ll submit an app to, but companies you\u2019d be psyched to work for!  &lt;/p&gt;\n\n&lt;p&gt;Aim for 15.&lt;/p&gt;\n\n&lt;p&gt;You want ~3 of them to be &amp;quot;stretch&amp;quot; dream companies.&lt;/p&gt;\n\n&lt;p&gt;Then you want 8-10 of them to be &amp;quot;stepping stone&amp;quot; companies that meet your criteria and allow you to build experience while getting paid what you&amp;#39;re worth.&lt;/p&gt;\n\n&lt;p&gt;Finally, you want ~3 of them to be &amp;quot;sandbox&amp;quot; companies where you can test strategies in a low stakes environment.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2 Do deep research on those companies.&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;Listen to earnings calls, read articles, find interviews and podcasts with executives.  &lt;/p&gt;\n\n&lt;p&gt;What are their goals, challenges, and initiatives for the next 6-12 months?  &lt;/p&gt;\n\n&lt;p&gt;How can you add relevant value?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3 Make a list of contacts at those companies.&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;Find 10-15 people at each company who could influence your ability to get hired for the role you want.  &lt;/p&gt;\n\n&lt;p&gt;Find their email and save them in a spreadsheet.  &lt;/p&gt;\n\n&lt;p&gt;10-15 at each company = ~150 total people.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4 Schedule your emails for the new year.&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;One of the biggest complaints I hear from job seekers is that they don\u2019t have enough time.  &lt;/p&gt;\n\n&lt;p&gt;Both Gmail and Outlook have schedule send features.  &lt;/p&gt;\n\n&lt;p&gt;Draft up notes to every person on your contact list and schedule them to send in January.  &lt;/p&gt;\n\n&lt;p&gt;After that?  &lt;/p&gt;\n\n&lt;p&gt;Kick back, relax, and enjoy the end of this crazy year!&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zii77p", "is_robot_indexable": true, "report_reasons": null, "author": "Necessary-Factor8861", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zii77p/any_idea_on_how_to_add_relevant_value_as_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zii77p/any_idea_on_how_to_add_relevant_value_as_a_data/", "subreddit_subscribers": 82586, "created_utc": 1670740208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wrote down some observations, and would love to hear yours.   \n[https://www.asanverse.com/why-data-projects-get-failed-reverse-etl/](https://www.asanverse.com/why-data-projects-get-failed-reverse-etl/)", "author_fullname": "t2_i2j8bdtn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is a data project delivering valuable insights for users still get failed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zi2cw7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670703499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote down some observations, and would love to hear yours.&lt;br/&gt;\n&lt;a href=\"https://www.asanverse.com/why-data-projects-get-failed-reverse-etl/\"&gt;https://www.asanverse.com/why-data-projects-get-failed-reverse-etl/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?auto=webp&amp;s=1f6570962697e192f874632ce64d36651cddb5f2", "width": 1000, "height": 501}, "resolutions": [{"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=26bb4299318c0ebdf935f8ea381bd5446353da57", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=88152dd1e2af6554dae0e7c06646988dd38cb059", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=622137554b579d28bcb6b349824635bcc1dad1cb", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d81b70acd5107625ae5670d59e1eed6c9d652730", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ed5cad063043d95adda5d722bd84e9e8d0104070", "width": 960, "height": 480}], "variants": {}, "id": "IpHXWC29OWlk894t8bJEURrR9ND6_veRIcSz_wmjqak"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zi2cw7", "is_robot_indexable": true, "report_reasons": null, "author": "ubukhary", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zi2cw7/why_is_a_data_project_delivering_valuable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zi2cw7/why_is_a_data_project_delivering_valuable/", "subreddit_subscribers": 82586, "created_utc": 1670703499.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}