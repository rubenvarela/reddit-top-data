{"kind": "Listing", "data": {"after": "t3_zi18rt", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are you ready? The CERN released 200 TB from the LHC to the public", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_zi0pff", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 280, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_22pmb26r", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 280, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NQ9iRZvlMLQvaliYi6vwhXfnD_BJtNyOQ22trMOOtWc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "Physics", "selftext": "", "author_fullname": "t2_61hj4kpq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Large Hadron Collider Beauty releases first set of data to the public", "link_flair_richtext": [], "subreddit_name_prefixed": "r/Physics", "hidden": false, "pwls": 6, "link_flair_css_class": "article", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_zi01o9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 275, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 275, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NQ9iRZvlMLQvaliYi6vwhXfnD_BJtNyOQ22trMOOtWc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1670697915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "home.cern", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://home.cern/news/news/knowledge-sharing/lhcb-releases-first-set-data-public", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9dMGnerLVSXi5wKf-9Wj9zrx7GZbjxheGslxHFRbli4.jpg?auto=webp&amp;s=a41d07dd00dd16510e84be529836a2c19c715aaa", "width": 561, "height": 284}, "resolutions": [{"url": "https://external-preview.redd.it/9dMGnerLVSXi5wKf-9Wj9zrx7GZbjxheGslxHFRbli4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a26b9c56ab6a7b6155b304ea0e997da4f04b951", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9dMGnerLVSXi5wKf-9Wj9zrx7GZbjxheGslxHFRbli4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=79e4ee12f7ef35f77f96c2e32308532a6211b56b", "width": 216, "height": 109}, {"url": "https://external-preview.redd.it/9dMGnerLVSXi5wKf-9Wj9zrx7GZbjxheGslxHFRbli4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b7aa5b007e57d54f30ce97e9fa5743d367ca798", "width": 320, "height": 161}], "variants": {}, "id": "zfoFnou3uOXirLtOngSd0kVZtiTWIOvNyuVffiZW4ZI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qhi6", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zi01o9", "is_robot_indexable": true, "report_reasons": null, "author": "corona_virus_is_dead", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/Physics/comments/zi01o9/large_hadron_collider_beauty_releases_first_set/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://home.cern/news/news/knowledge-sharing/lhcb-releases-first-set-data-public", "subreddit_subscribers": 2094475, "created_utc": 1670697915.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1670699517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "home.cern", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://home.cern/news/news/knowledge-sharing/lhcb-releases-first-set-data-public", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9dMGnerLVSXi5wKf-9Wj9zrx7GZbjxheGslxHFRbli4.jpg?auto=webp&amp;s=a41d07dd00dd16510e84be529836a2c19c715aaa", "width": 561, "height": 284}, "resolutions": [{"url": "https://external-preview.redd.it/9dMGnerLVSXi5wKf-9Wj9zrx7GZbjxheGslxHFRbli4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a26b9c56ab6a7b6155b304ea0e997da4f04b951", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9dMGnerLVSXi5wKf-9Wj9zrx7GZbjxheGslxHFRbli4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=79e4ee12f7ef35f77f96c2e32308532a6211b56b", "width": 216, "height": 109}, {"url": "https://external-preview.redd.it/9dMGnerLVSXi5wKf-9Wj9zrx7GZbjxheGslxHFRbli4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b7aa5b007e57d54f30ce97e9fa5743d367ca798", "width": 320, "height": 161}], "variants": {}, "id": "zfoFnou3uOXirLtOngSd0kVZtiTWIOvNyuVffiZW4ZI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zi0pff", "is_robot_indexable": true, "report_reasons": null, "author": "sersoniko", "discussion_type": null, "num_comments": 24, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_zi01o9", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zi0pff/are_you_ready_the_cern_released_200_tb_from_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://home.cern/news/news/knowledge-sharing/lhcb-releases-first-set-data-public", "subreddit_subscribers": 658692, "created_utc": 1670699517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, in a bit of shock.\n\nI just noticed I accidentally deleted almost all of my Logic sessions this week.\n\nTo clean up my desktop I put everything in a folder, and by mistake I deleted that folder.\n\nI have some backups, but not of recent sessions and not of component files like drum kits.\n\nPlease, give me all your suggestions for what I should do to go about recovering my life\u2019s work. I have contacted a local data recovery service about an appointment. NYC area. 2016 MBP OS Catalina.\n\nThis post may be slightly orthogonal to the subreddit but this is devastating to me.", "author_fullname": "t2_72q8zrxm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accidentally deleted 300 music sessions. Please help.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhxaqb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 73, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 73, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670691284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, in a bit of shock.&lt;/p&gt;\n\n&lt;p&gt;I just noticed I accidentally deleted almost all of my Logic sessions this week.&lt;/p&gt;\n\n&lt;p&gt;To clean up my desktop I put everything in a folder, and by mistake I deleted that folder.&lt;/p&gt;\n\n&lt;p&gt;I have some backups, but not of recent sessions and not of component files like drum kits.&lt;/p&gt;\n\n&lt;p&gt;Please, give me all your suggestions for what I should do to go about recovering my life\u2019s work. I have contacted a local data recovery service about an appointment. NYC area. 2016 MBP OS Catalina.&lt;/p&gt;\n\n&lt;p&gt;This post may be slightly orthogonal to the subreddit but this is devastating to me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhxaqb", "is_robot_indexable": true, "report_reasons": null, "author": "quick_advert", "discussion_type": null, "num_comments": 74, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zhxaqb/accidentally_deleted_300_music_sessions_please/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zhxaqb/accidentally_deleted_300_music_sessions_please/", "subreddit_subscribers": 658692, "created_utc": 1670691284.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_2emtczf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Twitter to begin purging accounts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhxv1x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Twitter will soon start freeing the name space of 1.5 billion accounts&lt;/p&gt;&amp;mdash; Elon Musk (@elonmusk) &lt;a href=\"https://twitter.com/elonmusk/status/1601124219009409024?ref_src=twsrc%5Etfw\"&gt;December 9, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "height": 200}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/elonmusk/status/1601124219009409024", "author_name": "Elon Musk", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Twitter will soon start freeing the name space of 1.5 billion accounts&lt;/p&gt;&amp;mdash; Elon Musk (@elonmusk) &lt;a href=\"https://twitter.com/elonmusk/status/1601124219009409024?ref_src=twsrc%5Etfw\"&gt;December 9, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/elonmusk", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Twitter will soon start freeing the name space of 1.5 billion accounts&lt;/p&gt;&amp;mdash; Elon Musk (@elonmusk) &lt;a href=\"https://twitter.com/elonmusk/status/1601124219009409024?ref_src=twsrc%5Etfw\"&gt;December 9, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/zhxv1x", "height": 200}, "link_flair_text": "News", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1670692665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "twitter.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://twitter.com/elonmusk/status/1601124219009409024", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhxv1x", "is_robot_indexable": true, "report_reasons": null, "author": "themadprogramer", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zhxv1x/twitter_to_begin_purging_accounts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://twitter.com/elonmusk/status/1601124219009409024", "subreddit_subscribers": 658692, "created_utc": 1670692665.0, "num_crossposts": 0, "media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/elonmusk/status/1601124219009409024", "author_name": "Elon Musk", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Twitter will soon start freeing the name space of 1.5 billion accounts&lt;/p&gt;&amp;mdash; Elon Musk (@elonmusk) &lt;a href=\"https://twitter.com/elonmusk/status/1601124219009409024?ref_src=twsrc%5Etfw\"&gt;December 9, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/elonmusk", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_rmiv8u6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Made a python script to download your Wattpad library in EPUB format", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_zhwimk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EGHcdAKuaAPLJZFIF1NH7Fd2ayjPZbP2OJ57pqPASHc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670689268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/MRtecno98/wattpad-archiver", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rGCC9rRPs4fHX-So2Xfsisq-f2R2Aaqbs6efxFmT8lo.jpg?auto=webp&amp;s=c2cb9d755e0645a136c9babedeeb91878a5fa87b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/rGCC9rRPs4fHX-So2Xfsisq-f2R2Aaqbs6efxFmT8lo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fdf01f2b052eefd556ea3334a5ed05681ac02dda", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/rGCC9rRPs4fHX-So2Xfsisq-f2R2Aaqbs6efxFmT8lo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ade60eafb4e3c3f6ca3db8e2cf934ba812b0cea4", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/rGCC9rRPs4fHX-So2Xfsisq-f2R2Aaqbs6efxFmT8lo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d38b4c29b8650ffce02b29ff2b0f02b15c03543c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/rGCC9rRPs4fHX-So2Xfsisq-f2R2Aaqbs6efxFmT8lo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2aacbf8e6a42c724264d5484112366ecd3eaa362", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/rGCC9rRPs4fHX-So2Xfsisq-f2R2Aaqbs6efxFmT8lo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9403d845467891f30cb725fa040a0b29e1e2dcd9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/rGCC9rRPs4fHX-So2Xfsisq-f2R2Aaqbs6efxFmT8lo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72700dd414392e56f8af3abaa1e1fc6c8559c76c", "width": 1080, "height": 540}], "variants": {}, "id": "ghy8xwLI1gbVk2AjeIsRzEJkWTcqbn_jERUZaizJHwk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhwimk", "is_robot_indexable": true, "report_reasons": null, "author": "MRtecno98", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zhwimk/made_a_python_script_to_download_your_wattpad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/MRtecno98/wattpad-archiver", "subreddit_subscribers": 658692, "created_utc": 1670689268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_i9ste", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Registration Issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 62, "top_awarded_type": null, "hide_score": false, "name": "t3_zhyp59", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZJQEFoQ5QYEbABGTJwM063Lf0_4Q5kHH2igtpd2UHcE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670694674.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/xviaonpez35a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/xviaonpez35a1.png?auto=webp&amp;s=1cc51c8acbf4d0d584c1ed83cb8f7f50b97c29b1", "width": 550, "height": 245}, "resolutions": [{"url": "https://preview.redd.it/xviaonpez35a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=338b41109b6607838df4ab2156045b0b908256c6", "width": 108, "height": 48}, {"url": "https://preview.redd.it/xviaonpez35a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e9bcb355399400b8b0e73f8ad2d952a8d206887", "width": 216, "height": 96}, {"url": "https://preview.redd.it/xviaonpez35a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a576dab66730204eaa12689cd164189ec4b51680", "width": 320, "height": 142}], "variants": {}, "id": "dkN-AwCA67GISid9-4gN_QKnCNEVAyOF9pomepyBmps"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhyp59", "is_robot_indexable": true, "report_reasons": null, "author": "root54", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zhyp59/wd_registration_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/xviaonpez35a1.png", "subreddit_subscribers": 658692, "created_utc": 1670694674.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I am building a media server and have amassed a lot of series over the years. I thought it would be really cool if I could find out where each episode starts so I can skip the intros/recaps/outros. I DO NOT want to go through all the eps (1000s) individually and write it down since it is likely that someone else has already done this. Any idea how to find this data online??\n\nAlso, one source I know is streaming sites obviously, as they have implemented this feature year ago. Any ideas on how to acquire this data from the sites or from individuals who have done so before? Thank.", "author_fullname": "t2_5atwpwyg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get \"skip intro\" data from streaming sites or elsewhere?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhhngr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": "", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670642979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am building a media server and have amassed a lot of series over the years. I thought it would be really cool if I could find out where each episode starts so I can skip the intros/recaps/outros. I DO NOT want to go through all the eps (1000s) individually and write it down since it is likely that someone else has already done this. Any idea how to find this data online??&lt;/p&gt;\n\n&lt;p&gt;Also, one source I know is streaming sites obviously, as they have implemented this feature year ago. Any ideas on how to acquire this data from the sites or from individuals who have done so before? Thank.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhhngr", "is_robot_indexable": true, "report_reasons": null, "author": "checkmyfancypants", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zhhngr/how_to_get_skip_intro_data_from_streaming_sites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zhhngr/how_to_get_skip_intro_data_from_streaming_sites/", "subreddit_subscribers": 658692, "created_utc": 1670642979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a tool out there that I can use to help backup my Audible purchases in an open audio format?", "author_fullname": "t2_3kmky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backing up Audible purchases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zi430s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670707617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a tool out there that I can use to help backup my Audible purchases in an open audio format?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zi430s", "is_robot_indexable": true, "report_reasons": null, "author": "Shazzbot", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zi430s/backing_up_audible_purchases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zi430s/backing_up_audible_purchases/", "subreddit_subscribers": 658692, "created_utc": 1670707617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want a new case that will only hold 2 3.5\" drives, so I would need a pseudo-nas or nas, o whatever to put the other drives in.\n\nI need at minimum 6 extra drives in the second case, and I don't need transcoding nor anything fancy, just be able to access through my computer and save there torrents and the torrent client be able to access as if they were normal drives (qbittorrent)\n\nI can't aford some sinology things and I would also like to be the less power consumption option\n\nI looked at some external cases like the sabrent 4 hdds's through usb 3, but those cases don't hold the drives, turn off the drives after 5 minutes even if you don't want to, and between that and virabtions, it ends killing the hdd's at mid term\n\nso, what can I do? any idea?", "author_fullname": "t2_59mxp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what would be my best and cheapest option to have multiple drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhhk3u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670642697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want a new case that will only hold 2 3.5&amp;quot; drives, so I would need a pseudo-nas or nas, o whatever to put the other drives in.&lt;/p&gt;\n\n&lt;p&gt;I need at minimum 6 extra drives in the second case, and I don&amp;#39;t need transcoding nor anything fancy, just be able to access through my computer and save there torrents and the torrent client be able to access as if they were normal drives (qbittorrent)&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t aford some sinology things and I would also like to be the less power consumption option&lt;/p&gt;\n\n&lt;p&gt;I looked at some external cases like the sabrent 4 hdds&amp;#39;s through usb 3, but those cases don&amp;#39;t hold the drives, turn off the drives after 5 minutes even if you don&amp;#39;t want to, and between that and virabtions, it ends killing the hdd&amp;#39;s at mid term&lt;/p&gt;\n\n&lt;p&gt;so, what can I do? any idea?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhhk3u", "is_robot_indexable": true, "report_reasons": null, "author": "DrKersh", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zhhk3u/what_would_be_my_best_and_cheapest_option_to_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zhhk3u/what_would_be_my_best_and_cheapest_option_to_have/", "subreddit_subscribers": 658692, "created_utc": 1670642697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm likely purchasing a 'small' harddrive by this community's standards, probably 1TB. I want to store old games, movies, tv, art, ''etc.'' on it, and I don't think I can store steam game data off the main PC, right? Then it doesn't need to be very fast. I also have a laptop not PC, so this seems to be the best way, right? Is a high capacity USB better?  \n\n\nIf speed isn't an issue, can I buy a cheaper one without having issues? I've found this Toshiba Canvio Ready 1TB for only $63 AUD = $42 USD\n\n&lt;[https://www.officeworks.com.au/shop/officeworks/p/toshiba-canvio-ready-hard-drive-1tb-tocvrdy1tb](https://www.officeworks.com.au/shop/officeworks/p/toshiba-canvio-ready-hard-drive-1tb-tocvrdy1tb)\\&gt;\n\nAgain sorry for being so clueless but noone irl I know would be able to advise me. Thanks in advance", "author_fullname": "t2_35mf5s4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it worth it to spend more on a more expensive brand of hard drive, or is failure rare enough not to bother?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhls6e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670656316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m likely purchasing a &amp;#39;small&amp;#39; harddrive by this community&amp;#39;s standards, probably 1TB. I want to store old games, movies, tv, art, &amp;#39;&amp;#39;etc.&amp;#39;&amp;#39; on it, and I don&amp;#39;t think I can store steam game data off the main PC, right? Then it doesn&amp;#39;t need to be very fast. I also have a laptop not PC, so this seems to be the best way, right? Is a high capacity USB better?  &lt;/p&gt;\n\n&lt;p&gt;If speed isn&amp;#39;t an issue, can I buy a cheaper one without having issues? I&amp;#39;ve found this Toshiba Canvio Ready 1TB for only $63 AUD = $42 USD&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;&lt;a href=\"https://www.officeworks.com.au/shop/officeworks/p/toshiba-canvio-ready-hard-drive-1tb-tocvrdy1tb\"&gt;https://www.officeworks.com.au/shop/officeworks/p/toshiba-canvio-ready-hard-drive-1tb-tocvrdy1tb&lt;/a&gt;&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;Again sorry for being so clueless but noone irl I know would be able to advise me. Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7FZyhpUah9sJzQiYiVCR9yMbC7JLf1zWYGg_TcZA_G4.jpg?auto=webp&amp;s=1451486d413255dcc836e7859801658ac2003643", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/7FZyhpUah9sJzQiYiVCR9yMbC7JLf1zWYGg_TcZA_G4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1ebeace0506af14956d7c246fa1230fc77a0cdf", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/7FZyhpUah9sJzQiYiVCR9yMbC7JLf1zWYGg_TcZA_G4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2656a2ac76b96064af8deb904c3c177be5bbbb59", "width": 216, "height": 216}], "variants": {}, "id": "UtZ3uFqD8luF8lBQLxpmX28-0-1LhoISyZy5OoeN7zA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhls6e", "is_robot_indexable": true, "report_reasons": null, "author": "Piranha_Plant05", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zhls6e/is_it_worth_it_to_spend_more_on_a_more_expensive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zhls6e/is_it_worth_it_to_spend_more_on_a_more_expensive/", "subreddit_subscribers": 658692, "created_utc": 1670656316.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a flatbed scanner and will use CmdTwain so you only have to click the .bat once and it will scan without having to enter anything or do any settings.\n\nNow cmdtwain overwrites the output file everytime if I dont change the filename before, so my plan is to use the \"running\" bash file to copy the scanned image into a destination folder, and rename it with an incrementing number (like scan00000, scan00001, scan00002,....)\n\nIs there a not too complex way of doing this?", "author_fullname": "t2_4i4hd8gq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archiving photos, how to copy and rename with incrementing number one by one?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zi5r9d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670711530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a flatbed scanner and will use CmdTwain so you only have to click the .bat once and it will scan without having to enter anything or do any settings.&lt;/p&gt;\n\n&lt;p&gt;Now cmdtwain overwrites the output file everytime if I dont change the filename before, so my plan is to use the &amp;quot;running&amp;quot; bash file to copy the scanned image into a destination folder, and rename it with an incrementing number (like scan00000, scan00001, scan00002,....)&lt;/p&gt;\n\n&lt;p&gt;Is there a not too complex way of doing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "24TB Raw/12TB ZFS/+18TB Active-Backup/+2x 8TB Cold-Storage ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zi5r9d", "is_robot_indexable": true, "report_reasons": null, "author": "PyroRider", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zi5r9d/archiving_photos_how_to_copy_and_rename_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zi5r9d/archiving_photos_how_to_copy_and_rename_with/", "subreddit_subscribers": 658692, "created_utc": 1670711530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not hearing back from their sales team at all, their website has said 'will be back in stock Mid-July' since June of this year. Getting more and more discouraged that my hopes for a SFF NAS are shot.  The NSC-810 seems one-of-a-kind (cheap knockoffs notwithstanding).", "author_fullname": "t2_f5smtb3l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has U-NAS gone out of business?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zi00j3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670697837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not hearing back from their sales team at all, their website has said &amp;#39;will be back in stock Mid-July&amp;#39; since June of this year. Getting more and more discouraged that my hopes for a SFF NAS are shot.  The NSC-810 seems one-of-a-kind (cheap knockoffs notwithstanding).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zi00j3", "is_robot_indexable": true, "report_reasons": null, "author": "stempoweredu", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zi00j3/has_unas_gone_out_of_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zi00j3/has_unas_gone_out_of_business/", "subreddit_subscribers": 658692, "created_utc": 1670697837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,  \nis it good idea to use NAS NVMe drive as system drive for OS in a desktop workstation?  \nI'm deciding between WD Red and Black.  \nReason I'm looking at NAS drive, is their endurance and lifespan.   \nThis would be a Win10 OS drive as well as cache drive for video editing (AE, Premiere).   \nHowever, I'm not sure how would TLER behave in non-raid situation.  \nShould I go for WD black instead?", "author_fullname": "t2_m1dmt3if", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I use NAS M.2 SSD drive as System drive in desktop?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhgf7r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670639395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nis it good idea to use NAS NVMe drive as system drive for OS in a desktop workstation?&lt;br/&gt;\nI&amp;#39;m deciding between WD Red and Black.&lt;br/&gt;\nReason I&amp;#39;m looking at NAS drive, is their endurance and lifespan.&lt;br/&gt;\nThis would be a Win10 OS drive as well as cache drive for video editing (AE, Premiere).&lt;br/&gt;\nHowever, I&amp;#39;m not sure how would TLER behave in non-raid situation.&lt;br/&gt;\nShould I go for WD black instead?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhgf7r", "is_robot_indexable": true, "report_reasons": null, "author": "a_moral_dilemma", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zhgf7r/can_i_use_nas_m2_ssd_drive_as_system_drive_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zhgf7r/can_i_use_nas_m2_ssd_drive_as_system_drive_in/", "subreddit_subscribers": 658692, "created_utc": 1670639395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi!  Not sure if this is the right thread but I\u2019m struggling to find the best place to post. \n\nMy mom made a purchase on Rakuten late last month (11/28) and they are now trying to claim the item was excluded. I am 100% positive it wasn\u2019t on the exclusion list and they have dodged her emails until today. I checked the exclusion list earlier and it still was not showing as excluded. \n\nNow, they have emailed back and when I just checked the exclusion list, they have added the exact item she purchased. It absolutely was not there at the time of purchase. \n\nIs there any way to see the archived version of the site if it was a few clicks into the page. I\u2019m very inexperienced with web archives but haven\u2019t had any luck with it. \n\nThanks so much!", "author_fullname": "t2_c9yps3r1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ziaf2j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670723416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!  Not sure if this is the right thread but I\u2019m struggling to find the best place to post. &lt;/p&gt;\n\n&lt;p&gt;My mom made a purchase on Rakuten late last month (11/28) and they are now trying to claim the item was excluded. I am 100% positive it wasn\u2019t on the exclusion list and they have dodged her emails until today. I checked the exclusion list earlier and it still was not showing as excluded. &lt;/p&gt;\n\n&lt;p&gt;Now, they have emailed back and when I just checked the exclusion list, they have added the exact item she purchased. It absolutely was not there at the time of purchase. &lt;/p&gt;\n\n&lt;p&gt;Is there any way to see the archived version of the site if it was a few clicks into the page. I\u2019m very inexperienced with web archives but haven\u2019t had any luck with it. &lt;/p&gt;\n\n&lt;p&gt;Thanks so much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ziaf2j", "is_robot_indexable": true, "report_reasons": null, "author": "WouldaBeenDinah", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ziaf2j/web_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ziaf2j/web_archive/", "subreddit_subscribers": 658692, "created_utc": 1670723416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've always used badblocks when adding drives to my servers, but that's no longer an option apparently. The 18tb drives I got are too large for badblocks, and every search I found on the topic says badblocks will never support higher capacity drives. So how does everyone on this sub burn in their drives? Or do you all just hope and pray they aren't duds?", "author_fullname": "t2_ao0okp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are you testing large capacity drives before putting them into production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zi4ryq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670709236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve always used badblocks when adding drives to my servers, but that&amp;#39;s no longer an option apparently. The 18tb drives I got are too large for badblocks, and every search I found on the topic says badblocks will never support higher capacity drives. So how does everyone on this sub burn in their drives? Or do you all just hope and pray they aren&amp;#39;t duds?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "156.99TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zi4ryq", "is_robot_indexable": true, "report_reasons": null, "author": "DGenerateKane", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zi4ryq/how_are_you_testing_large_capacity_drives_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zi4ryq/how_are_you_testing_large_capacity_drives_before/", "subreddit_subscribers": 658692, "created_utc": 1670709236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I might be mistaken here but is refreshing the charges in SSD NAND as easy as reading the files? Could we not just run something like `dd if=/dev/{SSD} of=/dev/null` periodically to ensure the data is healthy?", "author_fullname": "t2_12mhf9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to refresh data on an SSD for long term storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhx6zw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670691004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I might be mistaken here but is refreshing the charges in SSD NAND as easy as reading the files? Could we not just run something like &lt;code&gt;dd if=/dev/{SSD} of=/dev/null&lt;/code&gt; periodically to ensure the data is healthy?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Trinary = tiddie storage", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhx6zw", "is_robot_indexable": true, "report_reasons": null, "author": "oeCake", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zhx6zw/best_way_to_refresh_data_on_an_ssd_for_long_term/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zhx6zw/best_way_to_refresh_data_on_an_ssd_for_long_term/", "subreddit_subscribers": 658692, "created_utc": 1670691004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have an old TV with a built-in DVR that does not have an output. The only way to get video out of it is to screenrecord with a camera.\n\nDoes anyone have a recommendation for a camera setup that would be good for capturing it?\n\nI have an old logitech c270, it is not very good at capture, constantly autofocusing and its brightness response is poor so when it goes from dark to light scene it gets washed out for a second before it normalizes. I also have an old Olympus Micro 4/3, its slightly better video but still has some washout and its mic is much worse, and its files are huge compared to webcam(capped with OBS).\n\nDo any modern webcams or standalone cameras have the ability to capture TV quality video without washouts?", "author_fullname": "t2_p863w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Camera setup recommendations for screencapturing a TV?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhrjy4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670675883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an old TV with a built-in DVR that does not have an output. The only way to get video out of it is to screenrecord with a camera.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have a recommendation for a camera setup that would be good for capturing it?&lt;/p&gt;\n\n&lt;p&gt;I have an old logitech c270, it is not very good at capture, constantly autofocusing and its brightness response is poor so when it goes from dark to light scene it gets washed out for a second before it normalizes. I also have an old Olympus Micro 4/3, its slightly better video but still has some washout and its mic is much worse, and its files are huge compared to webcam(capped with OBS).&lt;/p&gt;\n\n&lt;p&gt;Do any modern webcams or standalone cameras have the ability to capture TV quality video without washouts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhrjy4", "is_robot_indexable": true, "report_reasons": null, "author": "idzero", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zhrjy4/camera_setup_recommendations_for_screencapturing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zhrjy4/camera_setup_recommendations_for_screencapturing/", "subreddit_subscribers": 658692, "created_utc": 1670675883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have compressed many files using bash find-exec tar.xz.\nNow I'd like to protect it for long term storage with ECCs.\nMy data is very mixed -- images, text, documents, html and associated directory, -- and assorted media and zip files.\nI plan to create small directories -- say, 1 to 10mb -- &amp; compress.\nThen put each file in its own directory along with its ECCs. \n\nfind . -type f -name \"*.tar.xz\" -exec par2 create -s 1k -n 3 -r 10 \"{}\" \\;\n\nThat's my script so far, guessing at appropriate block size, number of ECC files, and percent redundancy. I can't find good examples or discussion on how to set parameters with variant data.\n\nDoes this seem reasonable? Suggestions much appreciated !!", "author_fullname": "t2_4tyix39s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimal par2 Switches for Mixed Data . .", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhivzk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670649924.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670646673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have compressed many files using bash find-exec tar.xz.\nNow I&amp;#39;d like to protect it for long term storage with ECCs.\nMy data is very mixed -- images, text, documents, html and associated directory, -- and assorted media and zip files.\nI plan to create small directories -- say, 1 to 10mb -- &amp;amp; compress.\nThen put each file in its own directory along with its ECCs. &lt;/p&gt;\n\n&lt;p&gt;find . -type f -name &amp;quot;*.tar.xz&amp;quot; -exec par2 create -s 1k -n 3 -r 10 &amp;quot;{}&amp;quot; \\;&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s my script so far, guessing at appropriate block size, number of ECC files, and percent redundancy. I can&amp;#39;t find good examples or discussion on how to set parameters with variant data.&lt;/p&gt;\n\n&lt;p&gt;Does this seem reasonable? Suggestions much appreciated !!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhivzk", "is_robot_indexable": true, "report_reasons": null, "author": "ericlindellnyc", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zhivzk/optimal_par2_switches_for_mixed_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zhivzk/optimal_par2_switches_for_mixed_data/", "subreddit_subscribers": 658692, "created_utc": 1670646673.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Pretty self explanatory, just wanna download my deceased Grandpas whole facebook page for photos and videos - any help would be appreciated!", "author_fullname": "t2_ct8lqncf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to archive/download all of someone elses photos &amp; videos from facebook?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zi8x68", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670719605.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty self explanatory, just wanna download my deceased Grandpas whole facebook page for photos and videos - any help would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zi8x68", "is_robot_indexable": true, "report_reasons": null, "author": "Hellboymeep", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zi8x68/is_there_a_way_to_archivedownload_all_of_someone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zi8x68/is_there_a_way_to_archivedownload_all_of_someone/", "subreddit_subscribers": 658692, "created_utc": 1670719605.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Thinking about consolidating several spanned AFS Fusion drives \u201c3 disk pool\u201d to a single disk.\n\nAnyone that understands read times from a sleep state to fully being able to read the entire file system , how should I select a disk if that is my biggest focus to try and resolve without wanting deep enough pockets to go full SSD?\n\nAre there drives I should be targeting, so when good deals come up I can confidently select a disk?", "author_fullname": "t2_3tjx3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacing 3 disk APFS Fusion Pool, Reference to learn which ~20TB drives to select for wake/fully readable times?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zi3yla", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670707336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thinking about consolidating several spanned AFS Fusion drives \u201c3 disk pool\u201d to a single disk.&lt;/p&gt;\n\n&lt;p&gt;Anyone that understands read times from a sleep state to fully being able to read the entire file system , how should I select a disk if that is my biggest focus to try and resolve without wanting deep enough pockets to go full SSD?&lt;/p&gt;\n\n&lt;p&gt;Are there drives I should be targeting, so when good deals come up I can confidently select a disk?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zi3yla", "is_robot_indexable": true, "report_reasons": null, "author": "rotarypower101", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zi3yla/replacing_3_disk_apfs_fusion_pool_reference_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zi3yla/replacing_3_disk_apfs_fusion_pool_reference_to/", "subreddit_subscribers": 658692, "created_utc": 1670707336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought a few 16TB Red Pro drives from the WD store during the WD Black Friday sale. I tried to register them, but 2 drives came back saying invalid S/N. Has anyone run into this lately? \n\nThe drives arrived in good condition, correct packaging and sealed anti-static bags. Unfortunately, one of the bad serial numbers is from a DOA drive that needs to be exchanged. I will call support on Monday. I'm just interested if anyone else has experienced this from WD Store fulfilled drives. Thanks", "author_fullname": "t2_4kdg2kyu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Red Pro serial number not found", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhym9p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670694481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a few 16TB Red Pro drives from the WD store during the WD Black Friday sale. I tried to register them, but 2 drives came back saying invalid S/N. Has anyone run into this lately? &lt;/p&gt;\n\n&lt;p&gt;The drives arrived in good condition, correct packaging and sealed anti-static bags. Unfortunately, one of the bad serial numbers is from a DOA drive that needs to be exchanged. I will call support on Monday. I&amp;#39;m just interested if anyone else has experienced this from WD Store fulfilled drives. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhym9p", "is_robot_indexable": true, "report_reasons": null, "author": "GroundWireNeutral", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zhym9p/wd_red_pro_serial_number_not_found/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zhym9p/wd_red_pro_serial_number_not_found/", "subreddit_subscribers": 658692, "created_utc": 1670694481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So what\u2019s the latest way to download stories without getting your account locked and temp banned? All the sites I was using the past few years all now trigger the account lock instantly. Anyone have a solution to this?", "author_fullname": "t2_lfnxe06", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IG story downloading", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhyex1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670693983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So what\u2019s the latest way to download stories without getting your account locked and temp banned? All the sites I was using the past few years all now trigger the account lock instantly. Anyone have a solution to this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhyex1", "is_robot_indexable": true, "report_reasons": null, "author": "haggard929", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zhyex1/ig_story_downloading/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zhyex1/ig_story_downloading/", "subreddit_subscribers": 658692, "created_utc": 1670693983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've found 8TB WD Black drives for $100 but they aren't advertised as NAS drives. Would this be OK for home use in raidZ. I'm using some tooles drive bays and I'm worried about the drives being SMR. \n\nhttps://pcpartpicker.com/product/6gVG3C/western-digital-black-8-tb-35-7200rpm-internal-hard-drive-wd8001fzbx", "author_fullname": "t2_ntpfs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Black 8TB for NAS use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhq89j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670671501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve found 8TB WD Black drives for $100 but they aren&amp;#39;t advertised as NAS drives. Would this be OK for home use in raidZ. I&amp;#39;m using some tooles drive bays and I&amp;#39;m worried about the drives being SMR. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pcpartpicker.com/product/6gVG3C/western-digital-black-8-tb-35-7200rpm-internal-hard-drive-wd8001fzbx\"&gt;https://pcpartpicker.com/product/6gVG3C/western-digital-black-8-tb-35-7200rpm-internal-hard-drive-wd8001fzbx&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "8TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zhq89j", "is_robot_indexable": true, "report_reasons": null, "author": "Bob_Cat_Stevens", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zhq89j/wd_black_8tb_for_nas_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zhq89j/wd_black_8tb_for_nas_use/", "subreddit_subscribers": 658692, "created_utc": 1670671501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5zija7cw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Ironwolf, TrueNAS Showing lots of read/write/checksum errors shows as faulted, but SMART data is good?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zi4hc0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/K7VartBMflHdRxIr-ZbD0sESIKj_Ki4GCW2xoc5N4-s.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670708530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/jjs5f67l455a1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/jjs5f67l455a1.png?auto=webp&amp;s=d672426ef5b4ff3f980c27126a4b3381cbfbd376", "width": 668, "height": 790}, "resolutions": [{"url": "https://preview.redd.it/jjs5f67l455a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=103a146d481ee2ec1ce2add9a6deeb6dcf764cd8", "width": 108, "height": 127}, {"url": "https://preview.redd.it/jjs5f67l455a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d5e29eb58169b19ac22b9894a60b454a8c5cc9b", "width": 216, "height": 255}, {"url": "https://preview.redd.it/jjs5f67l455a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a2e2e65155e9d15309f8377aa708624b66a1cd31", "width": 320, "height": 378}, {"url": "https://preview.redd.it/jjs5f67l455a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=55b76e5955cc7cfc2f14c3169bfeb639f454cb19", "width": 640, "height": 756}], "variants": {}, "id": "ZP_hQmh-rrjWiTj2YwOHvDuMPZpXHM6Hzs56RZDnyhk"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "8TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zi4hc0", "is_robot_indexable": true, "report_reasons": null, "author": "SumSnowMan", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zi4hc0/seagate_ironwolf_truenas_showing_lots_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/jjs5f67l455a1.png", "subreddit_subscribers": 658692, "created_utc": 1670708530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Finally had a drive fail.  Have like 50 drives running 24/7 so I guess I can\u2019t complain.  \n\nIt\u2019s a 14tb WD easystore from bestbuy I bought 12 months ago.  I have a bunch of easystore boxes and the housing and stuff, but I don\u2019t know which housing and box matches the one that died.  Does that matter?  Or can I just put any other housing and box on it?", "author_fullname": "t2_ytkgh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Process to get a shucked Easystore rmaed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zi36ao", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670705496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally had a drive fail.  Have like 50 drives running 24/7 so I guess I can\u2019t complain.  &lt;/p&gt;\n\n&lt;p&gt;It\u2019s a 14tb WD easystore from bestbuy I bought 12 months ago.  I have a bunch of easystore boxes and the housing and stuff, but I don\u2019t know which housing and box matches the one that died.  Does that matter?  Or can I just put any other housing and box on it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "476TB Unraid", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zi36ao", "is_robot_indexable": true, "report_reasons": null, "author": "sittingmongoose", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zi36ao/process_to_get_a_shucked_easystore_rmaed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zi36ao/process_to_get_a_shucked_easystore_rmaed/", "subreddit_subscribers": 658692, "created_utc": 1670705496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently installed a 2 TB Samsung Pro 980 M.2 Nvme to use as my boot drive. I cloned my original boot drive which was about 250 GB to the M.2. After a week of computer use, my M.2 states it has 1.2 TB written to it in Samsung magician. Is this normal? I have not installed anything else other than to copy the boot drive to the Nvme.", "author_fullname": "t2_iqasx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about TBW for Samsung 980 Pro", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zi18rt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670700872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently installed a 2 TB Samsung Pro 980 M.2 Nvme to use as my boot drive. I cloned my original boot drive which was about 250 GB to the M.2. After a week of computer use, my M.2 states it has 1.2 TB written to it in Samsung magician. Is this normal? I have not installed anything else other than to copy the boot drive to the Nvme.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zi18rt", "is_robot_indexable": true, "report_reasons": null, "author": "Insanity8016", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zi18rt/question_about_tbw_for_samsung_980_pro/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zi18rt/question_about_tbw_for_samsung_980_pro/", "subreddit_subscribers": 658692, "created_utc": 1670700872.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}