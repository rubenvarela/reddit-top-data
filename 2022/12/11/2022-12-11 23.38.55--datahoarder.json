{"kind": "Listing", "data": {"after": "t3_zibwgv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Pretty sure Memorex isn't a recommended brand anyway, but I figured it can't hurt to put it out there in case someone just didn't know.\n\nI received a stack of 19 Memorex DVDs from a relative to put them on a flash drive for Christmas. I've gone through 13 so far and 6 have come back with issues.\n\nIn MakeMKV, all but one disc reported:\n\nError 'Scsi error - MEDIUM ERROR:L-EC UNCORRECTABLE ERROR' occurred while reading\n\nThe other disc had a different error I don't remember and haven't seen since. Some others were completely unreadable. Some discs were accessible through other means, but any video I could retrieve are severely corrupted so I'll have to copy a bunch of tapes as well to fill in the gaps.\n\nNo discs had any visible damage. No bit rot no scratches. They've been untouched since 2005.\n\nI'm not asking for any advice, just recording my experience with these discs for others to reference and maybe kick their butt into gear to update their storage situation.", "author_fullname": "t2_2yo9if6c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA to anyone who used Memorex DVD-R from 2005", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ziwl8o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 141, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 141, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670772843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty sure Memorex isn&amp;#39;t a recommended brand anyway, but I figured it can&amp;#39;t hurt to put it out there in case someone just didn&amp;#39;t know.&lt;/p&gt;\n\n&lt;p&gt;I received a stack of 19 Memorex DVDs from a relative to put them on a flash drive for Christmas. I&amp;#39;ve gone through 13 so far and 6 have come back with issues.&lt;/p&gt;\n\n&lt;p&gt;In MakeMKV, all but one disc reported:&lt;/p&gt;\n\n&lt;p&gt;Error &amp;#39;Scsi error - MEDIUM ERROR:L-EC UNCORRECTABLE ERROR&amp;#39; occurred while reading&lt;/p&gt;\n\n&lt;p&gt;The other disc had a different error I don&amp;#39;t remember and haven&amp;#39;t seen since. Some others were completely unreadable. Some discs were accessible through other means, but any video I could retrieve are severely corrupted so I&amp;#39;ll have to copy a bunch of tapes as well to fill in the gaps.&lt;/p&gt;\n\n&lt;p&gt;No discs had any visible damage. No bit rot no scratches. They&amp;#39;ve been untouched since 2005.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not asking for any advice, just recording my experience with these discs for others to reference and maybe kick their butt into gear to update their storage situation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "ziwl8o", "is_robot_indexable": true, "report_reasons": null, "author": "DeckardTBechard", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ziwl8o/psa_to_anyone_who_used_memorex_dvdr_from_2005/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ziwl8o/psa_to_anyone_who_used_memorex_dvdr_from_2005/", "subreddit_subscribers": 658838, "created_utc": 1670772843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Archive Of A Vanishing World | Albert Kahn sought to preserve a world he perceived to be disappearing. A century later, his \u201cArchives de la Plan\u00e8te\u201d connects disparate lands, dying ecosystems and cultures, and a world being utterly transformed by modernity.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 103, "top_awarded_type": null, "hide_score": false, "name": "t3_zidk9g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 112, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_5uj3gfdz", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 112, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nSE5wMFlwq-7M5dE2PIflnKhdTgYZyxlieSriZfzgGw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "longform", "selftext": "", "author_fullname": "t2_i9130lnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Archive Of A Vanishing World | Albert Kahn sought to preserve a world he perceived to be disappearing. A century later, his \u201cArchives de la Plan\u00e8te\u201d connects disparate lands, dying ecosystems and cultures, and a world being utterly transformed by modernity.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/longform", "hidden": false, "pwls": 0, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 103, "top_awarded_type": null, "hide_score": false, "name": "t3_wxql4s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/nSE5wMFlwq-7M5dE2PIflnKhdTgYZyxlieSriZfzgGw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1661464174.0, "link_flair_type": "text", "wls": 0, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "noemamag.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.noemamag.com/the-archive-of-a-vanishing-world/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?auto=webp&amp;s=5fa23c5797ce0f314ebe80f50453d29c7068e614", "width": 2000, "height": 1483}, "resolutions": [{"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=62fe2cec7ca18e760a4304daae0b2fd627f10479", "width": 108, "height": 80}, {"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d59b271df4a7de9cf47f0c3a59b965c8841a4395", "width": 216, "height": 160}, {"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6075aabef09ce217ab4965df5b79518760d514df", "width": 320, "height": 237}, {"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a11ff3f53ee8d023569ca28740167293dfe62f00", "width": 640, "height": 474}, {"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=06bd9dc51c2bd23121495cc139ea6bf49ea2da91", "width": 960, "height": 711}, {"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5efa1d055e76dc8e7070beab7dfd637417709f4e", "width": 1080, "height": 800}], "variants": {}, "id": "dmqVbCAW_PWPbL0j8VThH7pHcUksTJF8O_JiUPO74gU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qht5", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "wxql4s", "is_robot_indexable": true, "report_reasons": null, "author": "bethany_mcguire", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "no_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/longform/comments/wxql4s/the_archive_of_a_vanishing_world_albert_kahn/", "parent_whitelist_status": "no_ads", "stickied": false, "url": "https://www.noemamag.com/the-archive-of-a-vanishing-world/", "subreddit_subscribers": 11999, "created_utc": 1661464174.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1670730066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "noemamag.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.noemamag.com/the-archive-of-a-vanishing-world/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?auto=webp&amp;s=5fa23c5797ce0f314ebe80f50453d29c7068e614", "width": 2000, "height": 1483}, "resolutions": [{"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=62fe2cec7ca18e760a4304daae0b2fd627f10479", "width": 108, "height": 80}, {"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d59b271df4a7de9cf47f0c3a59b965c8841a4395", "width": 216, "height": 160}, {"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6075aabef09ce217ab4965df5b79518760d514df", "width": 320, "height": 237}, {"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a11ff3f53ee8d023569ca28740167293dfe62f00", "width": 640, "height": 474}, {"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=06bd9dc51c2bd23121495cc139ea6bf49ea2da91", "width": 960, "height": 711}, {"url": "https://external-preview.redd.it/gF4WTeMZ7ALPTPO7wx5qjEhXeal0gnR6iY1yjUOAdNI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5efa1d055e76dc8e7070beab7dfd637417709f4e", "width": 1080, "height": 800}], "variants": {}, "id": "dmqVbCAW_PWPbL0j8VThH7pHcUksTJF8O_JiUPO74gU"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 250, "id": "award_a67d649d-5aa5-407e-a98b-32fd9e3a9696", "penny_donate": null, "award_sub_type": "APPRECIATION", "coin_reward": 100, "icon_url": "https://i.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=16&amp;height=16&amp;auto=webp&amp;s=bbfa251092cce139b37d74237ec28a8c4e8f06b0", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=32&amp;height=32&amp;auto=webp&amp;s=e1f9dd28741e2551b1fbbd341b006cc316f48fa1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=48&amp;height=48&amp;auto=webp&amp;s=d93434d26563a534397ff748cce71d4b733c32d9", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=64&amp;height=64&amp;auto=webp&amp;s=cf4a1ddb8474d11682f0d88aa32562f9fcbf30b0", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=128&amp;height=128&amp;auto=webp&amp;s=70b1596cdd0ae75b52db5c2732d8c336d300cc11", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "The more you know... Gives %{coin_symbol}100 Coins to both the author and the community.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 100, "count": 1, "static_icon_height": 2048, "name": "Today I Learned", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=16&amp;height=16&amp;auto=webp&amp;s=bbfa251092cce139b37d74237ec28a8c4e8f06b0", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=32&amp;height=32&amp;auto=webp&amp;s=e1f9dd28741e2551b1fbbd341b006cc316f48fa1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=48&amp;height=48&amp;auto=webp&amp;s=d93434d26563a534397ff748cce71d4b733c32d9", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=64&amp;height=64&amp;auto=webp&amp;s=cf4a1ddb8474d11682f0d88aa32562f9fcbf30b0", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png?width=128&amp;height=128&amp;auto=webp&amp;s=70b1596cdd0ae75b52db5c2732d8c336d300cc11", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/bph2png4ajz31_TodayILearned.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zidk9g", "is_robot_indexable": true, "report_reasons": null, "author": "kraft-skunk", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_wxql4s", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zidk9g/the_archive_of_a_vanishing_world_albert_kahn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.noemamag.com/the-archive-of-a-vanishing-world/", "subreddit_subscribers": 658838, "created_utc": 1670730066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_h6kxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How 45 Drives Open Source Houston Command Center Makes ZFS On Linux Easy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_ziognu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 48, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/07uTKOB1z7E?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How 45 Drives Open Source Houston Command Center Makes ZFS On Linux Easy\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How 45 Drives Open Source Houston Command Center Makes ZFS On Linux Easy", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/07uTKOB1z7E?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How 45 Drives Open Source Houston Command Center Makes ZFS On Linux Easy\"&gt;&lt;/iframe&gt;", "author_name": "Lawrence Systems", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/07uTKOB1z7E/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@LAWRENCESYSTEMS"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/07uTKOB1z7E?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How 45 Drives Open Source Houston Command Center Makes ZFS On Linux Easy\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/ziognu", "height": 200}, "link_flair_text": "Video", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/K-SJm8OQcs7j5BXias9NZdNlSINQyZtJog_OUa2NHXI.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670757690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=07uTKOB1z7E", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QSt43o6FlOwL5od-fJPC1lDVthKeGhjoK068srqkQG4.jpg?auto=webp&amp;s=306b7ddfb448fcc8423057c899d248ff84d6ab0c", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/QSt43o6FlOwL5od-fJPC1lDVthKeGhjoK068srqkQG4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=11cfab0791905228777a90a7934cda94b8cb9b5d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/QSt43o6FlOwL5od-fJPC1lDVthKeGhjoK068srqkQG4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5188d89ad88ae2ad3e9fd7707cc108f0ffa01048", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/QSt43o6FlOwL5od-fJPC1lDVthKeGhjoK068srqkQG4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b61a30be4f580e2bd063906dfdb8260a54aa2d65", "width": 320, "height": 240}], "variants": {}, "id": "ZRD4ZV5bW5cnOu7k8S2Q7FXH1yik2jwraBNvo3tp4Q0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "400TB raw", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "ziognu", "is_robot_indexable": true, "report_reasons": null, "author": "It_Is1-24PM", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/ziognu/how_45_drives_open_source_houston_command_center/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=07uTKOB1z7E", "subreddit_subscribers": 658838, "created_utc": 1670757690.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How 45 Drives Open Source Houston Command Center Makes ZFS On Linux Easy", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/07uTKOB1z7E?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How 45 Drives Open Source Houston Command Center Makes ZFS On Linux Easy\"&gt;&lt;/iframe&gt;", "author_name": "Lawrence Systems", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/07uTKOB1z7E/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@LAWRENCESYSTEMS"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was just reading the arch wiki on setting up RAID and it mentions it's highly recommended to partition the device used for RAID. But it doesn't really explain why. Could someone enlighten me please? Aside from the later suggestion of leaving 100 MiB on the end to make replacement easier is there a reason I should partition the drive before putting it into RAID?\n\nhttps://wiki.archlinux.org/title/RAID#Partition_the_devices\n\nNote: yesterday I did just this but using LVM. So I had 2 18TiB drives, I created 32 partitions about 0.5TiB each and then put them into an LV with total size 18TiB and RAID1 across each device.", "author_fullname": "t2_4fuysild", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is it recommended to partition drives in RAID?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ziozxs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670759186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was just reading the arch wiki on setting up RAID and it mentions it&amp;#39;s highly recommended to partition the device used for RAID. But it doesn&amp;#39;t really explain why. Could someone enlighten me please? Aside from the later suggestion of leaving 100 MiB on the end to make replacement easier is there a reason I should partition the drive before putting it into RAID?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://wiki.archlinux.org/title/RAID#Partition_the_devices\"&gt;https://wiki.archlinux.org/title/RAID#Partition_the_devices&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Note: yesterday I did just this but using LVM. So I had 2 18TiB drives, I created 32 partitions about 0.5TiB each and then put them into an LV with total size 18TiB and RAID1 across each device.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ziozxs", "is_robot_indexable": true, "report_reasons": null, "author": "emax-gomax", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ziozxs/why_is_it_recommended_to_partition_drives_in_raid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ziozxs/why_is_it_recommended_to_partition_drives_in_raid/", "subreddit_subscribers": 658838, "created_utc": 1670759186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m not too sure where to post this but my Aunts birthday is coming up and I wanted to suppose her by digitizing a huge collection of Mini CDs we had from recording in the Sony Handicam. The problem is I have no idea how to accomplish this. Ive been looking online saying I have to finalize the disks but we lost the Camera a long time ago. And the DVD player I have now is too big for the disk and it doesn\u2019t reach the laser for it to be read. i\u2019ve submitted some pictures for context. Links to products I could use and just advice in general would be much appreciated. I thank any and all who answer in advance. \n\nhttps://imgur.com/a/XxFOpAU/", "author_fullname": "t2_1gzox78b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Convert / Digitize Mini CDs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zj1in9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670781143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m not too sure where to post this but my Aunts birthday is coming up and I wanted to suppose her by digitizing a huge collection of Mini CDs we had from recording in the Sony Handicam. The problem is I have no idea how to accomplish this. Ive been looking online saying I have to finalize the disks but we lost the Camera a long time ago. And the DVD player I have now is too big for the disk and it doesn\u2019t reach the laser for it to be read. i\u2019ve submitted some pictures for context. Links to products I could use and just advice in general would be much appreciated. I thank any and all who answer in advance. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/XxFOpAU/\"&gt;https://imgur.com/a/XxFOpAU/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IFCCfA1gzcM3rYXncct0pGnXMtPEHimLbTZv4SAwghE.jpg?auto=webp&amp;s=2eb91746775e5c5b9d8ae9968bf0cac1d051e989", "width": 1500, "height": 2000}, "resolutions": [{"url": "https://external-preview.redd.it/IFCCfA1gzcM3rYXncct0pGnXMtPEHimLbTZv4SAwghE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b50b6fabcec68eaf05513ba9f17851e8d3232d5c", "width": 108, "height": 144}, {"url": "https://external-preview.redd.it/IFCCfA1gzcM3rYXncct0pGnXMtPEHimLbTZv4SAwghE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1a85222f148675de890c42ba824ae8093bc8cbc", "width": 216, "height": 288}, {"url": "https://external-preview.redd.it/IFCCfA1gzcM3rYXncct0pGnXMtPEHimLbTZv4SAwghE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1df31c78c061c24303689bc6967448951b3c9f66", "width": 320, "height": 426}, {"url": "https://external-preview.redd.it/IFCCfA1gzcM3rYXncct0pGnXMtPEHimLbTZv4SAwghE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4a0890e5b734910843fb5c30aa2891ffa44d220", "width": 640, "height": 853}, {"url": "https://external-preview.redd.it/IFCCfA1gzcM3rYXncct0pGnXMtPEHimLbTZv4SAwghE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e37644eae4001d3755b2e8dca6f36e75fdc2d68", "width": 960, "height": 1280}, {"url": "https://external-preview.redd.it/IFCCfA1gzcM3rYXncct0pGnXMtPEHimLbTZv4SAwghE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eba881871f7561dd5e50bc21c8569a5ea58721df", "width": 1080, "height": 1440}], "variants": {}, "id": "Aocy3QeJzyQ0zog21FfBmD2vqr7bvvqDi8LSqGeMkZ0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zj1in9", "is_robot_indexable": true, "report_reasons": null, "author": "SuperSpirito", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zj1in9/how_to_convert_digitize_mini_cds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zj1in9/how_to_convert_digitize_mini_cds/", "subreddit_subscribers": 658838, "created_utc": 1670781143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Is there a way to download the contents of my account in case yahoo goes belly up unexpectedly or something..", "author_fullname": "t2_8cdiegha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Found my old yahoo account from 2007 and it has some old emails between my and some family members I'd like to keep.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zj4zl1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670786714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to download the contents of my account in case yahoo goes belly up unexpectedly or something..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zj4zl1", "is_robot_indexable": true, "report_reasons": null, "author": "Perfect_Salamander_2", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zj4zl1/found_my_old_yahoo_account_from_2007_and_it_has/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zj4zl1/found_my_old_yahoo_account_from_2007_and_it_has/", "subreddit_subscribers": 658838, "created_utc": 1670786714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Pretty self explanatory, just wanna download my deceased Grandpas whole facebook page for photos and videos - any help would be appreciated!", "author_fullname": "t2_ct8lqncf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to archive/download all of someone elses photos &amp; videos from facebook?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zi8x68", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670719605.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty self explanatory, just wanna download my deceased Grandpas whole facebook page for photos and videos - any help would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zi8x68", "is_robot_indexable": true, "report_reasons": null, "author": "Hellboymeep", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zi8x68/is_there_a_way_to_archivedownload_all_of_someone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zi8x68/is_there_a_way_to_archivedownload_all_of_someone/", "subreddit_subscribers": 658838, "created_utc": 1670719605.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Every pixiv scraper I can find is purely for scraping tags, but I want to bulk download all my followings, coz I follow over 1k people. Any suggestions?", "author_fullname": "t2_10no10lp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any pixiv scraper that can target my follow list or list of users?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zjbah6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670796273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every pixiv scraper I can find is purely for scraping tags, but I want to bulk download all my followings, coz I follow over 1k people. Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zjbah6", "is_robot_indexable": true, "report_reasons": null, "author": "MayonnaisalSpray", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zjbah6/any_pixiv_scraper_that_can_target_my_follow_list/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zjbah6/any_pixiv_scraper_that_can_target_my_follow_list/", "subreddit_subscribers": 658838, "created_utc": 1670796273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All\n\nI tend to find myself having weird 'me only' issues when hoarding. Im currently sat at over 1m images, from deviantart, pixiv, pinterest but then i discovered wallhaven.....you can see where this is going.\n\nAssuming you are using gallery-dl with the '--write-metadata' option to download an ID search or batch of single images it will check the json for the file for the username, create a folder structure and then sort the files for that user to the correct folder.\n\nit'll then add the user account to a txt file for batch downloading of that users uploads. Eventually, ill no doubt have all the user accounts (if they have uploaded) but i then use this for more downloads.\n\nAs an example, i download all the images tagged for cyberpunk 2077 lets say around 300, this is parsed i then have 90 users ready to go and end up with 100k images after running that list. They state they only have about 1m images themselves so shouldn't take long to get it all then just append to my collection.\n\nI couldn't find a list of all users, a search all(\\*) or a list of how many IDs they have. But if you know, let me know. I could have just iterated over IDs in a loop 1 to 999999 but thats possibly a lot of waste.\n\nAnyway......\n\nAs i say, this fits my need for how i scrape stuff from there but thought id share it should someone want to improve/make use of it. Its powershell btw\n\n[https://pastebin.com/q6JXgTL7](https://pastebin.com/q6JXgTL7)", "author_fullname": "t2_8088jmi3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wallhaven Organiser", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zjcsp9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670798375.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All&lt;/p&gt;\n\n&lt;p&gt;I tend to find myself having weird &amp;#39;me only&amp;#39; issues when hoarding. Im currently sat at over 1m images, from deviantart, pixiv, pinterest but then i discovered wallhaven.....you can see where this is going.&lt;/p&gt;\n\n&lt;p&gt;Assuming you are using gallery-dl with the &amp;#39;--write-metadata&amp;#39; option to download an ID search or batch of single images it will check the json for the file for the username, create a folder structure and then sort the files for that user to the correct folder.&lt;/p&gt;\n\n&lt;p&gt;it&amp;#39;ll then add the user account to a txt file for batch downloading of that users uploads. Eventually, ill no doubt have all the user accounts (if they have uploaded) but i then use this for more downloads.&lt;/p&gt;\n\n&lt;p&gt;As an example, i download all the images tagged for cyberpunk 2077 lets say around 300, this is parsed i then have 90 users ready to go and end up with 100k images after running that list. They state they only have about 1m images themselves so shouldn&amp;#39;t take long to get it all then just append to my collection.&lt;/p&gt;\n\n&lt;p&gt;I couldn&amp;#39;t find a list of all users, a search all(*) or a list of how many IDs they have. But if you know, let me know. I could have just iterated over IDs in a loop 1 to 999999 but thats possibly a lot of waste.&lt;/p&gt;\n\n&lt;p&gt;Anyway......&lt;/p&gt;\n\n&lt;p&gt;As i say, this fits my need for how i scrape stuff from there but thought id share it should someone want to improve/make use of it. Its powershell btw&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pastebin.com/q6JXgTL7\"&gt;https://pastebin.com/q6JXgTL7&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?auto=webp&amp;s=07c121a0180003f7373863af66192b6ff6a937da", "width": 150, "height": 150}, "resolutions": [{"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df9c6a296446d05d873c629a30253398c4d29c1b", "width": 108, "height": 108}], "variants": {}, "id": "OgFzGCIRw1ZxjMOSkfV1OiH-_nQiZl8rzSonmOAuhGs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zjcsp9", "is_robot_indexable": true, "report_reasons": null, "author": "Obvious-Viking", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zjcsp9/wallhaven_organiser/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zjcsp9/wallhaven_organiser/", "subreddit_subscribers": 658838, "created_utc": 1670798375.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have about 250-300 blurays that I have made over time. I do not have originals any longer as I sold them off when Bluray was \"hot\". These are now 5+ years old and worried about bluRay rot.\n\n90+% of the blurays are LTH 25gb\n\nWould a 8TB be good enough to store all them? Since streaming is so abundant now and looks to be for the forseeable future unless something serious happened to the world ( nowadays who can say, would it be better to find them on the net when I want?\n\nWill an external HD be ok if written to and then only accessed when needed ( once or twice a month) and then shut off?  \n\nI am getting older and plan on retiring in next 10 years. I will have plenty of time to watch all my \"classic\" movies a that time. So I am wanting to store all this until I retire.  Then my family can deal with it when I pass.... HAHA  dont know what they are in for\n\n\n32TB so far not including blurays", "author_fullname": "t2_lzwh7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thinking of moving my copied BluRays to external hard drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zjbo3e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670796815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have about 250-300 blurays that I have made over time. I do not have originals any longer as I sold them off when Bluray was &amp;quot;hot&amp;quot;. These are now 5+ years old and worried about bluRay rot.&lt;/p&gt;\n\n&lt;p&gt;90+% of the blurays are LTH 25gb&lt;/p&gt;\n\n&lt;p&gt;Would a 8TB be good enough to store all them? Since streaming is so abundant now and looks to be for the forseeable future unless something serious happened to the world ( nowadays who can say, would it be better to find them on the net when I want?&lt;/p&gt;\n\n&lt;p&gt;Will an external HD be ok if written to and then only accessed when needed ( once or twice a month) and then shut off?  &lt;/p&gt;\n\n&lt;p&gt;I am getting older and plan on retiring in next 10 years. I will have plenty of time to watch all my &amp;quot;classic&amp;quot; movies a that time. So I am wanting to store all this until I retire.  Then my family can deal with it when I pass.... HAHA  dont know what they are in for&lt;/p&gt;\n\n&lt;p&gt;32TB so far not including blurays&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zjbo3e", "is_robot_indexable": true, "report_reasons": null, "author": "cmdrmcgarrett", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zjbo3e/thinking_of_moving_my_copied_blurays_to_external/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zjbo3e/thinking_of_moving_my_copied_blurays_to_external/", "subreddit_subscribers": 658838, "created_utc": 1670796815.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After losing a couple SSD's the pain of losing my files was too great. I started doing some research and bought a TerraMaster D4-300 and two 12Tb HDD. I am unsure what would be the best next steps for backing up my files. \n\nMy initial strategy was to start backing up on one HDD 1 and then set up HDD 2 as a mirrored backup. Now that I have it all plugged in, I am unsure about the best way to do that and looking for advice. I am working from a 4TB Macbook Pro that will go back and forth to work with me and ideally, the TM will be a part of a desk/monitor set up I can just connect laptop to when working from home. \n\nSomething like drivepool seems perfect for what I need but I can see from other posts that's not an option for Mac. Should I just set up a ChronoSync task to mirror HDD 1 to HDD 2 and sync whenever I add files?\n\nI'm a noob at all this so any advice would be greatly appreciated, thanks!", "author_fullname": "t2_11nnr2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for starting down the path of a hoarder", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zjbi2f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670796570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After losing a couple SSD&amp;#39;s the pain of losing my files was too great. I started doing some research and bought a TerraMaster D4-300 and two 12Tb HDD. I am unsure what would be the best next steps for backing up my files. &lt;/p&gt;\n\n&lt;p&gt;My initial strategy was to start backing up on one HDD 1 and then set up HDD 2 as a mirrored backup. Now that I have it all plugged in, I am unsure about the best way to do that and looking for advice. I am working from a 4TB Macbook Pro that will go back and forth to work with me and ideally, the TM will be a part of a desk/monitor set up I can just connect laptop to when working from home. &lt;/p&gt;\n\n&lt;p&gt;Something like drivepool seems perfect for what I need but I can see from other posts that&amp;#39;s not an option for Mac. Should I just set up a ChronoSync task to mirror HDD 1 to HDD 2 and sync whenever I add files?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a noob at all this so any advice would be greatly appreciated, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zjbi2f", "is_robot_indexable": true, "report_reasons": null, "author": "DaBeigeMage", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zjbi2f/advice_for_starting_down_the_path_of_a_hoarder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zjbi2f/advice_for_starting_down_the_path_of_a_hoarder/", "subreddit_subscribers": 658838, "created_utc": 1670796570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not sure if this is the right place to ask. At the moment just have a small Pi nas running openmediavault with a 4tb 3.5\" hdd in a sabrent powered enclosure. My one issue: the enclosure has a built in power save/sleep function that I cannot disable. Tried going through and making sure APM was disabled, spindown was disabled, etc, but after exhausting all resources I am fairly sure this is a function of the enclosure(even though it is not listed in the product info anywhere I can find). this causes extremely slow load times for Plex initially and can cause temporary hangups browsing files, etc. Anyone know of a good 3.5\" enclosure that does not have a sleep function? I have been looking online and this was the best option that did not list powersave, and ended up having it anyway. Not ready to pull the trigger on an add-on board yet for the Pi. Thanks for any replies!", "author_fullname": "t2_1p8cryl3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External enclosure question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zjavhw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670795640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if this is the right place to ask. At the moment just have a small Pi nas running openmediavault with a 4tb 3.5&amp;quot; hdd in a sabrent powered enclosure. My one issue: the enclosure has a built in power save/sleep function that I cannot disable. Tried going through and making sure APM was disabled, spindown was disabled, etc, but after exhausting all resources I am fairly sure this is a function of the enclosure(even though it is not listed in the product info anywhere I can find). this causes extremely slow load times for Plex initially and can cause temporary hangups browsing files, etc. Anyone know of a good 3.5&amp;quot; enclosure that does not have a sleep function? I have been looking online and this was the best option that did not list powersave, and ended up having it anyway. Not ready to pull the trigger on an add-on board yet for the Pi. Thanks for any replies!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zjavhw", "is_robot_indexable": true, "report_reasons": null, "author": "dyno241", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zjavhw/external_enclosure_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zjavhw/external_enclosure_question/", "subreddit_subscribers": 658838, "created_utc": 1670795640.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8rszy3ze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "I am coming to the hoarder side", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": null, "name": "t3_zit8lu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": null, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670767454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zit8lu", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "60 GB sheet music", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zit8lu", "is_robot_indexable": true, "report_reasons": null, "author": "WhyIsIsTakenTaken", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zit8lu/i_am_coming_to_the_hoarder_side/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zit8lu", "subreddit_subscribers": 658838, "created_utc": 1670767454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Question above -- currently debating about buying 20TB hard drives, but unsure if they are supported with my OS and unsure how to check if they are supported.\n\nI have an i3-10105, 64gb of ram and running Ubuntu Server 22.04.", "author_fullname": "t2_4quxu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is &gt;16TB HDDs an issue with Ubuntu Server 22.04 LTS on modern hardware? I remember reading ext4 only supporting up to 16TB.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ziev62", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670733001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Question above -- currently debating about buying 20TB hard drives, but unsure if they are supported with my OS and unsure how to check if they are supported.&lt;/p&gt;\n\n&lt;p&gt;I have an i3-10105, 64gb of ram and running Ubuntu Server 22.04.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ziev62", "is_robot_indexable": true, "report_reasons": null, "author": "DigitalSpeed", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ziev62/is_16tb_hdds_an_issue_with_ubuntu_server_2204_lts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ziev62/is_16tb_hdds_an_issue_with_ubuntu_server_2204_lts/", "subreddit_subscribers": 658838, "created_utc": 1670733001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Also, would being able to hold this content make it harder for the authorities to deal with book piracy? See here: \n\nhttps://join.substack.com/p/copyrights-costs\n\n&gt;Anna\u2019s Archive is active on the normal internet\u2014no need for Tor\u2014and has an \u201cAbout\u201d page that says: \u201cThis website was created by Anna, the person behind the Pirate Library Mirror, which is a backup of the Z-Library shadow library.\u201d And I estimate that people with 300 terabytes of disk space have the ability to personally mirror the totality of the shadow-library material that exists\u2014maybe that\u2019s irrelevant, but the fact that people can personally mirror the totality of the content in question might make it harder to crack down on shadow libraries.", "author_fullname": "t2_dranep8p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much space would it take to hold all of the shadow-library content in existence?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zj9hmh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670793576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Also, would being able to hold this content make it harder for the authorities to deal with book piracy? See here: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://join.substack.com/p/copyrights-costs\"&gt;https://join.substack.com/p/copyrights-costs&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Anna\u2019s Archive is active on the normal internet\u2014no need for Tor\u2014and has an \u201cAbout\u201d page that says: \u201cThis website was created by Anna, the person behind the Pirate Library Mirror, which is a backup of the Z-Library shadow library.\u201d And I estimate that people with 300 terabytes of disk space have the ability to personally mirror the totality of the shadow-library material that exists\u2014maybe that\u2019s irrelevant, but the fact that people can personally mirror the totality of the content in question might make it harder to crack down on shadow libraries.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iMgAG5khu2IybQzzkE1u5WD9_vAJy2WSuYo8Ter7P-A.jpg?auto=webp&amp;s=1dac45b95bcde1a6a11afb045a681ba428dc15a4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/iMgAG5khu2IybQzzkE1u5WD9_vAJy2WSuYo8Ter7P-A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=edf6bd18331f88fd08808623c1f30035e06b67e4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/iMgAG5khu2IybQzzkE1u5WD9_vAJy2WSuYo8Ter7P-A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e608d25e66098fcd3479c7075d8a08b600c9e028", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/iMgAG5khu2IybQzzkE1u5WD9_vAJy2WSuYo8Ter7P-A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0af745763d96b0c944c93fc6ffc2cf49bfd6f8d0", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/iMgAG5khu2IybQzzkE1u5WD9_vAJy2WSuYo8Ter7P-A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f88c59bad9cfb4635f5a073822a576f19722556b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/iMgAG5khu2IybQzzkE1u5WD9_vAJy2WSuYo8Ter7P-A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=becae61703e4b6c49961761b2f2eb10a020a9ce6", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/iMgAG5khu2IybQzzkE1u5WD9_vAJy2WSuYo8Ter7P-A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=51ef7bf4183c3a7acc9b7e3c259dbf6b750be974", "width": 1080, "height": 540}], "variants": {}, "id": "IWGyv6nQC-VPVF3n6rO3yLMKDPn78cqxe6flJKhikuc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zj9hmh", "is_robot_indexable": true, "report_reasons": null, "author": "LinguisticsTurtle", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zj9hmh/how_much_space_would_it_take_to_hold_all_of_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zj9hmh/how_much_space_would_it_take_to_hold_all_of_the/", "subreddit_subscribers": 658838, "created_utc": 1670793576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi! How to download from Internet Archive using ia downloader, but files keep their original title and not be renamed by doc file \"identifier\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zj4w86", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_59rlp3yo", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "internetarchive", "selftext": "I'm trying to download doc in bulk using ia downloader but each is named after their identifier. What command line to use so files keep their original title?", "author_fullname": "t2_59rlp3yo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download all pdf from a collection using ia downloader, but files keep their original title and not be renamed by doc file identifier?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/internetarchive", "hidden": false, "pwls": null, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zj4pau", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670786235.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.internetarchive", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to download doc in bulk using ia downloader but each is named after their identifier. What command line to use so files keep their original title?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x0rs", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zj4pau", "is_robot_indexable": true, "report_reasons": null, "author": "mataka54321", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/internetarchive/comments/zj4pau/how_to_download_all_pdf_from_a_collection_using/", "parent_whitelist_status": null, "stickied": false, "url": "https://old.reddit.com/r/internetarchive/comments/zj4pau/how_to_download_all_pdf_from_a_collection_using/", "subreddit_subscribers": 485, "created_utc": 1670786235.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1670786554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.internetarchive", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/internetarchive/comments/zj4pau/how_to_download_all_pdf_from_a_collection_using/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zj4w86", "is_robot_indexable": true, "report_reasons": null, "author": "mataka54321", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_zj4pau", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zj4w86/hi_how_to_download_from_internet_archive_using_ia/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/internetarchive/comments/zj4pau/how_to_download_all_pdf_from_a_collection_using/", "subreddit_subscribers": 658838, "created_utc": 1670786554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi ,  \n\n\nI have 2 x \" WD Black D10 8Tb \" drives in enclosures.     \nThey have been great however both \" apd \" 12V 3.0 A power     \nadaptors have since failed ..\n\nI'm looking for 2 - 4 adaptors and happy to pay 5 usd each ,  \nplus shipping costs to Portland. For the original \" APD \" ones    \nto match what I had ..  \n\n\nWanted to buy :      \n2 - 4 \" APD \" 12V  \\* 3.0 A \\* power adaptors    \n( US ones are ok even though I'm in AUS )  \n    \nHere's a photo of them :       \n\\[ [https://imgur.com/a/QE1ZzMe](https://imgur.com/a/QE1ZzMe) \\]      \n\n\nThanks ,       \n\u00a0   \n\u2014 Chuan", "author_fullname": "t2_fghh5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[WTB] \" WD Black D10 \" 8Tb \u2014 Replacement 12V / 3.0 A power adaptors", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ziluh5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670750334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi ,  &lt;/p&gt;\n\n&lt;p&gt;I have 2 x &amp;quot; WD Black D10 8Tb &amp;quot; drives in enclosures.&lt;br/&gt;\nThey have been great however both &amp;quot; apd &amp;quot; 12V 3.0 A power&lt;br/&gt;\nadaptors have since failed ..&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for 2 - 4 adaptors and happy to pay 5 usd each ,&lt;br/&gt;\nplus shipping costs to Portland. For the original &amp;quot; APD &amp;quot; ones&lt;br/&gt;\nto match what I had ..  &lt;/p&gt;\n\n&lt;p&gt;Wanted to buy :&lt;br/&gt;\n2 - 4 &amp;quot; APD &amp;quot; 12V  * 3.0 A * power adaptors&lt;br/&gt;\n( US ones are ok even though I&amp;#39;m in AUS )  &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a photo of them :&lt;br/&gt;\n[ &lt;a href=\"https://imgur.com/a/QE1ZzMe\"&gt;https://imgur.com/a/QE1ZzMe&lt;/a&gt; ]      &lt;/p&gt;\n\n&lt;p&gt;Thanks ,&lt;br/&gt;\n\u00a0&lt;br/&gt;\n\u2014 Chuan&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Qx8m4rNE2o-fKJ0GQS9h9GrYXMl3pzKL1HWrPxifiO8.jpg?auto=webp&amp;s=1d25718b4175b608f9f3716d65d22b2c6c61f58d", "width": 2736, "height": 3648}, "resolutions": [{"url": "https://external-preview.redd.it/Qx8m4rNE2o-fKJ0GQS9h9GrYXMl3pzKL1HWrPxifiO8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a16af017be45e38cc048ab43d7ff7e10633cd760", "width": 108, "height": 144}, {"url": "https://external-preview.redd.it/Qx8m4rNE2o-fKJ0GQS9h9GrYXMl3pzKL1HWrPxifiO8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bfa295667e16b3e534996d7a7ccc40d3b48ece4a", "width": 216, "height": 288}, {"url": "https://external-preview.redd.it/Qx8m4rNE2o-fKJ0GQS9h9GrYXMl3pzKL1HWrPxifiO8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f20ba1a23628a4933e6ce9258da20fb6fad2cb99", "width": 320, "height": 426}, {"url": "https://external-preview.redd.it/Qx8m4rNE2o-fKJ0GQS9h9GrYXMl3pzKL1HWrPxifiO8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db88511c256c923d19a024af9b0f9d9fab69d308", "width": 640, "height": 853}, {"url": "https://external-preview.redd.it/Qx8m4rNE2o-fKJ0GQS9h9GrYXMl3pzKL1HWrPxifiO8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f0fa0a86d1f52722a732f9c3c4a16a3409a9061", "width": 960, "height": 1280}, {"url": "https://external-preview.redd.it/Qx8m4rNE2o-fKJ0GQS9h9GrYXMl3pzKL1HWrPxifiO8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=95935e2413fcca5a9008fdce92c2ce3d114ada53", "width": 1080, "height": 1440}], "variants": {}, "id": "Bz2szLUv3fNZ42DlgD7ZIHG4Oj9RSxNMahL6hN5z534"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ziluh5", "is_robot_indexable": true, "report_reasons": null, "author": "chuan_l", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ziluh5/wtb_wd_black_d10_8tb_replacement_12v_30_a_power/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ziluh5/wtb_wd_black_d10_8tb_replacement_12v_30_a_power/", "subreddit_subscribers": 658838, "created_utc": 1670750334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was a little disappointed when I installed a new external HDD in an enclosure and it maxed out at 50 mb/s copying data from my internal nvme SSD, but accepted it for what it was. I just purchased a second external HDD and am copying between the two and have hit 180+ mbs per second which I was not expecting.\n\nTo add further confusion the new drive is plugged into my slower USB port which (they are both 3.0) ruled so by testing an external drive in each port on opposite sides of the laptop, this slower port gets 15 mb/s when transferring from my internal drive.\n\nWondering what the bottleneck could be and if there's anything I can do.", "author_fullname": "t2_5hpsob4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD transfer speeds question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zj4bsc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670785621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was a little disappointed when I installed a new external HDD in an enclosure and it maxed out at 50 mb/s copying data from my internal nvme SSD, but accepted it for what it was. I just purchased a second external HDD and am copying between the two and have hit 180+ mbs per second which I was not expecting.&lt;/p&gt;\n\n&lt;p&gt;To add further confusion the new drive is plugged into my slower USB port which (they are both 3.0) ruled so by testing an external drive in each port on opposite sides of the laptop, this slower port gets 15 mb/s when transferring from my internal drive.&lt;/p&gt;\n\n&lt;p&gt;Wondering what the bottleneck could be and if there&amp;#39;s anything I can do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zj4bsc", "is_robot_indexable": true, "report_reasons": null, "author": "Artifact-O", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zj4bsc/hdd_transfer_speeds_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zj4bsc/hdd_transfer_speeds_question/", "subreddit_subscribers": 658838, "created_utc": 1670785621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I purchased a set of PNY flashdrives before, 16GB. The problem is they are slow, practically unusably slow. Even some of the cheaper USB 3.0 drives I got from Sandisk are about 1/10th the speed of some more expensive Samsung 3.0 drives I got. \n \nThere are tons of random no-name USB drive packs on Amazon and the like, but I trust those about as much as Google's privacy policies. Are there any good USB drive packs on Amazon or anywhere else? Or do I just have to buy individual usb drives?", "author_fullname": "t2_9njdn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any good packs of USB Flashdrives anywhere, or do I have to purchase shingle?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zijdq7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670743025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I purchased a set of PNY flashdrives before, 16GB. The problem is they are slow, practically unusably slow. Even some of the cheaper USB 3.0 drives I got from Sandisk are about 1/10th the speed of some more expensive Samsung 3.0 drives I got. &lt;/p&gt;\n\n&lt;p&gt;There are tons of random no-name USB drive packs on Amazon and the like, but I trust those about as much as Google&amp;#39;s privacy policies. Are there any good USB drive packs on Amazon or anywhere else? Or do I just have to buy individual usb drives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zijdq7", "is_robot_indexable": true, "report_reasons": null, "author": "Cyber_Akuma", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zijdq7/are_there_any_good_packs_of_usb_flashdrives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zijdq7/are_there_any_good_packs_of_usb_flashdrives/", "subreddit_subscribers": 658838, "created_utc": 1670743025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My daughter's First Grade teacher took a quick video of her in class and posted it a few years ago. She left the school and we haven't been able to reach her. We'd like to be able to download the video so save it as it's very cute. Do y'all have any advice on how to do that?", "author_fullname": "t2_59mtpzg1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading Video From Twitter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zicafb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670727255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My daughter&amp;#39;s First Grade teacher took a quick video of her in class and posted it a few years ago. She left the school and we haven&amp;#39;t been able to reach her. We&amp;#39;d like to be able to download the video so save it as it&amp;#39;s very cute. Do y&amp;#39;all have any advice on how to do that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zicafb", "is_robot_indexable": true, "report_reasons": null, "author": "RR1904", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zicafb/downloading_video_from_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zicafb/downloading_video_from_twitter/", "subreddit_subscribers": 658838, "created_utc": 1670727255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi!  Not sure if this is the right thread but I\u2019m struggling to find the best place to post. \n\nMy mom made a purchase on Rakuten late last month (11/28) and they are now trying to claim the item was excluded. I am 100% positive it wasn\u2019t on the exclusion list and they have dodged her emails until today. I checked the exclusion list earlier and it still was not showing as excluded. \n\nNow, they have emailed back and when I just checked the exclusion list, they have added the exact item she purchased. It absolutely was not there at the time of purchase. \n\nIs there any way to see the archived version of the site if it was a few clicks into the page. I\u2019m very inexperienced with web archives but haven\u2019t had any luck with it. \n\nThanks so much!", "author_fullname": "t2_c9yps3r1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ziaf2j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670723416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!  Not sure if this is the right thread but I\u2019m struggling to find the best place to post. &lt;/p&gt;\n\n&lt;p&gt;My mom made a purchase on Rakuten late last month (11/28) and they are now trying to claim the item was excluded. I am 100% positive it wasn\u2019t on the exclusion list and they have dodged her emails until today. I checked the exclusion list earlier and it still was not showing as excluded. &lt;/p&gt;\n\n&lt;p&gt;Now, they have emailed back and when I just checked the exclusion list, they have added the exact item she purchased. It absolutely was not there at the time of purchase. &lt;/p&gt;\n\n&lt;p&gt;Is there any way to see the archived version of the site if it was a few clicks into the page. I\u2019m very inexperienced with web archives but haven\u2019t had any luck with it. &lt;/p&gt;\n\n&lt;p&gt;Thanks so much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ziaf2j", "is_robot_indexable": true, "report_reasons": null, "author": "WouldaBeenDinah", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ziaf2j/web_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ziaf2j/web_archive/", "subreddit_subscribers": 658838, "created_utc": 1670723416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got an ST18000NE000 and it's passed  doing 2 read/write surfaces scans and such but it causes the program to die.  \n[This is the drive before being scanned.](https://i.imgur.com/R2VIBln.png)  \n\n\n[And after  2 full read/write scans in HDD Sentinel.](https://i.imgur.com/GsVlx5z.png)  \n\n\nIt seems fine to me?", "author_fullname": "t2_7snrz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD Sentinel Hates a Drive.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zj4dlx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670785704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got an ST18000NE000 and it&amp;#39;s passed  doing 2 read/write surfaces scans and such but it causes the program to die.&lt;br/&gt;\n&lt;a href=\"https://i.imgur.com/R2VIBln.png\"&gt;This is the drive before being scanned.&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/GsVlx5z.png\"&gt;And after  2 full read/write scans in HDD Sentinel.&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;It seems fine to me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BJO_olifyouTxfmN4HgPgwVa4Sh3jG0C7lgRdbdkKUY.png?auto=webp&amp;s=9a0693ca263083a1bcce3544afcb15301b65a7bc", "width": 592, "height": 442}, "resolutions": [{"url": "https://external-preview.redd.it/BJO_olifyouTxfmN4HgPgwVa4Sh3jG0C7lgRdbdkKUY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=acc7e50e3f71b94af60152c431d20a5f0f57e179", "width": 108, "height": 80}, {"url": "https://external-preview.redd.it/BJO_olifyouTxfmN4HgPgwVa4Sh3jG0C7lgRdbdkKUY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4572556ac8de27e7914bab59f782f3fd58ad7dd1", "width": 216, "height": 161}, {"url": "https://external-preview.redd.it/BJO_olifyouTxfmN4HgPgwVa4Sh3jG0C7lgRdbdkKUY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e90ed9dc8078a50da42ab7434300ead99a8ea626", "width": 320, "height": 238}], "variants": {}, "id": "_vhIgU04WxEtsaBhWeHf1d92U_kAjM9vc2FkBu0BZUs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zj4dlx", "is_robot_indexable": true, "report_reasons": null, "author": "Eagle1337", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zj4dlx/hdd_sentinel_hates_a_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zj4dlx/hdd_sentinel_hates_a_drive/", "subreddit_subscribers": 658838, "created_utc": 1670785704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "When I start a new project there are these filters: \n\n\\+\\*.png +\\*.gif +\\*.jpg +\\*.jpeg +\\*.css +\\*.js -ad.doubleclick.net/\\* -mime:application/foobar \n\nI delete these filters because I am worried that by leaving them then only .png .gif .jpg .jpeg .css .js files will be downloaded and all the other files won't be downloaded, am I correct? \n\nSo I delete the filters and I leave the page blank, but what does happen when the page is left blank? I tried to use the filter +\\* to see if it is the same thing but then other files are downloaded if I leave the page blank and different files are downloaded if I use the +\\* filter.\n\nMy question is what is downloaded when no filter is used?\n\nAnother question, what's the difference between this filters:\n\n\\+\\*.png +\\*.gif +\\*.jpg +\\*.jpeg +\\*.css +\\*.js -ad.doubleclick.net/\\* -mime:application/foobar \n\nand this filters:\n\n\\-\\*  \n\\+\\*.png +\\*.gif +\\*.jpg +\\*.jpeg +\\*.css +\\*.js -ad.doubleclick.net/\\* -mime:application/foobar", "author_fullname": "t2_xtwzx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HTTrack, what does happen if I leave the Scan Rules (filters) page blank?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ziyrse", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670776556.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I start a new project there are these filters: &lt;/p&gt;\n\n&lt;p&gt;+*.png +*.gif +*.jpg +*.jpeg +*.css +*.js -ad.doubleclick.net/* -mime:application/foobar &lt;/p&gt;\n\n&lt;p&gt;I delete these filters because I am worried that by leaving them then only .png .gif .jpg .jpeg .css .js files will be downloaded and all the other files won&amp;#39;t be downloaded, am I correct? &lt;/p&gt;\n\n&lt;p&gt;So I delete the filters and I leave the page blank, but what does happen when the page is left blank? I tried to use the filter +* to see if it is the same thing but then other files are downloaded if I leave the page blank and different files are downloaded if I use the +* filter.&lt;/p&gt;\n\n&lt;p&gt;My question is what is downloaded when no filter is used?&lt;/p&gt;\n\n&lt;p&gt;Another question, what&amp;#39;s the difference between this filters:&lt;/p&gt;\n\n&lt;p&gt;+*.png +*.gif +*.jpg +*.jpeg +*.css +*.js -ad.doubleclick.net/* -mime:application/foobar &lt;/p&gt;\n\n&lt;p&gt;and this filters:&lt;/p&gt;\n\n&lt;p&gt;-*&lt;br/&gt;\n+*.png +*.gif +*.jpg +*.jpeg +*.css +*.js -ad.doubleclick.net/* -mime:application/foobar&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ziyrse", "is_robot_indexable": true, "report_reasons": null, "author": "fjnk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ziyrse/httrack_what_does_happen_if_i_leave_the_scan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ziyrse/httrack_what_does_happen_if_i_leave_the_scan/", "subreddit_subscribers": 658838, "created_utc": 1670776556.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a solution (preferably free) that can download over 5k songs in .wav format preferably though .mp3 320kbps should also do the trick. I can make a playlist on any service except Apple Music and also have a stored .txt and .csv version of the songs I'm looking for. All of the solutions I have seen require inputting each song into the downloader individually and I was wondering if there was a solution that can mass-download all of them at once. I'm on Win10 and am comfortable using command prompt.", "author_fullname": "t2_12225d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to download large amounts of high-quality music?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zie4ch", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.41, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670731266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a solution (preferably free) that can download over 5k songs in .wav format preferably though .mp3 320kbps should also do the trick. I can make a playlist on any service except Apple Music and also have a stored .txt and .csv version of the songs I&amp;#39;m looking for. All of the solutions I have seen require inputting each song into the downloader individually and I was wondering if there was a solution that can mass-download all of them at once. I&amp;#39;m on Win10 and am comfortable using command prompt.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "16TB + 3TB NAS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zie4ch", "is_robot_indexable": true, "report_reasons": null, "author": "shoey9998", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zie4ch/best_way_to_download_large_amounts_of_highquality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zie4ch/best_way_to_download_large_amounts_of_highquality/", "subreddit_subscribers": 658838, "created_utc": 1670731266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was about to buy a WD 4TB \u201cportable\u201d external hard drive, but then I noticed I could get an internal 3.5\u201d WD Blue with the same capacity for significantly cheaper, or a WD Red for a bit cheaper than the portable.\n\nI already have a hard drive case with USB adapter, although I don\u2019t know how durable it is. I bought it to keep on my desk for occasional use, not to take with me places. I don\u2019t think it\u2019s especially flimsy though. \n\nIs there a hidden cost if I go for the WD Blue and put it in the case instead of just getting the portable? For example, does the portable have more protection against physical damage? Or is it the same guts inside a cheap case?", "author_fullname": "t2_tggommtv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need an external hard drive I can take with me places. Is there a disadvantage to buying an internal drive and putting it in a SATA to USB adapter shell, instead of getting one advertised as \u201cportable\u201d?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zibwgv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670726391.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was about to buy a WD 4TB \u201cportable\u201d external hard drive, but then I noticed I could get an internal 3.5\u201d WD Blue with the same capacity for significantly cheaper, or a WD Red for a bit cheaper than the portable.&lt;/p&gt;\n\n&lt;p&gt;I already have a hard drive case with USB adapter, although I don\u2019t know how durable it is. I bought it to keep on my desk for occasional use, not to take with me places. I don\u2019t think it\u2019s especially flimsy though. &lt;/p&gt;\n\n&lt;p&gt;Is there a hidden cost if I go for the WD Blue and put it in the case instead of just getting the portable? For example, does the portable have more protection against physical damage? Or is it the same guts inside a cheap case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zibwgv", "is_robot_indexable": true, "report_reasons": null, "author": "bobisnotmyuncIe", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zibwgv/i_need_an_external_hard_drive_i_can_take_with_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zibwgv/i_need_an_external_hard_drive_i_can_take_with_me/", "subreddit_subscribers": 658838, "created_utc": 1670726391.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}