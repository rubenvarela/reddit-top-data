{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_wqct3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Conversation with chatGPT on Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_zhm66m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 65, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 65, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/j6QJXqBiJhPLVEgoUN1oygxPn2o_MWUFDLoNQpFWl74.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670657750.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "link.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://link.medium.com/ScfLkXrHDvb", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?auto=webp&amp;s=802152dcad572430c26b31d3441447118d6b7686", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d9933bfc52c090d2b690b0632c57808fada55ed", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9fee743a6a013e2943dd90a7ff0e352d8344726", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33309caadaaa69438579868ea0bfcbd1bcc1b3ac", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=730ff9e726b3520935bfcefcdd17f9a9e704a6ac", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dd6d9af53c22efdd0fa0896d3023265f19330222", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c25ce605074dbe468c8fc1b5d3ce67f36ed0c657", "width": 1080, "height": 607}], "variants": {}, "id": "jKaQlVsP7O4k3Ibxba0Vhjw-0dOfpUpXCJJ_yFlcxew"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zhm66m", "is_robot_indexable": true, "report_reasons": null, "author": "Koushik5586", "discussion_type": null, "num_comments": 27, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhm66m/a_conversation_with_chatgpt_on_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://link.medium.com/ScfLkXrHDvb", "subreddit_subscribers": 82516, "created_utc": 1670657750.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_nrfxa5al", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Future of Big Data Systems by Spark creator Matei Zaharia", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_zi7hrj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 22, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/aRk6Lk6L5gA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Future of Big Data Systems by Spark creator Matei Zaharia\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Future of Big Data Systems by Spark creator Matei Zaharia", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/aRk6Lk6L5gA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Future of Big Data Systems by Spark creator Matei Zaharia\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/aRk6Lk6L5gA/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/aRk6Lk6L5gA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Future of Big Data Systems by Spark creator Matei Zaharia\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/zi7hrj", "height": 200}, "link_flair_text": "Interview", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Nx2_TecTpkre5K9-IDjOsTlKMzcKzvVON0bM5eadaIc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670715844.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/aRk6Lk6L5gA", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NjXpxJvHD561QkDdsfd6VaIRpafEWOIr5FB58g_Osj8.jpg?auto=webp&amp;s=6163850426c519392d194763812b65ead23f9969", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/NjXpxJvHD561QkDdsfd6VaIRpafEWOIr5FB58g_Osj8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d8bad56c4220db5d34b06edb5fc83ab3b279376", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/NjXpxJvHD561QkDdsfd6VaIRpafEWOIr5FB58g_Osj8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ea11c283bdaba19fb02f718aa5a1685707505e93", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/NjXpxJvHD561QkDdsfd6VaIRpafEWOIr5FB58g_Osj8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5df9a64eff9702d07b15ce6ccfef15fcf1e88212", "width": 320, "height": 240}], "variants": {}, "id": "VrUmpf7gHYb0jVrTvpkyd98lRn3v5vdJxtDeGa9WdNc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "zi7hrj", "is_robot_indexable": true, "report_reasons": null, "author": "CatanNicollo", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zi7hrj/future_of_big_data_systems_by_spark_creator_matei/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/aRk6Lk6L5gA", "subreddit_subscribers": 82516, "created_utc": 1670715844.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Future of Big Data Systems by Spark creator Matei Zaharia", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/aRk6Lk6L5gA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Future of Big Data Systems by Spark creator Matei Zaharia\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/aRk6Lk6L5gA/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are many SaaS applications which allows users to basically store data with various attributes and data types. i.e a collection of a custom objects with user defined fields/attributes of any primitive datatype. These applications then allow efficient querying of these custom objects.\n\nA few examples that come to mind are:\n\n* [Salesforce](https://trailhead.salesforce.com/content/learn/modules/data_modeling/objects_intro)\n   * Allows users to define tables with fields which they can query against.\n   * Allows defining relations (FK) between multiple objects\n* [Hubspot](https://www.hubspot.com/products/custom-objects)\n   * Same thing as salesforce.\n* [Segment.io](https://segment.com/docs/guides/filtering-data/)\n   * Allows sending large amounts of data and defining filters to segment data into user defined buckets\n* [Heap Analytics](https://help.heap.io/definitions/properties/how-to-create-defined-properties/)\n   * Similar to to [segment.io](https://segment.io)\n* [Klaviyo](https://help.klaviyo.com/hc/en-us/articles/115000250912-About-Custom-Properties) \\- customer data platform\n   * Allows adding custom properties to 1000s of customer profiles and then segment/filter them into various groups.\n\nTraditionally, developers would model schemas and add indexes to various fields based on expected query patterns as they know this information ahead of time. However, in user defined objects/schemas, this is unknown.\n\nHow do these products/features work? Whats the technology behind this? How is the data stored? How are the queries made to execute efficiently? What's the underlying implementations to allow this to work at scale?\n\n&amp;#x200B;\n\nP.S: Are there any opensource technologies/databases which allow implementing features like these?", "author_fullname": "t2_h81s7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does storing and querying user defined schema work at scale?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhtbjs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670681002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are many SaaS applications which allows users to basically store data with various attributes and data types. i.e a collection of a custom objects with user defined fields/attributes of any primitive datatype. These applications then allow efficient querying of these custom objects.&lt;/p&gt;\n\n&lt;p&gt;A few examples that come to mind are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://trailhead.salesforce.com/content/learn/modules/data_modeling/objects_intro\"&gt;Salesforce&lt;/a&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Allows users to define tables with fields which they can query against.&lt;/li&gt;\n&lt;li&gt;Allows defining relations (FK) between multiple objects&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.hubspot.com/products/custom-objects\"&gt;Hubspot&lt;/a&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Same thing as salesforce.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://segment.com/docs/guides/filtering-data/\"&gt;Segment.io&lt;/a&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Allows sending large amounts of data and defining filters to segment data into user defined buckets&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://help.heap.io/definitions/properties/how-to-create-defined-properties/\"&gt;Heap Analytics&lt;/a&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Similar to to &lt;a href=\"https://segment.io\"&gt;segment.io&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://help.klaviyo.com/hc/en-us/articles/115000250912-About-Custom-Properties\"&gt;Klaviyo&lt;/a&gt; - customer data platform\n\n&lt;ul&gt;\n&lt;li&gt;Allows adding custom properties to 1000s of customer profiles and then segment/filter them into various groups.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Traditionally, developers would model schemas and add indexes to various fields based on expected query patterns as they know this information ahead of time. However, in user defined objects/schemas, this is unknown.&lt;/p&gt;\n\n&lt;p&gt;How do these products/features work? Whats the technology behind this? How is the data stored? How are the queries made to execute efficiently? What&amp;#39;s the underlying implementations to allow this to work at scale?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;P.S: Are there any opensource technologies/databases which allow implementing features like these?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rO6oUQYwnpvj6zBnRO2KQyzxmAEb9-79YUfQeMeERLk.jpg?auto=webp&amp;s=e7d093b708a39b10f67496c315a48f92fddb2b28", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/rO6oUQYwnpvj6zBnRO2KQyzxmAEb9-79YUfQeMeERLk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=af3528e6cd5e832aad44fd7ca8d1fe5d6bd379c6", "width": 108, "height": 108}], "variants": {}, "id": "qFPKnsa5pUm7UbgtvJLjONWckFvkmJRapYOlAH8MdoM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zhtbjs", "is_robot_indexable": true, "report_reasons": null, "author": "Krimson1911", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhtbjs/how_does_storing_and_querying_user_defined_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhtbjs/how_does_storing_and_querying_user_defined_schema/", "subreddit_subscribers": 82516, "created_utc": 1670681002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nI'm working in a project to aggregate data from three different systems (psql, mysql and mongodb). I have a huge problem with incremental updates once one of the systems does hard deletes in their tables (mysql). What would be the best strategy to handle such problem? CDC is not an option right now.\n\nI'm using Airflow as data ingestion and dbt to perform the data modeling and transformation.\n\nThanks all in advance!", "author_fullname": "t2_hanh3wo6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle deletes in source systems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhov20", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670667187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working in a project to aggregate data from three different systems (psql, mysql and mongodb). I have a huge problem with incremental updates once one of the systems does hard deletes in their tables (mysql). What would be the best strategy to handle such problem? CDC is not an option right now.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Airflow as data ingestion and dbt to perform the data modeling and transformation.&lt;/p&gt;\n\n&lt;p&gt;Thanks all in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zhov20", "is_robot_indexable": true, "report_reasons": null, "author": "Other-Government-796", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhov20/how_to_handle_deletes_in_source_systems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhov20/how_to_handle_deletes_in_source_systems/", "subreddit_subscribers": 82516, "created_utc": 1670667187.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Given the following two trends, it seems to me a lot of the technical/coding parts of DE responsibilities will decrease and/or radically change:\n\n* Vendors tools and cloud perform are maturing a lot, exposing higher level abstractions to us developers (e.g. today's Spark is an order of magnitude easier to code with than 8 years ago) and becoming more seamlessly integrated (e.g. recent announcement of Aurora to Redshift integration)\n\n* AI coding assistant are now usable, as famously illustrated by Copilot and ChatGPT, it's reasonable to assume they will ultimately be integrated by cloud platforms and tool vendors in the coming years, reducing there again the amount of work for developers.\n\nNow I'm convinced there will always be tasks for humans to build and maintain data pipelines and data oriented solution, although it's likely going to be quite different from today.\n\nWhat do you think data engineer job will be like in 5 years? What's your move today to prepare for it?", "author_fullname": "t2_hf4d8zql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What DE tasks do you think will look like in 5 years?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhow0i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670667273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given the following two trends, it seems to me a lot of the technical/coding parts of DE responsibilities will decrease and/or radically change:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Vendors tools and cloud perform are maturing a lot, exposing higher level abstractions to us developers (e.g. today&amp;#39;s Spark is an order of magnitude easier to code with than 8 years ago) and becoming more seamlessly integrated (e.g. recent announcement of Aurora to Redshift integration)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;AI coding assistant are now usable, as famously illustrated by Copilot and ChatGPT, it&amp;#39;s reasonable to assume they will ultimately be integrated by cloud platforms and tool vendors in the coming years, reducing there again the amount of work for developers.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now I&amp;#39;m convinced there will always be tasks for humans to build and maintain data pipelines and data oriented solution, although it&amp;#39;s likely going to be quite different from today.&lt;/p&gt;\n\n&lt;p&gt;What do you think data engineer job will be like in 5 years? What&amp;#39;s your move today to prepare for it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zhow0i", "is_robot_indexable": true, "report_reasons": null, "author": "sv3ndk", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhow0i/what_de_tasks_do_you_think_will_look_like_in_5/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhow0i/what_de_tasks_do_you_think_will_look_like_in_5/", "subreddit_subscribers": 82516, "created_utc": 1670667273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7g8ahtr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advanced DBT Macros", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "name": "t3_zhjvcw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kUnox4hHhEct0IHebUENb9Hb_Bq57dHmF2u2b8AVlOI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670649765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@kadek/advanced-dbt-macros-c274f1cfa989", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?auto=webp&amp;s=319ba75d9c5d15b9ee79ef24fe04b672f5ab2d25", "width": 1200, "height": 658}, "resolutions": [{"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6daed398df2e0a759ca234ad545f82d5dd8bb0e", "width": 108, "height": 59}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6599dc4486e9c8775acfd4240f604832ce65ef0e", "width": 216, "height": 118}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdd29e283e196351479b87c4b93f1bde00cad3fc", "width": 320, "height": 175}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=018afecb2358addb0ef361e6db0e3abfa59ff4ed", "width": 640, "height": 350}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4a419a06364430f23db47e24443c356ec63cc3ef", "width": 960, "height": 526}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=739089bae6a0ba18df34b02b8ef8145a9f7b067a", "width": 1080, "height": 592}], "variants": {}, "id": "EIbAEFWYS3HzVZSKuLBYDA8qdb6Kgl7d7hGOqBbvNVw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zhjvcw", "is_robot_indexable": true, "report_reasons": null, "author": "hjkl_ornah", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhjvcw/advanced_dbt_macros/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@kadek/advanced-dbt-macros-c274f1cfa989", "subreddit_subscribers": 82516, "created_utc": 1670649765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a junior (~2 years experience) De and my current work contract ends in April. I was wondering, for those who are in a similar position and will be looking for jobs in the New Year, what are some things you will take in to consideration? any criteria for a role? any advice for looking for roles?", "author_fullname": "t2_a3m6qw38", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Applying or Looking for Jobs in the New Year?? What to think about?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhpew8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670668913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a junior (~2 years experience) De and my current work contract ends in April. I was wondering, for those who are in a similar position and will be looking for jobs in the New Year, what are some things you will take in to consideration? any criteria for a role? any advice for looking for roles?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zhpew8", "is_robot_indexable": true, "report_reasons": null, "author": "sk808mafia", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhpew8/applying_or_looking_for_jobs_in_the_new_year_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhpew8/applying_or_looking_for_jobs_in_the_new_year_what/", "subreddit_subscribers": 82516, "created_utc": 1670668913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hoping to find someone to hop on a call and look over a take home assignment for a mid-level DE job with me. \n\nTo be clear, I'm not looking for anyone to do any work for me-- just critique the answers I have already written. At my current job nobody writes python, so I don't have any experience writing python in a shared codebase and the conventions that might come along with that. As a result, I'm concerned about my code coming off as amateurish. \n\nIf any generous soul would be willing to help me out for a little bit, it would be much appreciated. Am willing to compensate at your hourly. Feel free to dm or comment. Can be discord/teams/Skype etc.", "author_fullname": "t2_kdj5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an experienced DE to walk through/critique take-home python assignment Saturday morning/afternoon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhm02s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670657136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hoping to find someone to hop on a call and look over a take home assignment for a mid-level DE job with me. &lt;/p&gt;\n\n&lt;p&gt;To be clear, I&amp;#39;m not looking for anyone to do any work for me-- just critique the answers I have already written. At my current job nobody writes python, so I don&amp;#39;t have any experience writing python in a shared codebase and the conventions that might come along with that. As a result, I&amp;#39;m concerned about my code coming off as amateurish. &lt;/p&gt;\n\n&lt;p&gt;If any generous soul would be willing to help me out for a little bit, it would be much appreciated. Am willing to compensate at your hourly. Feel free to dm or comment. Can be discord/teams/Skype etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zhm02s", "is_robot_indexable": true, "report_reasons": null, "author": "Touvejs", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhm02s/looking_for_an_experienced_de_to_walk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhm02s/looking_for_an_experienced_de_to_walk/", "subreddit_subscribers": 82516, "created_utc": 1670657136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_88bdez9q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Designing and Planning an Event Store System", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": true, "name": "t3_zic6ms", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9MU7sxu7HTEYzMsquTaJnEs1cc3we45tebm46SuSKps.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670727001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "coraspe-ramses.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://coraspe-ramses.medium.com/designing-and-planning-an-event-store-system-be4df7519442", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?auto=webp&amp;s=3eaddf9e51bf4c5cefeaad71031e28b6a84f1f7c", "width": 1200, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e573daa31d80943c4285481fcb704c15e16ce29", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c973763e9fdcb0b33da6f550d516c3aa252af87", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6a0f62eba8a22f321c369c70f67b39363c9e247", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=948994eb15b1ecc7835f5b341dd7279175b3e417", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=221b994d8c4285c1683ae509a2c468e3713023f2", "width": 960, "height": 576}, {"url": "https://external-preview.redd.it/5gF7HGT8A5IdYtI_jOgdhMgxujJ3r_kxfueLiO5a_mI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=abfd065ae2bda0e79e763b13db228d67aecfa990", "width": 1080, "height": 648}], "variants": {}, "id": "R8w61k1rvY7F-US8sPnunbp6rR3lonn0z685fjPeP9k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zic6ms", "is_robot_indexable": true, "report_reasons": null, "author": "ramses-coraspe", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/zic6ms/designing_and_planning_an_event_store_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://coraspe-ramses.medium.com/designing-and-planning-an-event-store-system-be4df7519442", "subreddit_subscribers": 82516, "created_utc": 1670727001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to load some data into a db table and i wanted to know how to handle a situation where some of my partitions didn't get loaded into the table.\n\nShould I do a left anti join with the data in the table and then load the remaining data or is there a better approach available?", "author_fullname": "t2_t1rd5ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle failure of data load by spark job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhlykc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670656978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to load some data into a db table and i wanted to know how to handle a situation where some of my partitions didn&amp;#39;t get loaded into the table.&lt;/p&gt;\n\n&lt;p&gt;Should I do a left anti join with the data in the table and then load the remaining data or is there a better approach available?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zhlykc", "is_robot_indexable": true, "report_reasons": null, "author": "PR0K1NG", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/zhlykc/how_to_handle_failure_of_data_load_by_spark_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhlykc/how_to_handle_failure_of_data_load_by_spark_job/", "subreddit_subscribers": 82516, "created_utc": 1670656978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to apply for a job and it requires me to know recommendation systems, streaming and big data. \n\nI have all the other ML amd infrastructure skills mentioned in the description but I want to add a project for this. \n\nWhat are some easy project ideas that can help me get an interview call?", "author_fullname": "t2_c2zeqd7i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick project ideas pyspark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zid408", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670729075.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to apply for a job and it requires me to know recommendation systems, streaming and big data. &lt;/p&gt;\n\n&lt;p&gt;I have all the other ML amd infrastructure skills mentioned in the description but I want to add a project for this. &lt;/p&gt;\n\n&lt;p&gt;What are some easy project ideas that can help me get an interview call?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zid408", "is_robot_indexable": true, "report_reasons": null, "author": "LetsJustGrowUp", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zid408/quick_project_ideas_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zid408/quick_project_ideas_pyspark/", "subreddit_subscribers": 82516, "created_utc": 1670729075.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI have a service that will push data each second to some data storage. The data will then be used to populate results in a web app, where users will have access to certain data based on their permission level but they won't have to write any data.\n\n&amp;#x200B;\n\n[High-level schema](https://preview.redd.it/1kqdnsxj265a1.png?width=960&amp;format=png&amp;auto=webp&amp;s=50fbb0fce1c1f80c29abceb520c21639d1ea4a23)\n\nWhat would be the best data solution for this system? \n\nI used Firebase in the past with was quite good to handle real time updates, but I suspect there are better solutions out there.", "author_fullname": "t2_3fyu9j5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best data storage solution for high-frequency (near real-time) updates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"1kqdnsxj265a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/1kqdnsxj265a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f36314f882772bbf093842a51cba56884114ef8"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/1kqdnsxj265a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c8ae5ea07cd6fcd4f5edeba5b2a8e502a2be1d7"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/1kqdnsxj265a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8892dd81e25e6ba2c0be325eb17cb18cb03c67fb"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/1kqdnsxj265a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d8bf390fd7596af6a377c162f6148654c0b484dc"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/1kqdnsxj265a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=997099a2767cd04bcb0c079a10a3487ea5f24307"}], "s": {"y": 720, "x": 960, "u": "https://preview.redd.it/1kqdnsxj265a1.png?width=960&amp;format=png&amp;auto=webp&amp;s=50fbb0fce1c1f80c29abceb520c21639d1ea4a23"}, "id": "1kqdnsxj265a1"}}, "name": "t3_zibr6p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/dO_pp5zdtLnGxAdYYt2IDI2n1BbsIXWUvMKtkCtOLg8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670726105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I have a service that will push data each second to some data storage. The data will then be used to populate results in a web app, where users will have access to certain data based on their permission level but they won&amp;#39;t have to write any data.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1kqdnsxj265a1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50fbb0fce1c1f80c29abceb520c21639d1ea4a23\"&gt;High-level schema&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What would be the best data solution for this system? &lt;/p&gt;\n\n&lt;p&gt;I used Firebase in the past with was quite good to handle real time updates, but I suspect there are better solutions out there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zibr6p", "is_robot_indexable": true, "report_reasons": null, "author": "EntropyRX", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zibr6p/what_is_the_best_data_storage_solution_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zibr6p/what_is_the_best_data_storage_solution_for/", "subreddit_subscribers": 82516, "created_utc": 1670726105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on a personal project for learning new skills. I have a large usage dataset that has member Id, usage qty, and epoch_timestamp. Each member usage is sent every 10 minutes unless there is an issue with the source.  For a given member they could have several records that end up being corrected later or a new historical record comes in when the source may come back online. Think store sales data that may get delayed.\n\nFor example \n\nMember 123 has 3 records Dec 1 for the 1pm hour. I pull that in a batch.\n\nOn Dec 2 I  pull the next days data which also includes a record from November that had been delayed.\n\nUltimately I need to roll these up to the hour for each member but this is a large (billions of rows) set that is increasing every day. Therefore, I'd like to avoid processing all records and just deal with changed or new and how to do that at the roll-up level.\n\nIs there an example somewhere on how to go about handling this efficiently?", "author_fullname": "t2_11qvgaee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental Time Series Usage Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zi3u6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670724153.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670707062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a personal project for learning new skills. I have a large usage dataset that has member Id, usage qty, and epoch_timestamp. Each member usage is sent every 10 minutes unless there is an issue with the source.  For a given member they could have several records that end up being corrected later or a new historical record comes in when the source may come back online. Think store sales data that may get delayed.&lt;/p&gt;\n\n&lt;p&gt;For example &lt;/p&gt;\n\n&lt;p&gt;Member 123 has 3 records Dec 1 for the 1pm hour. I pull that in a batch.&lt;/p&gt;\n\n&lt;p&gt;On Dec 2 I  pull the next days data which also includes a record from November that had been delayed.&lt;/p&gt;\n\n&lt;p&gt;Ultimately I need to roll these up to the hour for each member but this is a large (billions of rows) set that is increasing every day. Therefore, I&amp;#39;d like to avoid processing all records and just deal with changed or new and how to do that at the roll-up level.&lt;/p&gt;\n\n&lt;p&gt;Is there an example somewhere on how to go about handling this efficiently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zi3u6t", "is_robot_indexable": true, "report_reasons": null, "author": "Yankee1423", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zi3u6t/incremental_time_series_usage_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zi3u6t/incremental_time_series_usage_data/", "subreddit_subscribers": 82516, "created_utc": 1670707062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone did this swap? Is it doable?", "author_fullname": "t2_54sksh03", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "from data engineering to data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhzww2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670697575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone did this swap? Is it doable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zhzww2", "is_robot_indexable": true, "report_reasons": null, "author": "kiesket", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhzww2/from_data_engineering_to_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhzww2/from_data_engineering_to_data_science/", "subreddit_subscribers": 82516, "created_utc": 1670697575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How we guarantee ACID integration over arbitrary SQL databases without a central monolith", "author_fullname": "t2_1d8tsh6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loosely coupled monoliths and where to find them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_zhpeit", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/P6-74TnPmtfZcjAXAE5hh2KUX61-IsZ7q-Iktr86am8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670668881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "itnext.io", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How we guarantee ACID integration over arbitrary SQL databases without a central monolith&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://itnext.io/loosely-coupled-monoliths-and-where-to-find-them-4004fac8ecc1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fFfbGLE0B-fJf3h9dQwq8asBhjwnQJT4aaQi_3L_b9g.jpg?auto=webp&amp;s=6b7d9871cd44e99c3b04b9fe56685aa54514b404", "width": 957, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/fFfbGLE0B-fJf3h9dQwq8asBhjwnQJT4aaQi_3L_b9g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a987bfc36fa2e7d6cb45ea00d26eb2dbbcfdf23", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/fFfbGLE0B-fJf3h9dQwq8asBhjwnQJT4aaQi_3L_b9g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=24861d0b9575680ac82b004ba70647afa07acb61", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/fFfbGLE0B-fJf3h9dQwq8asBhjwnQJT4aaQi_3L_b9g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a6fc0b52ed8473cc40f264445fb24fce14bf859", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/fFfbGLE0B-fJf3h9dQwq8asBhjwnQJT4aaQi_3L_b9g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=67153255c4d2b33ff7d80cbd2a305df3b954a75a", "width": 640, "height": 481}], "variants": {}, "id": "0cTQ2NWyC01Dq-h8MHCOUI-ikfcBaC2ht4B6s1xf9eY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zhpeit", "is_robot_indexable": true, "report_reasons": null, "author": "andras_gerlits", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhpeit/loosely_coupled_monoliths_and_where_to_find_them/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://itnext.io/loosely-coupled-monoliths-and-where-to-find-them-4004fac8ecc1", "subreddit_subscribers": 82516, "created_utc": 1670668881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, \n\nI have been using data factory successfully to move data from cosmos db to sql tables with data flows and JSON flattening options.   \n\n\nHowever I have come across a situation with data in cosmos  that requires some custom handling and I have successfully created the python script to do just that.   \n\n\nI am confused where and how I can run this piece of code to take data from the query running in data flow, pass it to python, flatten it with python, and then write the dataset to sql table, if possible using the sink connector in datafactory?  I am ok to use python to write the table with python if the sink is not possible.   \n\n\nThank you in advance.", "author_fullname": "t2_l1znu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to run python code in either datafactory or databricks to parse JSON data from cosmos db to SQL db in azure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhmgac", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670658790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I have been using data factory successfully to move data from cosmos db to sql tables with data flows and JSON flattening options.   &lt;/p&gt;\n\n&lt;p&gt;However I have come across a situation with data in cosmos  that requires some custom handling and I have successfully created the python script to do just that.   &lt;/p&gt;\n\n&lt;p&gt;I am confused where and how I can run this piece of code to take data from the query running in data flow, pass it to python, flatten it with python, and then write the dataset to sql table, if possible using the sink connector in datafactory?  I am ok to use python to write the table with python if the sink is not possible.   &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zhmgac", "is_robot_indexable": true, "report_reasons": null, "author": "boston101", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhmgac/where_to_run_python_code_in_either_datafactory_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhmgac/where_to_run_python_code_in_either_datafactory_or/", "subreddit_subscribers": 82516, "created_utc": 1670658790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wrote down some observations, and would love to hear yours.   \n[https://www.asanverse.com/why-data-projects-get-failed-reverse-etl/](https://www.asanverse.com/why-data-projects-get-failed-reverse-etl/)", "author_fullname": "t2_i2j8bdtn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is a data project delivering valuable insights for users still get failed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zi2cw7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670703499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote down some observations, and would love to hear yours.&lt;br/&gt;\n&lt;a href=\"https://www.asanverse.com/why-data-projects-get-failed-reverse-etl/\"&gt;https://www.asanverse.com/why-data-projects-get-failed-reverse-etl/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?auto=webp&amp;s=1f6570962697e192f874632ce64d36651cddb5f2", "width": 1000, "height": 501}, "resolutions": [{"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=26bb4299318c0ebdf935f8ea381bd5446353da57", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=88152dd1e2af6554dae0e7c06646988dd38cb059", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=622137554b579d28bcb6b349824635bcc1dad1cb", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d81b70acd5107625ae5670d59e1eed6c9d652730", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ed5cad063043d95adda5d722bd84e9e8d0104070", "width": 960, "height": 480}], "variants": {}, "id": "IpHXWC29OWlk894t8bJEURrR9ND6_veRIcSz_wmjqak"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zi2cw7", "is_robot_indexable": true, "report_reasons": null, "author": "ubukhary", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zi2cw7/why_is_a_data_project_delivering_valuable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zi2cw7/why_is_a_data_project_delivering_valuable/", "subreddit_subscribers": 82516, "created_utc": 1670703499.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}