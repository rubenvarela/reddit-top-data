{"kind": "Listing", "data": {"after": "t3_zq2b2d", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_14z3t1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The real reason ChatGPT was created", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_zpraee", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 605, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 605, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/I3qmOhxHk8m1KrglQM1-6HA-33jZgxlJZeZVdUTLP6o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671455711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/g5z2t4zeuu6a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/g5z2t4zeuu6a1.png?auto=webp&amp;s=88894e844011e00446480ff28bb04cd59c4c5f53", "width": 2880, "height": 1548}, "resolutions": [{"url": "https://preview.redd.it/g5z2t4zeuu6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4e9f6720120c9d217f7fdc85b8ad4e8383c35a3", "width": 108, "height": 58}, {"url": "https://preview.redd.it/g5z2t4zeuu6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d4225677c898c548cfe640271ea2051eb6a92d9", "width": 216, "height": 116}, {"url": "https://preview.redd.it/g5z2t4zeuu6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a17d949009da80cfeed89e91a5c6521297e18bb3", "width": 320, "height": 172}, {"url": "https://preview.redd.it/g5z2t4zeuu6a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9efd35e36e031ee8332528645423f24f38feb429", "width": 640, "height": 344}, {"url": "https://preview.redd.it/g5z2t4zeuu6a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ed11f66e66b288d8f28a578f1f0cc66ec09d3c2e", "width": 960, "height": 516}, {"url": "https://preview.redd.it/g5z2t4zeuu6a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8321fe83c82d50d7d07146e0042463c2533466d4", "width": 1080, "height": 580}], "variants": {}, "id": "s7-gx_SuRZ-m6pZfP60Koagg9hEurc51DaBGZFqvy0w"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zpraee", "is_robot_indexable": true, "report_reasons": null, "author": "xdonvanx", "discussion_type": null, "num_comments": 61, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zpraee/the_real_reason_chatgpt_was_created/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/g5z2t4zeuu6a1.png", "subreddit_subscribers": 828564, "created_utc": 1671455711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_oxgj60x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why business data science irritates me", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_zq30tn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 80, "domain": "shakoist.substack.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 80, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AXe9r7il4LBu36QlAizp5N8-d1-6dhVq2_yRoDsxKOc.jpg", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671482748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://shakoist.substack.com/p/why-business-data-science-irritates?utm_source=twitter&amp;sd=pf", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GkOmXq5e_3MS_cWaAQ7RWi0eLrP118sKvkUfKy-XSFU.jpg?auto=webp&amp;s=f56ca257ce707b018be2510c20de4906e4417958", "width": 1066, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/GkOmXq5e_3MS_cWaAQ7RWi0eLrP118sKvkUfKy-XSFU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4367004bca7ed1c2bc42667dd6871ceb60797929", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/GkOmXq5e_3MS_cWaAQ7RWi0eLrP118sKvkUfKy-XSFU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=818e3eebc2e8c7b511d195a9c150f4040178a9c0", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/GkOmXq5e_3MS_cWaAQ7RWi0eLrP118sKvkUfKy-XSFU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9db3ee316c83cd00c84899870b344b7cb131120", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/GkOmXq5e_3MS_cWaAQ7RWi0eLrP118sKvkUfKy-XSFU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=17e80500e6b8248056239ec840fb54559203b1db", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/GkOmXq5e_3MS_cWaAQ7RWi0eLrP118sKvkUfKy-XSFU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=25ca766a3b513bccffaaea46abd7e866e51dcdf3", "width": 960, "height": 540}], "variants": {}, "id": "Gd29rlJxSoGdKrc45YavqlPTDWd364kSymEjm_TyAa8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zq30tn", "is_robot_indexable": true, "report_reasons": null, "author": "jerrylessthanthree", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zq30tn/why_business_data_science_irritates_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://shakoist.substack.com/p/why-business-data-science-irritates?utm_source=twitter&amp;sd=pf", "subreddit_subscribers": 828564, "created_utc": 1671482748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So i have historical human classification labels for some data and am training a model with it. I got the same people to label the data a second time more recently and saw that their labels only matched 70% of the time. Does this mean than my model can only ever be 70% accurate also? (I appreciate accuracy isnt the best measure, im using it here just for explination).", "author_fullname": "t2_gm8b3iu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If im building a classification model using human labelled data, does that mean it can only ever be as accurate as the humans that originally labelled the data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zprd3e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671455913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i have historical human classification labels for some data and am training a model with it. I got the same people to label the data a second time more recently and saw that their labels only matched 70% of the time. Does this mean than my model can only ever be 70% accurate also? (I appreciate accuracy isnt the best measure, im using it here just for explination).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zprd3e", "is_robot_indexable": true, "report_reasons": null, "author": "poppycocknbalderdash", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zprd3e/if_im_building_a_classification_model_using_human/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zprd3e/if_im_building_a_classification_model_using_human/", "subreddit_subscribers": 828564, "created_utc": 1671455913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_jcdyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How an AI Stole $35 Million", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_zptdjz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Y-bneGyk4k0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How an AI Stole $35 Million\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How an AI Stole $35 Million", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Y-bneGyk4k0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How an AI Stole $35 Million\"&gt;&lt;/iframe&gt;", "author_name": "Chill Fuel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Y-bneGyk4k0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ChillFuel"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Y-bneGyk4k0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How an AI Stole $35 Million\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/zptdjz", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uVen1tja_bGfwG5cjUEVVQwwa2csHAKnxJfrvw99-Io.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671460909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=Y-bneGyk4k0", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XDxxy_ulkit2juQYZcf6XMvpSy7wbjus0rOH9awiyiw.jpg?auto=webp&amp;s=869b3e7a7d794fb58ed35cca14cc7b4bda0b8ff5", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/XDxxy_ulkit2juQYZcf6XMvpSy7wbjus0rOH9awiyiw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a681269a11342c5764b3e0c490f2847ba3800566", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/XDxxy_ulkit2juQYZcf6XMvpSy7wbjus0rOH9awiyiw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=95fad9a10798240cdca5b99d3f38174dcb469dc1", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/XDxxy_ulkit2juQYZcf6XMvpSy7wbjus0rOH9awiyiw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5e4c2384ca5c79dcc639b8fdc0a71058a7dda0b", "width": 320, "height": 240}], "variants": {}, "id": "JZv8jw1RIwUyS3h6l6GtUUERv1gAn41welyoe6rBhfo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zptdjz", "is_robot_indexable": true, "report_reasons": null, "author": "gordon22", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zptdjz/how_an_ai_stole_35_million/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=Y-bneGyk4k0", "subreddit_subscribers": 828564, "created_utc": 1671460909.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How an AI Stole $35 Million", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Y-bneGyk4k0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How an AI Stole $35 Million\"&gt;&lt;/iframe&gt;", "author_name": "Chill Fuel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Y-bneGyk4k0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ChillFuel"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My boss is providing funds to further my education for my Inventory Analyst role. I currently use Power BI and Power Automate at a slightly higher than entry level. Can anyone recommend courses that would help me progress in this type of role? Thank you in advance.", "author_fullname": "t2_7w35o79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Analyst Courses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqamfk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671500784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My boss is providing funds to further my education for my Inventory Analyst role. I currently use Power BI and Power Automate at a slightly higher than entry level. Can anyone recommend courses that would help me progress in this type of role? Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zqamfk", "is_robot_indexable": true, "report_reasons": null, "author": "babulthegreat", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zqamfk/data_analyst_courses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zqamfk/data_analyst_courses/", "subreddit_subscribers": 828564, "created_utc": 1671500784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "when performing logistic regression, the predicted outputs are typically binomial. A typical threshold value of 0.5 is applied, and yhat = 1 if sigmoid(z) &gt;0.5, and 0 if &lt;0.5. Would it be reasonable to split this into more categories? Or do i have to use softmax regression.  \n\nFor example im working on a cancer prediction project, and would like to generate the following outputs:  \n\nIf sigmoid(z) &lt; 0.05, output= benign  \nIf sigmoid(z) 0.05 to 0.95, output = recommend dr's review  \nIf sigmoid(z) &gt;0.95, output = malignant  \n\n\nWould this be incorrect?", "author_fullname": "t2_qe43i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "multi-categorical outputs from logistic regression", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpp7gv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671449714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;when performing logistic regression, the predicted outputs are typically binomial. A typical threshold value of 0.5 is applied, and yhat = 1 if sigmoid(z) &amp;gt;0.5, and 0 if &amp;lt;0.5. Would it be reasonable to split this into more categories? Or do i have to use softmax regression.  &lt;/p&gt;\n\n&lt;p&gt;For example im working on a cancer prediction project, and would like to generate the following outputs:  &lt;/p&gt;\n\n&lt;p&gt;If sigmoid(z) &amp;lt; 0.05, output= benign&lt;br/&gt;\nIf sigmoid(z) 0.05 to 0.95, output = recommend dr&amp;#39;s review&lt;br/&gt;\nIf sigmoid(z) &amp;gt;0.95, output = malignant  &lt;/p&gt;\n\n&lt;p&gt;Would this be incorrect?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zpp7gv", "is_robot_indexable": true, "report_reasons": null, "author": "GoodboyBuddy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zpp7gv/multicategorical_outputs_from_logistic_regression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zpp7gv/multicategorical_outputs_from_logistic_regression/", "subreddit_subscribers": 828564, "created_utc": 1671449714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Sorry if this isn't a place for questions like this but I was trying to implement the research paper  [Siamese neural networks for one-shot image recognition | Papers With Code](https://paperswithcode.com/paper/siamese-neural-networks-for-one-shot-image) but the approach I am trying to use is not giving any results at all it's giving the worst possible result with at max only two correct answers out of 250 cases, I took inspiration from this Github repo  [One-Shot-Learning-with-Siamese-Networks/Siamese on Omniglot Dataset.ipynb at master \u00b7 hlamba28/One-Shot-Learning-with-Siamese-Networks (github.com)](https://github.com/hlamba28/One-Shot-Learning-with-Siamese-Networks/blob/master/Siamese%20on%20Omniglot%20Dataset.ipynb) and for the most part my notebook is exactly similar with a few minor changes here and there nothing that should affect the result this drastically.\n\n&amp;#x200B;\n\nI wanted to know if anyone had any experience in working with Siamese Models before who could help understand my issues.", "author_fullname": "t2_5dba85st", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am working on a Siamese model for one-shot image classification but the approach I am using to implement my model is not working at all", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpwdg6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671467846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this isn&amp;#39;t a place for questions like this but I was trying to implement the research paper  &lt;a href=\"https://paperswithcode.com/paper/siamese-neural-networks-for-one-shot-image\"&gt;Siamese neural networks for one-shot image recognition | Papers With Code&lt;/a&gt; but the approach I am trying to use is not giving any results at all it&amp;#39;s giving the worst possible result with at max only two correct answers out of 250 cases, I took inspiration from this Github repo  &lt;a href=\"https://github.com/hlamba28/One-Shot-Learning-with-Siamese-Networks/blob/master/Siamese%20on%20Omniglot%20Dataset.ipynb\"&gt;One-Shot-Learning-with-Siamese-Networks/Siamese on Omniglot Dataset.ipynb at master \u00b7 hlamba28/One-Shot-Learning-with-Siamese-Networks (github.com)&lt;/a&gt; and for the most part my notebook is exactly similar with a few minor changes here and there nothing that should affect the result this drastically.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I wanted to know if anyone had any experience in working with Siamese Models before who could help understand my issues.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3PKDHjR7bNNRtXKDSAgi8fBSNMgT1ynm04bHL_w3zQo.jpg?auto=webp&amp;s=1f2f3358ee695685b7da48813bb70483fc865e5f", "width": 382, "height": 248}, "resolutions": [{"url": "https://external-preview.redd.it/3PKDHjR7bNNRtXKDSAgi8fBSNMgT1ynm04bHL_w3zQo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f41664ed766d816568c3a121b08b53edb46de0a6", "width": 108, "height": 70}, {"url": "https://external-preview.redd.it/3PKDHjR7bNNRtXKDSAgi8fBSNMgT1ynm04bHL_w3zQo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bec7130e4da02889a0a1d828ee0fd3f8bac8a631", "width": 216, "height": 140}, {"url": "https://external-preview.redd.it/3PKDHjR7bNNRtXKDSAgi8fBSNMgT1ynm04bHL_w3zQo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2182341665c014ae1e774c45f26128f93acf0db", "width": 320, "height": 207}], "variants": {}, "id": "aAMnSpDxn5pvDSe_CLquRhqBevtKHywF6qOTdN12ofM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zpwdg6", "is_robot_indexable": true, "report_reasons": null, "author": "DemonCyborg27", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zpwdg6/i_am_working_on_a_siamese_model_for_oneshot_image/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zpwdg6/i_am_working_on_a_siamese_model_for_oneshot_image/", "subreddit_subscribers": 828564, "created_utc": 1671467846.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I know MS in Data Science/Analytics degrees are scoffed at by most of the DS community, but that more often than not has to do with the egregious price. I imagine if the majority of programs out there were cheap, they would be viewed in a different light. GT's program is a whopping \\~$12k or so after fees and if you complete it part time as intended, it's theoretically free as most large size employers offer to pay for it since it's below their annual tax credit threshold. So, thoughts? Degree mill or actually decent for people seeking a Masters degree?", "author_fullname": "t2_3kg3ev32", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is this sub's opinion of Georgia Tech's online MS Analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqbn9o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671503360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know MS in Data Science/Analytics degrees are scoffed at by most of the DS community, but that more often than not has to do with the egregious price. I imagine if the majority of programs out there were cheap, they would be viewed in a different light. GT&amp;#39;s program is a whopping ~$12k or so after fees and if you complete it part time as intended, it&amp;#39;s theoretically free as most large size employers offer to pay for it since it&amp;#39;s below their annual tax credit threshold. So, thoughts? Degree mill or actually decent for people seeking a Masters degree?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zqbn9o", "is_robot_indexable": true, "report_reasons": null, "author": "terraninteractive", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zqbn9o/what_is_this_subs_opinion_of_georgia_techs_online/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zqbn9o/what_is_this_subs_opinion_of_georgia_techs_online/", "subreddit_subscribers": 828564, "created_utc": 1671503360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm training a Poisson model on count data and passing in log(distance) as my offset term.  This is how I called I made the model:\n\n`model &lt;- glm(next_target_both_collision_count ~ weekday_daytime_brake_count +`\n\n`weekend_daytime_brake_count + weekend_nighttime_distraction_count`\n\n`+ weekday_daytime_distraction_count`\n\n`+ weekend_daytime_following_distance_count +`\n\n`weekday_nighttime_following_distance_count +`\n\n`weekday_daytime_following_distance_count,`\n\n`maxit = 100,data = train_df, family = poisson(),`\n\n`offset=log(train_df$distance))`\n\n&amp;#x200B;\n\nHowever, the length of my train set is about 100,000 and the length of my test set is about 10,000.  As a result, when I try to run the following code:\n\n&amp;#x200B;\n\n`predictions &lt;- predict(model, type = \"response\", newdata = test_df)`\n\nI get the following error message:\n\nWarning in offset + eval(object$call$offset, newdata) :\n\nlonger object length is not a multiple of shorter object length\n\nWarning in predictor + offset :\n\nlonger object length is not a multiple of shorter object length\n\n&amp;#x200B;\n\nHow do I properly call the predict function so I don't get this error?  I tried updating the model's offset by doing\n\n`model$offset &lt;- log(test_df$distance)`\n\n`predictions &lt;- predict(model, type = \"response\", newdata = test_df)`\n\n&amp;#x200B;\n\nBut the warning still pops up.", "author_fullname": "t2_8avdky0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In R, how would you account for the offset term when doing predict? [Q]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq3rux", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671484502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m training a Poisson model on count data and passing in log(distance) as my offset term.  This is how I called I made the model:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;model &amp;lt;- glm(next_target_both_collision_count ~ weekday_daytime_brake_count +&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;weekend_daytime_brake_count + weekend_nighttime_distraction_count&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;+ weekday_daytime_distraction_count&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;+ weekend_daytime_following_distance_count +&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;weekday_nighttime_following_distance_count +&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;weekday_daytime_following_distance_count,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;maxit = 100,data = train_df, family = poisson(),&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;offset=log(train_df$distance))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;However, the length of my train set is about 100,000 and the length of my test set is about 10,000.  As a result, when I try to run the following code:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;predictions &amp;lt;- predict(model, type = &amp;quot;response&amp;quot;, newdata = test_df)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I get the following error message:&lt;/p&gt;\n\n&lt;p&gt;Warning in offset + eval(object$call$offset, newdata) :&lt;/p&gt;\n\n&lt;p&gt;longer object length is not a multiple of shorter object length&lt;/p&gt;\n\n&lt;p&gt;Warning in predictor + offset :&lt;/p&gt;\n\n&lt;p&gt;longer object length is not a multiple of shorter object length&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How do I properly call the predict function so I don&amp;#39;t get this error?  I tried updating the model&amp;#39;s offset by doing&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;model$offset &amp;lt;- log(test_df$distance)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;predictions &amp;lt;- predict(model, type = &amp;quot;response&amp;quot;, newdata = test_df)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But the warning still pops up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zq3rux", "is_robot_indexable": true, "report_reasons": null, "author": "Significant-Work-204", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zq3rux/in_r_how_would_you_account_for_the_offset_term/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zq3rux/in_r_how_would_you_account_for_the_offset_term/", "subreddit_subscribers": 828564, "created_utc": 1671484502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "It's brutal. When I have used ImportXML the first time, it looked like a miracle! I can retrieve data from external website *AND extract data from HTML in one function call*, wow! Now I am sick of it.\n\n**Custom processing and conversions of data is painful in Google Sheets.** Let's say you want to import just 32.22 number from \"cost: 32.22 USD\" string extracted from some website. It is possible to do it in Google Sheets, with some fiddling around formulas and scratching your head, but obviously it is much easier to accomplish this task Javascript or Python, especially if you have lots and lots of data to cleanup!\n\n**There is no proper launch and cache control in ImportXML.** There is no \"scrape now\" button in your Google Sheet. You can't control how often the ImportXML is triggered (well, unless you are a huge fan of Goole Apps Script!) and if you have many cells populated with importXML, it is very easy to occasionally trigger an avalanche of external http calls when opening and editing your sheet. There are also no caching mechanisms in Google Sheets, so if the last call to ImportXML fails, the cell will get *ERR!* value.\n\n**ImportXML only works with basic websites** (no SPAs rendered in browsers can be scraped this way, any basic web scraping protection or connectivity issue breaks the process, no control over HTTP request geo location, or number of retries)\n\nImportXML just fails to extract data from a huge amount of websites for me.\n\nI have finally found an alternative solution which I have been using for some time, it's more complicated to start but is just infinitely flexible and it just.. works for web scraping. My recipe consists of two ingredients:\n\n1. Proper automation framework: I choose [Make.com](https://make.com/) because it is essentially a cheaper and more techy Zapier competitor, it is mature, and it works great. Free plan.\n2. Web scraping API: I use [ScrapeNinja.net](https://scrapeninja.net/) because it allows to write custom Javascript extractors which allow me to extract any data from infinitely complex HTML of the target website, and convert it into arbitrary JSON structure for later usage. It has free plan. **And it can render SPAs like a real browser.**\n\nI am now using Google Sheets like a plain, stupid database - read data, write data back. No external HTTP calls.\n\nHere is my recipe in action: [**https://youtu.be/Uu1uw\\_koznA**](https://youtu.be/Uu1uw_koznA)", "author_fullname": "t2_13hqmc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am sick of Google Sheets ImportXML and I have finally replaced it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq1bef", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671478778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s brutal. When I have used ImportXML the first time, it looked like a miracle! I can retrieve data from external website &lt;em&gt;AND extract data from HTML in one function call&lt;/em&gt;, wow! Now I am sick of it.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Custom processing and conversions of data is painful in Google Sheets.&lt;/strong&gt; Let&amp;#39;s say you want to import just 32.22 number from &amp;quot;cost: 32.22 USD&amp;quot; string extracted from some website. It is possible to do it in Google Sheets, with some fiddling around formulas and scratching your head, but obviously it is much easier to accomplish this task Javascript or Python, especially if you have lots and lots of data to cleanup!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;There is no proper launch and cache control in ImportXML.&lt;/strong&gt; There is no &amp;quot;scrape now&amp;quot; button in your Google Sheet. You can&amp;#39;t control how often the ImportXML is triggered (well, unless you are a huge fan of Goole Apps Script!) and if you have many cells populated with importXML, it is very easy to occasionally trigger an avalanche of external http calls when opening and editing your sheet. There are also no caching mechanisms in Google Sheets, so if the last call to ImportXML fails, the cell will get &lt;em&gt;ERR!&lt;/em&gt; value.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;ImportXML only works with basic websites&lt;/strong&gt; (no SPAs rendered in browsers can be scraped this way, any basic web scraping protection or connectivity issue breaks the process, no control over HTTP request geo location, or number of retries)&lt;/p&gt;\n\n&lt;p&gt;ImportXML just fails to extract data from a huge amount of websites for me.&lt;/p&gt;\n\n&lt;p&gt;I have finally found an alternative solution which I have been using for some time, it&amp;#39;s more complicated to start but is just infinitely flexible and it just.. works for web scraping. My recipe consists of two ingredients:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Proper automation framework: I choose &lt;a href=\"https://make.com/\"&gt;Make.com&lt;/a&gt; because it is essentially a cheaper and more techy Zapier competitor, it is mature, and it works great. Free plan.&lt;/li&gt;\n&lt;li&gt;Web scraping API: I use &lt;a href=\"https://scrapeninja.net/\"&gt;ScrapeNinja.net&lt;/a&gt; because it allows to write custom Javascript extractors which allow me to extract any data from infinitely complex HTML of the target website, and convert it into arbitrary JSON structure for later usage. It has free plan. &lt;strong&gt;And it can render SPAs like a real browser.&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I am now using Google Sheets like a plain, stupid database - read data, write data back. No external HTTP calls.&lt;/p&gt;\n\n&lt;p&gt;Here is my recipe in action: &lt;a href=\"https://youtu.be/Uu1uw_koznA\"&gt;&lt;strong&gt;https://youtu.be/Uu1uw_koznA&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zq1bef", "is_robot_indexable": true, "report_reasons": null, "author": "superjet1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zq1bef/i_am_sick_of_google_sheets_importxml_and_i_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zq1bef/i_am_sick_of_google_sheets_importxml_and_i_have/", "subreddit_subscribers": 828564, "created_utc": 1671478778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Are y'all aware of any online programs or group hackathons that involve mentorship from other people? There's the WiDS hackathon and the WiBD mentorship program but apart from these, any other things y'all can tell me about?\nThanks!", "author_fullname": "t2_v3e14wl8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any programs or online events that include things like a group hackathon and mentoring?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zqg8fj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671516079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are y&amp;#39;all aware of any online programs or group hackathons that involve mentorship from other people? There&amp;#39;s the WiDS hackathon and the WiBD mentorship program but apart from these, any other things y&amp;#39;all can tell me about?\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zqg8fj", "is_robot_indexable": true, "report_reasons": null, "author": "carrotbean_14", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zqg8fj/any_programs_or_online_events_that_include_things/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zqg8fj/any_programs_or_online_events_that_include_things/", "subreddit_subscribers": 828564, "created_utc": 1671516079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all. Longtime SPSS user using statsmodels in Python for the first time. I have a dataset with the following factors: \n\nFactor A: Group with two levels\nFactor B: Group with two levels\nThe outcome measure (Y) is repeated for each subject over two time points. Each subject is in a different group. \n\nI\u2019m trying to see if there is an interaction effect of group allocation on the outcome measure as a function of time. I think I know how to model the groups (Y ~ A*B), but I\u2019m lost as how to model the repeated measure factor of Time. \n\nCan anyone help me with this and/or direct me to some resources for learning how to write model formulas?", "author_fullname": "t2_w9k2y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up a Linear Mixed Effect Model for repeated measures in statsmodels", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq63ur", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671489913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all. Longtime SPSS user using statsmodels in Python for the first time. I have a dataset with the following factors: &lt;/p&gt;\n\n&lt;p&gt;Factor A: Group with two levels\nFactor B: Group with two levels\nThe outcome measure (Y) is repeated for each subject over two time points. Each subject is in a different group. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to see if there is an interaction effect of group allocation on the outcome measure as a function of time. I think I know how to model the groups (Y ~ A*B), but I\u2019m lost as how to model the repeated measure factor of Time. &lt;/p&gt;\n\n&lt;p&gt;Can anyone help me with this and/or direct me to some resources for learning how to write model formulas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zq63ur", "is_robot_indexable": true, "report_reasons": null, "author": "tweedrobot", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zq63ur/setting_up_a_linear_mixed_effect_model_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zq63ur/setting_up_a_linear_mixed_effect_model_for/", "subreddit_subscribers": 828564, "created_utc": 1671489913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone,\n\nI've been laid off for about 2 months now and recently just got a job offer. The company is located in the UK and I am located in the US. The contract states I would be an \"independent contractor\" not an \"employee\" and the pay is in GBP and there is no healthcare offered.\n\nWhat does independent contrator mean? Is this a standard practice for hiring somebody from a foreign country? What does this mean for my taxes? Should I even take this job? Thanks!", "author_fullname": "t2_mmsqh3aw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got offered a job but as an independent contractor ? help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq5t69", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671489213.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been laid off for about 2 months now and recently just got a job offer. The company is located in the UK and I am located in the US. The contract states I would be an &amp;quot;independent contractor&amp;quot; not an &amp;quot;employee&amp;quot; and the pay is in GBP and there is no healthcare offered.&lt;/p&gt;\n\n&lt;p&gt;What does independent contrator mean? Is this a standard practice for hiring somebody from a foreign country? What does this mean for my taxes? Should I even take this job? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zq5t69", "is_robot_indexable": true, "report_reasons": null, "author": "almightynem", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zq5t69/got_offered_a_job_but_as_an_independent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zq5t69/got_offered_a_job_but_as_an_independent/", "subreddit_subscribers": 828564, "created_utc": 1671489213.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So im trying to train a tensorflow multoheaded neural network, passed and POC and everything works fine. However, the pipeline in our team is based on sklearn pipelines. I tried to figure out a way to split the labels from two dimensional numpy array to lists, and pass it through the pipeline but it doesnt work well, the transformers expect ndarrays and not list of arrays. Any advice?", "author_fullname": "t2_rq3qsw3g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scikit learn multioutput pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq542n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671487584.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So im trying to train a tensorflow multoheaded neural network, passed and POC and everything works fine. However, the pipeline in our team is based on sklearn pipelines. I tried to figure out a way to split the labels from two dimensional numpy array to lists, and pass it through the pipeline but it doesnt work well, the transformers expect ndarrays and not list of arrays. Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zq542n", "is_robot_indexable": true, "report_reasons": null, "author": "No-Front-4346", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zq542n/scikit_learn_multioutput_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zq542n/scikit_learn_multioutput_pipelines/", "subreddit_subscribers": 828564, "created_utc": 1671487584.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Say I build a classification model with a training dataset that has a column Favourite Colour which has values Yellow, Red, and Blue. Then there is the dependent variable column Gender which has values 0 - for Male, and 1 - for Female. After categorical encoding the Favourite Colour column I'll end up with 2 Independent variable (IV) columns (as we drop one column cause of the dummy variable trap). \n\nBut then in a new test dataset the Favourite Color column only contains values Red, and Blue. After encoding I'll then have 1 IV column, but my model was trained on a dataset with 2 IVs. How will my model be able to run considering the input size is now different?\n\nI know that I have run into this problem when creating a Random Forest Model (it throws an array that the model was expecting a certain input size but got a different one), but I am asking about models in general.\n\nThanks", "author_fullname": "t2_frwys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do to do when test input size is not the same as training input size?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq3wnd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671484810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say I build a classification model with a training dataset that has a column Favourite Colour which has values Yellow, Red, and Blue. Then there is the dependent variable column Gender which has values 0 - for Male, and 1 - for Female. After categorical encoding the Favourite Colour column I&amp;#39;ll end up with 2 Independent variable (IV) columns (as we drop one column cause of the dummy variable trap). &lt;/p&gt;\n\n&lt;p&gt;But then in a new test dataset the Favourite Color column only contains values Red, and Blue. After encoding I&amp;#39;ll then have 1 IV column, but my model was trained on a dataset with 2 IVs. How will my model be able to run considering the input size is now different?&lt;/p&gt;\n\n&lt;p&gt;I know that I have run into this problem when creating a Random Forest Model (it throws an array that the model was expecting a certain input size but got a different one), but I am asking about models in general.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zq3wnd", "is_robot_indexable": true, "report_reasons": null, "author": "fouried96", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zq3wnd/what_do_to_do_when_test_input_size_is_not_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zq3wnd/what_do_to_do_when_test_input_size_is_not_the/", "subreddit_subscribers": 828564, "created_utc": 1671484810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_c25i55k3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a digital library of data science books. I have looked at Kindle Unlimited and Scribd. Does anyone know of an good source to read / refer to data science books?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq2xer", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671482525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zq2xer", "is_robot_indexable": true, "report_reasons": null, "author": "moltra_1", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zq2xer/looking_for_a_digital_library_of_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zq2xer/looking_for_a_digital_library_of_data_science/", "subreddit_subscribers": 828564, "created_utc": 1671482525.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all hoping someone can answer this question. \n\nI am working on a project where health care providers will be collecting data about the patient. The patient is fully aware we are collecting the data as part of the program. My organization is not HIPAA compliment so we need all data de-identified. \n\n\nWhat way can this be done? The offices will be sending me monthly blood pressure updates so they have to be able to look at the chart and be able to determine which patient is which I can\u2019t just number them 1-100 or something. My first thought was patient first/last name initial plus birth day/month but I believe for hipaa that is not stripped enough. Appreciate the help and fully understand y\u2019all are not lawyers but I am not finding any concrete resources online!", "author_fullname": "t2_3bxtfsum", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "De-Identify Health Info", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq2ike", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671481536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all hoping someone can answer this question. &lt;/p&gt;\n\n&lt;p&gt;I am working on a project where health care providers will be collecting data about the patient. The patient is fully aware we are collecting the data as part of the program. My organization is not HIPAA compliment so we need all data de-identified. &lt;/p&gt;\n\n&lt;p&gt;What way can this be done? The offices will be sending me monthly blood pressure updates so they have to be able to look at the chart and be able to determine which patient is which I can\u2019t just number them 1-100 or something. My first thought was patient first/last name initial plus birth day/month but I believe for hipaa that is not stripped enough. Appreciate the help and fully understand y\u2019all are not lawyers but I am not finding any concrete resources online!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zq2ike", "is_robot_indexable": true, "report_reasons": null, "author": "HPGOTTOP", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zq2ike/deidentify_health_info/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zq2ike/deidentify_health_info/", "subreddit_subscribers": 828564, "created_utc": 1671481536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi there,\n\nBeing into the topic of web scraping and deep learning lately, I am asking myself how well the two topics could be combined. How feasible would it for example be to build a deep learning model that could extract the relevant information from a screenshot of a website? More generally, I would say that a lot of e-commerce sites are build up almost the same way (structure and product display) when it comes to their visual appearance. Would it here be possibel to build and train a generalizable model that could extract the necessary product information of other e-commerce sites by getting a screenshot of a page as input? Additionally, what architecture would you probably use, a RNN like model and are there already similar solutions?", "author_fullname": "t2_qrf0fsnc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web scraping with Deep Learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq133q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671478248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;Being into the topic of web scraping and deep learning lately, I am asking myself how well the two topics could be combined. How feasible would it for example be to build a deep learning model that could extract the relevant information from a screenshot of a website? More generally, I would say that a lot of e-commerce sites are build up almost the same way (structure and product display) when it comes to their visual appearance. Would it here be possibel to build and train a generalizable model that could extract the necessary product information of other e-commerce sites by getting a screenshot of a page as input? Additionally, what architecture would you probably use, a RNN like model and are there already similar solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zq133q", "is_robot_indexable": true, "report_reasons": null, "author": "Odd-Concert-4591", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zq133q/web_scraping_with_deep_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zq133q/web_scraping_with_deep_learning/", "subreddit_subscribers": 828564, "created_utc": 1671478248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_sppgx30r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Docker Basics and data science devops integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_zpwh4l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/2_YGrAbX5Ck?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Master Docker in 30 Minutes | Introduction, Architecture and Commands | DevOps Tutorial\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Master Docker in 30 Minutes | Introduction, Architecture and Commands | DevOps Tutorial", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/2_YGrAbX5Ck?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Master Docker in 30 Minutes | Introduction, Architecture and Commands | DevOps Tutorial\"&gt;&lt;/iframe&gt;", "author_name": " Code with Scaler", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/2_YGrAbX5Ck/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CodewithScaler"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/2_YGrAbX5Ck?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Master Docker in 30 Minutes | Introduction, Architecture and Commands | DevOps Tutorial\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/zpwh4l", "height": 200}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/m_6y3EPsbFDjMWkt1rdzJfxIR5Bf41vvNAlrbuEtqD4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671468089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/2_YGrAbX5Ck", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ujSq04ppJ8wTypANKd9k2bxbgTcyjUbdhPgBuCMH020.jpg?auto=webp&amp;s=9dc857126f2b0d16487bd3dae962f3a7bda5e1f2", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ujSq04ppJ8wTypANKd9k2bxbgTcyjUbdhPgBuCMH020.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=565c6585b4be6cee9720f692fe84acdcfff628ad", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ujSq04ppJ8wTypANKd9k2bxbgTcyjUbdhPgBuCMH020.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e2bd6ef8689bd8d18c6c9de9e1b97d8fc0053f05", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ujSq04ppJ8wTypANKd9k2bxbgTcyjUbdhPgBuCMH020.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=be10cf13c2888e562e2e34b63a87404281feee07", "width": 320, "height": 240}], "variants": {}, "id": "KabfInI-6CSIuonyYX_CsxqIFlsvM_q4cIuXIpJXy_I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zpwh4l", "is_robot_indexable": true, "report_reasons": null, "author": "thetech_learner", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zpwh4l/docker_basics_and_data_science_devops_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/2_YGrAbX5Ck", "subreddit_subscribers": 828564, "created_utc": 1671468089.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Master Docker in 30 Minutes | Introduction, Architecture and Commands | DevOps Tutorial", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/2_YGrAbX5Ck?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Master Docker in 30 Minutes | Introduction, Architecture and Commands | DevOps Tutorial\"&gt;&lt;/iframe&gt;", "author_name": " Code with Scaler", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/2_YGrAbX5Ck/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CodewithScaler"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello Everyone. I hope guys are doing well. I am about to start a new job as a data analyst with major focus on building dashboards in various tools like Excel, Microsoft PowerBI and Tableau to derive meaningful insights while I am also tasked with making executive level presentations displaying those insights strategically.\n\nMy question is that what frameworks, methods or processes do I follow to derive those insights in the 1st place? Any suggestions would be highly helpful.", "author_fullname": "t2_lw641n38", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I derive meaningful insights from a raw database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpsxpf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671459828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone. I hope guys are doing well. I am about to start a new job as a data analyst with major focus on building dashboards in various tools like Excel, Microsoft PowerBI and Tableau to derive meaningful insights while I am also tasked with making executive level presentations displaying those insights strategically.&lt;/p&gt;\n\n&lt;p&gt;My question is that what frameworks, methods or processes do I follow to derive those insights in the 1st place? Any suggestions would be highly helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zpsxpf", "is_robot_indexable": true, "report_reasons": null, "author": "The_Alexander_3141", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zpsxpf/how_do_i_derive_meaningful_insights_from_a_raw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zpsxpf/how_do_i_derive_meaningful_insights_from_a_raw/", "subreddit_subscribers": 828564, "created_utc": 1671459828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi,\n\nI'm a newbie to Data Science, been in this ML space just for 6 months, looking for a transition to DS and then into ML and as a part of my resume building, need a small help on a project idea. \n\nHaven't yet worked on this, but is this a correct route to get anything fruitful to showcase in a resume.\n\nUsing Twitter API to get hashtags regarding tech investments for 3-4 year timeline\nTrying to find any correlation of this Twitter info with job postings and key skills required for the same time as above or said time +2 years\n\nI know a bit of web scraping, have idea on all basic ML algos other than deep learning..\n\nAny help/direction on this is deeply appreciated.", "author_fullname": "t2_85v6qfqc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with a small self project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zplsr5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671437336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a newbie to Data Science, been in this ML space just for 6 months, looking for a transition to DS and then into ML and as a part of my resume building, need a small help on a project idea. &lt;/p&gt;\n\n&lt;p&gt;Haven&amp;#39;t yet worked on this, but is this a correct route to get anything fruitful to showcase in a resume.&lt;/p&gt;\n\n&lt;p&gt;Using Twitter API to get hashtags regarding tech investments for 3-4 year timeline\nTrying to find any correlation of this Twitter info with job postings and key skills required for the same time as above or said time +2 years&lt;/p&gt;\n\n&lt;p&gt;I know a bit of web scraping, have idea on all basic ML algos other than deep learning..&lt;/p&gt;\n\n&lt;p&gt;Any help/direction on this is deeply appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zplsr5", "is_robot_indexable": true, "report_reasons": null, "author": "dreamy_poppy_nutty", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zplsr5/help_with_a_small_self_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zplsr5/help_with_a_small_self_project/", "subreddit_subscribers": 828564, "created_utc": 1671437336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\n\ud83d\udd25 We\u2019re thrilled to introduce BastionLab, our simple privacy framework for data science collaboration! To see what remote data exploration looks like when all privacy concerns are automatically handled for you, you can check [our GitHub](https://github.com/mithril-security/bastionlab/) or directly go to our [Quick Tour](https://bastionlab.readthedocs.io/en/latest/docs/quick-tour/quick-tour/) tutorial, which is also available on [Colab ](https://colab.research.google.com/github/mithril-security/bastionlab/blob/v0.3.5/docs/docs/quick-tour/quick-tour.ipynb)\ud83d\udd12\n\n# Built for sensitive data collaboration\n\nCollaboration between data owners and data scientists is a big challenge for highly regulated fields like health, finance, or advertising due to security and privacy issues. When collaborating remotely, data owners have to open their whole dataset, often through a Jupyter notebook. This too-broad access creates huge privacy gaps because too many operations are allowed, which enables data scientists to extract information from the remote infrastructure (print the whole database, save the dataset in the weights, etc). \n\n\u2699\ufe0f BastionLab solves this problem by providing fine-grained access control. It guarantees data owners that data scientists can only perform privacy-friendly operations on their data and that only anonymized outputs are shared with them.\n\n# How does BastionLab work?\n\nBastionLab makes sure that the data owner\u2019s remote data is never accessed directly by the data scientist. Three main elements ensure this: \n\n* First, a \u2018safe zone\u2019 is defined by the data owner to filter the data scientist\u2019s queries, which enforces control while allowing for interactivity. \n* Second, expressivity is limited. This means that the type of operations that can be executed by the data scientists is restricted to avoid arbitrary code execution. \n* Finally, the data scientist never accesses the dataset locally. They only manipulate a local object that contains metadata to interact with the remotely hosted dataset - and data owners can always see the calls made by that object. \n\n# Ready to try?\n\nIf you like the project, drop a \u2b50 on our [GitHub](https://github.com/mithril-security/bastionlab/)! We\u2019re open-source, so it\u2019s a big help \\^\\^", "author_fullname": "t2_dwuoduiq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing BastionLab - A simple privacy framework for data science collaboration!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zqfuzi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671514964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\ud83d\udd25 We\u2019re thrilled to introduce BastionLab, our simple privacy framework for data science collaboration! To see what remote data exploration looks like when all privacy concerns are automatically handled for you, you can check &lt;a href=\"https://github.com/mithril-security/bastionlab/\"&gt;our GitHub&lt;/a&gt; or directly go to our &lt;a href=\"https://bastionlab.readthedocs.io/en/latest/docs/quick-tour/quick-tour/\"&gt;Quick Tour&lt;/a&gt; tutorial, which is also available on &lt;a href=\"https://colab.research.google.com/github/mithril-security/bastionlab/blob/v0.3.5/docs/docs/quick-tour/quick-tour.ipynb\"&gt;Colab &lt;/a&gt;\ud83d\udd12&lt;/p&gt;\n\n&lt;h1&gt;Built for sensitive data collaboration&lt;/h1&gt;\n\n&lt;p&gt;Collaboration between data owners and data scientists is a big challenge for highly regulated fields like health, finance, or advertising due to security and privacy issues. When collaborating remotely, data owners have to open their whole dataset, often through a Jupyter notebook. This too-broad access creates huge privacy gaps because too many operations are allowed, which enables data scientists to extract information from the remote infrastructure (print the whole database, save the dataset in the weights, etc). &lt;/p&gt;\n\n&lt;p&gt;\u2699\ufe0f BastionLab solves this problem by providing fine-grained access control. It guarantees data owners that data scientists can only perform privacy-friendly operations on their data and that only anonymized outputs are shared with them.&lt;/p&gt;\n\n&lt;h1&gt;How does BastionLab work?&lt;/h1&gt;\n\n&lt;p&gt;BastionLab makes sure that the data owner\u2019s remote data is never accessed directly by the data scientist. Three main elements ensure this: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;First, a \u2018safe zone\u2019 is defined by the data owner to filter the data scientist\u2019s queries, which enforces control while allowing for interactivity. &lt;/li&gt;\n&lt;li&gt;Second, expressivity is limited. This means that the type of operations that can be executed by the data scientists is restricted to avoid arbitrary code execution. &lt;/li&gt;\n&lt;li&gt;Finally, the data scientist never accesses the dataset locally. They only manipulate a local object that contains metadata to interact with the remotely hosted dataset - and data owners can always see the calls made by that object. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Ready to try?&lt;/h1&gt;\n\n&lt;p&gt;If you like the project, drop a \u2b50 on our &lt;a href=\"https://github.com/mithril-security/bastionlab/\"&gt;GitHub&lt;/a&gt;! We\u2019re open-source, so it\u2019s a big help ^^&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8l1gCfTSVG63eDA7vFvFsBRQiHDkGmHtUd4k4PukPVE.jpg?auto=webp&amp;s=2de6f12ed23c4f54c28751887e7ccc14a4aaaefa", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/8l1gCfTSVG63eDA7vFvFsBRQiHDkGmHtUd4k4PukPVE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ebcdedc34e047780945fe53807dc8403cc5dc76", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/8l1gCfTSVG63eDA7vFvFsBRQiHDkGmHtUd4k4PukPVE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=74f45f7bd67da8ca6cdf837ec79b5c3eb6258b2d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/8l1gCfTSVG63eDA7vFvFsBRQiHDkGmHtUd4k4PukPVE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=de5496bf970e334f81781ca7c83199a5290c8de4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/8l1gCfTSVG63eDA7vFvFsBRQiHDkGmHtUd4k4PukPVE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=500be71718c43cf5483e0dfed22431627794ddea", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/8l1gCfTSVG63eDA7vFvFsBRQiHDkGmHtUd4k4PukPVE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4931a8e07568a2f9003d5fbbefd5ce0170f62823", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/8l1gCfTSVG63eDA7vFvFsBRQiHDkGmHtUd4k4PukPVE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8a0360728028f8b37456d9f676f8a0b51b9702c5", "width": 1080, "height": 540}], "variants": {}, "id": "4I5yjoJax0kR52ybWpt_vvGQ6G0zj4fk8619okQsBrg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zqfuzi", "is_robot_indexable": true, "report_reasons": null, "author": "Separate-Still3770", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zqfuzi/introducing_bastionlab_a_simple_privacy_framework/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zqfuzi/introducing_bastionlab_a_simple_privacy_framework/", "subreddit_subscribers": 828564, "created_utc": 1671514964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_jeooiop5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3D COMPARISON: HEAT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_zqdaub", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/cAkv60mjBeY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"3D COMPARISON: HEAT\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "3D COMPARISON: HEAT", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/cAkv60mjBeY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"3D COMPARISON: HEAT\"&gt;&lt;/iframe&gt;", "author_name": "Data Centre", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/cAkv60mjBeY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@SangamNews"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/cAkv60mjBeY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"3D COMPARISON: HEAT\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/zqdaub", "height": 200}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/SizGoz-LMOWtC8YQDoyzPOMUmbBqvzesmuXTYoVuF40.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671507729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://youtube.com/watch?v=cAkv60mjBeY&amp;feature=share", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7N_GFyBL45tdZJ3-ow9eFzDkBbotq-LyV_8L-zzBKRg.jpg?auto=webp&amp;s=6b5a6e7803c74e7b6958090db9c6543e87f0f554", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/7N_GFyBL45tdZJ3-ow9eFzDkBbotq-LyV_8L-zzBKRg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e49dd0f3afb2e7dc2cc4e0d180f3bd49167993da", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/7N_GFyBL45tdZJ3-ow9eFzDkBbotq-LyV_8L-zzBKRg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e1b5db68c435d81df97db5fad5a30e5144e732e", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/7N_GFyBL45tdZJ3-ow9eFzDkBbotq-LyV_8L-zzBKRg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8635843bc0928602ba03f7d58005a7c71e5e05a0", "width": 320, "height": 240}], "variants": {}, "id": "id0-g2W7ehunak_bDv4VEuYhmixZbbr-ZhJaRmUl4jk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zqdaub", "is_robot_indexable": true, "report_reasons": null, "author": "guptasangam", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zqdaub/3d_comparison_heat/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtube.com/watch?v=cAkv60mjBeY&amp;feature=share", "subreddit_subscribers": 828564, "created_utc": 1671507729.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "3D COMPARISON: HEAT", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/cAkv60mjBeY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"3D COMPARISON: HEAT\"&gt;&lt;/iframe&gt;", "author_name": "Data Centre", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/cAkv60mjBeY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@SangamNews"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_afysg7cd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For someone without any prior experience in this field(did my bachelors in biology) what would be a good major do to my masters in? What do y\u2019all think about MIS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq2v4p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671482372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zq2v4p", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Dark-170", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zq2v4p/for_someone_without_any_prior_experience_in_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zq2v4p/for_someone_without_any_prior_experience_in_this/", "subreddit_subscribers": 828564, "created_utc": 1671482372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_k0j19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Short-term precipitation forecasting using Convolutional LSTM neural networks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zq2b2d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mcK9oMjlEjBlFluu7TzvXp516iFc4K-lG2Uaslg9A0Q.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671481044.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/p/f347db1b5f1d", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mVgiYnXMD1IXCLwSKvtvNx8yVpYSlLKACJyqdw3T6-w.jpg?auto=webp&amp;s=d8600699a92433ae5a7185e8c8e30b69b1038544", "width": 700, "height": 765}, "resolutions": [{"url": "https://external-preview.redd.it/mVgiYnXMD1IXCLwSKvtvNx8yVpYSlLKACJyqdw3T6-w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=447c813d7aedac75cb02aadd5a7dd7f35bceedda", "width": 108, "height": 118}, {"url": "https://external-preview.redd.it/mVgiYnXMD1IXCLwSKvtvNx8yVpYSlLKACJyqdw3T6-w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cbaaf1ad21ee1ffbd1b8ea54d2bab07fb7ff0c18", "width": 216, "height": 236}, {"url": "https://external-preview.redd.it/mVgiYnXMD1IXCLwSKvtvNx8yVpYSlLKACJyqdw3T6-w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4118cf1358381b11e2ed233cb350edff83b3bb7a", "width": 320, "height": 349}, {"url": "https://external-preview.redd.it/mVgiYnXMD1IXCLwSKvtvNx8yVpYSlLKACJyqdw3T6-w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5221f6d339a492d762a262cc4268868a4554aecc", "width": 640, "height": 699}], "variants": {}, "id": "0IDXQCqsuw67pRbMpM7dfspUJA9KKRRbvKg038cUKeo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zq2b2d", "is_robot_indexable": true, "report_reasons": null, "author": "psdemetrako", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zq2b2d/shortterm_precipitation_forecasting_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/p/f347db1b5f1d", "subreddit_subscribers": 828564, "created_utc": 1671481044.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}