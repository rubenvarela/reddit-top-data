{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_69a5f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "explaining what the data modeler on the team does", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_zq4eg6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 156, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 156, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/oKNUA7sY1Eh2aloDHp4Q-2QbFqqpb4h5homJ0tTEae8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671485970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/4svii7iyty6a1.gif", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/4svii7iyty6a1.gif?format=png8&amp;s=4bd9c72626681c8ab5f84b9f177e29fc9e05f6c4", "width": 480, "height": 360}, "resolutions": [{"url": "https://preview.redd.it/4svii7iyty6a1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=fe04087d34b32b14c5d236d9b58bb5840f693c1f", "width": 108, "height": 81}, {"url": "https://preview.redd.it/4svii7iyty6a1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=7ca3dbf483553b27cd31ce551b1b040379cf5ce2", "width": 216, "height": 162}, {"url": "https://preview.redd.it/4svii7iyty6a1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=c280a6b4d28e30ed54f2e5fd5a81b54dcb884ca9", "width": 320, "height": 240}], "variants": {"gif": {"source": {"url": "https://preview.redd.it/4svii7iyty6a1.gif?s=e7723bc482be0b0026b05fe53ee196f60f98aa6d", "width": 480, "height": 360}, "resolutions": [{"url": "https://preview.redd.it/4svii7iyty6a1.gif?width=108&amp;crop=smart&amp;s=f89dbb860b65ec70b99a348a3cd7dd4ec108e602", "width": 108, "height": 81}, {"url": "https://preview.redd.it/4svii7iyty6a1.gif?width=216&amp;crop=smart&amp;s=6e814c7d821378c262c3345090f92e2e76f199ed", "width": 216, "height": 162}, {"url": "https://preview.redd.it/4svii7iyty6a1.gif?width=320&amp;crop=smart&amp;s=dcb652318b88d353cfd9091b4c44599fdd0ee95d", "width": 320, "height": 240}]}, "mp4": {"source": {"url": "https://preview.redd.it/4svii7iyty6a1.gif?format=mp4&amp;s=79c646a6091a2054ecc51083894f393f601296e0", "width": 480, "height": 360}, "resolutions": [{"url": "https://preview.redd.it/4svii7iyty6a1.gif?width=108&amp;format=mp4&amp;s=70fad6d8c2188e0ffcde10e091198fadcadeaa9b", "width": 108, "height": 81}, {"url": "https://preview.redd.it/4svii7iyty6a1.gif?width=216&amp;format=mp4&amp;s=a4b6826456166838c71df8356d2f4a3ee4f46dee", "width": 216, "height": 162}, {"url": "https://preview.redd.it/4svii7iyty6a1.gif?width=320&amp;format=mp4&amp;s=3e090612250734014da0751f7e93c82c54d17efb", "width": 320, "height": 240}]}}, "id": "v7U3z_ZRcPCLfYaFGIe2eihy6KZVGMFdg-7Mzcfk9Sg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "zq4eg6", "is_robot_indexable": true, "report_reasons": null, "author": "DiceboyT", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zq4eg6/explaining_what_the_data_modeler_on_the_team_does/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/4svii7iyty6a1.gif", "subreddit_subscribers": 83403, "created_utc": 1671485970.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "ELT: \u201cshift your cost center to your warehouse\u201d  \n\n\nModern Data Stack - \u201cshift your cost center to your warehouse\u201d  \n\n\nZero ETL:\u00a0 \u201cshift your cost center to your warehouse \\*now with more lock in!\\*\u201d  \n\n\nCredits:\u00a0 \u201cshift your costs to\u2026.variable\u201d  \n\n\nNo code: \u201cshift to needing two tools for the same job\u201d  \n\n\nLow code: \u201cshift to coding normally\u201d  \n\n\nBatch:\u00a0 \u201cBusiness model for NYSE:SNOW\u201d  \n\n\nReal-time: \u201csomewhere between nano seconds and hours\u201d  \n\n\nData quality: \u201cthe thing we keep talking about and would like to get to someday\u201d  \n\n\nStreaming SQL: \u201cVendor-specific mashups of various strategies for bolting notions of time variance into a language not designed for it\u201d  \n\n\nSchemaless: \u201cthere is a schema, but we don\u2019t know what it is\u201d  \n\n\nBonus alternative ELT definition: \"we changed our schema and broke the data pipeline, but we can make the analysts deal with it\"  \n\n\nWhat others are we missing?  \n\n\nGreat thread of comments on this prompt as well: [https://www.linkedin.com/feed/update/urn:li:activity:7009593010644557825/](https://www.linkedin.com/feed/update/urn:li:activity:7009593010644557825/)", "author_fullname": "t2_sa3mbz4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2022 data buzzwords translated to their actual meaning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqqsqx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 96, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 96, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671548815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ELT: \u201cshift your cost center to your warehouse\u201d  &lt;/p&gt;\n\n&lt;p&gt;Modern Data Stack - \u201cshift your cost center to your warehouse\u201d  &lt;/p&gt;\n\n&lt;p&gt;Zero ETL:\u00a0 \u201cshift your cost center to your warehouse *now with more lock in!*\u201d  &lt;/p&gt;\n\n&lt;p&gt;Credits:\u00a0 \u201cshift your costs to\u2026.variable\u201d  &lt;/p&gt;\n\n&lt;p&gt;No code: \u201cshift to needing two tools for the same job\u201d  &lt;/p&gt;\n\n&lt;p&gt;Low code: \u201cshift to coding normally\u201d  &lt;/p&gt;\n\n&lt;p&gt;Batch:\u00a0 \u201cBusiness model for NYSE:SNOW\u201d  &lt;/p&gt;\n\n&lt;p&gt;Real-time: \u201csomewhere between nano seconds and hours\u201d  &lt;/p&gt;\n\n&lt;p&gt;Data quality: \u201cthe thing we keep talking about and would like to get to someday\u201d  &lt;/p&gt;\n\n&lt;p&gt;Streaming SQL: \u201cVendor-specific mashups of various strategies for bolting notions of time variance into a language not designed for it\u201d  &lt;/p&gt;\n\n&lt;p&gt;Schemaless: \u201cthere is a schema, but we don\u2019t know what it is\u201d  &lt;/p&gt;\n\n&lt;p&gt;Bonus alternative ELT definition: &amp;quot;we changed our schema and broke the data pipeline, but we can make the analysts deal with it&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;What others are we missing?  &lt;/p&gt;\n\n&lt;p&gt;Great thread of comments on this prompt as well: &lt;a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7009593010644557825/\"&gt;https://www.linkedin.com/feed/update/urn:li:activity:7009593010644557825/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?auto=webp&amp;s=18689041ed1ad1829e19b3f16bdf0619c3add354", "width": 1400, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd3104eb2954de61b63b4aded5889d7f481667e4", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c971a4bc289c8f96921326f627cebe12f3ac1660", "width": 216, "height": 123}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4aa8c0a35a98b8aba18c141f215320d65e017724", "width": 320, "height": 182}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b4185fb0b2f226c0a0e3e33b8be8f2575bf9ab5", "width": 640, "height": 365}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f9aebe95893f1568f8433661797ed06581b8b5b", "width": 960, "height": 548}, {"url": "https://external-preview.redd.it/HXoO2jOmLFHYLOp5uqz0uMk2GNP7Gqo8zf2a1hzURE4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4632741c44875873f4766540a85113c0e703b906", "width": 1080, "height": 617}], "variants": {}, "id": "CjbMbFq2MEqSKWpNCjv-ipCLADmRBQ1ZCG3w2yy71f0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "zqqsqx", "is_robot_indexable": true, "report_reasons": null, "author": "MooJerseyCreamery", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqqsqx/2022_data_buzzwords_translated_to_their_actual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqqsqx/2022_data_buzzwords_translated_to_their_actual/", "subreddit_subscribers": 83403, "created_utc": 1671548815.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for good technical interview questions and tips for interviewing entry to mid-level 'Data &amp; Analytics Engineers'. \n\nI've interviewed a number of people already for this position but want to make sure I'm asking good questions and being fair to the candidates \n\nI'm a young software engineer at a large IT consulting firm. I have a strong background in MS SQL Server, ETL, MDM and tuning queries for large transactional databases\n\nHowever.. I have little to NO experience with Azure/AWS, data warehousing, machine learning, Python, R, data visualization tools like Tableau, etc. This can make interviews difficult because the candidates often have these tools/disciplines listed on their resume.. \n\nI usually end up asking broad questions about their past project/work to gauge their communication skills (important because this is consulting). Then asking if they have experience with source control, performance tuning, or have worked with sensitive data. Then finish by asking basic SQL/database questions like: what is the difference between INNER vs LEFT join, what are some ways to eliminate duplicates in a query, what is a temp table, what is a database index, etc..", "author_fullname": "t2_8jq30m4x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good technical interview questions for 'Data &amp; Analytics Engineer'?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqeco3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671510570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for good technical interview questions and tips for interviewing entry to mid-level &amp;#39;Data &amp;amp; Analytics Engineers&amp;#39;. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve interviewed a number of people already for this position but want to make sure I&amp;#39;m asking good questions and being fair to the candidates &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a young software engineer at a large IT consulting firm. I have a strong background in MS SQL Server, ETL, MDM and tuning queries for large transactional databases&lt;/p&gt;\n\n&lt;p&gt;However.. I have little to NO experience with Azure/AWS, data warehousing, machine learning, Python, R, data visualization tools like Tableau, etc. This can make interviews difficult because the candidates often have these tools/disciplines listed on their resume.. &lt;/p&gt;\n\n&lt;p&gt;I usually end up asking broad questions about their past project/work to gauge their communication skills (important because this is consulting). Then asking if they have experience with source control, performance tuning, or have worked with sensitive data. Then finish by asking basic SQL/database questions like: what is the difference between INNER vs LEFT join, what are some ways to eliminate duplicates in a query, what is a temp table, what is a database index, etc..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "zqeco3", "is_robot_indexable": true, "report_reasons": null, "author": "patheticadam", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqeco3/good_technical_interview_questions_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqeco3/good_technical_interview_questions_for_data/", "subreddit_subscribers": 83403, "created_utc": 1671510570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r570xlc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Project - Personal Finances with Airflow, Docker, Great Expectations and Metabase", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqkowp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1671530903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "eliasbenaddouidrissi.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://eliasbenaddouidrissi.dev/posts/data_engineering_project_monzo/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "zqkowp", "is_robot_indexable": true, "report_reasons": null, "author": "homosapienhomodeus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqkowp/data_engineering_project_personal_finances_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://eliasbenaddouidrissi.dev/posts/data_engineering_project_monzo/", "subreddit_subscribers": 83403, "created_utc": 1671530903.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI would like to request for your guidance/advice to understand if the streaming pipeline I am working on can be somehow optimised, both in term of costs and performances.\n\nThis pipeline uses Debezium to ingest data from different MySQL databases (each of them containing \\~20-30 schemas, each of them containing \\~450-500 tables). Data is written by Debezium to a Kafka broker, and from there it is consumed by a Spark Structured streaming application that reads these events and performs upserts to Delta Tables stored on S3. These delta tables are accessed directly by our BI/reporting tools, that currently rely on a 1:1 copy of the source database (long-term solution to this at the end).\n\n&amp;#x200B;\n\nIn the current implementation of this pipeline, I have a Debezium worker for each MySQL server, and I am rerouting all events from all tables/schemas of each server to a single topic, named like the source MySQL server, so I have one connector and one topic for each MySQL server. Each topic has 12 partitions (number picked randomly, to be \"future-proof\"). For each topic there is a dedicated Spark Streaming application reading events from Kafka and upserting them to Delta tables.\n\nThis implementation is currently running, but I have some concerns and I hope someone with more experience on similar use cases could help:\n\n* I need to have one Spark streaming application for each mysql server, this can make costs levitate in case instances increase to high numbers (e.g. 100-200)\n* The Spark streaming job needs to group events by table and each batch contains events belonging to \\~70-100 different tables, therefore for each batch the same amount of upserts is performed, resulting in a latency of 20-30 minutes, which for now is acceptable.\n* I was thinking about reorganising topics by table, but:\n   * I should have \\~450-500 active Spark Streaming apps/queries, even though this would solve the problem of \\~70-100 different upserts per batch. \n   * There are tables that are written more frequently than others, meaning the upserts will take longer, thus producing potential inconsistency between tables (if I need to join two or more tables, the largest ones might not be up to date). The current solution of using a dedicated Spark App for each DB prevents this from happening\n\nIn the long term the strategy would be to convince the product team to implement CQRS pattern, in such a way I can directly consume events that already contain most of the context, so I won't have to copy so many tables from our relational DB. \n\nBut for the time being, is there any change that you would suggest to reduce both latency and potential costs growth?", "author_fullname": "t2_do2bj7xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to optimize a streaming pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqia5w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671522521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I would like to request for your guidance/advice to understand if the streaming pipeline I am working on can be somehow optimised, both in term of costs and performances.&lt;/p&gt;\n\n&lt;p&gt;This pipeline uses Debezium to ingest data from different MySQL databases (each of them containing ~20-30 schemas, each of them containing ~450-500 tables). Data is written by Debezium to a Kafka broker, and from there it is consumed by a Spark Structured streaming application that reads these events and performs upserts to Delta Tables stored on S3. These delta tables are accessed directly by our BI/reporting tools, that currently rely on a 1:1 copy of the source database (long-term solution to this at the end).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In the current implementation of this pipeline, I have a Debezium worker for each MySQL server, and I am rerouting all events from all tables/schemas of each server to a single topic, named like the source MySQL server, so I have one connector and one topic for each MySQL server. Each topic has 12 partitions (number picked randomly, to be &amp;quot;future-proof&amp;quot;). For each topic there is a dedicated Spark Streaming application reading events from Kafka and upserting them to Delta tables.&lt;/p&gt;\n\n&lt;p&gt;This implementation is currently running, but I have some concerns and I hope someone with more experience on similar use cases could help:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I need to have one Spark streaming application for each mysql server, this can make costs levitate in case instances increase to high numbers (e.g. 100-200)&lt;/li&gt;\n&lt;li&gt;The Spark streaming job needs to group events by table and each batch contains events belonging to ~70-100 different tables, therefore for each batch the same amount of upserts is performed, resulting in a latency of 20-30 minutes, which for now is acceptable.&lt;/li&gt;\n&lt;li&gt;I was thinking about reorganising topics by table, but:\n\n&lt;ul&gt;\n&lt;li&gt;I should have ~450-500 active Spark Streaming apps/queries, even though this would solve the problem of ~70-100 different upserts per batch. &lt;/li&gt;\n&lt;li&gt;There are tables that are written more frequently than others, meaning the upserts will take longer, thus producing potential inconsistency between tables (if I need to join two or more tables, the largest ones might not be up to date). The current solution of using a dedicated Spark App for each DB prevents this from happening&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In the long term the strategy would be to convince the product team to implement CQRS pattern, in such a way I can directly consume events that already contain most of the context, so I won&amp;#39;t have to copy so many tables from our relational DB. &lt;/p&gt;\n\n&lt;p&gt;But for the time being, is there any change that you would suggest to reduce both latency and potential costs growth?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zqia5w", "is_robot_indexable": true, "report_reasons": null, "author": "somerandomdataeng", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/zqia5w/best_way_to_optimize_a_streaming_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqia5w/best_way_to_optimize_a_streaming_pipeline/", "subreddit_subscribers": 83403, "created_utc": 1671522521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI've studied data engineering for almost two years. My last role was a BI Developer and now I have an interview for a Senior data integration engineer role in two weeks. I want and need this job.  What are some things I can mention to be impressive in this interview.  I'm prepared to discuss the following:\n\n ETL design patterns, incremental load, query optimization, audit tables, batch and streaming processing and ETL monitoring", "author_fullname": "t2_6ard8gzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Prep - Senior Data Integration role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqmnxe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671537882.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve studied data engineering for almost two years. My last role was a BI Developer and now I have an interview for a Senior data integration engineer role in two weeks. I want and need this job.  What are some things I can mention to be impressive in this interview.  I&amp;#39;m prepared to discuss the following:&lt;/p&gt;\n\n&lt;p&gt;ETL design patterns, incremental load, query optimization, audit tables, batch and streaming processing and ETL monitoring&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "zqmnxe", "is_robot_indexable": true, "report_reasons": null, "author": "py_vel26", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqmnxe/interview_prep_senior_data_integration_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqmnxe/interview_prep_senior_data_integration_role/", "subreddit_subscribers": 83403, "created_utc": 1671537882.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to figure it out, but am feeling a little lost. Is there any good documentation or videos where I can learn about it?\n\n\"LMDB is written in C with API Bindings for several programming languages.\"\n\nWhere can I get a list of these languages? Is it available for Go or JavaScript? Or is it a library for query languages like SQL?\n\nI am working on a personal project that is scraping large amounts of data using JavaScript. A friend recommended LMDB. Where do I start?\n\nEDIT: After a few responses I have realized that this will most likely not be the right tool for what I am doing. However, I feel like I learned a lot about something I have never heard of and am excited to dive deeper into the topic for personal gain. Thank you everyone for the responses.", "author_fullname": "t2_7y9rju54", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Lightning Memory Mapped Database (LMDB)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq13us", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671545491.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671478295.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to figure it out, but am feeling a little lost. Is there any good documentation or videos where I can learn about it?&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;LMDB is written in C with API Bindings for several programming languages.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Where can I get a list of these languages? Is it available for Go or JavaScript? Or is it a library for query languages like SQL?&lt;/p&gt;\n\n&lt;p&gt;I am working on a personal project that is scraping large amounts of data using JavaScript. A friend recommended LMDB. Where do I start?&lt;/p&gt;\n\n&lt;p&gt;EDIT: After a few responses I have realized that this will most likely not be the right tool for what I am doing. However, I feel like I learned a lot about something I have never heard of and am excited to dive deeper into the topic for personal gain. Thank you everyone for the responses.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zq13us", "is_robot_indexable": true, "report_reasons": null, "author": "emosk8rboi4206969", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zq13us/what_is_lightning_memory_mapped_database_lmdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zq13us/what_is_lightning_memory_mapped_database_lmdb/", "subreddit_subscribers": 83403, "created_utc": 1671478295.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Need some industry standard suggestions on building a ETL or ELT pipeline.\nWe are a small product based company focussed on billing software. We get to win new customers and that comes from a legacy database/application. We are looking to adopt either ETL or ELT model based on open source tools from market. Please suggest a method which could take in low code and time to develop a model.\n\nDiscussion made so far: \n\nDevelop a customer end windows batch executable file. This should enable the customer to run the executable and validate data based on the business rules to create CSV files. (They would run the batch file until the errors are phased out and a proper csv file is captured)\n\nCustomer should be able to upload final version of CSVs into Amazon S3 ( object versioning enabled).\n\nWe should be able to download those files and upload into Postgresql database ( minimum data validation here as data is already cured at the customer end)\n\nI am new to DE field and trying to share here for optimized way to develop the model in less time with low or no code.", "author_fullname": "t2_m6gnxiuj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CSV to Database based on data rules", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zqva84", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671559849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need some industry standard suggestions on building a ETL or ELT pipeline.\nWe are a small product based company focussed on billing software. We get to win new customers and that comes from a legacy database/application. We are looking to adopt either ETL or ELT model based on open source tools from market. Please suggest a method which could take in low code and time to develop a model.&lt;/p&gt;\n\n&lt;p&gt;Discussion made so far: &lt;/p&gt;\n\n&lt;p&gt;Develop a customer end windows batch executable file. This should enable the customer to run the executable and validate data based on the business rules to create CSV files. (They would run the batch file until the errors are phased out and a proper csv file is captured)&lt;/p&gt;\n\n&lt;p&gt;Customer should be able to upload final version of CSVs into Amazon S3 ( object versioning enabled).&lt;/p&gt;\n\n&lt;p&gt;We should be able to download those files and upload into Postgresql database ( minimum data validation here as data is already cured at the customer end)&lt;/p&gt;\n\n&lt;p&gt;I am new to DE field and trying to share here for optimized way to develop the model in less time with low or no code.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "zqva84", "is_robot_indexable": true, "report_reasons": null, "author": "rajekum512", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqva84/csv_to_database_based_on_data_rules/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqva84/csv_to_database_based_on_data_rules/", "subreddit_subscribers": 83403, "created_utc": 1671559849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to upskill in my general knowledge of databases; currently, I can fumble about and work with whatever database management system reasonably well and am most familiar with relational databases, but I know my knowledge of database models (e.g. relational, document, graph) and when, how, and why to use them is lacking.\n\nI've found bits and pieces online, but I'm looking for something more comprehensive / book-length, ideally walking through different case studies comparing database models/data designs, how they integrate with systems, how efficient/resilient/changeable they are for different tasks etc. If anyone knows of such resources, that would be amazing. Alternatively, would it be best to find separate books on these different models and work through them to get a feel for each? If so, any recommendations?", "author_fullname": "t2_bh9ilzdq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resouces comprehensively comparing different database models?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqd8l3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671507565.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to upskill in my general knowledge of databases; currently, I can fumble about and work with whatever database management system reasonably well and am most familiar with relational databases, but I know my knowledge of database models (e.g. relational, document, graph) and when, how, and why to use them is lacking.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found bits and pieces online, but I&amp;#39;m looking for something more comprehensive / book-length, ideally walking through different case studies comparing database models/data designs, how they integrate with systems, how efficient/resilient/changeable they are for different tasks etc. If anyone knows of such resources, that would be amazing. Alternatively, would it be best to find separate books on these different models and work through them to get a feel for each? If so, any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zqd8l3", "is_robot_indexable": true, "report_reasons": null, "author": "zazzedcoffee", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqd8l3/resouces_comprehensively_comparing_different/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqd8l3/resouces_comprehensively_comparing_different/", "subreddit_subscribers": 83403, "created_utc": 1671507565.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all! At the companies where I've worked as a developer, I've found that business stakeholders typically want a *concrete* way to check and assure the quality of data that pipelines are producing, before other downstream systems and users get impacted. I've tested solutions like Deequ, but I found that it made building compliance and data rules a bit more complicated and put a greater emphasis on developers to get the rules right that business was expecting. I also experienced issues with running checks in parallel and getting row level details about the failures.\n\nSo I'm trying to create an easier solution that both dev teams and business stakeholders can use to write consistent data quality checks and rules that run on various data sources and tell the truth about the data and catch potential failures in the data.\n\nIf you want to follow along or provide any feedback, please checkout: [DATAQLTY](https://dataqlty.webflow.io/)", "author_fullname": "t2_127oi7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a data quality solution for devs and business people", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqd0lp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671507961.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671506979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all! At the companies where I&amp;#39;ve worked as a developer, I&amp;#39;ve found that business stakeholders typically want a &lt;em&gt;concrete&lt;/em&gt; way to check and assure the quality of data that pipelines are producing, before other downstream systems and users get impacted. I&amp;#39;ve tested solutions like Deequ, but I found that it made building compliance and data rules a bit more complicated and put a greater emphasis on developers to get the rules right that business was expecting. I also experienced issues with running checks in parallel and getting row level details about the failures.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m trying to create an easier solution that both dev teams and business stakeholders can use to write consistent data quality checks and rules that run on various data sources and tell the truth about the data and catch potential failures in the data.&lt;/p&gt;\n\n&lt;p&gt;If you want to follow along or provide any feedback, please checkout: &lt;a href=\"https://dataqlty.webflow.io/\"&gt;DATAQLTY&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zqd0lp", "is_robot_indexable": true, "report_reasons": null, "author": "guru223", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqd0lp/building_a_data_quality_solution_for_devs_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqd0lp/building_a_data_quality_solution_for_devs_and/", "subreddit_subscribers": 83403, "created_utc": 1671506979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are pulling data from on-premises to AWS S3 bucket in parquet format using a Glue job. Planning to copy the data from S3 buck to Redshift tables in parallel. No transformation is needed. \n\nHave created an event notification for the S3 bucket (upon put and post of object) to invoke a lambda function that executes the Step function. In the Step function, have the state to start the Glue job that ingests data to S3 bucket and then do a parallel state of COPY lambda functions. \n\nIf I am testing manually by uploading files to S3 bucket, every time the Step function executes, it is loading the data multiple times into the tables due to parallel state. How can I make the Lambda functions in the Parallel state of Step functions execute only when that respective table's parquet file is uploaded into S3 bucket.\n\nIf Step functions is not the appropriate approach for this use case, what is the best solution to upload data to Redshift tables in parallel from S3 bucket with COPY command. This COPY process should be dependent on the Glue job completion.\n\nRead that Redshift auto copy but it is still in preview mode.\n\nAny help is appreciated.", "author_fullname": "t2_mspamalq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data flow to Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zquy11", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671559060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are pulling data from on-premises to AWS S3 bucket in parquet format using a Glue job. Planning to copy the data from S3 buck to Redshift tables in parallel. No transformation is needed. &lt;/p&gt;\n\n&lt;p&gt;Have created an event notification for the S3 bucket (upon put and post of object) to invoke a lambda function that executes the Step function. In the Step function, have the state to start the Glue job that ingests data to S3 bucket and then do a parallel state of COPY lambda functions. &lt;/p&gt;\n\n&lt;p&gt;If I am testing manually by uploading files to S3 bucket, every time the Step function executes, it is loading the data multiple times into the tables due to parallel state. How can I make the Lambda functions in the Parallel state of Step functions execute only when that respective table&amp;#39;s parquet file is uploaded into S3 bucket.&lt;/p&gt;\n\n&lt;p&gt;If Step functions is not the appropriate approach for this use case, what is the best solution to upload data to Redshift tables in parallel from S3 bucket with COPY command. This COPY process should be dependent on the Glue job completion.&lt;/p&gt;\n\n&lt;p&gt;Read that Redshift auto copy but it is still in preview mode.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zquy11", "is_robot_indexable": true, "report_reasons": null, "author": "Awsmason", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zquy11/data_flow_to_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zquy11/data_flow_to_cloud/", "subreddit_subscribers": 83403, "created_utc": 1671559060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "let's say I have source data like this:\n\ncustomers:\n\nid | name | updated_at\n---|---------|-----------------\n1 | bob | 2022-12-19\n2 | frank | 2022-12-18\n1 | bob2 | 2022-12-20\n\nand I want to create an SCD2 out of it\n\nwhat is the general process to do this in DBT?\n\nAs far as I understand, Snapshots work only with mutable data. Using a snapshot directly would have duplicates\n\nI can create an SCD1 based on source.customers latest data, and then snapshot that\n\nBut I'm not sure how I'd build the SCD2 with full history...\n\nTIA", "author_fullname": "t2_z3784il", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Slowly Changing Dimension Type 2 (SCD2) when source data is immutable / append-only?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqu7ix", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671557268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;let&amp;#39;s say I have source data like this:&lt;/p&gt;\n\n&lt;p&gt;customers:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;id&lt;/th&gt;\n&lt;th&gt;name&lt;/th&gt;\n&lt;th&gt;updated_at&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;bob&lt;/td&gt;\n&lt;td&gt;2022-12-19&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;frank&lt;/td&gt;\n&lt;td&gt;2022-12-18&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;bob2&lt;/td&gt;\n&lt;td&gt;2022-12-20&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;and I want to create an SCD2 out of it&lt;/p&gt;\n\n&lt;p&gt;what is the general process to do this in DBT?&lt;/p&gt;\n\n&lt;p&gt;As far as I understand, Snapshots work only with mutable data. Using a snapshot directly would have duplicates&lt;/p&gt;\n\n&lt;p&gt;I can create an SCD1 based on source.customers latest data, and then snapshot that&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m not sure how I&amp;#39;d build the SCD2 with full history...&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zqu7ix", "is_robot_indexable": true, "report_reasons": null, "author": "boy_named_su", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqu7ix/dbt_slowly_changing_dimension_type_2_scd2_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqu7ix/dbt_slowly_changing_dimension_type_2_scd2_when/", "subreddit_subscribers": 83403, "created_utc": 1671557268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\n&amp;#x200B;\n\nInfo: first of all, I know it's already a very good salary for the little experience I have, but I live alone and the price increases are hurting me too, as I also support relatives abroad.\n\nI've been officially working as a data engineer for 1 year now, but I've had experience with Python and Azure before that, during my three years of corporate studies. My starting salary after graduation was 55k, which is good in Germany. It has now been raised to 58k after a year.\n\nI feel like an imposter because that's too much money for the little experience I have, but data engineers and people who know about cloud are apparently very much in demand at the moment, and I'm not complaining about the money, of course.\n\nI now want to change jobs and have an offer from a large German company that is famous for paying good money and having a lot of bonuses.\n\nI know that you can get a lot more money after changing jobs, but somehow I feel stupid asking for more, given my limited experience.\n\nThe company seems very interested and wants to know in the interview how much salary I would want.\n\nHow would you proceed, should I ask for the same as I currently earn so they don't think I'm insane, or would you ask for a few thousand more and take advantage of the situation but possibly lose out on a good job?\n\nIt has to be said that the company where I would change jobs also pays a Christmas bonus, holiday bonus and annual profit sharing. So that would add up to an additional bonus of around 10-12k depending on the salary i get.\n\nIf anybody of you is a data engineer in germany, how much do you earn and what experience do you have?", "author_fullname": "t2_ocvc1cn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Salary in Germany with 1 year of experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqq3au", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671546892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Info: first of all, I know it&amp;#39;s already a very good salary for the little experience I have, but I live alone and the price increases are hurting me too, as I also support relatives abroad.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been officially working as a data engineer for 1 year now, but I&amp;#39;ve had experience with Python and Azure before that, during my three years of corporate studies. My starting salary after graduation was 55k, which is good in Germany. It has now been raised to 58k after a year.&lt;/p&gt;\n\n&lt;p&gt;I feel like an imposter because that&amp;#39;s too much money for the little experience I have, but data engineers and people who know about cloud are apparently very much in demand at the moment, and I&amp;#39;m not complaining about the money, of course.&lt;/p&gt;\n\n&lt;p&gt;I now want to change jobs and have an offer from a large German company that is famous for paying good money and having a lot of bonuses.&lt;/p&gt;\n\n&lt;p&gt;I know that you can get a lot more money after changing jobs, but somehow I feel stupid asking for more, given my limited experience.&lt;/p&gt;\n\n&lt;p&gt;The company seems very interested and wants to know in the interview how much salary I would want.&lt;/p&gt;\n\n&lt;p&gt;How would you proceed, should I ask for the same as I currently earn so they don&amp;#39;t think I&amp;#39;m insane, or would you ask for a few thousand more and take advantage of the situation but possibly lose out on a good job?&lt;/p&gt;\n\n&lt;p&gt;It has to be said that the company where I would change jobs also pays a Christmas bonus, holiday bonus and annual profit sharing. So that would add up to an additional bonus of around 10-12k depending on the salary i get.&lt;/p&gt;\n\n&lt;p&gt;If anybody of you is a data engineer in germany, how much do you earn and what experience do you have?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zqq3au", "is_robot_indexable": true, "report_reasons": null, "author": "Other-Name5179", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqq3au/data_engineering_salary_in_germany_with_1_year_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqq3au/data_engineering_salary_in_germany_with_1_year_of/", "subreddit_subscribers": 83403, "created_utc": 1671546892.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello. I am looking for courses or youtube playlists where people perform DE task in the cloud directly. Alike when I used to learn BI, I could gather videos where people made reports from stratch, show how to move and share them etc. I found it pretty hard to see as most of learning sources are either text or some sort of presentations. Ideally Id like to learn azure but if such materials are available for other cloud providers I am fine with that\n\nThank you in advance", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE tasks performed on video", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqmq0w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671538075.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I am looking for courses or youtube playlists where people perform DE task in the cloud directly. Alike when I used to learn BI, I could gather videos where people made reports from stratch, show how to move and share them etc. I found it pretty hard to see as most of learning sources are either text or some sort of presentations. Ideally Id like to learn azure but if such materials are available for other cloud providers I am fine with that&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zqmq0w", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqmq0w/de_tasks_performed_on_video/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqmq0w/de_tasks_performed_on_video/", "subreddit_subscribers": 83403, "created_utc": 1671538075.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nSo In my use case I need to constantly read new files which arrive in a GCS bucket. I don't want to use event base like Cloud function. I am running a batch spark process on Gcp dataproc. Is there some workaround or way via which we can only read unprocessed files ? (Something like job bookmarking feature in AWS Glue)", "author_fullname": "t2_57e44nxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to develop something like Job bookmarking (AWS glue feature) in Google Cloud ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqjvmy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671528024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So In my use case I need to constantly read new files which arrive in a GCS bucket. I don&amp;#39;t want to use event base like Cloud function. I am running a batch spark process on Gcp dataproc. Is there some workaround or way via which we can only read unprocessed files ? (Something like job bookmarking feature in AWS Glue)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zqjvmy", "is_robot_indexable": true, "report_reasons": null, "author": "RstarPhoneix", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqjvmy/how_to_develop_something_like_job_bookmarking_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqjvmy/how_to_develop_something_like_job_bookmarking_aws/", "subreddit_subscribers": 83403, "created_utc": 1671528024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Last week I had my first session in taking the Azure for data engineers webinar.  During the session I covered various tasks:  \n\\- Introduction and data manipulation using PySpark.\n\n\\-  Introduction to Databricks (UI, Cluster &amp; Dataset).\n\n\\- Mounting Azure Storage in Databricks (Blob Storage &amp; Data Lake).\n\n\\- Activating Transform Activity from Azure Data Factory.\n\n\\- Connect Transformed Data to Microsoft Power BI.  \n\ud83c\udfa5 YouTube Link: https://youtu.be/zTwevLbRfpE \n\n\u2022GitHub Repo: https://github.com/kiddojazz/Data-Transformation-using-Azure-Databricks", "author_fullname": "t2_6fwa4j9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Transformation using Azure Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq5jlb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671488582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last week I had my first session in taking the Azure for data engineers webinar.  During the session I covered various tasks:&lt;br/&gt;\n- Introduction and data manipulation using PySpark.&lt;/p&gt;\n\n&lt;p&gt;-  Introduction to Databricks (UI, Cluster &amp;amp; Dataset).&lt;/p&gt;\n\n&lt;p&gt;- Mounting Azure Storage in Databricks (Blob Storage &amp;amp; Data Lake).&lt;/p&gt;\n\n&lt;p&gt;- Activating Transform Activity from Azure Data Factory.&lt;/p&gt;\n\n&lt;p&gt;- Connect Transformed Data to Microsoft Power BI.&lt;br/&gt;\n\ud83c\udfa5 YouTube Link: &lt;a href=\"https://youtu.be/zTwevLbRfpE\"&gt;https://youtu.be/zTwevLbRfpE&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;\u2022GitHub Repo: &lt;a href=\"https://github.com/kiddojazz/Data-Transformation-using-Azure-Databricks\"&gt;https://github.com/kiddojazz/Data-Transformation-using-Azure-Databricks&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/60hu9Woxfra3pwTH9A7GtZrX2h3bjHAnExttROIR5VM.jpg?auto=webp&amp;s=69ee3f3c553f2fd0fe85a7dd6721908154ec3c71", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/60hu9Woxfra3pwTH9A7GtZrX2h3bjHAnExttROIR5VM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d28e721f2a4a8eade8e982ec2cb7efdc155e920e", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/60hu9Woxfra3pwTH9A7GtZrX2h3bjHAnExttROIR5VM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=192c1cd4113196c542b8727f428c05b9768ba5b1", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/60hu9Woxfra3pwTH9A7GtZrX2h3bjHAnExttROIR5VM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a9963513139eb524d670a5a9f08e53d312a3e502", "width": 320, "height": 240}], "variants": {}, "id": "XP66OiJjgYCGmtcsqMtj_zrpniIltUg-88PfnRaP8mA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zq5jlb", "is_robot_indexable": true, "report_reasons": null, "author": "kiddojazz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zq5jlb/data_transformation_using_azure_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zq5jlb/data_transformation_using_azure_databricks/", "subreddit_subscribers": 83403, "created_utc": 1671488582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We used text to SQL to build a \"Webflow for Web3 Dashboards\". We discovered loads of problems in the entire blockchain data space outlined in the thread.\n\nDemo: Design your dashboard, type your query in English.\n\nWe're open-sourcing everything.\n\n[https://twitter.com/vatsal\\_aggarwal/status/1604995355468173312?s=20&amp;t=DJ-zmyaV7g5jgbUGpsssmQ](https://twitter.com/vatsal_aggarwal/status/1604995355468173312?s=20&amp;t=DJ-zmyaV7g5jgbUGpsssmQ)", "author_fullname": "t2_ea8d3w57", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open-sourcing text-to-SQL for blockchain data!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq9hyk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.42, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671498014.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We used text to SQL to build a &amp;quot;Webflow for Web3 Dashboards&amp;quot;. We discovered loads of problems in the entire blockchain data space outlined in the thread.&lt;/p&gt;\n\n&lt;p&gt;Demo: Design your dashboard, type your query in English.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re open-sourcing everything.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/vatsal_aggarwal/status/1604995355468173312?s=20&amp;amp;t=DJ-zmyaV7g5jgbUGpsssmQ\"&gt;https://twitter.com/vatsal_aggarwal/status/1604995355468173312?s=20&amp;amp;t=DJ-zmyaV7g5jgbUGpsssmQ&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/soVXxY3in1C5Xb1xu9X0epwwcbmX2yQZDLHcDwYl0Go.jpg?auto=webp&amp;s=28b738ba55e892a6dd8d8dffd2e23f97ed59837f", "width": 140, "height": 90}, "resolutions": [{"url": "https://external-preview.redd.it/soVXxY3in1C5Xb1xu9X0epwwcbmX2yQZDLHcDwYl0Go.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aba91d8de11ec54f0a9f913c4274c369a2263b16", "width": 108, "height": 69}], "variants": {}, "id": "atjhhl11daWVVuu8SYGQEqyjtZXvbh4CM2nQ97YctLI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zq9hyk", "is_robot_indexable": true, "report_reasons": null, "author": "Apprehensive-Tax-214", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zq9hyk/opensourcing_texttosql_for_blockchain_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zq9hyk/opensourcing_texttosql_for_blockchain_data/", "subreddit_subscribers": 83403, "created_utc": 1671498014.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}