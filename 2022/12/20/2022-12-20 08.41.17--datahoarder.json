{"kind": "Listing", "data": {"after": "t3_zqgzyb", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "How would I go about cleaning this out properly? I figured all this dust cant be good for the tape or the drive", "author_fullname": "t2_17jter", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Got this LTO-5 Drive off eBay, inside of it is pretty dusty", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zojwxjhjtw6a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b2f7a7fd7278a62b8b3cd070e4ffcb698d0b1d6"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2015d73bded00c0b1d993fb6864949f132109fed"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=750f323558600d5ff8e2481377a67826bd2ddd07"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2badf36f170a71e3ee70afccbb6d88479f82ff9"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1f3b51a52597c238b4b8f275238de5dc798f91f"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=66cc46cd6403ed34b2418b0d53bad7a16f504485"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=5d9f60490687be8d09b9218f55fbf182ecf41640"}, "id": "zojwxjhjtw6a1"}, "7pz5ejhjtw6a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2400d448c834d4c9af2458c038edf93b99216ded"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=66572fa6e68152688db64535d1509f35bbb61003"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=91d2fcb3f3ada387ff5162b07dfc3a0039c6c8a4"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eea21df7a3e58b31572a32fc4776af144a733b79"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a35a493780e562d36c5b59c80fcaba21bf70071"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=58eb95df1f6da057d0d9d476a5a7e4515aae68c2"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=93717ea4626d106605ac098b78a320883ee57d23"}, "id": "7pz5ejhjtw6a1"}}, "name": "t3_zq1oer", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 246, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "zojwxjhjtw6a1", "id": 220856999}, {"media_id": "7pz5ejhjtw6a1", "id": 220857000}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 246, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/I8nI1PFChUJ3MG4k_g-rHxh3015T9xFHw5nvKkZRRis.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671479616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How would I go about cleaning this out properly? I figured all this dust cant be good for the tape or the drive&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zq1oer", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zq1oer", "is_robot_indexable": true, "report_reasons": null, "author": "JustKeKe23", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zq1oer/got_this_lto5_drive_off_ebay_inside_of_it_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zq1oer", "subreddit_subscribers": 660092, "created_utc": 1671479616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My dad passed recently, and he left behind a treasured collection of 5,000+ CD's. I'd like to archive it all as I have many fond memories of listening to them; MP3's are sufficient.\n\nI was originally thinking of setting up something [similar to this](https://www.reddit.com/r/DataHoarder/comments/saca3t/comment/htsodr6/), with many drives. Problem is my main machine which has the space and processing power is at my own home; moving either the machine or the CD's between my house and my dad's isn't practical, and the work involved in setting it up starts to sound blocking.\n\nI also considered just cataloguing them and feeding that into a torrent/Usenet/purchasing script, but many of these are old and do not exist online (and are not sorted), so that doesn't sound any faster.\n\nHow do you suggest I go about this? I'm open to jerry-rigging something together, buying a commercial solution, etc, just not sure what's most efficient. My biggest constraint is I don't get much time at his house, so I want a solution with a high CD throughput. Should I just be ripping images, and then transporting those to my own home for transcoding/tagging/etc?\n\nThanks all!\n\nEDIT: As many of you have mentioned, tagging and metadata is a big problem, especially since many of these CD's have no online presence and can't be auto-tagged. Since the CD's aren't sorted (and I don't want to upset the physical state of things too much), retrieving a disc after ripping isn't feasible. So whatever strategy is use needs to record disc data (e.g. photographs of the case) when it's ripped.", "author_fullname": "t2_egsdu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Efficient method to rip 5,000 audio CD's on-site?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq4bv3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 101, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 101, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671512828.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671485799.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My dad passed recently, and he left behind a treasured collection of 5,000+ CD&amp;#39;s. I&amp;#39;d like to archive it all as I have many fond memories of listening to them; MP3&amp;#39;s are sufficient.&lt;/p&gt;\n\n&lt;p&gt;I was originally thinking of setting up something &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/saca3t/comment/htsodr6/\"&gt;similar to this&lt;/a&gt;, with many drives. Problem is my main machine which has the space and processing power is at my own home; moving either the machine or the CD&amp;#39;s between my house and my dad&amp;#39;s isn&amp;#39;t practical, and the work involved in setting it up starts to sound blocking.&lt;/p&gt;\n\n&lt;p&gt;I also considered just cataloguing them and feeding that into a torrent/Usenet/purchasing script, but many of these are old and do not exist online (and are not sorted), so that doesn&amp;#39;t sound any faster.&lt;/p&gt;\n\n&lt;p&gt;How do you suggest I go about this? I&amp;#39;m open to jerry-rigging something together, buying a commercial solution, etc, just not sure what&amp;#39;s most efficient. My biggest constraint is I don&amp;#39;t get much time at his house, so I want a solution with a high CD throughput. Should I just be ripping images, and then transporting those to my own home for transcoding/tagging/etc?&lt;/p&gt;\n\n&lt;p&gt;Thanks all!&lt;/p&gt;\n\n&lt;p&gt;EDIT: As many of you have mentioned, tagging and metadata is a big problem, especially since many of these CD&amp;#39;s have no online presence and can&amp;#39;t be auto-tagged. Since the CD&amp;#39;s aren&amp;#39;t sorted (and I don&amp;#39;t want to upset the physical state of things too much), retrieving a disc after ripping isn&amp;#39;t feasible. So whatever strategy is use needs to record disc data (e.g. photographs of the case) when it&amp;#39;s ripped.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zq4bv3", "is_robot_indexable": true, "report_reasons": null, "author": "Gatherix", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zq4bv3/efficient_method_to_rip_5000_audio_cds_onsite/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zq4bv3/efficient_method_to_rip_5000_audio_cds_onsite/", "subreddit_subscribers": 660092, "created_utc": 1671485799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_9qnme", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "These be dead, right? Don't use them any more?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_zq1vy9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 95, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 95, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/OGYPngUSzUrdoQArVSKVEvhpHp0Mw-CpLmQK0fwbqWM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671480070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/f7mmndvtuw6a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/f7mmndvtuw6a1.png?auto=webp&amp;s=d161aa9dc58f3b8a269ebf5aacaf460184b3d81c", "width": 1348, "height": 816}, "resolutions": [{"url": "https://preview.redd.it/f7mmndvtuw6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=32949479eb9e110c6f73540f182dad687f9a702b", "width": 108, "height": 65}, {"url": "https://preview.redd.it/f7mmndvtuw6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=01bd95cd4d0fa03cf9602966bd54699a7c1426a6", "width": 216, "height": 130}, {"url": "https://preview.redd.it/f7mmndvtuw6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=753b605921ac9385b08e100bce4037b340eb9497", "width": 320, "height": 193}, {"url": "https://preview.redd.it/f7mmndvtuw6a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3aadbad17db0c3363e5ab77c235d09b9bc6a370", "width": 640, "height": 387}, {"url": "https://preview.redd.it/f7mmndvtuw6a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5056adebbe6bf1f40ddb75d5b1b5debb8c2c414", "width": 960, "height": 581}, {"url": "https://preview.redd.it/f7mmndvtuw6a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb996fb0eb0d181a474c543e88fe2abef06d8741", "width": 1080, "height": 653}], "variants": {}, "id": "Uin8-z9wY7-QItvNWLAY0Ksq8DRMoXXv8ubv1tuUvvU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zq1vy9", "is_robot_indexable": true, "report_reasons": null, "author": "skeptibat", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zq1vy9/these_be_dead_right_dont_use_them_any_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/f7mmndvtuw6a1.png", "subreddit_subscribers": 660092, "created_utc": 1671480070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I make this post to get an update of current state of the storage technology and also seek to find answer for wheather i should make backups to HDDs vs SSD.\n\nCurrent Situation:-\nI have around 500 gb of Family photos from 2001 on a Seagate external HDD, it lasted for 7 years and data is well and good right now.\n\nI already have backups on 2 different machines and the external HDD. It's now time again to migrate my external HDD to new Hardware and I am conflicted on what should I choose moving further.\n\nUntil now my photos have been jumping CDs to HDD and I am at a crossroads again weather to switch from HDD to SSD or HDD are still better for cold storage long term.\n\nI did fair bit of research and I am aware Optical Media would be my best bet, namely M Disk or BD disks. Unfortunately where I live I cannot source them reliably and affordably enough.\n\nI browsed reddit threads from past few years.\nLike [this](https://www.reddit.com/r/DataHoarder/comments/jiwqqy/are_ssds_more_reliable_than_hard_drives) from 2 years ago which says SSDs are better.\n\nI have consistently found a narrative that newer SSDs are better alternative than HDDs.\n\nMy primary concern is not number of read writes in SSDs. Often they are in 100s of TBW which I presume I won't hit because of the nature of my storage needs.\n\nI fear data corruption and chip failure rather than running out of read writes.\n\nThe disk I chose weather SSD or an HDD will probably be left on shelf with about twice a year plugging into PC to add new photos.\n\n\nWhat do you guys think would be a good choice ?\n\nShould I keep moving forward with a new HDD or are SSD a smarter choice?\n\nWhatever I choose I would probably rely on for at least next 4-5 years, with backups of course.", "author_fullname": "t2_6meaucy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Long term storage: SSDs vs HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpt6es", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 75, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 75, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671460405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I make this post to get an update of current state of the storage technology and also seek to find answer for wheather i should make backups to HDDs vs SSD.&lt;/p&gt;\n\n&lt;p&gt;Current Situation:-\nI have around 500 gb of Family photos from 2001 on a Seagate external HDD, it lasted for 7 years and data is well and good right now.&lt;/p&gt;\n\n&lt;p&gt;I already have backups on 2 different machines and the external HDD. It&amp;#39;s now time again to migrate my external HDD to new Hardware and I am conflicted on what should I choose moving further.&lt;/p&gt;\n\n&lt;p&gt;Until now my photos have been jumping CDs to HDD and I am at a crossroads again weather to switch from HDD to SSD or HDD are still better for cold storage long term.&lt;/p&gt;\n\n&lt;p&gt;I did fair bit of research and I am aware Optical Media would be my best bet, namely M Disk or BD disks. Unfortunately where I live I cannot source them reliably and affordably enough.&lt;/p&gt;\n\n&lt;p&gt;I browsed reddit threads from past few years.\nLike &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/jiwqqy/are_ssds_more_reliable_than_hard_drives\"&gt;this&lt;/a&gt; from 2 years ago which says SSDs are better.&lt;/p&gt;\n\n&lt;p&gt;I have consistently found a narrative that newer SSDs are better alternative than HDDs.&lt;/p&gt;\n\n&lt;p&gt;My primary concern is not number of read writes in SSDs. Often they are in 100s of TBW which I presume I won&amp;#39;t hit because of the nature of my storage needs.&lt;/p&gt;\n\n&lt;p&gt;I fear data corruption and chip failure rather than running out of read writes.&lt;/p&gt;\n\n&lt;p&gt;The disk I chose weather SSD or an HDD will probably be left on shelf with about twice a year plugging into PC to add new photos.&lt;/p&gt;\n\n&lt;p&gt;What do you guys think would be a good choice ?&lt;/p&gt;\n\n&lt;p&gt;Should I keep moving forward with a new HDD or are SSD a smarter choice?&lt;/p&gt;\n\n&lt;p&gt;Whatever I choose I would probably rely on for at least next 4-5 years, with backups of course.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zpt6es", "is_robot_indexable": true, "report_reasons": null, "author": "alsu2launda", "discussion_type": null, "num_comments": 94, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpt6es/long_term_storage_ssds_vs_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpt6es/long_term_storage_ssds_vs_hdd/", "subreddit_subscribers": 660092, "created_utc": 1671460405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using gallery-dl to download media and metadata for a list of people I follow. However, despite using an archive file, it still iterates through their entire tweet history. It does skip downloading old tweets, but I'd rather it just get everything new and then move onto the next URL once it encounters a tweet it already has. That would save me a TON of time (and rate limits).\n\nAll the documentation and suggestions I've found thus far involve skipping downloads, which it's already doing.\n\nAlso, since moving to a new computer and setting things up again, files it already downloaded are displaying as `./gallery-dl/twitter/[user]/?` in the terminal output instead of showing the filenames. Not sure why that is happening.\n\nConfig is below. Any suggestions?\n\n\t{\n\t\t\"extractor\": {\n\t\t\t\"twitter\": {\n\t\t\t\t\"cookies\": {\n\t\t\t\t\t\"auth_token\": \"[redacted]\"\n\t\t\t\t},\n\t\t\t\t\"archive\": \"~/twitter/archive-twitter.sqlite3\",\n\t\t\t\t\"postprocessors\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"name\": \"metadata\",\n\t\t\t\t\t\t\"event\": \"post\",\n\t\t\t\t\t\t\"filename\": \"{tweet_id}.json\"\n\t\t\t\t\t}\n\t\t\t\t],\n\t\t\t\t\"expand\": false,\n\t\t\t\t\"cards\": false,\n\t\t\t\t\"quoted\": false,\n\t\t\t\t\"retweets\": false,\n\t\t\t\t\"text-tweets\": false,\n\t\t\t\t\"unique\": true,\n\t\t\t\t\"videos\": true,\n\t\t\t\t\"timeline\": {\n\t\t\t\t\t\"strategy\": \"media\"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\nI run with the following command: `gallery-dl -c ./gallery-dl.no-text.conf -i urls.txt`", "author_fullname": "t2_4m88o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[gallery-dl / Twitter] Is there a way to skip all remaining tweets for a user once everything new has been collected?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpwmt5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671468428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using gallery-dl to download media and metadata for a list of people I follow. However, despite using an archive file, it still iterates through their entire tweet history. It does skip downloading old tweets, but I&amp;#39;d rather it just get everything new and then move onto the next URL once it encounters a tweet it already has. That would save me a TON of time (and rate limits).&lt;/p&gt;\n\n&lt;p&gt;All the documentation and suggestions I&amp;#39;ve found thus far involve skipping downloads, which it&amp;#39;s already doing.&lt;/p&gt;\n\n&lt;p&gt;Also, since moving to a new computer and setting things up again, files it already downloaded are displaying as &lt;code&gt;./gallery-dl/twitter/[user]/?&lt;/code&gt; in the terminal output instead of showing the filenames. Not sure why that is happening.&lt;/p&gt;\n\n&lt;p&gt;Config is below. Any suggestions?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n    &amp;quot;extractor&amp;quot;: {\n        &amp;quot;twitter&amp;quot;: {\n            &amp;quot;cookies&amp;quot;: {\n                &amp;quot;auth_token&amp;quot;: &amp;quot;[redacted]&amp;quot;\n            },\n            &amp;quot;archive&amp;quot;: &amp;quot;~/twitter/archive-twitter.sqlite3&amp;quot;,\n            &amp;quot;postprocessors&amp;quot;: [\n                {\n                    &amp;quot;name&amp;quot;: &amp;quot;metadata&amp;quot;,\n                    &amp;quot;event&amp;quot;: &amp;quot;post&amp;quot;,\n                    &amp;quot;filename&amp;quot;: &amp;quot;{tweet_id}.json&amp;quot;\n                }\n            ],\n            &amp;quot;expand&amp;quot;: false,\n            &amp;quot;cards&amp;quot;: false,\n            &amp;quot;quoted&amp;quot;: false,\n            &amp;quot;retweets&amp;quot;: false,\n            &amp;quot;text-tweets&amp;quot;: false,\n            &amp;quot;unique&amp;quot;: true,\n            &amp;quot;videos&amp;quot;: true,\n            &amp;quot;timeline&amp;quot;: {\n                &amp;quot;strategy&amp;quot;: &amp;quot;media&amp;quot;\n            }\n        }\n    }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I run with the following command: &lt;code&gt;gallery-dl -c ./gallery-dl.no-text.conf -i urls.txt&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpwmt5", "is_robot_indexable": true, "report_reasons": null, "author": "turaiel", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpwmt5/gallerydl_twitter_is_there_a_way_to_skip_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpwmt5/gallerydl_twitter_is_there_a_way_to_skip_all/", "subreddit_subscribers": 660092, "created_utc": 1671468428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyone know if unburned BDXL discs that use inorganic dye have a shelf life?  In other words, should I only buy enough discs that I will burn, for example, in 1 year?  Or does it matter if I buy them in bulk and only finish burning all of them, for example, in 5 to 7 years?", "author_fullname": "t2_mgmtm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do Unburned BDXL Discs Have a Shelf Life", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqg162", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671515475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone know if unburned BDXL discs that use inorganic dye have a shelf life?  In other words, should I only buy enough discs that I will burn, for example, in 1 year?  Or does it matter if I buy them in bulk and only finish burning all of them, for example, in 5 to 7 years?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zqg162", "is_robot_indexable": true, "report_reasons": null, "author": "HarryMuscle", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zqg162/do_unburned_bdxl_discs_have_a_shelf_life/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zqg162/do_unburned_bdxl_discs_have_a_shelf_life/", "subreddit_subscribers": 660092, "created_utc": 1671515475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i am quite out of the loop with computer technology, with reliable brands and etc.\nmy computer is from 2011, and will only be ugraded in 2025 when windows 10 dies.\nbut i do need hard drives as i am running out of space.\n\ni tried searching but could not find a simple and realistic comparrison showing that NAS disk are really better for desktop storage.\n======================================\nit's mostly for flac and wave music, png, svg and jpg images  and mkv, avi and mp4 movies\ngoing to migrate to (03) 4tb hard disks  , so actually i need to buy 6 for backup reasons,  \n\ni keep hearing i should buy the seagate ironwolf instead of barracuda, and begs the quesiton if its not overkill?  with 6 drives i would spend  u$ 120 more with ironwolf.\nfrom what i read NAS drives are made to run hot and not get affected from vibrations. isn't that because nas drives are all cramped together in a tiny box?\n\nso in a big server case both vibrations and temperature will not be a concern right?\n\nmy computer runs rather cold, hard disks usually around 30~35 celsius, (a big old server case from the 2000's) \n\n\nthanks!\n\n\nps: not raid, just manual backup using software that does incremental copy", "author_fullname": "t2_hs76g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "are nas drives not overkill for desktop?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpykxu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671472739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i am quite out of the loop with computer technology, with reliable brands and etc.\nmy computer is from 2011, and will only be ugraded in 2025 when windows 10 dies.\nbut i do need hard drives as i am running out of space.&lt;/p&gt;\n\n&lt;h1&gt;i tried searching but could not find a simple and realistic comparrison showing that NAS disk are really better for desktop storage.&lt;/h1&gt;\n\n&lt;p&gt;it&amp;#39;s mostly for flac and wave music, png, svg and jpg images  and mkv, avi and mp4 movies\ngoing to migrate to (03) 4tb hard disks  , so actually i need to buy 6 for backup reasons,  &lt;/p&gt;\n\n&lt;p&gt;i keep hearing i should buy the seagate ironwolf instead of barracuda, and begs the quesiton if its not overkill?  with 6 drives i would spend  u$ 120 more with ironwolf.\nfrom what i read NAS drives are made to run hot and not get affected from vibrations. isn&amp;#39;t that because nas drives are all cramped together in a tiny box?&lt;/p&gt;\n\n&lt;p&gt;so in a big server case both vibrations and temperature will not be a concern right?&lt;/p&gt;\n\n&lt;p&gt;my computer runs rather cold, hard disks usually around 30~35 celsius, (a big old server case from the 2000&amp;#39;s) &lt;/p&gt;\n\n&lt;p&gt;thanks!&lt;/p&gt;\n\n&lt;p&gt;ps: not raid, just manual backup using software that does incremental copy&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpykxu", "is_robot_indexable": true, "report_reasons": null, "author": "vanderzee", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpykxu/are_nas_drives_not_overkill_for_desktop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpykxu/are_nas_drives_not_overkill_for_desktop/", "subreddit_subscribers": 660092, "created_utc": 1671472739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know you guys shuck drives, could I please have the enclosures you don't need? I live in Canada, thanks", "author_fullname": "t2_n8gn68va", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Could someone send or sell me a 2.5 inch USB enclosure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqcv7d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671506571.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know you guys shuck drives, could I please have the enclosures you don&amp;#39;t need? I live in Canada, thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zqcv7d", "is_robot_indexable": true, "report_reasons": null, "author": "LichtensteinIsBased", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zqcv7d/could_someone_send_or_sell_me_a_25_inch_usb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zqcv7d/could_someone_send_or_sell_me_a_25_inch_usb/", "subreddit_subscribers": 660092, "created_utc": 1671506571.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 32T (total) UnRaid server. Never thought I'd fill up my 12TB USB external device as a backup. Now that its filled up, I would like to purchase another 12+TB external USB, but how do I not backup the already backed up files?  \n\n\nIdeally, I would write all the backed up files to a text file, then tell 'rsync' to exclude all the files and directories in that file. Certainly I'm not the first one, so curious how others have done this? TIA", "author_fullname": "t2_3073b6o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to backup across multiple external USB devices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqaax6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671499987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 32T (total) UnRaid server. Never thought I&amp;#39;d fill up my 12TB USB external device as a backup. Now that its filled up, I would like to purchase another 12+TB external USB, but how do I not backup the already backed up files?  &lt;/p&gt;\n\n&lt;p&gt;Ideally, I would write all the backed up files to a text file, then tell &amp;#39;rsync&amp;#39; to exclude all the files and directories in that file. Certainly I&amp;#39;m not the first one, so curious how others have done this? TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zqaax6", "is_robot_indexable": true, "report_reasons": null, "author": "leonj1", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zqaax6/how_to_backup_across_multiple_external_usb_devices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zqaax6/how_to_backup_across_multiple_external_usb_devices/", "subreddit_subscribers": 660092, "created_utc": 1671499987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don\u2019t particularly enjoy socialgrep and ihsoyct seems to be down. Anyone have an answer lol", "author_fullname": "t2_lzup5se2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit Archive Simular To Ihsoyct?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq3aej", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671483378.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don\u2019t particularly enjoy socialgrep and ihsoyct seems to be down. Anyone have an answer lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zq3aej", "is_robot_indexable": true, "report_reasons": null, "author": "Impressive_Bass_1537", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zq3aej/reddit_archive_simular_to_ihsoyct/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zq3aej/reddit_archive_simular_to_ihsoyct/", "subreddit_subscribers": 660092, "created_utc": 1671483378.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone!\n\nRecently, I've learned about the Kiwix project that lets you download valuable sites completely in one \".zim\" file that can be browsed offline. I found several interesting zims in the Kiwix library, such as ArchWiki, StackExchange, AllTheTropes. I wonder, is there a way to have a local TV Tropes copy for offline browsing?", "author_fullname": "t2_26qth9a3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to download TV Tropes for offline browsing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zptksv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671461395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;Recently, I&amp;#39;ve learned about the Kiwix project that lets you download valuable sites completely in one &amp;quot;.zim&amp;quot; file that can be browsed offline. I found several interesting zims in the Kiwix library, such as ArchWiki, StackExchange, AllTheTropes. I wonder, is there a way to have a local TV Tropes copy for offline browsing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zptksv", "is_robot_indexable": true, "report_reasons": null, "author": "ChrysoliteAzalea", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zptksv/is_there_a_way_to_download_tv_tropes_for_offline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zptksv/is_there_a_way_to_download_tv_tropes_for_offline/", "subreddit_subscribers": 660092, "created_utc": 1671461395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So far what I have been doing for important backups I need to ensure isn't corrupted is hashing over the uncompressed file, usually a tar and writing the output to a separate file, and then compressing the archive. So, for data.tar, I would end up with data.tar.xz or data.tar.gz, and a separate data.tar.sha256. When I uncompressed, I then check the extracted data against the hash file. Is this actually necessary? What kind of error checking is in gz and xz files? Do they store the hash of the original uncompressed data and check it during decompression, or do they only checksum the final compressed data?", "author_fullname": "t2_1h1hfeve", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What kind of error checking do gz or xz files have?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqczin", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671506895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So far what I have been doing for important backups I need to ensure isn&amp;#39;t corrupted is hashing over the uncompressed file, usually a tar and writing the output to a separate file, and then compressing the archive. So, for data.tar, I would end up with data.tar.xz or data.tar.gz, and a separate data.tar.sha256. When I uncompressed, I then check the extracted data against the hash file. Is this actually necessary? What kind of error checking is in gz and xz files? Do they store the hash of the original uncompressed data and check it during decompression, or do they only checksum the final compressed data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zqczin", "is_robot_indexable": true, "report_reasons": null, "author": "AgreeableLandscape3", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zqczin/what_kind_of_error_checking_do_gz_or_xz_files_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zqczin/what_kind_of_error_checking_do_gz_or_xz_files_have/", "subreddit_subscribers": 660092, "created_utc": 1671506895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[This](https://github.com/aliparlakci/bulk-downloader-for-reddit) is what I'm talking about. I'm trying to figure out how to use it but I'm having issues as I've never used Python before. I managed to install Python 3.10.9 and I believe I installed bdfr by typing\n\n    py -m pip install bdfr --upgrade\n\ninto cmd.exe. I can't actually figure out how to use bdfr now, though. I read the instructions and put together a command that seems like it's supposed to work, but it doesn't. I tried\n\n    bdfr download E:\\subredditarchives\\aww --subreddit aww --sort top --time month --no-dupes --file-scheme {SUBREDDIT}_{UPVOTES}_{TITLE}_{POSTID}\n\nin cmd.exe and got \n\n    'bdfr' is not recognized as an internal or external command, operable program or batch file.\n\nback.", "author_fullname": "t2_fxa6t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone here used Bulk-Downloader-For-Reddit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqb5qy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671502131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/aliparlakci/bulk-downloader-for-reddit\"&gt;This&lt;/a&gt; is what I&amp;#39;m talking about. I&amp;#39;m trying to figure out how to use it but I&amp;#39;m having issues as I&amp;#39;ve never used Python before. I managed to install Python 3.10.9 and I believe I installed bdfr by typing&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;py -m pip install bdfr --upgrade\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;into cmd.exe. I can&amp;#39;t actually figure out how to use bdfr now, though. I read the instructions and put together a command that seems like it&amp;#39;s supposed to work, but it doesn&amp;#39;t. I tried&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;bdfr download E:\\subredditarchives\\aww --subreddit aww --sort top --time month --no-dupes --file-scheme {SUBREDDIT}_{UPVOTES}_{TITLE}_{POSTID}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;in cmd.exe and got &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;#39;bdfr&amp;#39; is not recognized as an internal or external command, operable program or batch file.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;back.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?auto=webp&amp;s=79964651227925e81dab2225e179f766b7ce9cd9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d9db54a5f92c02b4ac3561a278cacd6a798ce38", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c15e8eb33f2dec8663f718ef091fdd1814ad7e9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b278fef3bbf93b8f9e90248f62d0a61dbd0c97f3", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a3e26ce2879ce29570b5fcd6fa3ee1b896ee8fbc", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b53ccbe0a13a14e258e90118afa420f37b8de454", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e70d92f1bcd7d55ee949fcff06d63bf88dd2a0c6", "width": 1080, "height": 540}], "variants": {}, "id": "t6fY0ytyGKU6aQ5wNobIAf9tnsQeHpyD7J-f0ZOYDsk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zqb5qy", "is_robot_indexable": true, "report_reasons": null, "author": "onlytoask", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zqb5qy/has_anyone_here_used_bulkdownloaderforreddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zqb5qy/has_anyone_here_used_bulkdownloaderforreddit/", "subreddit_subscribers": 660092, "created_utc": 1671502131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hopefully i am in the right place\n\nI am looking at best solution to get all backup in one place. Currently i am using external hard drives as back up for music, photos, home video's, important documents etc. Would it be best for me to just get NAS or should i look at cloud back instead?", "author_fullname": "t2_csizh1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to set up proper backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqb425", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671502016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hopefully i am in the right place&lt;/p&gt;\n\n&lt;p&gt;I am looking at best solution to get all backup in one place. Currently i am using external hard drives as back up for music, photos, home video&amp;#39;s, important documents etc. Would it be best for me to just get NAS or should i look at cloud back instead?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zqb425", "is_robot_indexable": true, "report_reasons": null, "author": "looker009", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zqb425/how_to_set_up_proper_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zqb425/how_to_set_up_proper_backup/", "subreddit_subscribers": 660092, "created_utc": 1671502016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Usually, I've kept my data on a server that was also doing routing and other stuff - a E7500 with many, many pre-SMR era 3.5\" HDDs in RAID1's. Sometimes they die, and replacing them is not much of a problem.\n\nBut now I've gone digital nomad, and I need a different solution. I can't take the server with me, or take proper care of it, so it is now \"unreliable\". Therefore, at least for some stuff, I need something I can carry with me.\n\nThe plan is to get a Sabrent DS-4SSD DAS (any better suggestions?). I have two 1-Tb 2.5\" Hitachi Travelstars, which should be very reliable compared to today's standards - I also expect them to be more resilient to shaking in a bag than 3.5\" drives. I also have a 500 Gb Samsung drive that I'm fully expecting to fail at any moment - it's for non-essential use for all kinds of temporary stuff.\n\nNow, 1 Tb (mirrored, of course) is barely enough, and I still have two more slots available. So I'm considering my options:\n\n- Buy two 2 Tb HDDs and mirror them. More than 2 Tb are likely more vulnerable to shaking, and are likely SMR. But a good kick to the DAS might kill all four of them.\n\n- Buy one 2 Tb SSD and mirror both of the Hitachis to it. This seems like the best solution. I will have one slot left for the Samsung, and I will have protection against both SSD damage and mechanical damage... as long as the SSD does not turn out to have failed after HDDs die.\n\nBut now that I look at what's available... The days of \"MLC are crap, buy SLC only\" are long gone; you're lucky to not get a QLC, and TLC are considered \"reliable\". And no one will even disclose what memory chips do they use, and swapping them (even along with the controller!) without changing the model name is a normal everyday occurrence.\n\nI see cheap TeamGroup SSDs which claim a relatively good duty (1600 TBW for a 2 Tb drive), but do they actually deliver on that? Are there any brands/models known to be good (and not bait-and-switch the insides after earning that)? And by the way, are there any caveats for running an SSD in a software RAID1 (md-raid) with an HDD?", "author_fullname": "t2_summwsvu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hints to find a reliable SATA SSD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq82pk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671494568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Usually, I&amp;#39;ve kept my data on a server that was also doing routing and other stuff - a E7500 with many, many pre-SMR era 3.5&amp;quot; HDDs in RAID1&amp;#39;s. Sometimes they die, and replacing them is not much of a problem.&lt;/p&gt;\n\n&lt;p&gt;But now I&amp;#39;ve gone digital nomad, and I need a different solution. I can&amp;#39;t take the server with me, or take proper care of it, so it is now &amp;quot;unreliable&amp;quot;. Therefore, at least for some stuff, I need something I can carry with me.&lt;/p&gt;\n\n&lt;p&gt;The plan is to get a Sabrent DS-4SSD DAS (any better suggestions?). I have two 1-Tb 2.5&amp;quot; Hitachi Travelstars, which should be very reliable compared to today&amp;#39;s standards - I also expect them to be more resilient to shaking in a bag than 3.5&amp;quot; drives. I also have a 500 Gb Samsung drive that I&amp;#39;m fully expecting to fail at any moment - it&amp;#39;s for non-essential use for all kinds of temporary stuff.&lt;/p&gt;\n\n&lt;p&gt;Now, 1 Tb (mirrored, of course) is barely enough, and I still have two more slots available. So I&amp;#39;m considering my options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Buy two 2 Tb HDDs and mirror them. More than 2 Tb are likely more vulnerable to shaking, and are likely SMR. But a good kick to the DAS might kill all four of them.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Buy one 2 Tb SSD and mirror both of the Hitachis to it. This seems like the best solution. I will have one slot left for the Samsung, and I will have protection against both SSD damage and mechanical damage... as long as the SSD does not turn out to have failed after HDDs die.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;But now that I look at what&amp;#39;s available... The days of &amp;quot;MLC are crap, buy SLC only&amp;quot; are long gone; you&amp;#39;re lucky to not get a QLC, and TLC are considered &amp;quot;reliable&amp;quot;. And no one will even disclose what memory chips do they use, and swapping them (even along with the controller!) without changing the model name is a normal everyday occurrence.&lt;/p&gt;\n\n&lt;p&gt;I see cheap TeamGroup SSDs which claim a relatively good duty (1600 TBW for a 2 Tb drive), but do they actually deliver on that? Are there any brands/models known to be good (and not bait-and-switch the insides after earning that)? And by the way, are there any caveats for running an SSD in a software RAID1 (md-raid) with an HDD?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zq82pk", "is_robot_indexable": true, "report_reasons": null, "author": "mad-jester69", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zq82pk/hints_to_find_a_reliable_sata_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zq82pk/hints_to_find_a_reliable_sata_ssd/", "subreddit_subscribers": 660092, "created_utc": 1671494568.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://github.com/mxmlnkn/ratarmount](https://github.com/mxmlnkn/ratarmount)\n\nYou might already know ratarmount thanks to it being mentioned [here](https://www.reddit.com/r/DataHoarder/comments/z29axm/help_seed_zlibrary_on_ipfs/).\n\nI've been writing ratarmount as an alternative to archivemount because the latter was too slow for large archives. Contrary to the name, it also supports ZIP and RAR archives to some extent. I've been using it to bundle collections of small files, e.g., pidgin chat logs, into TAR archives because HDDs don't like reading and writing and copying those. Furthermore, as a data hoarder, those files are only used read-only anyway, so storing them in an archive is not an issue. Because I now know that TAR archives are only a simple concatenation of the actual files preceded by 512 B of metadata, I'm not even worried about data format rot anymore. It would be simple enough to write a crude TAR extractor from scratch.\n\nAt this point, ratarmount has too many features to name them all but one of my favorites is the recursive bind mounting of folders. You could for example do `ratarmount --recursion-depth 1 /media/my-large-drive /media/mld-mounted` and then you would be able to view my-large-drive via mld-mounted as if all archives in the former were extracted.\n\nCurrently, I'm dedicating my time to one of ratarmount's backends: pragzip, a multithreaded parallel gzip decoder with random access support. This should speed things up further for gzip-compressed TAR archives.\n\nI'd be happy to hear about suggestions or even bug reports if bugs were encountered.", "author_fullname": "t2_upy204yv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Access your archives without extracting them with ratarmount", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpmi44", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671440078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/mxmlnkn/ratarmount\"&gt;https://github.com/mxmlnkn/ratarmount&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You might already know ratarmount thanks to it being mentioned &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/z29axm/help_seed_zlibrary_on_ipfs/\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been writing ratarmount as an alternative to archivemount because the latter was too slow for large archives. Contrary to the name, it also supports ZIP and RAR archives to some extent. I&amp;#39;ve been using it to bundle collections of small files, e.g., pidgin chat logs, into TAR archives because HDDs don&amp;#39;t like reading and writing and copying those. Furthermore, as a data hoarder, those files are only used read-only anyway, so storing them in an archive is not an issue. Because I now know that TAR archives are only a simple concatenation of the actual files preceded by 512 B of metadata, I&amp;#39;m not even worried about data format rot anymore. It would be simple enough to write a crude TAR extractor from scratch.&lt;/p&gt;\n\n&lt;p&gt;At this point, ratarmount has too many features to name them all but one of my favorites is the recursive bind mounting of folders. You could for example do &lt;code&gt;ratarmount --recursion-depth 1 /media/my-large-drive /media/mld-mounted&lt;/code&gt; and then you would be able to view my-large-drive via mld-mounted as if all archives in the former were extracted.&lt;/p&gt;\n\n&lt;p&gt;Currently, I&amp;#39;m dedicating my time to one of ratarmount&amp;#39;s backends: pragzip, a multithreaded parallel gzip decoder with random access support. This should speed things up further for gzip-compressed TAR archives.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d be happy to hear about suggestions or even bug reports if bugs were encountered.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?auto=webp&amp;s=18c94eb1a9806e8d732e747144a42e65922bfd1f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4c4a0b2b3230c3246202b8eda828b8c676c34aa5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1780b9b08d081324f2e248c3c13f68f352ec65b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c75f5ed7ba04f01298b7e65fe1eda0d81272db1b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=92897dd24d79979b5d06d85fb06073465ab862e1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=80dd2205c70656937d97500612a1851f03747aa5", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=81544b3361a3105d62ebfd99f93571482ea2a1e1", "width": 1080, "height": 540}], "variants": {}, "id": "E_-IiemZPNkstx_dji0Br7cF26Y0H42JpsdEwRT38EA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpmi44", "is_robot_indexable": true, "report_reasons": null, "author": "mxmlnkn", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpmi44/access_your_archives_without_extracting_them_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpmi44/access_your_archives_without_extracting_them_with/", "subreddit_subscribers": 660092, "created_utc": 1671440078.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought a Nas with a single hdd a few months ago to double backup some important things, and move some less important things from my pc to it. Also to stream things.\n\nBut after seeing all the posts in the sub about setups with 2-3-10 hdds in raid and even more I'm starting to worry: is having a single hdd a problem? (I'm slowly saving up to get another one) Does it happen often that a drive goes from working to completely unrecoverable in a day? \n\nThe data that I absolutely don't want to lose is also on pc and some also on some cloud hostings, but there are still things that I would prefer not to lose that are in single copy there due to their size. Is there any app or settings on a terramaster Nas (tos4) to check drive health or to get  notified if bad sectors start to accumulate?", "author_fullname": "t2_50bwjoy1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is a single hdd setup on my Nas a problem?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zqhoea", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671520567.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a Nas with a single hdd a few months ago to double backup some important things, and move some less important things from my pc to it. Also to stream things.&lt;/p&gt;\n\n&lt;p&gt;But after seeing all the posts in the sub about setups with 2-3-10 hdds in raid and even more I&amp;#39;m starting to worry: is having a single hdd a problem? (I&amp;#39;m slowly saving up to get another one) Does it happen often that a drive goes from working to completely unrecoverable in a day? &lt;/p&gt;\n\n&lt;p&gt;The data that I absolutely don&amp;#39;t want to lose is also on pc and some also on some cloud hostings, but there are still things that I would prefer not to lose that are in single copy there due to their size. Is there any app or settings on a terramaster Nas (tos4) to check drive health or to get  notified if bad sectors start to accumulate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zqhoea", "is_robot_indexable": true, "report_reasons": null, "author": "TheXade", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zqhoea/is_a_single_hdd_setup_on_my_nas_a_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zqhoea/is_a_single_hdd_setup_on_my_nas_a_problem/", "subreddit_subscribers": 660092, "created_utc": 1671520567.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyone know of a site that tracks/compares Blueray disc prices?  I'm trying to find the cheapest way to buy about 10 to 20 100gb BDXL discs.", "author_fullname": "t2_mgmtm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Site that Tracks Blueray Disc Prices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqfanw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671513286.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone know of a site that tracks/compares Blueray disc prices?  I&amp;#39;m trying to find the cheapest way to buy about 10 to 20 100gb BDXL discs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zqfanw", "is_robot_indexable": true, "report_reasons": null, "author": "HarryMuscle", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zqfanw/site_that_tracks_blueray_disc_prices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zqfanw/site_that_tracks_blueray_disc_prices/", "subreddit_subscribers": 660092, "created_utc": 1671513286.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is a 25G card worth it to put in this NAS? I have a Switch which supports 25G. The server will typically be pushing UHD quality vido files. 50gb-100gb approx\n\nSince I am guessing the max read rate of Seagate EXOS 18 18TB in RAID 6 qty of 12 is much slower than even 10G is it even worth the upgrade? I just purchased it because the costs were so similar between the two compatible cards. (10G/25G)", "author_fullname": "t2_972426g5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "QNAP TS-EC1280U 12 Bay NAS 25G Add-on Card Upgrade", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqcfmn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671505391.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is a 25G card worth it to put in this NAS? I have a Switch which supports 25G. The server will typically be pushing UHD quality vido files. 50gb-100gb approx&lt;/p&gt;\n\n&lt;p&gt;Since I am guessing the max read rate of Seagate EXOS 18 18TB in RAID 6 qty of 12 is much slower than even 10G is it even worth the upgrade? I just purchased it because the costs were so similar between the two compatible cards. (10G/25G)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zqcfmn", "is_robot_indexable": true, "report_reasons": null, "author": "Avionics_Engineer06", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zqcfmn/qnap_tsec1280u_12_bay_nas_25g_addon_card_upgrade/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zqcfmn/qnap_tsec1280u_12_bay_nas_25g_addon_card_upgrade/", "subreddit_subscribers": 660092, "created_utc": 1671505391.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Before everyone starts saying **rclone,** please read on.\n\nCurrently I use cyberduck along with carotdav to upload files to google drive. I am not looking to mount it as a drive or to setup sync. I want to manually upload different files from different directories at different times manually. I need it to be able to drag and drop to upload multiple files (which I wasn't able to do with rclone browser and I don't want to upload it as a folder, correct me if I'm wrong)\n\nMy current issue with cyberduck is for some reason, it uses much more CPU compared to other programs. The issue with carotDAV is it only upload files one by one even if you dump in a bunch of files, which I cannot max out my connection.\n\nTo what I understand, rclone doesn't fit my needs. If anyone knows how to setup rclone to work like I how I need it, I would happily use rclone. Filezilla Pro doesn't have a trial so I can't test how well it would work for me.\n\n&amp;#x200B;\n\nAre there any alternative programs on windows for this use case or how do I properly setup rclone for this use case? I'm ready to learn.\n\n&amp;#x200B;\n\nThanks.", "author_fullname": "t2_lcuai", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good alternatives to Cyberduck (on windows) for uploading to GDrive? (please read before commenting rclone)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq4xjk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671487386.0, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671487195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Before everyone starts saying &lt;strong&gt;rclone,&lt;/strong&gt; please read on.&lt;/p&gt;\n\n&lt;p&gt;Currently I use cyberduck along with carotdav to upload files to google drive. I am not looking to mount it as a drive or to setup sync. I want to manually upload different files from different directories at different times manually. I need it to be able to drag and drop to upload multiple files (which I wasn&amp;#39;t able to do with rclone browser and I don&amp;#39;t want to upload it as a folder, correct me if I&amp;#39;m wrong)&lt;/p&gt;\n\n&lt;p&gt;My current issue with cyberduck is for some reason, it uses much more CPU compared to other programs. The issue with carotDAV is it only upload files one by one even if you dump in a bunch of files, which I cannot max out my connection.&lt;/p&gt;\n\n&lt;p&gt;To what I understand, rclone doesn&amp;#39;t fit my needs. If anyone knows how to setup rclone to work like I how I need it, I would happily use rclone. Filezilla Pro doesn&amp;#39;t have a trial so I can&amp;#39;t test how well it would work for me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Are there any alternative programs on windows for this use case or how do I properly setup rclone for this use case? I&amp;#39;m ready to learn.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "17TB+To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zq4xjk", "is_robot_indexable": true, "report_reasons": null, "author": "chorong761", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zq4xjk/what_are_some_good_alternatives_to_cyberduck_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zq4xjk/what_are_some_good_alternatives_to_cyberduck_on/", "subreddit_subscribers": 660092, "created_utc": 1671487195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I finally received a dud drive. Never happened before to me. Sold and shipped by newegg, the 16TB Seagate EXos drive I received wouldn't even spin up. It was packed decently well but I guess not well enough. I have to wait until tomorrow to start the return process.  \n\n\nOriginally I thought it must be a bum external drive dock so tried popping it in my single drive synology to test and it wouldnt show up at all. Glad I didnt jump straight into degrading the pool it is supposed to go into.\n\nAre there any tips for making sure I get a replacement quickly and efficiently? I am planning on returning the drive to newegg under the replacement option rather than RMA through Seagate.", "author_fullname": "t2_11bkpp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for my bad drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq3cbe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671483507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I finally received a dud drive. Never happened before to me. Sold and shipped by newegg, the 16TB Seagate EXos drive I received wouldn&amp;#39;t even spin up. It was packed decently well but I guess not well enough. I have to wait until tomorrow to start the return process.  &lt;/p&gt;\n\n&lt;p&gt;Originally I thought it must be a bum external drive dock so tried popping it in my single drive synology to test and it wouldnt show up at all. Glad I didnt jump straight into degrading the pool it is supposed to go into.&lt;/p&gt;\n\n&lt;p&gt;Are there any tips for making sure I get a replacement quickly and efficiently? I am planning on returning the drive to newegg under the replacement option rather than RMA through Seagate.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "28TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zq3cbe", "is_robot_indexable": true, "report_reasons": null, "author": "haloid2013", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zq3cbe/tips_for_my_bad_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zq3cbe/tips_for_my_bad_drive/", "subreddit_subscribers": 660092, "created_utc": 1671483507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everybody,\n\nhave you got some experiences with the kind of items in the title?\n\nThere are tons of models for each vendor and except vantec I dont know others. I've seen some vantec products - I have and old pata nexstar, it's still doing the job when needed), but some amazon clients' online reviews seem to say that old quality has gone (and they cost 3x, compared with other firm).\n\nI had two sabrent but after a couple year, one after another, stop to work.\n\nPlus: they should work good with linux and ssd disks, some chip may have trim problems (sob). It seems so complicate build an external 2.5 usb disks (I dont trust the ones sale by wd or seagate or toshiba... there are hdd smr disk; I also prefer to not buy a \"usb ssd\" like Sandisk Extreme, because they are a finished product and I cannot shuck them or change the disk, if needed).\n\nSome one of you has tested / evaluated these case? Should I evaluate a solution like the FANTEC QB-35US3R. Please note I would not buy a NAS, I have already have enough pc (family says...) and they are too expansive.\n\nThanks a lot for reporting you experiences!\n\n&amp;#x200B;\n\n\\----\n\nedit: added the fantec paragraph.", "author_fullname": "t2_9gmmn58m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "external usb3 case: vantec, inateck, ugreen or... which one? No sabrent please, I've already had bad experiences with them.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zq2kaw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671482481.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671481660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody,&lt;/p&gt;\n\n&lt;p&gt;have you got some experiences with the kind of items in the title?&lt;/p&gt;\n\n&lt;p&gt;There are tons of models for each vendor and except vantec I dont know others. I&amp;#39;ve seen some vantec products - I have and old pata nexstar, it&amp;#39;s still doing the job when needed), but some amazon clients&amp;#39; online reviews seem to say that old quality has gone (and they cost 3x, compared with other firm).&lt;/p&gt;\n\n&lt;p&gt;I had two sabrent but after a couple year, one after another, stop to work.&lt;/p&gt;\n\n&lt;p&gt;Plus: they should work good with linux and ssd disks, some chip may have trim problems (sob). It seems so complicate build an external 2.5 usb disks (I dont trust the ones sale by wd or seagate or toshiba... there are hdd smr disk; I also prefer to not buy a &amp;quot;usb ssd&amp;quot; like Sandisk Extreme, because they are a finished product and I cannot shuck them or change the disk, if needed).&lt;/p&gt;\n\n&lt;p&gt;Some one of you has tested / evaluated these case? Should I evaluate a solution like the FANTEC QB-35US3R. Please note I would not buy a NAS, I have already have enough pc (family says...) and they are too expansive.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for reporting you experiences!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;----&lt;/p&gt;\n\n&lt;p&gt;edit: added the fantec paragraph.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zq2kaw", "is_robot_indexable": true, "report_reasons": null, "author": "wireless82", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zq2kaw/external_usb3_case_vantec_inateck_ugreen_or_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zq2kaw/external_usb3_case_vantec_inateck_ugreen_or_which/", "subreddit_subscribers": 660092, "created_utc": 1671481660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Surprisingly few sources on this - officially WD say it's 16 TB, which after some time magically became 20 TB on a 2-bay NAS.\n\nI don't get what's imposing the limitation though - it's the same controller, same power - what is there to stop me from putting 2x 16 TB drives in there? Or is it just a hard-limit set by WD for whatever reasons and the NAS simply will not boot with such drives?\n\nOne theory is that the \"advertised\" capacity is what it is, going by the maximum available HDD size **at that time.** I can't confirm that though.\n\nI'd like to find out if maybe someone's already tried it before I buy the drives. Thoughts ?", "author_fullname": "t2_2zwlr2wk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD MyCloud EX2 Ultra - max capacity ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zptm0k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671461469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Surprisingly few sources on this - officially WD say it&amp;#39;s 16 TB, which after some time magically became 20 TB on a 2-bay NAS.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t get what&amp;#39;s imposing the limitation though - it&amp;#39;s the same controller, same power - what is there to stop me from putting 2x 16 TB drives in there? Or is it just a hard-limit set by WD for whatever reasons and the NAS simply will not boot with such drives?&lt;/p&gt;\n\n&lt;p&gt;One theory is that the &amp;quot;advertised&amp;quot; capacity is what it is, going by the maximum available HDD size &lt;strong&gt;at that time.&lt;/strong&gt; I can&amp;#39;t confirm that though.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to find out if maybe someone&amp;#39;s already tried it before I buy the drives. Thoughts ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zptm0k", "is_robot_indexable": true, "report_reasons": null, "author": "Teacan83", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zptm0k/wd_mycloud_ex2_ultra_max_capacity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zptm0k/wd_mycloud_ex2_ultra_max_capacity/", "subreddit_subscribers": 660092, "created_utc": 1671461469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Howdy, fellow hoarders!\n\nI'm looking for a solution to use to store about 2 TB of data (family photos and videos backup) online that does not require me to install an application to do so.\n\nI've looked at some of the other posts here and have an idea of the major suppliers already, but not which ones offer an option to just map a drive (open to any protocols sorted by linux).\n\nDoes anyone else have this use case and doing this with a provider, please?\n\nIdeally I'll go with the cheapest cost/year. I don't want nor need anything fancy, just storage space I can map to have another backip of family photos and videos. \n\nThank you in advance.\n\nPs- I asked iDrive and tgey responded that I must have their app installed.", "author_fullname": "t2_emara", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Storage w/o Install", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zptkha", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671461745.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671461373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy, fellow hoarders!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a solution to use to store about 2 TB of data (family photos and videos backup) online that does not require me to install an application to do so.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked at some of the other posts here and have an idea of the major suppliers already, but not which ones offer an option to just map a drive (open to any protocols sorted by linux).&lt;/p&gt;\n\n&lt;p&gt;Does anyone else have this use case and doing this with a provider, please?&lt;/p&gt;\n\n&lt;p&gt;Ideally I&amp;#39;ll go with the cheapest cost/year. I don&amp;#39;t want nor need anything fancy, just storage space I can map to have another backip of family photos and videos. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n\n&lt;p&gt;Ps- I asked iDrive and tgey responded that I must have their app installed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zptkha", "is_robot_indexable": true, "report_reasons": null, "author": "trancekat", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zptkha/cloud_storage_wo_install/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zptkha/cloud_storage_wo_install/", "subreddit_subscribers": 660092, "created_utc": 1671461373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all, as the title implies. How?", "author_fullname": "t2_c7muzduq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use WaybackMachine to find Facebook Marketplace post", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqgzyb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671518427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, as the title implies. How?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zqgzyb", "is_robot_indexable": true, "report_reasons": null, "author": "lookielookiehi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zqgzyb/use_waybackmachine_to_find_facebook_marketplace/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zqgzyb/use_waybackmachine_to_find_facebook_marketplace/", "subreddit_subscribers": 660092, "created_utc": 1671518427.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}