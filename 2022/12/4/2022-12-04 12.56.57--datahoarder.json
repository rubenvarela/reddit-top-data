{"kind": "Listing", "data": {"after": "t3_zc18q8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reminder to backup up your data! 5 month old ADATA SSD Failure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": false, "name": "t3_zbsc58", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 229, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_odtvi", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 229, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/c6ZFKQ7JVOCF0jg0AO52CeWpTuzSk6hq8KQ7B4KP-WI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "pcmasterrace", "selftext": "", "author_fullname": "t2_odtvi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Reminder to backup up your data! 5 month old ADATA SSD Failure", "link_flair_richtext": [{"e": "text", "t": "NSFMR"}], "subreddit_name_prefixed": "r/pcmasterrace", "hidden": false, "pwls": 6, "link_flair_css_class": "red", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": false, "media_metadata": {"8gh0drxx8r3a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 68, "x": 108, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=467c6b9aeca1853aae4afedede89671e468ed6d9"}, {"y": 136, "x": 216, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de01e9c1b36b463b62456e3f6571c69151ed6d86"}, {"y": 201, "x": 320, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f1b4050501da4df186d3012704b967e4e62dd3af"}, {"y": 403, "x": 640, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d652b57fb88f89a47060aa6eba4ca090446bae86"}, {"y": 605, "x": 960, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7e154adf575548c1e8c7519e94c81a299ce197a"}, {"y": 680, "x": 1080, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=23ca023ae7e4d2b5db23023fbfef46af21ca4596"}], "s": {"y": 2199, "x": 3489, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=3489&amp;format=pjpg&amp;auto=webp&amp;s=c664512290e4e874b038cee82d88d4f9dcfd0230"}, "id": "8gh0drxx8r3a1"}, "8li9yryx8r3a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0be58dc406ebfc3060a3b9950f96e04919d0f5f9"}, {"y": 152, "x": 216, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be2d802ea06dd13e01a688d864087906c7384695"}, {"y": 225, "x": 320, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ca8f5f1e5338faf6c176691cae54983c121e3f1"}, {"y": 451, "x": 640, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=013bd21e2dfd6141f53c776893291f2a47810d75"}, {"y": 676, "x": 960, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ed35fba989543c53543f759f175d6809095e48e5"}, {"y": 761, "x": 1080, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=901c4629a266928d0c3fd2d9432723ce29d7b28a"}], "s": {"y": 2816, "x": 3994, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=3994&amp;format=pjpg&amp;auto=webp&amp;s=e6ffe2ff213f2a61014d949bed019d450321bc8b"}, "id": "8li9yryx8r3a1"}, "f2j1efzx8r3a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8fafd849d4e87690af25a3769c71b1f72def8ef2"}, {"y": 125, "x": 216, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c251b4eefd6ea98fe5354c45efabde285db131f"}, {"y": 185, "x": 320, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=04806382b74b637dfead07d632ab90bb75838866"}, {"y": 371, "x": 640, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=957edbc397f6e587bf129efa4c7ee879e5706c89"}, {"y": 557, "x": 960, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=96053e9b4b138e52ae0c2580172abe216ff7d017"}, {"y": 626, "x": 1080, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6966b64aee329d93f347e4d7dd93d4878c1ed8dc"}], "s": {"y": 1998, "x": 3442, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=3442&amp;format=pjpg&amp;auto=webp&amp;s=61b590496d3fec4ba0778b279e24bce32710051d"}, "id": "f2j1efzx8r3a1"}, "lak43zxx8r3a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 64, "x": 108, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f2d1e9403f49050ecca445266abe5ac94904c2c"}, {"y": 129, "x": 216, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a701d655f6550ade97f26713e9fc9044c5e31448"}, {"y": 192, "x": 320, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=be4d9aa835c8c9524ae1a0abb2d0b98bf37996e0"}, {"y": 384, "x": 640, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4cb99076471feb6426c41481707e01db5d707dc4"}, {"y": 576, "x": 960, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=52607c9a610d5afd13f5c7beebb3315abc09fa2a"}, {"y": 648, "x": 1080, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8dc7600018490b28d77cf29fd89d18cd3182a625"}], "s": {"y": 2056, "x": 3425, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=3425&amp;format=pjpg&amp;auto=webp&amp;s=58ec36c8ad0a08a882e379bae5ad68e1c9d88060"}, "id": "lak43zxx8r3a1"}}, "name": "t3_zbs22g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 6, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "That's a lot of bad blocks", "media_id": "8gh0drxx8r3a1", "id": 215627376}, {"media_id": "8li9yryx8r3a1", "id": 215627377}, {"caption": "Even the log file is corrupt!", "media_id": "lak43zxx8r3a1", "id": 215627378}, {"media_id": "f2j1efzx8r3a1", "id": 215627379}]}, "link_flair_text": "NSFMR", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/c6ZFKQ7JVOCF0jg0AO52CeWpTuzSk6hq8KQ7B4KP-WI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670104896.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zbs22g", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4c6e9074-3133-11e4-b380-12313b12e896", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sgp1", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff0000", "id": "zbs22g", "is_robot_indexable": true, "report_reasons": null, "author": "hboyd2003", "discussion_type": null, "num_comments": 16, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/pcmasterrace/comments/zbs22g/reminder_to_backup_up_your_data_5_month_old_adata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zbs22g", "subreddit_subscribers": 6900561, "created_utc": 1670104896.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1670105590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zbs22g", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbsc58", "is_robot_indexable": true, "report_reasons": null, "author": "hboyd2003", "discussion_type": null, "num_comments": 68, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_zbs22g", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbsc58/reminder_to_backup_up_your_data_5_month_old_adata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zbs22g", "subreddit_subscribers": 657402, "created_utc": 1670105590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4pdfnevq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It ain\u2019t much but it\u2019s my first big boy storage drive. Question: I always run a full error scan on new HDD\u2019s with HD Tune, is this necessary to ensure a full working drive or is it overkill? Should I just check SMART attributes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 131, "top_awarded_type": null, "hide_score": false, "name": "t3_zbwgvx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 120, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 120, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7ft42o2OWVVk0FtqVgueHnuiThbRe9UxVA7XKluRyGs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670116527.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zw7rk3gwpt3a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?auto=webp&amp;s=e97b5aafd00bd941ba02381a65fc914e7bc9bf64", "width": 3230, "height": 3023}, "resolutions": [{"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f159d24e45a01001664e369166245b9e4a772a46", "width": 108, "height": 101}, {"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d7ab40bcc0d6c048d16a84f1610e70a25fdf98a", "width": 216, "height": 202}, {"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=62a8adb7a082bc89cc7e6a0a733b92afcd27a1fd", "width": 320, "height": 299}, {"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e345b8f89e9e3a68afd68fe4b5f4df13279f5fb1", "width": 640, "height": 598}, {"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ff5ce3629084f5b39ef48b839f8c5daefe8c7d5", "width": 960, "height": 898}, {"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4e74645d34dbbf9ba54ec09955c1ec780f4f18c1", "width": 1080, "height": 1010}], "variants": {}, "id": "Gp4u_8kPc6aAjHGiIxzVH_2Y6Ej3bwlFsPyeOTLCAzQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbwgvx", "is_robot_indexable": true, "report_reasons": null, "author": "Valor_X", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbwgvx/it_aint_much_but_its_my_first_big_boy_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zw7rk3gwpt3a1.jpg", "subreddit_subscribers": 657402, "created_utc": 1670116527.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I\u2019m trying to download a lot of YouTube videos in huge playlist. I have a really fast internet (5gbit/s), but the softwares that I tried (4K video downloaded and Open Video Downloader) are slow, like 3 MB/s for 4k video download and 1MB/s for Oen video downloader. I founded some online websites with a lot of stupid ads, like https://x2download.app/ , that download at a really fast speed, but they aren\u2019t good for download more than few videos at once. What do you use? I have both windows, Linux and Mac.", "author_fullname": "t2_b80xl4j2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best software for download YouTube videos and playlist in mass", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbn1ke", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 98, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 98, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670092438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019m trying to download a lot of YouTube videos in huge playlist. I have a really fast internet (5gbit/s), but the softwares that I tried (4K video downloaded and Open Video Downloader) are slow, like 3 MB/s for 4k video download and 1MB/s for Oen video downloader. I founded some online websites with a lot of stupid ads, like &lt;a href=\"https://x2download.app/\"&gt;https://x2download.app/&lt;/a&gt; , that download at a really fast speed, but they aren\u2019t good for download more than few videos at once. What do you use? I have both windows, Linux and Mac.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bBZtnOZdA4YFrPl8m5wpdkUIGtT2JyV2kLHwlKLfkI8.jpg?auto=webp&amp;s=2ed9d33e203acca30d6a9235bb3f536609a105df", "width": 496, "height": 251}, "resolutions": [{"url": "https://external-preview.redd.it/bBZtnOZdA4YFrPl8m5wpdkUIGtT2JyV2kLHwlKLfkI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b73398f420d1ea825f5cb7c61acd8e4b921825d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/bBZtnOZdA4YFrPl8m5wpdkUIGtT2JyV2kLHwlKLfkI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e505c0a39a37c1deb12ef8cc438f9e5e1ce6d39b", "width": 216, "height": 109}, {"url": "https://external-preview.redd.it/bBZtnOZdA4YFrPl8m5wpdkUIGtT2JyV2kLHwlKLfkI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b0758c1f43eae0dd0e9bd3287520e17df2bd82b", "width": 320, "height": 161}], "variants": {}, "id": "RzHaQHhYQQZo_4fJo6oQXmSdOd-gIK8_CoINdfSoBAA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbn1ke", "is_robot_indexable": true, "report_reasons": null, "author": "StrengthLocal2543", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbn1ke/best_software_for_download_youtube_videos_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbn1ke/best_software_for_download_youtube_videos_and/", "subreddit_subscribers": 657402, "created_utc": 1670092438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have around 5TB worth of data that I want to consolidate into one place and want to know how would I go around and do such a thing.   \nShort backstory, I had a system with 2 drives, a 500GB SSD and 2TB HDD. Bought a 1TB HDD during start of the pandemic for recording location of recorded meetings. Year 1 into Pandemic, 2TB HDD has an error and on a \"Caution\" status from CrystalDiskInfo. I was also planning to build a new rig that year so I bought one 1TB NVME SSD(not part of the situation), a 2TB Ironwolf HDD and 4TB Barracuda.  \nI didn't want to use my new drives on the PC so I used two 1TB External HDDs to store the data. Then a year later the 1TB I bought at the start of the pandemic was in \"Bad\" condition, so I gave in and plugged my 4TB HDD and transferred my files. For some reason I also plugged in the 2TB HDD. Now all of them are in use.  \n\n\nSo I did a disk analyze using WizTree and found out that I have half of my total storage full, not to mention earlier in the day I got an error that there was not enough space in my boot drive when I was saving stuff into there.  \n\n\nNow how would I go around completely copying over these files into a whole separate drive/drives, eliminating duplicates and make sure everything is copied properly, so that I can completely wipe my current system and basically start from scratch.  \n I would also like to ask on what kind of storage I should use to store this on. I think it will be just a cold storage, with the occasional rummage through, but it might also be another dump place I can continue adding to. I don't want to delete any of the files, more of its a hassle rather than sentimental stuff, but there are stuff that I definitely want to keep there.  I would also say that I don't have much money to put towards storing these. So I guess a solution where I can add on over time would be best.", "author_fullname": "t2_mu7c9au", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would I go about copying around 5TB worth of data, from multiple drives to a singular drive/drives (Shared Pools/Raid)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbhmth", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670077996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have around 5TB worth of data that I want to consolidate into one place and want to know how would I go around and do such a thing.&lt;br/&gt;\nShort backstory, I had a system with 2 drives, a 500GB SSD and 2TB HDD. Bought a 1TB HDD during start of the pandemic for recording location of recorded meetings. Year 1 into Pandemic, 2TB HDD has an error and on a &amp;quot;Caution&amp;quot; status from CrystalDiskInfo. I was also planning to build a new rig that year so I bought one 1TB NVME SSD(not part of the situation), a 2TB Ironwolf HDD and 4TB Barracuda.&lt;br/&gt;\nI didn&amp;#39;t want to use my new drives on the PC so I used two 1TB External HDDs to store the data. Then a year later the 1TB I bought at the start of the pandemic was in &amp;quot;Bad&amp;quot; condition, so I gave in and plugged my 4TB HDD and transferred my files. For some reason I also plugged in the 2TB HDD. Now all of them are in use.  &lt;/p&gt;\n\n&lt;p&gt;So I did a disk analyze using WizTree and found out that I have half of my total storage full, not to mention earlier in the day I got an error that there was not enough space in my boot drive when I was saving stuff into there.  &lt;/p&gt;\n\n&lt;p&gt;Now how would I go around completely copying over these files into a whole separate drive/drives, eliminating duplicates and make sure everything is copied properly, so that I can completely wipe my current system and basically start from scratch.&lt;br/&gt;\n I would also like to ask on what kind of storage I should use to store this on. I think it will be just a cold storage, with the occasional rummage through, but it might also be another dump place I can continue adding to. I don&amp;#39;t want to delete any of the files, more of its a hassle rather than sentimental stuff, but there are stuff that I definitely want to keep there.  I would also say that I don&amp;#39;t have much money to put towards storing these. So I guess a solution where I can add on over time would be best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbhmth", "is_robot_indexable": true, "report_reasons": null, "author": "InfraDelta", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbhmth/how_would_i_go_about_copying_around_5tb_worth_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbhmth/how_would_i_go_about_copying_around_5tb_worth_of/", "subreddit_subscribers": 657402, "created_utc": 1670077996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI'm planning a 6 drive (storage) truenas build + 1 ssd for the OS drive.\n\nMy mobo has 6 sata ports and so consequently, I'm one short. \n\nTo utilise my 6 sata ports for the 6 data drives. I understand there is the option to boot the OS from USB (converted to Sata). Alternatively, I could look to get an HBA card (pci express --&gt; sata) but hear there are compatibility issues via this route. If you know of a good HBA card thats not too expensive, I'd love to hear.\n\nAny guidance here would be appreciated!", "author_fullname": "t2_d2ja9fnm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "6 drive nas - HDA card question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbowdf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670097168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning a 6 drive (storage) truenas build + 1 ssd for the OS drive.&lt;/p&gt;\n\n&lt;p&gt;My mobo has 6 sata ports and so consequently, I&amp;#39;m one short. &lt;/p&gt;\n\n&lt;p&gt;To utilise my 6 sata ports for the 6 data drives. I understand there is the option to boot the OS from USB (converted to Sata). Alternatively, I could look to get an HBA card (pci express --&amp;gt; sata) but hear there are compatibility issues via this route. If you know of a good HBA card thats not too expensive, I&amp;#39;d love to hear.&lt;/p&gt;\n\n&lt;p&gt;Any guidance here would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbowdf", "is_robot_indexable": true, "report_reasons": null, "author": "Maximum-Warning-4186", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbowdf/6_drive_nas_hda_card_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbowdf/6_drive_nas_hda_card_question/", "subreddit_subscribers": 657402, "created_utc": 1670097168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought HP LTO-5 Ultrium 3280 FC drive from eBay and have to upgrade firmware.\nBut this file is locked to download and I don't have permission to download it.\nThe URL is; https://support.hpe.com/connect/s/softwaredetails?softwareId=co_154832_1\nDoes anyone help me? Thanks in advance!", "author_fullname": "t2_jnn18zst", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download locked file on HPE support center?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zbk72l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3Hv7ABYSZFObeFGJR-JMYdzL_xKFatrJnNldPXy8jzw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670085046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought HP LTO-5 Ultrium 3280 FC drive from eBay and have to upgrade firmware.\nBut this file is locked to download and I don&amp;#39;t have permission to download it.\nThe URL is; &lt;a href=\"https://support.hpe.com/connect/s/softwaredetails?softwareId=co_154832_1\"&gt;https://support.hpe.com/connect/s/softwaredetails?softwareId=co_154832_1&lt;/a&gt;\nDoes anyone help me? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/6qfgwoea4r3a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?auto=webp&amp;s=56ffe1538c200c8efe74db845f208432839f4867", "width": 1440, "height": 2862}, "resolutions": [{"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=36eba71941689b950e430c7a47313759d96cd4fb", "width": 108, "height": 214}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=01d491c9f50728ff74aaf9a41190c2ef49ff26bd", "width": 216, "height": 429}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=98610143610cf4095d700a321143e58803ac6143", "width": 320, "height": 636}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e2fcc45c42d717f4fdc84e09a7c88d3c17d898e", "width": 640, "height": 1272}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8a83b58f39ac6f150fd45247b554e4e611a59b2", "width": 960, "height": 1908}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fb16bce58540c7884d17bdf07670b1391b04b42", "width": 1080, "height": 2146}], "variants": {}, "id": "mNKFwNb_IuvqdSRmN5qqHuJNjmhyCAGfJ7ysIVhwghU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbk72l", "is_robot_indexable": true, "report_reasons": null, "author": "AMDRadeonHD6950", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbk72l/how_to_download_locked_file_on_hpe_support_center/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/6qfgwoea4r3a1.png", "subreddit_subscribers": 657402, "created_utc": 1670085046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "On windows explorer when I go searching through my hdd, theres a long pause on every folder as it scans and builds a directory file tree from scratch every single time. \n\nThese folders haven\u2019t changed in months, maybe years, theres no need to rebuild it so often.\n\nIs there a software / hardware combo that will place all the filenames and directories on a dedicated NVME for instant browsing? And then do a minor delta update as things change?\n\nI\u2019m ok with the actual hdd files themselves being slow and taking a while to load.\n\nThanks in advance", "author_fullname": "t2_3mjejfmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software/hardware to cache filenames &amp; directories for instantaneous windows explorer browsing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbx5wq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670119702.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670118561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On windows explorer when I go searching through my hdd, theres a long pause on every folder as it scans and builds a directory file tree from scratch every single time. &lt;/p&gt;\n\n&lt;p&gt;These folders haven\u2019t changed in months, maybe years, theres no need to rebuild it so often.&lt;/p&gt;\n\n&lt;p&gt;Is there a software / hardware combo that will place all the filenames and directories on a dedicated NVME for instant browsing? And then do a minor delta update as things change?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m ok with the actual hdd files themselves being slow and taking a while to load.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbx5wq", "is_robot_indexable": true, "report_reasons": null, "author": "freshairproject", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbx5wq/softwarehardware_to_cache_filenames_directories/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbx5wq/softwarehardware_to_cache_filenames_directories/", "subreddit_subscribers": 657402, "created_utc": 1670118561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I saw that pixiv is changing their content policy and I assume that means a lot of stuff is about to be deleted. If I want a good backup or content rip of the site, what's the best way to do that? Otherwise, if anyone has a recent backup, are you willing to share or explain how you made it?\n\nedit:\n\n[Announcement and request regarding updates to Service Master Terms of Use for certain transactions](https://www.pixiv.net/info.php?id=8788&amp;lang=en)\n\n[Notice of revision of Service Master Terms of Use and establishment of \"Prohibited Items\" and \"Items Requiring Revision\"](https://www.pixiv.net/info.php?id=8872&amp;lang=en)", "author_fullname": "t2_3ov1i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pixiv looks to be wiping a lot of content soon, what's the best way to go about creating a site rip or backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbve3r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670132621.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670113481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw that pixiv is changing their content policy and I assume that means a lot of stuff is about to be deleted. If I want a good backup or content rip of the site, what&amp;#39;s the best way to do that? Otherwise, if anyone has a recent backup, are you willing to share or explain how you made it?&lt;/p&gt;\n\n&lt;p&gt;edit:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.pixiv.net/info.php?id=8788&amp;amp;lang=en\"&gt;Announcement and request regarding updates to Service Master Terms of Use for certain transactions&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.pixiv.net/info.php?id=8872&amp;amp;lang=en\"&gt;Notice of revision of Service Master Terms of Use and establishment of &amp;quot;Prohibited Items&amp;quot; and &amp;quot;Items Requiring Revision&amp;quot;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?auto=webp&amp;s=6f770ce6f634848bbe711506f48af405cbf5d106", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad1f3e2162a25fbc67d62e2459a763085a4fc757", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4eab161ae404f1d1c43a6dd2ff2e15d3450a8a0c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0d4405a7e3d730eba191b631a18dfa2869fdf19", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9656bfb90f7e3f6be767b1edf16d6de4f3975279", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6372f3280b08409bc871cb0cae1dc66b0b95dcc0", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ebb1a620886f4e1dba109523459958ab343a395c", "width": 1080, "height": 567}], "variants": {}, "id": "Xrmjj7H8dRSWCfboRhrC_PjmPF5yR1nuTS8P5iY80-M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbve3r", "is_robot_indexable": true, "report_reasons": null, "author": "th_sth", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbve3r/pixiv_looks_to_be_wiping_a_lot_of_content_soon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbve3r/pixiv_looks_to_be_wiping_a_lot_of_content_soon/", "subreddit_subscribers": 657402, "created_utc": 1670113481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello there!\n\nFirst of all, sorry for my bad englisch german native here :) \n\nI have a question about ZFS-Replication, perhaps someone have some infos.\n\nI have a Main-Storage System with ZFS and ECC RAM, the Second-Storage-System is running ZFS without ECC.\n\nIn my opinion it's ok to run the second storage system without ECC, because the Main System have.\n\nThe Second System is only for storing zfs-snapshot-replication as Backup\n\nSince i'm safe against bit root on my main storage after the data are writen to disk there is no chance to transfer corrupted data on my second storage system even without ecc on the second system.\n\nIf i have a bit-root in ram on the second system while replication, zfs would check the different parity and resend it.\n\nDo I have a flaw in my thinking?", "author_fullname": "t2_8pe5532a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ZFS Snapshot Replication without ECC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbjjh1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670083236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there!&lt;/p&gt;\n\n&lt;p&gt;First of all, sorry for my bad englisch german native here :) &lt;/p&gt;\n\n&lt;p&gt;I have a question about ZFS-Replication, perhaps someone have some infos.&lt;/p&gt;\n\n&lt;p&gt;I have a Main-Storage System with ZFS and ECC RAM, the Second-Storage-System is running ZFS without ECC.&lt;/p&gt;\n\n&lt;p&gt;In my opinion it&amp;#39;s ok to run the second storage system without ECC, because the Main System have.&lt;/p&gt;\n\n&lt;p&gt;The Second System is only for storing zfs-snapshot-replication as Backup&lt;/p&gt;\n\n&lt;p&gt;Since i&amp;#39;m safe against bit root on my main storage after the data are writen to disk there is no chance to transfer corrupted data on my second storage system even without ecc on the second system.&lt;/p&gt;\n\n&lt;p&gt;If i have a bit-root in ram on the second system while replication, zfs would check the different parity and resend it.&lt;/p&gt;\n\n&lt;p&gt;Do I have a flaw in my thinking?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbjjh1", "is_robot_indexable": true, "report_reasons": null, "author": "Vertax1337", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbjjh1/zfs_snapshot_replication_without_ecc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbjjh1/zfs_snapshot_replication_without_ecc/", "subreddit_subscribers": 657402, "created_utc": 1670083236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A friend of mine picked up several hundred 250gb m.2 SSDs in the wake of a business closing down, and I'm trying to think of potential uses for them. Are there any relatively cheap / efficient ways to connect them in a large storage array, RAID or otherwise?", "author_fullname": "t2_6qrxw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to use a large amount of m.2 drives for data storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbwl8p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670116870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A friend of mine picked up several hundred 250gb m.2 SSDs in the wake of a business closing down, and I&amp;#39;m trying to think of potential uses for them. Are there any relatively cheap / efficient ways to connect them in a large storage array, RAID or otherwise?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbwl8p", "is_robot_indexable": true, "report_reasons": null, "author": "atrere", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbwl8p/how_to_use_a_large_amount_of_m2_drives_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbwl8p/how_to_use_a_large_amount_of_m2_drives_for_data/", "subreddit_subscribers": 657402, "created_utc": 1670116870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Now they have blocked my account alleging that I have to renew the service, but renewing is only possible by changing to monthly payments. This looks like scam to me", "author_fullname": "t2_xnkh1lx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zoolz sold me a lifetime license that lasted 3 years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbqhfu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670101089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now they have blocked my account alleging that I have to renew the service, but renewing is only possible by changing to monthly payments. This looks like scam to me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbqhfu", "is_robot_indexable": true, "report_reasons": null, "author": "rlopez7", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbqhfu/zoolz_sold_me_a_lifetime_license_that_lasted_3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbqhfu/zoolz_sold_me_a_lifetime_license_that_lasted_3/", "subreddit_subscribers": 657402, "created_utc": 1670101089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was planning on making a raid 5 array from 2 12 tb hdds and and 1 raid 0 array made from 3 4 tb hdds. The 12tb are 7200 rpm while the 3 \\* 4 hdds are 5400 rpm. Is this possible and more so not stupid? I know raid 10 is basically raid 1 + 0 but I'm not sure if it has to be implemented in a certain way to work. Using mdadm from linux kernel if your wondering. Google wasn't answering my question.", "author_fullname": "t2_1v9ndvba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "can a raid array be made from another raid array?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbny9t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "RAID configuration", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670094748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was planning on making a raid 5 array from 2 12 tb hdds and and 1 raid 0 array made from 3 4 tb hdds. The 12tb are 7200 rpm while the 3 * 4 hdds are 5400 rpm. Is this possible and more so not stupid? I know raid 10 is basically raid 1 + 0 but I&amp;#39;m not sure if it has to be implemented in a certain way to work. Using mdadm from linux kernel if your wondering. Google wasn&amp;#39;t answering my question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zbny9t", "is_robot_indexable": true, "report_reasons": null, "author": "quantumechanicalhose", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbny9t/can_a_raid_array_be_made_from_another_raid_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbny9t/can_a_raid_array_be_made_from_another_raid_array/", "subreddit_subscribers": 657402, "created_utc": 1670094748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Google drive calculates hash of files stored and removes it if it does checks out. Any good cloud service recommendations?", "author_fullname": "t2_su02gr9w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which is the best cloud service for keeping copyright stuff?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zc6gp3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670150187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google drive calculates hash of files stored and removes it if it does checks out. Any good cloud service recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zc6gp3", "is_robot_indexable": true, "report_reasons": null, "author": "LifeIsApparentlyHard", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zc6gp3/which_is_the_best_cloud_service_for_keeping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zc6gp3/which_is_the_best_cloud_service_for_keeping/", "subreddit_subscribers": 657402, "created_utc": 1670150187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Map of 18,237 files. More than 22 years of writing in text files. Plus OneNote to Markdown conversion.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "name": "t3_zbz2ob", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_5u03lo0g", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/0wRYgfL0sWiH27_64zi0ODCb5PGKnYSo0spWcyL7j70.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "ObsidianMD", "selftext": "More than 22 years of writing in text files, plus OneNote to Markdown conversion. Obsidian is a champion.\n\n&amp;#x200B;\n\n[Map of 18,237 files. More than 22 years of writing in text files. Plus OneNote to Markdown conversion.](https://preview.redd.it/dddbj34hbl3a1.png?width=1576&amp;format=png&amp;auto=webp&amp;s=4a9806d682cee92822545c05d0c3fd3e2f6b5d42)\n\nMany of these files were in LaTex, TeX, plain text, or Markdown. I wrote a script to add YAML front matter, then fed them to my Obsidian vault.\n\nObsidian synchronizes between four work computers via GitHub. I can even use Obsidian on my iPad.  I'm particular about having my writing accessible.\n\nVisual Studio Code helps with the cleanup of format and structure. My editor and assistant can access the GitHub repository.\n\nIn DOCX I have another eight thousand articles, marketing campaigns, and research. I will be using pandoc to convert, but I can search for them on Windows.\n\nWhat Obsidian has helped me do is write a book in 26 days. Now it's helping me with diagrams using LaTeX TikZ.\n\nMy workflow identified three prospect books (idea clusters). Obsidian helped me find a 22,000-word manuscript from 1998 plus a related 12-part newsletter series.\n\nI use a Kanban of my Work In Progress (WIP) and Editorial Status to pipeline deliverables. The focus now is finishing publications.\n\nTurns out I've been using Zettelkasten and commonplace books for years. But I won't be migrating my five thousand index cards and file cabinets soon.\n\nYet, I can reference engineering notebooks, card catalogs, and materials using tags. My physical records management already has tags I can search for in Obsidian.\n\nI have subject matter expertise in knowledge management, research methods, and publishing. Are you a knowledge worker who wants to be more productive? **Ask me anything.**", "author_fullname": "t2_5u03lo0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Map of 18,237 files. More than 22 years of writing in text files. Plus OneNote to Markdown conversion.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ObsidianMD", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 85, "top_awarded_type": null, "hide_score": false, "media_metadata": {"dddbj34hbl3a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 65, "x": 108, "u": "https://preview.redd.it/dddbj34hbl3a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ec0eab940de6a595bc386321f44e677396d161f"}, {"y": 131, "x": 216, "u": "https://preview.redd.it/dddbj34hbl3a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=00d90dddd6431b34247cce6234387e9fc6ac457c"}, {"y": 195, "x": 320, "u": "https://preview.redd.it/dddbj34hbl3a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=97e926854763517b2fd09a8ef66367286fc69d8a"}, {"y": 390, "x": 640, "u": "https://preview.redd.it/dddbj34hbl3a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9fd08930d7566bd96c9279fa860105ce5a5bec35"}, {"y": 585, "x": 960, "u": "https://preview.redd.it/dddbj34hbl3a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ed26bd48b8bf0fd90b296ed3d8a0796760c772e2"}, {"y": 658, "x": 1080, "u": "https://preview.redd.it/dddbj34hbl3a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6cf85c22b2943c8b9f486c344c54bd67154530e2"}], "s": {"y": 961, "x": 1576, "u": "https://preview.redd.it/dddbj34hbl3a1.png?width=1576&amp;format=png&amp;auto=webp&amp;s=4a9806d682cee92822545c05d0c3fd3e2f6b5d42"}, "id": "dddbj34hbl3a1"}}, "name": "t3_zb4okr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 114, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 114, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/0wRYgfL0sWiH27_64zi0ODCb5PGKnYSo0spWcyL7j70.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670032787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ObsidianMD", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;More than 22 years of writing in text files, plus OneNote to Markdown conversion. Obsidian is a champion.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dddbj34hbl3a1.png?width=1576&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a9806d682cee92822545c05d0c3fd3e2f6b5d42\"&gt;Map of 18,237 files. More than 22 years of writing in text files. Plus OneNote to Markdown conversion.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Many of these files were in LaTex, TeX, plain text, or Markdown. I wrote a script to add YAML front matter, then fed them to my Obsidian vault.&lt;/p&gt;\n\n&lt;p&gt;Obsidian synchronizes between four work computers via GitHub. I can even use Obsidian on my iPad.  I&amp;#39;m particular about having my writing accessible.&lt;/p&gt;\n\n&lt;p&gt;Visual Studio Code helps with the cleanup of format and structure. My editor and assistant can access the GitHub repository.&lt;/p&gt;\n\n&lt;p&gt;In DOCX I have another eight thousand articles, marketing campaigns, and research. I will be using pandoc to convert, but I can search for them on Windows.&lt;/p&gt;\n\n&lt;p&gt;What Obsidian has helped me do is write a book in 26 days. Now it&amp;#39;s helping me with diagrams using LaTeX TikZ.&lt;/p&gt;\n\n&lt;p&gt;My workflow identified three prospect books (idea clusters). Obsidian helped me find a 22,000-word manuscript from 1998 plus a related 12-part newsletter series.&lt;/p&gt;\n\n&lt;p&gt;I use a Kanban of my Work In Progress (WIP) and Editorial Status to pipeline deliverables. The focus now is finishing publications.&lt;/p&gt;\n\n&lt;p&gt;Turns out I&amp;#39;ve been using Zettelkasten and commonplace books for years. But I won&amp;#39;t be migrating my five thousand index cards and file cabinets soon.&lt;/p&gt;\n\n&lt;p&gt;Yet, I can reference engineering notebooks, card catalogs, and materials using tags. My physical records management already has tags I can search for in Obsidian.&lt;/p&gt;\n\n&lt;p&gt;I have subject matter expertise in knowledge management, research methods, and publishing. Are you a knowledge worker who wants to be more productive? &lt;strong&gt;Ask me anything.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2mz3dr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zb4okr", "is_robot_indexable": true, "report_reasons": null, "author": "jwhco", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ObsidianMD/comments/zb4okr/map_of_18237_files_more_than_22_years_of_writing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/ObsidianMD/comments/zb4okr/map_of_18237_files_more_than_22_years_of_writing/", "subreddit_subscribers": 51431, "created_utc": 1670032787.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1670124107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ObsidianMD", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/ObsidianMD/comments/zb4okr/map_of_18237_files_more_than_22_years_of_writing/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbz2ob", "is_robot_indexable": true, "report_reasons": null, "author": "jwhco", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_zb4okr", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbz2ob/map_of_18237_files_more_than_22_years_of_writing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/ObsidianMD/comments/zb4okr/map_of_18237_files_more_than_22_years_of_writing/", "subreddit_subscribers": 657402, "created_utc": 1670124107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m a code noob.\n\nAside from a few simple concepts I can\u2019t script or code. \n\nI\u2019m wondering if you think it\u2019s feasible to get consistent results with AI generated code.\n\nIt would make my life so much easier. \n\nThis is an example, wdyt?\n\n[Here is an example of a Python script that checks for changes at a URL, and if there are changes, takes a screenshot and saves it to a specific folder:](https://i.imgur.com/fMR00wz.jpg)", "author_fullname": "t2_akf0w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to USE AI to write scripts for data hoarding?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbm58l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670090171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a code noob.&lt;/p&gt;\n\n&lt;p&gt;Aside from a few simple concepts I can\u2019t script or code. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m wondering if you think it\u2019s feasible to get consistent results with AI generated code.&lt;/p&gt;\n\n&lt;p&gt;It would make my life so much easier. &lt;/p&gt;\n\n&lt;p&gt;This is an example, wdyt?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/fMR00wz.jpg\"&gt;Here is an example of a Python script that checks for changes at a URL, and if there are changes, takes a screenshot and saves it to a specific folder:&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?auto=webp&amp;s=bf755b85c339bc6079c1b3a1ae9cc0ce1a5ccc79", "width": 1048, "height": 1710}, "resolutions": [{"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=360ecb919246bc3e0207c69df707e99d0e0246d1", "width": 108, "height": 176}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ea811c86a0739ff9c729ea1ff0cab77f68c7bcf", "width": 216, "height": 352}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=caa8de9baa73d9cbfdb530833192eab9d6014d73", "width": 320, "height": 522}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f120a979a4bc351aa022af6f4215c667b2a73567", "width": 640, "height": 1044}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bdefd051dc61ac6e920b8aa5a20a2b2f6a33fd19", "width": 960, "height": 1566}], "variants": {}, "id": "D56t9ZeJJiAWGiIrwM0_68X5POaYTVxrgStV49UCEIk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbm58l", "is_robot_indexable": true, "report_reasons": null, "author": "badatmathdave", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbm58l/is_it_possible_to_use_ai_to_write_scripts_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbm58l/is_it_possible_to_use_ai_to_write_scripts_for/", "subreddit_subscribers": 657402, "created_utc": 1670090171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all,\n\nI am in the process of backing up a complete music library from a website (+- 10000 albums).  \nHere are always 2 options to download a flac and mp3 version. When the files are being downloaded individually they have always the same name whithout knowing if you have downloaded the flac or mp3 version.\n\nFilename.rar (contains 1 mp3 album)  \nFilename.rar (contains 1 flac album)\n\nby using jdownloader 2 i always download both and append a number\n\nFilename.rar  \nFilename\\_2.rar\n\nBut i still do not know if the archive contains mp3 or flac.\n\nIs there any piece of software that can open the archive, see what extensions the songs have and append this to the filename?\n\nFilename.rar -&gt; Filename\\_flac.rar  \nFilename\\_2.rar -&gt; Filename\\_2\\_mp3.rar\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_85rup3n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Append to archive name based on extension of contents.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbkp39", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670086453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I am in the process of backing up a complete music library from a website (+- 10000 albums).&lt;br/&gt;\nHere are always 2 options to download a flac and mp3 version. When the files are being downloaded individually they have always the same name whithout knowing if you have downloaded the flac or mp3 version.&lt;/p&gt;\n\n&lt;p&gt;Filename.rar (contains 1 mp3 album)&lt;br/&gt;\nFilename.rar (contains 1 flac album)&lt;/p&gt;\n\n&lt;p&gt;by using jdownloader 2 i always download both and append a number&lt;/p&gt;\n\n&lt;p&gt;Filename.rar&lt;br/&gt;\nFilename_2.rar&lt;/p&gt;\n\n&lt;p&gt;But i still do not know if the archive contains mp3 or flac.&lt;/p&gt;\n\n&lt;p&gt;Is there any piece of software that can open the archive, see what extensions the songs have and append this to the filename?&lt;/p&gt;\n\n&lt;p&gt;Filename.rar -&amp;gt; Filename_flac.rar&lt;br/&gt;\nFilename_2.rar -&amp;gt; Filename_2_mp3.rar&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbkp39", "is_robot_indexable": true, "report_reasons": null, "author": "carval444", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbkp39/append_to_archive_name_based_on_extension_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbkp39/append_to_archive_name_based_on_extension_of/", "subreddit_subscribers": 657402, "created_utc": 1670086453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\nI've already searched the threads but it looks like the provided links are now out of date.  \nI need to download a couple books off Scribd, not documents that are user-uploaded, but ebooks Scribd provides. They are too large to download one page at a time without taking too much time. If you know of any ebook rippers for Scribd I would be grateful.\n\nThank you!", "author_fullname": "t2_69i9rgfi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working downloader for Scribd books in late 2022 (not documents)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbhd7s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670077226.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve already searched the threads but it looks like the provided links are now out of date.&lt;br/&gt;\nI need to download a couple books off Scribd, not documents that are user-uploaded, but ebooks Scribd provides. They are too large to download one page at a time without taking too much time. If you know of any ebook rippers for Scribd I would be grateful.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbhd7s", "is_robot_indexable": true, "report_reasons": null, "author": "throw_away_bay_bay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbhd7s/working_downloader_for_scribd_books_in_late_2022/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbhd7s/working_downloader_for_scribd_books_in_late_2022/", "subreddit_subscribers": 657402, "created_utc": 1670077226.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey Guys,I've recently had a weird failure mode on my unraid box and thought I would consult the hive-mind for ideas &amp; thoughts.\n\n**TLDR:**\n\nI think that somehow port/channel 3 on the first port of my LSI 9240-8i is bad. Swapping breakout cable results in the same sata port being bad.\n\nHas anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?\n\n**Key system info:**\n\n* It's been up and happy for over a year.\n* I'm using a LSI 9240-8i in IT mode as an HBA\n* I'm using 3 x 14TB WD drives all attached to a single SAS SFF-8087 to 4x SATA Cable\n* 1 x Parity disk and 2 x data disks\n\n**What happened:**\n\n1. Woke up in the morning to a scheduled parity check having failed. Looking at the logs. It failed not long after it started. The second data disk was marked by unRaid as \"Device disabled, contents emulated\".\n2. Ok, no biggie - the disk has failed. Stopped the array, unassigned the failed disk and shut down. Ordered a replacement.\n3. While waiting for the replacement, I pulled the failed disk and had a look at it with CrystalDiskInfo. Good SMART health and zero errors of any kind. Hmmm.\n4. New disk arrives. Slap it into the machine (same cable as the 'failed' disk) and unraid offers to rebuild the array to it. Click go.\n5. Almost immediately, read errors are reported, rebuild fails and the array is stuck in some kind of read-check paused status. Shit.\n6. Hmm. Maybe it's a bad cable? Shut down and swap the SAS SFF-8087 to 4x SATA Cable for a spare. Plug in both the new drive as well as the 'failed' drive (I suspect the drive itself is actually fine, so I may add it as a second parity later). New drive is on port 3 of the breakout cable (where the failed drive had been) and the 'failed' drive is now on port 4.\n7. Boot up unraid and assign the original 'failed' drive to the array as a new disk. Start the array and unraid starts to rebuild the array with no issues!\n8. Go to my unassigned disks and I can see the new drive. Spin it up and run a SMART test. SMART test fails. Twice. Errors in the log:\n\n&amp;#x200B;\n\n    Dec  3 12:11:06 tower  emhttpd: spinning up /dev/sdd\n    Dec  3 12:11:17 tower  emhttpd: read SMART /dev/sdd\n    Dec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronizing SCSI cache\n    Dec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronize Cache(10) failed: Result: hostbyte=0x01 driverbyte=DRIVER_OK\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 27344764928 512-byte logical blocks: (14.0 TB/12.7 TiB)\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 4096-byte physical blocks\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write Protect is off\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Mode Sense: 7f 00 10 08\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write cache: enabled, read cache: enabled, supports DPO and FUA\n    Dec  3 12:12:53 tower kernel: sdd: sdd1\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Attached SCSI disk\n    Dec  3 12:12:54 tower unassigned.devices: Disk with ID 'TOSHIBA_HDWG21E_Z1A0A0CCFP7G (sdd)' is not set to auto mount.\n\n**Conclusion:**\n\nI think the cable was actually fine and somehow port/channel 3 on the first port of my LSI 9240-8i is somehow bad.\n\nHas anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?", "author_fullname": "t2_ckt3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debugging LSI 9240-8i in IT mode", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbfyvu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670073114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Guys,I&amp;#39;ve recently had a weird failure mode on my unraid box and thought I would consult the hive-mind for ideas &amp;amp; thoughts.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I think that somehow port/channel 3 on the first port of my LSI 9240-8i is bad. Swapping breakout cable results in the same sata port being bad.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key system info:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It&amp;#39;s been up and happy for over a year.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m using a LSI 9240-8i in IT mode as an HBA&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m using 3 x 14TB WD drives all attached to a single SAS SFF-8087 to 4x SATA Cable&lt;/li&gt;\n&lt;li&gt;1 x Parity disk and 2 x data disks&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What happened:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Woke up in the morning to a scheduled parity check having failed. Looking at the logs. It failed not long after it started. The second data disk was marked by unRaid as &amp;quot;Device disabled, contents emulated&amp;quot;.&lt;/li&gt;\n&lt;li&gt;Ok, no biggie - the disk has failed. Stopped the array, unassigned the failed disk and shut down. Ordered a replacement.&lt;/li&gt;\n&lt;li&gt;While waiting for the replacement, I pulled the failed disk and had a look at it with CrystalDiskInfo. Good SMART health and zero errors of any kind. Hmmm.&lt;/li&gt;\n&lt;li&gt;New disk arrives. Slap it into the machine (same cable as the &amp;#39;failed&amp;#39; disk) and unraid offers to rebuild the array to it. Click go.&lt;/li&gt;\n&lt;li&gt;Almost immediately, read errors are reported, rebuild fails and the array is stuck in some kind of read-check paused status. Shit.&lt;/li&gt;\n&lt;li&gt;Hmm. Maybe it&amp;#39;s a bad cable? Shut down and swap the SAS SFF-8087 to 4x SATA Cable for a spare. Plug in both the new drive as well as the &amp;#39;failed&amp;#39; drive (I suspect the drive itself is actually fine, so I may add it as a second parity later). New drive is on port 3 of the breakout cable (where the failed drive had been) and the &amp;#39;failed&amp;#39; drive is now on port 4.&lt;/li&gt;\n&lt;li&gt;Boot up unraid and assign the original &amp;#39;failed&amp;#39; drive to the array as a new disk. Start the array and unraid starts to rebuild the array with no issues!&lt;/li&gt;\n&lt;li&gt;Go to my unassigned disks and I can see the new drive. Spin it up and run a SMART test. SMART test fails. Twice. Errors in the log:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Dec  3 12:11:06 tower  emhttpd: spinning up /dev/sdd\nDec  3 12:11:17 tower  emhttpd: read SMART /dev/sdd\nDec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronizing SCSI cache\nDec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronize Cache(10) failed: Result: hostbyte=0x01 driverbyte=DRIVER_OK\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 27344764928 512-byte logical blocks: (14.0 TB/12.7 TiB)\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 4096-byte physical blocks\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write Protect is off\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Mode Sense: 7f 00 10 08\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write cache: enabled, read cache: enabled, supports DPO and FUA\nDec  3 12:12:53 tower kernel: sdd: sdd1\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Attached SCSI disk\nDec  3 12:12:54 tower unassigned.devices: Disk with ID &amp;#39;TOSHIBA_HDWG21E_Z1A0A0CCFP7G (sdd)&amp;#39; is not set to auto mount.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I think the cable was actually fine and somehow port/channel 3 on the first port of my LSI 9240-8i is somehow bad.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbfyvu", "is_robot_indexable": true, "report_reasons": null, "author": "kabadisha", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbfyvu/debugging_lsi_92408i_in_it_mode/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbfyvu/debugging_lsi_92408i_in_it_mode/", "subreddit_subscribers": 657402, "created_utc": 1670073114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello fellow hoarders! I have a feeling I'm going to use some of these words wrong so please excuse that and feel free to correct me. I've been a data hoarder for as long as I can remember but only recently ran into issues. From what I've learned online, I'm experiencing issues with fragmentation and it's causing my external HDD to be super slow. I'm a graphic designer and I regularly download large compressed folders of multiple file types only to keep one specific format (keeping like 100MB of a 1GB download). It was so bad I couldn't get Windows defrag, Defraggler, or UltraDefrag to get more than 1% defragged in like a 24 hour period. So I moved all the files from my Seagate HDD to my laptop's SSD and formatted my Seagate. I started adding things back and began with one 55GB folder that had 94% fragmentation. Defraggler has been doing a \"quick\" defrag for about 45 minutes and is still at 0%. Here are my questions:\n\n* Should I try transferring stuff over in smaller clusters, like 5GB at a time and then defragging them? \n* How can I prevent data fragging in this circumstance? Prep the download and what not on my laptop and then transfer only the files I was keeping? \n\nThank you so much for taking the time to read this! Have a great weekend :)", "author_fullname": "t2_87yliers", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Defrag tips for large downloads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbzdwn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670125056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow hoarders! I have a feeling I&amp;#39;m going to use some of these words wrong so please excuse that and feel free to correct me. I&amp;#39;ve been a data hoarder for as long as I can remember but only recently ran into issues. From what I&amp;#39;ve learned online, I&amp;#39;m experiencing issues with fragmentation and it&amp;#39;s causing my external HDD to be super slow. I&amp;#39;m a graphic designer and I regularly download large compressed folders of multiple file types only to keep one specific format (keeping like 100MB of a 1GB download). It was so bad I couldn&amp;#39;t get Windows defrag, Defraggler, or UltraDefrag to get more than 1% defragged in like a 24 hour period. So I moved all the files from my Seagate HDD to my laptop&amp;#39;s SSD and formatted my Seagate. I started adding things back and began with one 55GB folder that had 94% fragmentation. Defraggler has been doing a &amp;quot;quick&amp;quot; defrag for about 45 minutes and is still at 0%. Here are my questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Should I try transferring stuff over in smaller clusters, like 5GB at a time and then defragging them? &lt;/li&gt;\n&lt;li&gt;How can I prevent data fragging in this circumstance? Prep the download and what not on my laptop and then transfer only the files I was keeping? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you so much for taking the time to read this! Have a great weekend :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbzdwn", "is_robot_indexable": true, "report_reasons": null, "author": "lezzuhlss", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbzdwn/defrag_tips_for_large_downloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbzdwn/defrag_tips_for_large_downloads/", "subreddit_subscribers": 657402, "created_utc": 1670125056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a handful of directories that I want to easily mirror onto an external drive as a backup. I'm looking for a good GUI program for setting up a list of directories and where to mirror them on another drive.\n\nI've been using SyncToy for years, and it does still mostly work, but occasionally I hit cases where it doesn't work as well as I'd like. E.g. I got a new external and had to recreate each paired folders rather than being able to reuse my existing sets, even though the paths were the same.\n\nAny recommendations?", "author_fullname": "t2_fvtdwa50", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good GUI utility for Windows for mirroring data onto an external?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbz6zx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670124485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a handful of directories that I want to easily mirror onto an external drive as a backup. I&amp;#39;m looking for a good GUI program for setting up a list of directories and where to mirror them on another drive.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using SyncToy for years, and it does still mostly work, but occasionally I hit cases where it doesn&amp;#39;t work as well as I&amp;#39;d like. E.g. I got a new external and had to recreate each paired folders rather than being able to reuse my existing sets, even though the paths were the same.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbz6zx", "is_robot_indexable": true, "report_reasons": null, "author": "FeistyThunderhorse", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbz6zx/good_gui_utility_for_windows_for_mirroring_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbz6zx/good_gui_utility_for_windows_for_mirroring_data/", "subreddit_subscribers": 657402, "created_utc": 1670124485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just got an crucial P3 m.2 ssd and a Sabrent enclosure. I haven't written any data to it. Why is my read speed half my write speed? I have it plugged in directly to an m1 macbook pro.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/bb79nmahur3a1.png?width=1404&amp;format=png&amp;auto=webp&amp;s=9b9a8dc651e9bb59afe28ea30744a4daea316d3f\n\n[https://www.amazon.com/gp/product/B0B25MJ1YT/ref=ppx\\_yo\\_dt\\_b\\_asin\\_title\\_o00\\_s00?ie=UTF8&amp;th=1](https://www.amazon.com/gp/product/B0B25MJ1YT/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;th=1)\n\n[https://www.amazon.com/gp/product/B08RVC6F9Y/ref=ppx\\_yo\\_dt\\_b\\_asin\\_title\\_o00\\_s01?ie=UTF8&amp;th=1](https://www.amazon.com/gp/product/B08RVC6F9Y/ref=ppx_yo_dt_b_asin_title_o00_s01?ie=UTF8&amp;th=1)", "author_fullname": "t2_kpi41e3e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External SSD read speed half of write speed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"bb79nmahur3a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 112, "x": 108, "u": "https://preview.redd.it/bb79nmahur3a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4867ec2aa0647e16ccb7e0abd366dd5e5913e8de"}, {"y": 225, "x": 216, "u": "https://preview.redd.it/bb79nmahur3a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2570434641f9a5ecc2a6fc50aa2d7443bd404914"}, {"y": 334, "x": 320, "u": "https://preview.redd.it/bb79nmahur3a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c1268b4a1ce37fb1f1e504fe972185f710722ecb"}, {"y": 669, "x": 640, "u": "https://preview.redd.it/bb79nmahur3a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2bf19dd7c2e514e25b3c312435cf43d8da7483e2"}, {"y": 1003, "x": 960, "u": "https://preview.redd.it/bb79nmahur3a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=076425e351810ffa063e4410b31b7bd83e0fc6dc"}, {"y": 1129, "x": 1080, "u": "https://preview.redd.it/bb79nmahur3a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eaa8cc74d44498bbc1b5674376ac13abb2900153"}], "s": {"y": 1468, "x": 1404, "u": "https://preview.redd.it/bb79nmahur3a1.png?width=1404&amp;format=png&amp;auto=webp&amp;s=9b9a8dc651e9bb59afe28ea30744a4daea316d3f"}, "id": "bb79nmahur3a1"}}, "name": "t3_zbusfx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HZ8lt1NfrYSiLzwOAdZthY6V79oGtmziT0ImcgKQolc.jpg", "edited": 1670112125.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670111887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just got an crucial P3 m.2 ssd and a Sabrent enclosure. I haven&amp;#39;t written any data to it. Why is my read speed half my write speed? I have it plugged in directly to an m1 macbook pro.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bb79nmahur3a1.png?width=1404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9b9a8dc651e9bb59afe28ea30744a4daea316d3f\"&gt;https://preview.redd.it/bb79nmahur3a1.png?width=1404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9b9a8dc651e9bb59afe28ea30744a4daea316d3f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/gp/product/B0B25MJ1YT/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;amp;th=1\"&gt;https://www.amazon.com/gp/product/B0B25MJ1YT/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;amp;th=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/gp/product/B08RVC6F9Y/ref=ppx_yo_dt_b_asin_title_o00_s01?ie=UTF8&amp;amp;th=1\"&gt;https://www.amazon.com/gp/product/B08RVC6F9Y/ref=ppx_yo_dt_b_asin_title_o00_s01?ie=UTF8&amp;amp;th=1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbusfx", "is_robot_indexable": true, "report_reasons": null, "author": "_Emoji_Man", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbusfx/external_ssd_read_speed_half_of_write_speed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbusfx/external_ssd_read_speed_half_of_write_speed/", "subreddit_subscribers": 657402, "created_utc": 1670111887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a complete beginner when it comes to technology, and I really need your help because I genuinely don't understand specs.\n\nI'm getting an external hard drive for a laptop that doesn't have room for an internal hard drive. I'm trying to pick the most reliable and best of them.\n\n1. [Basic Portable Drive 2 TB](https://www.seagate.com/gb/en/products/external-hard-drives/basic-external-hard-drive/)\n2. [Toshiba Canvio Ready](https://www.toshiba-storage.com/products/toshiba-portable-hard-drives-canvio-ready/)\n3. [Toshiba Canvio Basics](https://storage.toshiba.com/consumer-hdd/external/canvio-basics)\n\nFinally, I have one more option - buying this [docking station](https://gembird.com/item.aspx?id=9536) to connect one of the internal HDDs (links below) to my laptop - I'm not sure if this is a good alternative or if it's reliable at all. \n\n1. [Toshiba P300 Desktop PC](https://www.toshiba-storage.com/products/toshiba-internal-hard-drives-p300/)\n2. [WD Blue PC Desktop Hard Drive](https://www.westerndigital.com/products/internal-drives/wd-blue-desktop-sata-hdd#WD20EZBX)\n\n**I apologize if this is a stupid and bothersome question**, but as a girl (who knows very little about tech and) is looking for a good photo and video storage solution, I hope you won't mind helping me with this.", "author_fullname": "t2_kh69bbbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which one of these is the best photo storage solution? Thank you in advance.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbqeu6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.48, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670100896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a complete beginner when it comes to technology, and I really need your help because I genuinely don&amp;#39;t understand specs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m getting an external hard drive for a laptop that doesn&amp;#39;t have room for an internal hard drive. I&amp;#39;m trying to pick the most reliable and best of them.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.seagate.com/gb/en/products/external-hard-drives/basic-external-hard-drive/\"&gt;Basic Portable Drive 2 TB&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.toshiba-storage.com/products/toshiba-portable-hard-drives-canvio-ready/\"&gt;Toshiba Canvio Ready&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://storage.toshiba.com/consumer-hdd/external/canvio-basics\"&gt;Toshiba Canvio Basics&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Finally, I have one more option - buying this &lt;a href=\"https://gembird.com/item.aspx?id=9536\"&gt;docking station&lt;/a&gt; to connect one of the internal HDDs (links below) to my laptop - I&amp;#39;m not sure if this is a good alternative or if it&amp;#39;s reliable at all. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.toshiba-storage.com/products/toshiba-internal-hard-drives-p300/\"&gt;Toshiba P300 Desktop PC&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.westerndigital.com/products/internal-drives/wd-blue-desktop-sata-hdd#WD20EZBX\"&gt;WD Blue PC Desktop Hard Drive&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;I apologize if this is a stupid and bothersome question&lt;/strong&gt;, but as a girl (who knows very little about tech and) is looking for a good photo and video storage solution, I hope you won&amp;#39;t mind helping me with this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbqeu6", "is_robot_indexable": true, "report_reasons": null, "author": "the-emotional-emu", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbqeu6/which_one_of_these_is_the_best_photo_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbqeu6/which_one_of_these_is_the_best_photo_storage/", "subreddit_subscribers": 657402, "created_utc": 1670100896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyone have a process they use to sort 1000s of unsorted photos? Cosplay rip downloaded from a site is like 4000 photos all in one folder. I'd like to get all the photos organized into the character\\\\shoot\\\\etc. Other than manually going through and moving to folder anyone have a program, app, script or something they are aware of which can determine similar photos (using AI maybe) and sort them?\n\nThanks", "author_fullname": "t2_1f6ljebz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Program\\Process to sort a bunch of unsorted photos.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbomv1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670096483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have a process they use to sort 1000s of unsorted photos? Cosplay rip downloaded from a site is like 4000 photos all in one folder. I&amp;#39;d like to get all the photos organized into the character\\shoot\\etc. Other than manually going through and moving to folder anyone have a program, app, script or something they are aware of which can determine similar photos (using AI maybe) and sort them?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbomv1", "is_robot_indexable": true, "report_reasons": null, "author": "basicallyahurricane", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbomv1/programprocess_to_sort_a_bunch_of_unsorted_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbomv1/programprocess_to_sort_a_bunch_of_unsorted_photos/", "subreddit_subscribers": 657402, "created_utc": 1670096483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all! I recently filled up my 40tb box that I keep in my local DC (1Gb connection from house to DC) and don\u2019t have room for more drives so I ended up buying another box with the exact same hardware for another 40tb but isn\u2019t in the same rack.\n\nDoes anybody know how to go about making 1 80tb volume? I\u2019d rather not pass the drives directly to the 1st host due to wanting to utilize the second host for processing power along with storage. \n\nI\u2019m using Windows Server Datacenter 2019 for my host OS (yes yes I know, it\u2019s just so easy to join them to my ADDC and create permissions extremely easy).\n\nMy initial thought was to use iscsi to pass the drives over a dedicated 1Gb link (no connection to the host would be more than 1Gb anyway) and just add them to the pool that way. Unless you guys have a better solution? Was thinking maybe clustering but it seems that you only get mirroring and not striping.", "author_fullname": "t2_xkr04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good way to pool storage across 2 physical boxes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbjweh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670084210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all! I recently filled up my 40tb box that I keep in my local DC (1Gb connection from house to DC) and don\u2019t have room for more drives so I ended up buying another box with the exact same hardware for another 40tb but isn\u2019t in the same rack.&lt;/p&gt;\n\n&lt;p&gt;Does anybody know how to go about making 1 80tb volume? I\u2019d rather not pass the drives directly to the 1st host due to wanting to utilize the second host for processing power along with storage. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m using Windows Server Datacenter 2019 for my host OS (yes yes I know, it\u2019s just so easy to join them to my ADDC and create permissions extremely easy).&lt;/p&gt;\n\n&lt;p&gt;My initial thought was to use iscsi to pass the drives over a dedicated 1Gb link (no connection to the host would be more than 1Gb anyway) and just add them to the pool that way. Unless you guys have a better solution? Was thinking maybe clustering but it seems that you only get mirroring and not striping.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbjweh", "is_robot_indexable": true, "report_reasons": null, "author": "Kawaiisampler", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zbjweh/any_good_way_to_pool_storage_across_2_physical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbjweh/any_good_way_to_pool_storage_across_2_physical/", "subreddit_subscribers": 657402, "created_utc": 1670084210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Gurus , I over-voltage the WD elements by plugging plug meant for laptop \n\nDismantled the drive from the enclosure and found the suspected component is burnt . \n\nJust wanna confirm if the burnt part is TVS Diode . \n\nThanks", "author_fullname": "t2_u547j6h5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help to identify WD 14TB HDD PCB burnt component", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zc18q8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670130840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Gurus , I over-voltage the WD elements by plugging plug meant for laptop &lt;/p&gt;\n\n&lt;p&gt;Dismantled the drive from the enclosure and found the suspected component is burnt . &lt;/p&gt;\n\n&lt;p&gt;Just wanna confirm if the burnt part is TVS Diode . &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zc18q8", "is_robot_indexable": true, "report_reasons": null, "author": "rextan123", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zc18q8/help_to_identify_wd_14tb_hdd_pcb_burnt_component/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zc18q8/help_to_identify_wd_14tb_hdd_pcb_burnt_component/", "subreddit_subscribers": 657402, "created_utc": 1670130840.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}