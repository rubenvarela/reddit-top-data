{"kind": "Listing", "data": {"after": "t3_zbzdwn", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reminder to backup up your data! 5 month old ADATA SSD Failure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": false, "name": "t3_zbsc58", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 185, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_odtvi", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 185, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/c6ZFKQ7JVOCF0jg0AO52CeWpTuzSk6hq8KQ7B4KP-WI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "pcmasterrace", "selftext": "", "author_fullname": "t2_odtvi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Reminder to backup up your data! 5 month old ADATA SSD Failure", "link_flair_richtext": [{"e": "text", "t": "NSFMR"}], "subreddit_name_prefixed": "r/pcmasterrace", "hidden": false, "pwls": 6, "link_flair_css_class": "red", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": false, "media_metadata": {"8gh0drxx8r3a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 68, "x": 108, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=467c6b9aeca1853aae4afedede89671e468ed6d9"}, {"y": 136, "x": 216, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de01e9c1b36b463b62456e3f6571c69151ed6d86"}, {"y": 201, "x": 320, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f1b4050501da4df186d3012704b967e4e62dd3af"}, {"y": 403, "x": 640, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d652b57fb88f89a47060aa6eba4ca090446bae86"}, {"y": 605, "x": 960, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7e154adf575548c1e8c7519e94c81a299ce197a"}, {"y": 680, "x": 1080, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=23ca023ae7e4d2b5db23023fbfef46af21ca4596"}], "s": {"y": 2199, "x": 3489, "u": "https://preview.redd.it/8gh0drxx8r3a1.jpg?width=3489&amp;format=pjpg&amp;auto=webp&amp;s=c664512290e4e874b038cee82d88d4f9dcfd0230"}, "id": "8gh0drxx8r3a1"}, "8li9yryx8r3a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 76, "x": 108, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0be58dc406ebfc3060a3b9950f96e04919d0f5f9"}, {"y": 152, "x": 216, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be2d802ea06dd13e01a688d864087906c7384695"}, {"y": 225, "x": 320, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ca8f5f1e5338faf6c176691cae54983c121e3f1"}, {"y": 451, "x": 640, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=013bd21e2dfd6141f53c776893291f2a47810d75"}, {"y": 676, "x": 960, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ed35fba989543c53543f759f175d6809095e48e5"}, {"y": 761, "x": 1080, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=901c4629a266928d0c3fd2d9432723ce29d7b28a"}], "s": {"y": 2816, "x": 3994, "u": "https://preview.redd.it/8li9yryx8r3a1.jpg?width=3994&amp;format=pjpg&amp;auto=webp&amp;s=e6ffe2ff213f2a61014d949bed019d450321bc8b"}, "id": "8li9yryx8r3a1"}, "f2j1efzx8r3a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8fafd849d4e87690af25a3769c71b1f72def8ef2"}, {"y": 125, "x": 216, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c251b4eefd6ea98fe5354c45efabde285db131f"}, {"y": 185, "x": 320, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=04806382b74b637dfead07d632ab90bb75838866"}, {"y": 371, "x": 640, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=957edbc397f6e587bf129efa4c7ee879e5706c89"}, {"y": 557, "x": 960, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=96053e9b4b138e52ae0c2580172abe216ff7d017"}, {"y": 626, "x": 1080, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6966b64aee329d93f347e4d7dd93d4878c1ed8dc"}], "s": {"y": 1998, "x": 3442, "u": "https://preview.redd.it/f2j1efzx8r3a1.jpg?width=3442&amp;format=pjpg&amp;auto=webp&amp;s=61b590496d3fec4ba0778b279e24bce32710051d"}, "id": "f2j1efzx8r3a1"}, "lak43zxx8r3a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 64, "x": 108, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f2d1e9403f49050ecca445266abe5ac94904c2c"}, {"y": 129, "x": 216, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a701d655f6550ade97f26713e9fc9044c5e31448"}, {"y": 192, "x": 320, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=be4d9aa835c8c9524ae1a0abb2d0b98bf37996e0"}, {"y": 384, "x": 640, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4cb99076471feb6426c41481707e01db5d707dc4"}, {"y": 576, "x": 960, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=52607c9a610d5afd13f5c7beebb3315abc09fa2a"}, {"y": 648, "x": 1080, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8dc7600018490b28d77cf29fd89d18cd3182a625"}], "s": {"y": 2056, "x": 3425, "u": "https://preview.redd.it/lak43zxx8r3a1.jpg?width=3425&amp;format=pjpg&amp;auto=webp&amp;s=58ec36c8ad0a08a882e379bae5ad68e1c9d88060"}, "id": "lak43zxx8r3a1"}}, "name": "t3_zbs22g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "That's a lot of bad blocks", "media_id": "8gh0drxx8r3a1", "id": 215627376}, {"media_id": "8li9yryx8r3a1", "id": 215627377}, {"caption": "Even the log file is corrupt!", "media_id": "lak43zxx8r3a1", "id": 215627378}, {"media_id": "f2j1efzx8r3a1", "id": 215627379}]}, "link_flair_text": "NSFMR", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/c6ZFKQ7JVOCF0jg0AO52CeWpTuzSk6hq8KQ7B4KP-WI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670104896.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zbs22g", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4c6e9074-3133-11e4-b380-12313b12e896", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sgp1", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff0000", "id": "zbs22g", "is_robot_indexable": true, "report_reasons": null, "author": "hboyd2003", "discussion_type": null, "num_comments": 16, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/pcmasterrace/comments/zbs22g/reminder_to_backup_up_your_data_5_month_old_adata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zbs22g", "subreddit_subscribers": 6899867, "created_utc": 1670104896.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1670105590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zbs22g", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbsc58", "is_robot_indexable": true, "report_reasons": null, "author": "hboyd2003", "discussion_type": null, "num_comments": 48, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_zbs22g", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbsc58/reminder_to_backup_up_your_data_5_month_old_adata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zbs22g", "subreddit_subscribers": 657386, "created_utc": 1670105590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I\u2019m trying to download a lot of YouTube videos in huge playlist. I have a really fast internet (5gbit/s), but the softwares that I tried (4K video downloaded and Open Video Downloader) are slow, like 3 MB/s for 4k video download and 1MB/s for Oen video downloader. I founded some online websites with a lot of stupid ads, like https://x2download.app/ , that download at a really fast speed, but they aren\u2019t good for download more than few videos at once. What do you use? I have both windows, Linux and Mac.", "author_fullname": "t2_b80xl4j2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best software for download YouTube videos and playlist in mass", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbn1ke", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 82, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 82, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670092438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019m trying to download a lot of YouTube videos in huge playlist. I have a really fast internet (5gbit/s), but the softwares that I tried (4K video downloaded and Open Video Downloader) are slow, like 3 MB/s for 4k video download and 1MB/s for Oen video downloader. I founded some online websites with a lot of stupid ads, like &lt;a href=\"https://x2download.app/\"&gt;https://x2download.app/&lt;/a&gt; , that download at a really fast speed, but they aren\u2019t good for download more than few videos at once. What do you use? I have both windows, Linux and Mac.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bBZtnOZdA4YFrPl8m5wpdkUIGtT2JyV2kLHwlKLfkI8.jpg?auto=webp&amp;s=2ed9d33e203acca30d6a9235bb3f536609a105df", "width": 496, "height": 251}, "resolutions": [{"url": "https://external-preview.redd.it/bBZtnOZdA4YFrPl8m5wpdkUIGtT2JyV2kLHwlKLfkI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b73398f420d1ea825f5cb7c61acd8e4b921825d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/bBZtnOZdA4YFrPl8m5wpdkUIGtT2JyV2kLHwlKLfkI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e505c0a39a37c1deb12ef8cc438f9e5e1ce6d39b", "width": 216, "height": 109}, {"url": "https://external-preview.redd.it/bBZtnOZdA4YFrPl8m5wpdkUIGtT2JyV2kLHwlKLfkI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b0758c1f43eae0dd0e9bd3287520e17df2bd82b", "width": 320, "height": 161}], "variants": {}, "id": "RzHaQHhYQQZo_4fJo6oQXmSdOd-gIK8_CoINdfSoBAA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbn1ke", "is_robot_indexable": true, "report_reasons": null, "author": "StrengthLocal2543", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbn1ke/best_software_for_download_youtube_videos_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbn1ke/best_software_for_download_youtube_videos_and/", "subreddit_subscribers": 657386, "created_utc": 1670092438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4pdfnevq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It ain\u2019t much but it\u2019s my first big boy storage drive. Question: I always run a full error scan on new HDD\u2019s with HD Tune, is this necessary to ensure a full working drive or is it overkill? Should I just check SMART attributes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 131, "top_awarded_type": null, "hide_score": false, "name": "t3_zbwgvx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7ft42o2OWVVk0FtqVgueHnuiThbRe9UxVA7XKluRyGs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670116527.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zw7rk3gwpt3a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?auto=webp&amp;s=e97b5aafd00bd941ba02381a65fc914e7bc9bf64", "width": 3230, "height": 3023}, "resolutions": [{"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f159d24e45a01001664e369166245b9e4a772a46", "width": 108, "height": 101}, {"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d7ab40bcc0d6c048d16a84f1610e70a25fdf98a", "width": 216, "height": 202}, {"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=62a8adb7a082bc89cc7e6a0a733b92afcd27a1fd", "width": 320, "height": 299}, {"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e345b8f89e9e3a68afd68fe4b5f4df13279f5fb1", "width": 640, "height": 598}, {"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ff5ce3629084f5b39ef48b839f8c5daefe8c7d5", "width": 960, "height": 898}, {"url": "https://preview.redd.it/zw7rk3gwpt3a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4e74645d34dbbf9ba54ec09955c1ec780f4f18c1", "width": 1080, "height": 1010}], "variants": {}, "id": "Gp4u_8kPc6aAjHGiIxzVH_2Y6Ej3bwlFsPyeOTLCAzQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbwgvx", "is_robot_indexable": true, "report_reasons": null, "author": "Valor_X", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbwgvx/it_aint_much_but_its_my_first_big_boy_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zw7rk3gwpt3a1.jpg", "subreddit_subscribers": 657386, "created_utc": 1670116527.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have around 5TB worth of data that I want to consolidate into one place and want to know how would I go around and do such a thing.   \nShort backstory, I had a system with 2 drives, a 500GB SSD and 2TB HDD. Bought a 1TB HDD during start of the pandemic for recording location of recorded meetings. Year 1 into Pandemic, 2TB HDD has an error and on a \"Caution\" status from CrystalDiskInfo. I was also planning to build a new rig that year so I bought one 1TB NVME SSD(not part of the situation), a 2TB Ironwolf HDD and 4TB Barracuda.  \nI didn't want to use my new drives on the PC so I used two 1TB External HDDs to store the data. Then a year later the 1TB I bought at the start of the pandemic was in \"Bad\" condition, so I gave in and plugged my 4TB HDD and transferred my files. For some reason I also plugged in the 2TB HDD. Now all of them are in use.  \n\n\nSo I did a disk analyze using WizTree and found out that I have half of my total storage full, not to mention earlier in the day I got an error that there was not enough space in my boot drive when I was saving stuff into there.  \n\n\nNow how would I go around completely copying over these files into a whole separate drive/drives, eliminating duplicates and make sure everything is copied properly, so that I can completely wipe my current system and basically start from scratch.  \n I would also like to ask on what kind of storage I should use to store this on. I think it will be just a cold storage, with the occasional rummage through, but it might also be another dump place I can continue adding to. I don't want to delete any of the files, more of its a hassle rather than sentimental stuff, but there are stuff that I definitely want to keep there.  I would also say that I don't have much money to put towards storing these. So I guess a solution where I can add on over time would be best.", "author_fullname": "t2_mu7c9au", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would I go about copying around 5TB worth of data, from multiple drives to a singular drive/drives (Shared Pools/Raid)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbhmth", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670077996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have around 5TB worth of data that I want to consolidate into one place and want to know how would I go around and do such a thing.&lt;br/&gt;\nShort backstory, I had a system with 2 drives, a 500GB SSD and 2TB HDD. Bought a 1TB HDD during start of the pandemic for recording location of recorded meetings. Year 1 into Pandemic, 2TB HDD has an error and on a &amp;quot;Caution&amp;quot; status from CrystalDiskInfo. I was also planning to build a new rig that year so I bought one 1TB NVME SSD(not part of the situation), a 2TB Ironwolf HDD and 4TB Barracuda.&lt;br/&gt;\nI didn&amp;#39;t want to use my new drives on the PC so I used two 1TB External HDDs to store the data. Then a year later the 1TB I bought at the start of the pandemic was in &amp;quot;Bad&amp;quot; condition, so I gave in and plugged my 4TB HDD and transferred my files. For some reason I also plugged in the 2TB HDD. Now all of them are in use.  &lt;/p&gt;\n\n&lt;p&gt;So I did a disk analyze using WizTree and found out that I have half of my total storage full, not to mention earlier in the day I got an error that there was not enough space in my boot drive when I was saving stuff into there.  &lt;/p&gt;\n\n&lt;p&gt;Now how would I go around completely copying over these files into a whole separate drive/drives, eliminating duplicates and make sure everything is copied properly, so that I can completely wipe my current system and basically start from scratch.&lt;br/&gt;\n I would also like to ask on what kind of storage I should use to store this on. I think it will be just a cold storage, with the occasional rummage through, but it might also be another dump place I can continue adding to. I don&amp;#39;t want to delete any of the files, more of its a hassle rather than sentimental stuff, but there are stuff that I definitely want to keep there.  I would also say that I don&amp;#39;t have much money to put towards storing these. So I guess a solution where I can add on over time would be best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbhmth", "is_robot_indexable": true, "report_reasons": null, "author": "InfraDelta", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbhmth/how_would_i_go_about_copying_around_5tb_worth_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbhmth/how_would_i_go_about_copying_around_5tb_worth_of/", "subreddit_subscribers": 657386, "created_utc": 1670077996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "On windows explorer when I go searching through my hdd, theres a long pause on every folder as it scans and builds a directory file tree from scratch every single time. \n\nThese folders haven\u2019t changed in months, maybe years, theres no need to rebuild it so often.\n\nIs there a software / hardware combo that will place all the filenames and directories on a dedicated NVME for instant browsing? And then do a minor delta update as things change?\n\nI\u2019m ok with the actual hdd files themselves being slow and taking a while to load.\n\nThanks in advance", "author_fullname": "t2_3mjejfmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software/hardware to cache filenames &amp; directories for instantaneous windows explorer browsing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbx5wq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670119702.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670118561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On windows explorer when I go searching through my hdd, theres a long pause on every folder as it scans and builds a directory file tree from scratch every single time. &lt;/p&gt;\n\n&lt;p&gt;These folders haven\u2019t changed in months, maybe years, theres no need to rebuild it so often.&lt;/p&gt;\n\n&lt;p&gt;Is there a software / hardware combo that will place all the filenames and directories on a dedicated NVME for instant browsing? And then do a minor delta update as things change?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m ok with the actual hdd files themselves being slow and taking a while to load.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbx5wq", "is_robot_indexable": true, "report_reasons": null, "author": "freshairproject", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbx5wq/softwarehardware_to_cache_filenames_directories/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbx5wq/softwarehardware_to_cache_filenames_directories/", "subreddit_subscribers": 657386, "created_utc": 1670118561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI'm planning a 6 drive (storage) truenas build + 1 ssd for the OS drive.\n\nMy mobo has 6 sata ports and so consequently, I'm one short. \n\nTo utilise my 6 sata ports for the 6 data drives. I understand there is the option to boot the OS from USB (converted to Sata). Alternatively, I could look to get an HBA card (pci express --&gt; sata) but hear there are compatibility issues via this route. If you know of a good HBA card thats not too expensive, I'd love to hear.\n\nAny guidance here would be appreciated!", "author_fullname": "t2_d2ja9fnm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "6 drive nas - HDA card question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbowdf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670097168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning a 6 drive (storage) truenas build + 1 ssd for the OS drive.&lt;/p&gt;\n\n&lt;p&gt;My mobo has 6 sata ports and so consequently, I&amp;#39;m one short. &lt;/p&gt;\n\n&lt;p&gt;To utilise my 6 sata ports for the 6 data drives. I understand there is the option to boot the OS from USB (converted to Sata). Alternatively, I could look to get an HBA card (pci express --&amp;gt; sata) but hear there are compatibility issues via this route. If you know of a good HBA card thats not too expensive, I&amp;#39;d love to hear.&lt;/p&gt;\n\n&lt;p&gt;Any guidance here would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbowdf", "is_robot_indexable": true, "report_reasons": null, "author": "Maximum-Warning-4186", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbowdf/6_drive_nas_hda_card_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbowdf/6_drive_nas_hda_card_question/", "subreddit_subscribers": 657386, "created_utc": 1670097168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought HP LTO-5 Ultrium 3280 FC drive from eBay and have to upgrade firmware.\nBut this file is locked to download and I don't have permission to download it.\nThe URL is; https://support.hpe.com/connect/s/softwaredetails?softwareId=co_154832_1\nDoes anyone help me? Thanks in advance!", "author_fullname": "t2_jnn18zst", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download locked file on HPE support center?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zbk72l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3Hv7ABYSZFObeFGJR-JMYdzL_xKFatrJnNldPXy8jzw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670085046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought HP LTO-5 Ultrium 3280 FC drive from eBay and have to upgrade firmware.\nBut this file is locked to download and I don&amp;#39;t have permission to download it.\nThe URL is; &lt;a href=\"https://support.hpe.com/connect/s/softwaredetails?softwareId=co_154832_1\"&gt;https://support.hpe.com/connect/s/softwaredetails?softwareId=co_154832_1&lt;/a&gt;\nDoes anyone help me? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/6qfgwoea4r3a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?auto=webp&amp;s=56ffe1538c200c8efe74db845f208432839f4867", "width": 1440, "height": 2862}, "resolutions": [{"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=36eba71941689b950e430c7a47313759d96cd4fb", "width": 108, "height": 214}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=01d491c9f50728ff74aaf9a41190c2ef49ff26bd", "width": 216, "height": 429}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=98610143610cf4095d700a321143e58803ac6143", "width": 320, "height": 636}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e2fcc45c42d717f4fdc84e09a7c88d3c17d898e", "width": 640, "height": 1272}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8a83b58f39ac6f150fd45247b554e4e611a59b2", "width": 960, "height": 1908}, {"url": "https://preview.redd.it/6qfgwoea4r3a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fb16bce58540c7884d17bdf07670b1391b04b42", "width": 1080, "height": 2146}], "variants": {}, "id": "mNKFwNb_IuvqdSRmN5qqHuJNjmhyCAGfJ7ysIVhwghU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbk72l", "is_robot_indexable": true, "report_reasons": null, "author": "AMDRadeonHD6950", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbk72l/how_to_download_locked_file_on_hpe_support_center/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/6qfgwoea4r3a1.png", "subreddit_subscribers": 657386, "created_utc": 1670085046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I've did some research, but rather ask here outright.\n\nThe worry is the availability of a reader 10 years down the line in order to transfer the data onto something newer.\n\nThe issue is I'm on budget and I just can't buy a NAS with data redundancy RAID and stuff right now. About half if not more of my data isn't changing so it might work if I go with write once disks.\n\nIf I do choose this option ... I have lots of HDDs already, I'd like to know some more details about Blu-Ray in terms of reliability, like DVDs would randomly get bad sectors no matter how well you cared about them (or maybe I just had cheap disks?) I would get 1 broken disk per 10 disks in a brand new fresh DVD pack.", "author_fullname": "t2_2hsxrobx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blu-Ray Disk as Data Archival?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbchcf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670059873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;ve did some research, but rather ask here outright.&lt;/p&gt;\n\n&lt;p&gt;The worry is the availability of a reader 10 years down the line in order to transfer the data onto something newer.&lt;/p&gt;\n\n&lt;p&gt;The issue is I&amp;#39;m on budget and I just can&amp;#39;t buy a NAS with data redundancy RAID and stuff right now. About half if not more of my data isn&amp;#39;t changing so it might work if I go with write once disks.&lt;/p&gt;\n\n&lt;p&gt;If I do choose this option ... I have lots of HDDs already, I&amp;#39;d like to know some more details about Blu-Ray in terms of reliability, like DVDs would randomly get bad sectors no matter how well you cared about them (or maybe I just had cheap disks?) I would get 1 broken disk per 10 disks in a brand new fresh DVD pack.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbchcf", "is_robot_indexable": true, "report_reasons": null, "author": "Sloperon", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbchcf/bluray_disk_as_data_archival/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbchcf/bluray_disk_as_data_archival/", "subreddit_subscribers": 657386, "created_utc": 1670059873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A friend of mine picked up several hundred 250gb m.2 SSDs in the wake of a business closing down, and I'm trying to think of potential uses for them. Are there any relatively cheap / efficient ways to connect them in a large storage array, RAID or otherwise?", "author_fullname": "t2_6qrxw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to use a large amount of m.2 drives for data storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbwl8p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670116870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A friend of mine picked up several hundred 250gb m.2 SSDs in the wake of a business closing down, and I&amp;#39;m trying to think of potential uses for them. Are there any relatively cheap / efficient ways to connect them in a large storage array, RAID or otherwise?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbwl8p", "is_robot_indexable": true, "report_reasons": null, "author": "atrere", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbwl8p/how_to_use_a_large_amount_of_m2_drives_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbwl8p/how_to_use_a_large_amount_of_m2_drives_for_data/", "subreddit_subscribers": 657386, "created_utc": 1670116870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I saw that pixiv is changing their content policy and I assume that means a lot of stuff is about to be deleted. If I want a good backup or content rip of the site, what's the best way to do that? Otherwise, if anyone has a recent backup, are you willing to share or explain how you made it?\n\nedit:\n\n[Announcement and request regarding updates to Service Master Terms of Use for certain transactions](https://www.pixiv.net/info.php?id=8788&amp;lang=en)\n\n[Notice of revision of Service Master Terms of Use and establishment of \"Prohibited Items\" and \"Items Requiring Revision\"](https://www.pixiv.net/info.php?id=8872&amp;lang=en)", "author_fullname": "t2_3ov1i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pixiv looks to be wiping a lot of content soon, what's the best way to go about creating a site rip or backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbve3r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670132621.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670113481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw that pixiv is changing their content policy and I assume that means a lot of stuff is about to be deleted. If I want a good backup or content rip of the site, what&amp;#39;s the best way to do that? Otherwise, if anyone has a recent backup, are you willing to share or explain how you made it?&lt;/p&gt;\n\n&lt;p&gt;edit:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.pixiv.net/info.php?id=8788&amp;amp;lang=en\"&gt;Announcement and request regarding updates to Service Master Terms of Use for certain transactions&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.pixiv.net/info.php?id=8872&amp;amp;lang=en\"&gt;Notice of revision of Service Master Terms of Use and establishment of &amp;quot;Prohibited Items&amp;quot; and &amp;quot;Items Requiring Revision&amp;quot;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?auto=webp&amp;s=6f770ce6f634848bbe711506f48af405cbf5d106", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad1f3e2162a25fbc67d62e2459a763085a4fc757", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4eab161ae404f1d1c43a6dd2ff2e15d3450a8a0c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0d4405a7e3d730eba191b631a18dfa2869fdf19", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9656bfb90f7e3f6be767b1edf16d6de4f3975279", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6372f3280b08409bc871cb0cae1dc66b0b95dcc0", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/X2BqQWwIBufV3_VZaqKAL2AgjCfd-XIarEcb3smEZFg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ebb1a620886f4e1dba109523459958ab343a395c", "width": 1080, "height": 567}], "variants": {}, "id": "Xrmjj7H8dRSWCfboRhrC_PjmPF5yR1nuTS8P5iY80-M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbve3r", "is_robot_indexable": true, "report_reasons": null, "author": "th_sth", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbve3r/pixiv_looks_to_be_wiping_a_lot_of_content_soon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbve3r/pixiv_looks_to_be_wiping_a_lot_of_content_soon/", "subreddit_subscribers": 657386, "created_utc": 1670113481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Now they have blocked my account alleging that I have to renew the service, but renewing is only possible by changing to monthly payments. This looks like scam to me", "author_fullname": "t2_xnkh1lx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zoolz sold me a lifetime license that lasted 3 years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbqhfu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670101089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now they have blocked my account alleging that I have to renew the service, but renewing is only possible by changing to monthly payments. This looks like scam to me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbqhfu", "is_robot_indexable": true, "report_reasons": null, "author": "rlopez7", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbqhfu/zoolz_sold_me_a_lifetime_license_that_lasted_3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbqhfu/zoolz_sold_me_a_lifetime_license_that_lasted_3/", "subreddit_subscribers": 657386, "created_utc": 1670101089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello there!\n\nFirst of all, sorry for my bad englisch german native here :) \n\nI have a question about ZFS-Replication, perhaps someone have some infos.\n\nI have a Main-Storage System with ZFS and ECC RAM, the Second-Storage-System is running ZFS without ECC.\n\nIn my opinion it's ok to run the second storage system without ECC, because the Main System have.\n\nThe Second System is only for storing zfs-snapshot-replication as Backup\n\nSince i'm safe against bit root on my main storage after the data are writen to disk there is no chance to transfer corrupted data on my second storage system even without ecc on the second system.\n\nIf i have a bit-root in ram on the second system while replication, zfs would check the different parity and resend it.\n\nDo I have a flaw in my thinking?", "author_fullname": "t2_8pe5532a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ZFS Snapshot Replication without ECC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbjjh1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670083236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there!&lt;/p&gt;\n\n&lt;p&gt;First of all, sorry for my bad englisch german native here :) &lt;/p&gt;\n\n&lt;p&gt;I have a question about ZFS-Replication, perhaps someone have some infos.&lt;/p&gt;\n\n&lt;p&gt;I have a Main-Storage System with ZFS and ECC RAM, the Second-Storage-System is running ZFS without ECC.&lt;/p&gt;\n\n&lt;p&gt;In my opinion it&amp;#39;s ok to run the second storage system without ECC, because the Main System have.&lt;/p&gt;\n\n&lt;p&gt;The Second System is only for storing zfs-snapshot-replication as Backup&lt;/p&gt;\n\n&lt;p&gt;Since i&amp;#39;m safe against bit root on my main storage after the data are writen to disk there is no chance to transfer corrupted data on my second storage system even without ecc on the second system.&lt;/p&gt;\n\n&lt;p&gt;If i have a bit-root in ram on the second system while replication, zfs would check the different parity and resend it.&lt;/p&gt;\n\n&lt;p&gt;Do I have a flaw in my thinking?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbjjh1", "is_robot_indexable": true, "report_reasons": null, "author": "Vertax1337", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbjjh1/zfs_snapshot_replication_without_ecc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbjjh1/zfs_snapshot_replication_without_ecc/", "subreddit_subscribers": 657386, "created_utc": 1670083236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello guys,\n\nI purchased a Storinator Q30 case and are looking to buy components for it ( CPU, RAM and MB)  \nI have 2 \\* lsi 16i cards and an 10gbe card that will be put into the computer aswell.\n\nStorage i have 5\\*10tb and 5\\*18tb with 20 more slots in the case empty, so a build that can handle all the pcie lanes ( im counting 20 lanes) \n\nWas thinking to get an ASUS PRIME Z690-P D4 (DDR4) and Intel Core i9-12900 together with 128gb ram but not sure thats the best path to take? read that QSV is favored because of plex and HW transcoding\n\nThe OS im going to be using for this build is going to be Unraid as im going to run some stuff besides plex. nothing that cpu heavy. ( pihole, \\*arr\u00b4s, grafana. etc) and were wondering if you guys could give me suggestions of what path would be the best to take. cost wise, not really any limits.\n\n&amp;#x200B;\n\nthanks in advice!", "author_fullname": "t2_14mlze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a new server for my data hoarding needs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbcosn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670060747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;/p&gt;\n\n&lt;p&gt;I purchased a Storinator Q30 case and are looking to buy components for it ( CPU, RAM and MB)&lt;br/&gt;\nI have 2 * lsi 16i cards and an 10gbe card that will be put into the computer aswell.&lt;/p&gt;\n\n&lt;p&gt;Storage i have 5*10tb and 5*18tb with 20 more slots in the case empty, so a build that can handle all the pcie lanes ( im counting 20 lanes) &lt;/p&gt;\n\n&lt;p&gt;Was thinking to get an ASUS PRIME Z690-P D4 (DDR4) and Intel Core i9-12900 together with 128gb ram but not sure thats the best path to take? read that QSV is favored because of plex and HW transcoding&lt;/p&gt;\n\n&lt;p&gt;The OS im going to be using for this build is going to be Unraid as im going to run some stuff besides plex. nothing that cpu heavy. ( pihole, *arr\u00b4s, grafana. etc) and were wondering if you guys could give me suggestions of what path would be the best to take. cost wise, not really any limits.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;thanks in advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbcosn", "is_robot_indexable": true, "report_reasons": null, "author": "yompe", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbcosn/building_a_new_server_for_my_data_hoarding_needs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbcosn/building_a_new_server_for_my_data_hoarding_needs/", "subreddit_subscribers": 657386, "created_utc": 1670060747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was planning on making a raid 5 array from 2 12 tb hdds and and 1 raid 0 array made from 3 4 tb hdds. The 12tb are 7200 rpm while the 3 \\* 4 hdds are 5400 rpm. Is this possible and more so not stupid? I know raid 10 is basically raid 1 + 0 but I'm not sure if it has to be implemented in a certain way to work. Using mdadm from linux kernel if your wondering. Google wasn't answering my question.", "author_fullname": "t2_1v9ndvba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "can a raid array be made from another raid array?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbny9t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "RAID configuration", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670094748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was planning on making a raid 5 array from 2 12 tb hdds and and 1 raid 0 array made from 3 4 tb hdds. The 12tb are 7200 rpm while the 3 * 4 hdds are 5400 rpm. Is this possible and more so not stupid? I know raid 10 is basically raid 1 + 0 but I&amp;#39;m not sure if it has to be implemented in a certain way to work. Using mdadm from linux kernel if your wondering. Google wasn&amp;#39;t answering my question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zbny9t", "is_robot_indexable": true, "report_reasons": null, "author": "quantumechanicalhose", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbny9t/can_a_raid_array_be_made_from_another_raid_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbny9t/can_a_raid_array_be_made_from_another_raid_array/", "subreddit_subscribers": 657386, "created_utc": 1670094748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m a code noob.\n\nAside from a few simple concepts I can\u2019t script or code. \n\nI\u2019m wondering if you think it\u2019s feasible to get consistent results with AI generated code.\n\nIt would make my life so much easier. \n\nThis is an example, wdyt?\n\n[Here is an example of a Python script that checks for changes at a URL, and if there are changes, takes a screenshot and saves it to a specific folder:](https://i.imgur.com/fMR00wz.jpg)", "author_fullname": "t2_akf0w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to USE AI to write scripts for data hoarding?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbm58l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670090171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a code noob.&lt;/p&gt;\n\n&lt;p&gt;Aside from a few simple concepts I can\u2019t script or code. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m wondering if you think it\u2019s feasible to get consistent results with AI generated code.&lt;/p&gt;\n\n&lt;p&gt;It would make my life so much easier. &lt;/p&gt;\n\n&lt;p&gt;This is an example, wdyt?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/fMR00wz.jpg\"&gt;Here is an example of a Python script that checks for changes at a URL, and if there are changes, takes a screenshot and saves it to a specific folder:&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?auto=webp&amp;s=bf755b85c339bc6079c1b3a1ae9cc0ce1a5ccc79", "width": 1048, "height": 1710}, "resolutions": [{"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=360ecb919246bc3e0207c69df707e99d0e0246d1", "width": 108, "height": 176}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ea811c86a0739ff9c729ea1ff0cab77f68c7bcf", "width": 216, "height": 352}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=caa8de9baa73d9cbfdb530833192eab9d6014d73", "width": 320, "height": 522}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f120a979a4bc351aa022af6f4215c667b2a73567", "width": 640, "height": 1044}, {"url": "https://external-preview.redd.it/Gj9D8dnmAuoo3MSs0pjZUPSGgdHz730TfMiL5fbEaQo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bdefd051dc61ac6e920b8aa5a20a2b2f6a33fd19", "width": 960, "height": 1566}], "variants": {}, "id": "D56t9ZeJJiAWGiIrwM0_68X5POaYTVxrgStV49UCEIk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbm58l", "is_robot_indexable": true, "report_reasons": null, "author": "badatmathdave", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbm58l/is_it_possible_to_use_ai_to_write_scripts_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbm58l/is_it_possible_to_use_ai_to_write_scripts_for/", "subreddit_subscribers": 657386, "created_utc": 1670090171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all,\n\nI am in the process of backing up a complete music library from a website (+- 10000 albums).  \nHere are always 2 options to download a flac and mp3 version. When the files are being downloaded individually they have always the same name whithout knowing if you have downloaded the flac or mp3 version.\n\nFilename.rar (contains 1 mp3 album)  \nFilename.rar (contains 1 flac album)\n\nby using jdownloader 2 i always download both and append a number\n\nFilename.rar  \nFilename\\_2.rar\n\nBut i still do not know if the archive contains mp3 or flac.\n\nIs there any piece of software that can open the archive, see what extensions the songs have and append this to the filename?\n\nFilename.rar -&gt; Filename\\_flac.rar  \nFilename\\_2.rar -&gt; Filename\\_2\\_mp3.rar\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_85rup3n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Append to archive name based on extension of contents.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbkp39", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670086453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I am in the process of backing up a complete music library from a website (+- 10000 albums).&lt;br/&gt;\nHere are always 2 options to download a flac and mp3 version. When the files are being downloaded individually they have always the same name whithout knowing if you have downloaded the flac or mp3 version.&lt;/p&gt;\n\n&lt;p&gt;Filename.rar (contains 1 mp3 album)&lt;br/&gt;\nFilename.rar (contains 1 flac album)&lt;/p&gt;\n\n&lt;p&gt;by using jdownloader 2 i always download both and append a number&lt;/p&gt;\n\n&lt;p&gt;Filename.rar&lt;br/&gt;\nFilename_2.rar&lt;/p&gt;\n\n&lt;p&gt;But i still do not know if the archive contains mp3 or flac.&lt;/p&gt;\n\n&lt;p&gt;Is there any piece of software that can open the archive, see what extensions the songs have and append this to the filename?&lt;/p&gt;\n\n&lt;p&gt;Filename.rar -&amp;gt; Filename_flac.rar&lt;br/&gt;\nFilename_2.rar -&amp;gt; Filename_2_mp3.rar&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbkp39", "is_robot_indexable": true, "report_reasons": null, "author": "carval444", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbkp39/append_to_archive_name_based_on_extension_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbkp39/append_to_archive_name_based_on_extension_of/", "subreddit_subscribers": 657386, "created_utc": 1670086453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\nI've already searched the threads but it looks like the provided links are now out of date.  \nI need to download a couple books off Scribd, not documents that are user-uploaded, but ebooks Scribd provides. They are too large to download one page at a time without taking too much time. If you know of any ebook rippers for Scribd I would be grateful.\n\nThank you!", "author_fullname": "t2_69i9rgfi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working downloader for Scribd books in late 2022 (not documents)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbhd7s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670077226.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve already searched the threads but it looks like the provided links are now out of date.&lt;br/&gt;\nI need to download a couple books off Scribd, not documents that are user-uploaded, but ebooks Scribd provides. They are too large to download one page at a time without taking too much time. If you know of any ebook rippers for Scribd I would be grateful.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbhd7s", "is_robot_indexable": true, "report_reasons": null, "author": "throw_away_bay_bay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbhd7s/working_downloader_for_scribd_books_in_late_2022/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbhd7s/working_downloader_for_scribd_books_in_late_2022/", "subreddit_subscribers": 657386, "created_utc": 1670077226.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey Guys,I've recently had a weird failure mode on my unraid box and thought I would consult the hive-mind for ideas &amp; thoughts.\n\n**TLDR:**\n\nI think that somehow port/channel 3 on the first port of my LSI 9240-8i is bad. Swapping breakout cable results in the same sata port being bad.\n\nHas anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?\n\n**Key system info:**\n\n* It's been up and happy for over a year.\n* I'm using a LSI 9240-8i in IT mode as an HBA\n* I'm using 3 x 14TB WD drives all attached to a single SAS SFF-8087 to 4x SATA Cable\n* 1 x Parity disk and 2 x data disks\n\n**What happened:**\n\n1. Woke up in the morning to a scheduled parity check having failed. Looking at the logs. It failed not long after it started. The second data disk was marked by unRaid as \"Device disabled, contents emulated\".\n2. Ok, no biggie - the disk has failed. Stopped the array, unassigned the failed disk and shut down. Ordered a replacement.\n3. While waiting for the replacement, I pulled the failed disk and had a look at it with CrystalDiskInfo. Good SMART health and zero errors of any kind. Hmmm.\n4. New disk arrives. Slap it into the machine (same cable as the 'failed' disk) and unraid offers to rebuild the array to it. Click go.\n5. Almost immediately, read errors are reported, rebuild fails and the array is stuck in some kind of read-check paused status. Shit.\n6. Hmm. Maybe it's a bad cable? Shut down and swap the SAS SFF-8087 to 4x SATA Cable for a spare. Plug in both the new drive as well as the 'failed' drive (I suspect the drive itself is actually fine, so I may add it as a second parity later). New drive is on port 3 of the breakout cable (where the failed drive had been) and the 'failed' drive is now on port 4.\n7. Boot up unraid and assign the original 'failed' drive to the array as a new disk. Start the array and unraid starts to rebuild the array with no issues!\n8. Go to my unassigned disks and I can see the new drive. Spin it up and run a SMART test. SMART test fails. Twice. Errors in the log:\n\n&amp;#x200B;\n\n    Dec  3 12:11:06 tower  emhttpd: spinning up /dev/sdd\n    Dec  3 12:11:17 tower  emhttpd: read SMART /dev/sdd\n    Dec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronizing SCSI cache\n    Dec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronize Cache(10) failed: Result: hostbyte=0x01 driverbyte=DRIVER_OK\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 27344764928 512-byte logical blocks: (14.0 TB/12.7 TiB)\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 4096-byte physical blocks\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write Protect is off\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Mode Sense: 7f 00 10 08\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write cache: enabled, read cache: enabled, supports DPO and FUA\n    Dec  3 12:12:53 tower kernel: sdd: sdd1\n    Dec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Attached SCSI disk\n    Dec  3 12:12:54 tower unassigned.devices: Disk with ID 'TOSHIBA_HDWG21E_Z1A0A0CCFP7G (sdd)' is not set to auto mount.\n\n**Conclusion:**\n\nI think the cable was actually fine and somehow port/channel 3 on the first port of my LSI 9240-8i is somehow bad.\n\nHas anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?", "author_fullname": "t2_ckt3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debugging LSI 9240-8i in IT mode", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbfyvu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670073114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Guys,I&amp;#39;ve recently had a weird failure mode on my unraid box and thought I would consult the hive-mind for ideas &amp;amp; thoughts.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I think that somehow port/channel 3 on the first port of my LSI 9240-8i is bad. Swapping breakout cable results in the same sata port being bad.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key system info:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It&amp;#39;s been up and happy for over a year.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m using a LSI 9240-8i in IT mode as an HBA&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m using 3 x 14TB WD drives all attached to a single SAS SFF-8087 to 4x SATA Cable&lt;/li&gt;\n&lt;li&gt;1 x Parity disk and 2 x data disks&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What happened:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Woke up in the morning to a scheduled parity check having failed. Looking at the logs. It failed not long after it started. The second data disk was marked by unRaid as &amp;quot;Device disabled, contents emulated&amp;quot;.&lt;/li&gt;\n&lt;li&gt;Ok, no biggie - the disk has failed. Stopped the array, unassigned the failed disk and shut down. Ordered a replacement.&lt;/li&gt;\n&lt;li&gt;While waiting for the replacement, I pulled the failed disk and had a look at it with CrystalDiskInfo. Good SMART health and zero errors of any kind. Hmmm.&lt;/li&gt;\n&lt;li&gt;New disk arrives. Slap it into the machine (same cable as the &amp;#39;failed&amp;#39; disk) and unraid offers to rebuild the array to it. Click go.&lt;/li&gt;\n&lt;li&gt;Almost immediately, read errors are reported, rebuild fails and the array is stuck in some kind of read-check paused status. Shit.&lt;/li&gt;\n&lt;li&gt;Hmm. Maybe it&amp;#39;s a bad cable? Shut down and swap the SAS SFF-8087 to 4x SATA Cable for a spare. Plug in both the new drive as well as the &amp;#39;failed&amp;#39; drive (I suspect the drive itself is actually fine, so I may add it as a second parity later). New drive is on port 3 of the breakout cable (where the failed drive had been) and the &amp;#39;failed&amp;#39; drive is now on port 4.&lt;/li&gt;\n&lt;li&gt;Boot up unraid and assign the original &amp;#39;failed&amp;#39; drive to the array as a new disk. Start the array and unraid starts to rebuild the array with no issues!&lt;/li&gt;\n&lt;li&gt;Go to my unassigned disks and I can see the new drive. Spin it up and run a SMART test. SMART test fails. Twice. Errors in the log:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Dec  3 12:11:06 tower  emhttpd: spinning up /dev/sdd\nDec  3 12:11:17 tower  emhttpd: read SMART /dev/sdd\nDec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronizing SCSI cache\nDec  3 12:12:42 tower kernel: sd 6:0:2:0: [sdd] Synchronize Cache(10) failed: Result: hostbyte=0x01 driverbyte=DRIVER_OK\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 27344764928 512-byte logical blocks: (14.0 TB/12.7 TiB)\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] 4096-byte physical blocks\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write Protect is off\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Mode Sense: 7f 00 10 08\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Write cache: enabled, read cache: enabled, supports DPO and FUA\nDec  3 12:12:53 tower kernel: sdd: sdd1\nDec  3 12:12:53 tower kernel: sd 6:0:4:0: [sdd] Attached SCSI disk\nDec  3 12:12:54 tower unassigned.devices: Disk with ID &amp;#39;TOSHIBA_HDWG21E_Z1A0A0CCFP7G (sdd)&amp;#39; is not set to auto mount.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I think the cable was actually fine and somehow port/channel 3 on the first port of my LSI 9240-8i is somehow bad.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else ever seen this type of failure mode? Is there a recommended way to test all the channels/ports on it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbfyvu", "is_robot_indexable": true, "report_reasons": null, "author": "kabadisha", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbfyvu/debugging_lsi_92408i_in_it_mode/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbfyvu/debugging_lsi_92408i_in_it_mode/", "subreddit_subscribers": 657386, "created_utc": 1670073114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello. To get a refund, I'm considering refusing my Western Digital items when they are delivered. I understand from their [Refusal Policy](https://www.westerndigital.com/support/store/return-policy) that I will be:\n\n&gt; credited for the product cost and tax, less shipping and handling \n\nI am now wondering how will they deal with orders that come with free shipping. Will they charge me for shipping (and handling) as if my purchase didn't come with free shipping? If this is true, how do I find out what this cost is?\n\nI'm hoping that someone with experience dealing with refusals enlighten me. Thanks!", "author_fullname": "t2_6czvm5cr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When delivery is refused, does Western Digital charge for shipping and handling if shipping was free?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zc2n7s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670135596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. To get a refund, I&amp;#39;m considering refusing my Western Digital items when they are delivered. I understand from their &lt;a href=\"https://www.westerndigital.com/support/store/return-policy\"&gt;Refusal Policy&lt;/a&gt; that I will be:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;credited for the product cost and tax, less shipping and handling &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I am now wondering how will they deal with orders that come with free shipping. Will they charge me for shipping (and handling) as if my purchase didn&amp;#39;t come with free shipping? If this is true, how do I find out what this cost is?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping that someone with experience dealing with refusals enlighten me. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-F2cowyFIaGx4Apq7yX1iKL_yGXcgpKYXauwmMNJdAo.jpg?auto=webp&amp;s=3c6a45dabbc6da8fc60cfd778ee0909d77bbd0bb", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/-F2cowyFIaGx4Apq7yX1iKL_yGXcgpKYXauwmMNJdAo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b344babe243a7a320a76136db06db46809edd1c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-F2cowyFIaGx4Apq7yX1iKL_yGXcgpKYXauwmMNJdAo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f5771dd42fdf5011dc5cf031718ebdae2604046c", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/-F2cowyFIaGx4Apq7yX1iKL_yGXcgpKYXauwmMNJdAo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=67dd9c52edc5452abf761e3a837cfdf532529b76", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/-F2cowyFIaGx4Apq7yX1iKL_yGXcgpKYXauwmMNJdAo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=529b29bfd15670226e60f8897c2c1eabad06eedf", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/-F2cowyFIaGx4Apq7yX1iKL_yGXcgpKYXauwmMNJdAo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e997318dbd3711a840c86ce3e9a8788096644a96", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/-F2cowyFIaGx4Apq7yX1iKL_yGXcgpKYXauwmMNJdAo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=55e461324a08b8555058e5d419919beee7cb5934", "width": 1080, "height": 564}], "variants": {}, "id": "zGEi2712E13UzihElDCXLsOTbDx-FLAvMkHvBo-bO2I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zc2n7s", "is_robot_indexable": true, "report_reasons": null, "author": "injeolmi-bingsoo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zc2n7s/when_delivery_is_refused_does_western_digital/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zc2n7s/when_delivery_is_refused_does_western_digital/", "subreddit_subscribers": 657386, "created_utc": 1670135596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Finally getting into offline data backup and archival. I don't have a tape drive or anything actually designed for long term offline storage, but I do have a drawer full of old hard drives and SSDs that I bought dirt cheap from thrift stores and eBay.\n\nHow long can they be mostly guaranteed to keep their data if I fill them up with archives and the leave them unplugged? I heard that SSDs are a lot worse at offline data storage than hard drives, is that true? Are they suitable for long term storage at all?", "author_fullname": "t2_1h1hfeve", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the longest a hard drive and SSD can be unplugged resoectively and still retain data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zc2jnz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670135235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally getting into offline data backup and archival. I don&amp;#39;t have a tape drive or anything actually designed for long term offline storage, but I do have a drawer full of old hard drives and SSDs that I bought dirt cheap from thrift stores and eBay.&lt;/p&gt;\n\n&lt;p&gt;How long can they be mostly guaranteed to keep their data if I fill them up with archives and the leave them unplugged? I heard that SSDs are a lot worse at offline data storage than hard drives, is that true? Are they suitable for long term storage at all?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zc2jnz", "is_robot_indexable": true, "report_reasons": null, "author": "AgreeableLandscape3", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zc2jnz/whats_the_longest_a_hard_drive_and_ssd_can_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zc2jnz/whats_the_longest_a_hard_drive_and_ssd_can_be/", "subreddit_subscribers": 657386, "created_utc": 1670135235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Gurus , I over-voltage the WD elements by plugging plug meant for laptop \n\nDismantled the drive from the enclosure and found the suspected component is burnt . \n\nJust wanna confirm if the burnt part is TVS Diode . \n\nThanks", "author_fullname": "t2_u547j6h5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help to identify WD 14TB HDD PCB burnt component", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zc18q8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670130840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Gurus , I over-voltage the WD elements by plugging plug meant for laptop &lt;/p&gt;\n\n&lt;p&gt;Dismantled the drive from the enclosure and found the suspected component is burnt . &lt;/p&gt;\n\n&lt;p&gt;Just wanna confirm if the burnt part is TVS Diode . &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zc18q8", "is_robot_indexable": true, "report_reasons": null, "author": "rextan123", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zc18q8/help_to_identify_wd_14tb_hdd_pcb_burnt_component/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zc18q8/help_to_identify_wd_14tb_hdd_pcb_burnt_component/", "subreddit_subscribers": 657386, "created_utc": 1670130840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Aspiring data hoarder flirting with different raid config ideas.  Maybe software raid but leaning towards enclosure or  hardware raid controller in tower.", "author_fullname": "t2_6l2op3dg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can a raid array via external disk enclosure be moved to and from other PCs with different hardware configurations? For the sake of data access is why I ask.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zc102n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670130058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Aspiring data hoarder flirting with different raid config ideas.  Maybe software raid but leaning towards enclosure or  hardware raid controller in tower.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zc102n", "is_robot_indexable": true, "report_reasons": null, "author": "History_guy2018", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zc102n/can_a_raid_array_via_external_disk_enclosure_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zc102n/can_a_raid_array_via_external_disk_enclosure_be/", "subreddit_subscribers": 657386, "created_utc": 1670130058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just purchased some HGST SN200 U.2 SSD\u2019s. While installing them I noticed they have what looks like a micro usb port on the opposite side of the U.2 connected. After searching the internet I can\u2019t find any reference to these ports. Does anyone know what they\u2019re used for?", "author_fullname": "t2_ru1of", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HGST SN200 USB Port?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zc0qzp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670129265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just purchased some HGST SN200 U.2 SSD\u2019s. While installing them I noticed they have what looks like a micro usb port on the opposite side of the U.2 connected. After searching the internet I can\u2019t find any reference to these ports. Does anyone know what they\u2019re used for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zc0qzp", "is_robot_indexable": true, "report_reasons": null, "author": "EricDArneson", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zc0qzp/hgst_sn200_usb_port/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zc0qzp/hgst_sn200_usb_port/", "subreddit_subscribers": 657386, "created_utc": 1670129265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello--\n\nNot sure if any of the other folks from the wetshaving or wicked_edge subreddits have stopped by, but it looks like a beloved website used by a lot of folks on those subs is going to be going offline around Dec 31 of this year.  Is there any tool that I could use to back up the contents of this website?  There are a lot of pages of descriptions of soaps/aftershaves/colognes that are no longer up on their respective vendors' websites anymore, so when https://trythatsoap.com/ goes offline, that information will be gone.\n\nI've got some extra TBs I can store the data in, but I'm not sure how to back up the site's contents.\n\nIn general I'm mainly concerned about the portion of the site dealing with Brand/product/description.  This [page](https://trythatsoap.com/collection/95/?product_type=soap) for example is a record of Stirling Soap's Executive Man shave soap-- which still happens to be up on Stirling's website, but lots and lots of scents from lots and lots of vendors aren't.. so this is just an example of what I'd like to be able to save for posterity.\n\nThanks in advance for any help that I could be pointed to.", "author_fullname": "t2_7ddwu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Options to backup https://trythatsoap.com/?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zc0huu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670129000.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670128443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello--&lt;/p&gt;\n\n&lt;p&gt;Not sure if any of the other folks from the wetshaving or wicked_edge subreddits have stopped by, but it looks like a beloved website used by a lot of folks on those subs is going to be going offline around Dec 31 of this year.  Is there any tool that I could use to back up the contents of this website?  There are a lot of pages of descriptions of soaps/aftershaves/colognes that are no longer up on their respective vendors&amp;#39; websites anymore, so when &lt;a href=\"https://trythatsoap.com/\"&gt;https://trythatsoap.com/&lt;/a&gt; goes offline, that information will be gone.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got some extra TBs I can store the data in, but I&amp;#39;m not sure how to back up the site&amp;#39;s contents.&lt;/p&gt;\n\n&lt;p&gt;In general I&amp;#39;m mainly concerned about the portion of the site dealing with Brand/product/description.  This &lt;a href=\"https://trythatsoap.com/collection/95/?product_type=soap\"&gt;page&lt;/a&gt; for example is a record of Stirling Soap&amp;#39;s Executive Man shave soap-- which still happens to be up on Stirling&amp;#39;s website, but lots and lots of scents from lots and lots of vendors aren&amp;#39;t.. so this is just an example of what I&amp;#39;d like to be able to save for posterity.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any help that I could be pointed to.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zc0huu", "is_robot_indexable": true, "report_reasons": null, "author": "grock1722", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zc0huu/options_to_backup_httpstrythatsoapcom/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zc0huu/options_to_backup_httpstrythatsoapcom/", "subreddit_subscribers": 657386, "created_utc": 1670128443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello fellow hoarders! I have a feeling I'm going to use some of these words wrong so please excuse that and feel free to correct me. I've been a data hoarder for as long as I can remember but only recently ran into issues. From what I've learned online, I'm experiencing issues with fragmentation and it's causing my external HDD to be super slow. I'm a graphic designer and I regularly download large compressed folders of multiple file types only to keep one specific format (keeping like 100MB of a 1GB download). It was so bad I couldn't get Windows defrag, Defraggler, or UltraDefrag to get more than 1% defragged in like a 24 hour period. So I moved all the files from my Seagate HDD to my laptop's SSD and formatted my Seagate. I started adding things back and began with one 55GB folder that had 94% fragmentation. Defraggler has been doing a \"quick\" defrag for about 45 minutes and is still at 0%. Here are my questions:\n\n* Should I try transferring stuff over in smaller clusters, like 5GB at a time and then defragging them? \n* How can I prevent data fragging in this circumstance? Prep the download and what not on my laptop and then transfer only the files I was keeping? \n\nThank you so much for taking the time to read this! Have a great weekend :)", "author_fullname": "t2_87yliers", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Defrag tips for large downloads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zbzdwn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670125056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow hoarders! I have a feeling I&amp;#39;m going to use some of these words wrong so please excuse that and feel free to correct me. I&amp;#39;ve been a data hoarder for as long as I can remember but only recently ran into issues. From what I&amp;#39;ve learned online, I&amp;#39;m experiencing issues with fragmentation and it&amp;#39;s causing my external HDD to be super slow. I&amp;#39;m a graphic designer and I regularly download large compressed folders of multiple file types only to keep one specific format (keeping like 100MB of a 1GB download). It was so bad I couldn&amp;#39;t get Windows defrag, Defraggler, or UltraDefrag to get more than 1% defragged in like a 24 hour period. So I moved all the files from my Seagate HDD to my laptop&amp;#39;s SSD and formatted my Seagate. I started adding things back and began with one 55GB folder that had 94% fragmentation. Defraggler has been doing a &amp;quot;quick&amp;quot; defrag for about 45 minutes and is still at 0%. Here are my questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Should I try transferring stuff over in smaller clusters, like 5GB at a time and then defragging them? &lt;/li&gt;\n&lt;li&gt;How can I prevent data fragging in this circumstance? Prep the download and what not on my laptop and then transfer only the files I was keeping? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you so much for taking the time to read this! Have a great weekend :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zbzdwn", "is_robot_indexable": true, "report_reasons": null, "author": "lezzuhlss", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zbzdwn/defrag_tips_for_large_downloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zbzdwn/defrag_tips_for_large_downloads/", "subreddit_subscribers": 657386, "created_utc": 1670125056.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}