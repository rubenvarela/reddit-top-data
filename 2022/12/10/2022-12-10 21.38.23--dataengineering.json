{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_wqct3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Conversation with chatGPT on Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_zhm66m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/j6QJXqBiJhPLVEgoUN1oygxPn2o_MWUFDLoNQpFWl74.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670657750.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "link.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://link.medium.com/ScfLkXrHDvb", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?auto=webp&amp;s=802152dcad572430c26b31d3441447118d6b7686", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d9933bfc52c090d2b690b0632c57808fada55ed", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9fee743a6a013e2943dd90a7ff0e352d8344726", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33309caadaaa69438579868ea0bfcbd1bcc1b3ac", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=730ff9e726b3520935bfcefcdd17f9a9e704a6ac", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dd6d9af53c22efdd0fa0896d3023265f19330222", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c25ce605074dbe468c8fc1b5d3ce67f36ed0c657", "width": 1080, "height": 607}], "variants": {}, "id": "jKaQlVsP7O4k3Ibxba0Vhjw-0dOfpUpXCJJ_yFlcxew"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zhm66m", "is_robot_indexable": true, "report_reasons": null, "author": "Koushik5586", "discussion_type": null, "num_comments": 25, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhm66m/a_conversation_with_chatgpt_on_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://link.medium.com/ScfLkXrHDvb", "subreddit_subscribers": 82489, "created_utc": 1670657750.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are many SaaS applications which allows users to basically store data with various attributes and data types. i.e a collection of a custom objects with user defined fields/attributes of any primitive datatype. These applications then allow efficient querying of these custom objects.\n\nA few examples that come to mind are:\n\n* [Salesforce](https://trailhead.salesforce.com/content/learn/modules/data_modeling/objects_intro)\n   * Allows users to define tables with fields which they can query against.\n   * Allows defining relations (FK) between multiple objects\n* [Hubspot](https://www.hubspot.com/products/custom-objects)\n   * Same thing as salesforce.\n* [Segment.io](https://segment.com/docs/guides/filtering-data/)\n   * Allows sending large amounts of data and defining filters to segment data into user defined buckets\n* [Heap Analytics](https://help.heap.io/definitions/properties/how-to-create-defined-properties/)\n   * Similar to to [segment.io](https://segment.io)\n* [Klaviyo](https://help.klaviyo.com/hc/en-us/articles/115000250912-About-Custom-Properties) \\- customer data platform\n   * Allows adding custom properties to 1000s of customer profiles and then segment/filter them into various groups.\n\nTraditionally, developers would model schemas and add indexes to various fields based on expected query patterns as they know this information ahead of time. However, in user defined objects/schemas, this is unknown.\n\nHow do these products/features work? Whats the technology behind this? How is the data stored? How are the queries made to execute efficiently? What's the underlying implementations to allow this to work at scale?\n\n&amp;#x200B;\n\nP.S: Are there any opensource technologies/databases which allow implementing features like these?", "author_fullname": "t2_h81s7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does storing and querying user defined schema work at scale?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhtbjs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670681002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are many SaaS applications which allows users to basically store data with various attributes and data types. i.e a collection of a custom objects with user defined fields/attributes of any primitive datatype. These applications then allow efficient querying of these custom objects.&lt;/p&gt;\n\n&lt;p&gt;A few examples that come to mind are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://trailhead.salesforce.com/content/learn/modules/data_modeling/objects_intro\"&gt;Salesforce&lt;/a&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Allows users to define tables with fields which they can query against.&lt;/li&gt;\n&lt;li&gt;Allows defining relations (FK) between multiple objects&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.hubspot.com/products/custom-objects\"&gt;Hubspot&lt;/a&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Same thing as salesforce.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://segment.com/docs/guides/filtering-data/\"&gt;Segment.io&lt;/a&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Allows sending large amounts of data and defining filters to segment data into user defined buckets&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://help.heap.io/definitions/properties/how-to-create-defined-properties/\"&gt;Heap Analytics&lt;/a&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Similar to to &lt;a href=\"https://segment.io\"&gt;segment.io&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://help.klaviyo.com/hc/en-us/articles/115000250912-About-Custom-Properties\"&gt;Klaviyo&lt;/a&gt; - customer data platform\n\n&lt;ul&gt;\n&lt;li&gt;Allows adding custom properties to 1000s of customer profiles and then segment/filter them into various groups.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Traditionally, developers would model schemas and add indexes to various fields based on expected query patterns as they know this information ahead of time. However, in user defined objects/schemas, this is unknown.&lt;/p&gt;\n\n&lt;p&gt;How do these products/features work? Whats the technology behind this? How is the data stored? How are the queries made to execute efficiently? What&amp;#39;s the underlying implementations to allow this to work at scale?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;P.S: Are there any opensource technologies/databases which allow implementing features like these?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rO6oUQYwnpvj6zBnRO2KQyzxmAEb9-79YUfQeMeERLk.jpg?auto=webp&amp;s=e7d093b708a39b10f67496c315a48f92fddb2b28", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/rO6oUQYwnpvj6zBnRO2KQyzxmAEb9-79YUfQeMeERLk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=af3528e6cd5e832aad44fd7ca8d1fe5d6bd379c6", "width": 108, "height": 108}], "variants": {}, "id": "qFPKnsa5pUm7UbgtvJLjONWckFvkmJRapYOlAH8MdoM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zhtbjs", "is_robot_indexable": true, "report_reasons": null, "author": "Krimson1911", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhtbjs/how_does_storing_and_querying_user_defined_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhtbjs/how_does_storing_and_querying_user_defined_schema/", "subreddit_subscribers": 82489, "created_utc": 1670681002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nI'm working in a project to aggregate data from three different systems (psql, mysql and mongodb). I have a huge problem with incremental updates once one of the systems does hard deletes in their tables (mysql). What would be the best strategy to handle such problem? CDC is not an option right now.\n\nI'm using Airflow as data ingestion and dbt to perform the data modeling and transformation.\n\nThanks all in advance!", "author_fullname": "t2_hanh3wo6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle deletes in source systems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhov20", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670667187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working in a project to aggregate data from three different systems (psql, mysql and mongodb). I have a huge problem with incremental updates once one of the systems does hard deletes in their tables (mysql). What would be the best strategy to handle such problem? CDC is not an option right now.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Airflow as data ingestion and dbt to perform the data modeling and transformation.&lt;/p&gt;\n\n&lt;p&gt;Thanks all in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zhov20", "is_robot_indexable": true, "report_reasons": null, "author": "Other-Government-796", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhov20/how_to_handle_deletes_in_source_systems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhov20/how_to_handle_deletes_in_source_systems/", "subreddit_subscribers": 82489, "created_utc": 1670667187.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Given the following two trends, it seems to me a lot of the technical/coding parts of DE responsibilities will decrease and/or radically change:\n\n* Vendors tools and cloud perform are maturing a lot, exposing higher level abstractions to us developers (e.g. today's Spark is an order of magnitude easier to code with than 8 years ago) and becoming more seamlessly integrated (e.g. recent announcement of Aurora to Redshift integration)\n\n* AI coding assistant are now usable, as famously illustrated by Copilot and ChatGPT, it's reasonable to assume they will ultimately be integrated by cloud platforms and tool vendors in the coming years, reducing there again the amount of work for developers.\n\nNow I'm convinced there will always be tasks for humans to build and maintain data pipelines and data oriented solution, although it's likely going to be quite different from today.\n\nWhat do you think data engineer job will be like in 5 years? What's your move today to prepare for it?", "author_fullname": "t2_hf4d8zql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What DE tasks do you think will look like in 5 years?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhow0i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670667273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given the following two trends, it seems to me a lot of the technical/coding parts of DE responsibilities will decrease and/or radically change:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Vendors tools and cloud perform are maturing a lot, exposing higher level abstractions to us developers (e.g. today&amp;#39;s Spark is an order of magnitude easier to code with than 8 years ago) and becoming more seamlessly integrated (e.g. recent announcement of Aurora to Redshift integration)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;AI coding assistant are now usable, as famously illustrated by Copilot and ChatGPT, it&amp;#39;s reasonable to assume they will ultimately be integrated by cloud platforms and tool vendors in the coming years, reducing there again the amount of work for developers.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now I&amp;#39;m convinced there will always be tasks for humans to build and maintain data pipelines and data oriented solution, although it&amp;#39;s likely going to be quite different from today.&lt;/p&gt;\n\n&lt;p&gt;What do you think data engineer job will be like in 5 years? What&amp;#39;s your move today to prepare for it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zhow0i", "is_robot_indexable": true, "report_reasons": null, "author": "sv3ndk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhow0i/what_de_tasks_do_you_think_will_look_like_in_5/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhow0i/what_de_tasks_do_you_think_will_look_like_in_5/", "subreddit_subscribers": 82489, "created_utc": 1670667273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey I have been trying to solve this issue for the past couple days. We are looking to bring in DBT and our snowflake accounts are split by environment. So we have 3 accounts (Dev, qa, prod). \n\nDoes anyone use DBT with 3 snowflake accounts? What is your branching strategy? \n\nI want to use trunk based (1 long lived branch) but I can\u2019t figure out how to do that currently. \n\nI would appreciate any help on this!", "author_fullname": "t2_1n3qfa0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT + Snowflake Envs Account level", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zha4fc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670623385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey I have been trying to solve this issue for the past couple days. We are looking to bring in DBT and our snowflake accounts are split by environment. So we have 3 accounts (Dev, qa, prod). &lt;/p&gt;\n\n&lt;p&gt;Does anyone use DBT with 3 snowflake accounts? What is your branching strategy? &lt;/p&gt;\n\n&lt;p&gt;I want to use trunk based (1 long lived branch) but I can\u2019t figure out how to do that currently. &lt;/p&gt;\n\n&lt;p&gt;I would appreciate any help on this!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zha4fc", "is_robot_indexable": true, "report_reasons": null, "author": "Culpgrant21", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zha4fc/dbt_snowflake_envs_account_level/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zha4fc/dbt_snowflake_envs_account_level/", "subreddit_subscribers": 82489, "created_utc": 1670623385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7g8ahtr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advanced DBT Macros", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "name": "t3_zhjvcw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kUnox4hHhEct0IHebUENb9Hb_Bq57dHmF2u2b8AVlOI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670649765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@kadek/advanced-dbt-macros-c274f1cfa989", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?auto=webp&amp;s=319ba75d9c5d15b9ee79ef24fe04b672f5ab2d25", "width": 1200, "height": 658}, "resolutions": [{"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6daed398df2e0a759ca234ad545f82d5dd8bb0e", "width": 108, "height": 59}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6599dc4486e9c8775acfd4240f604832ce65ef0e", "width": 216, "height": 118}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdd29e283e196351479b87c4b93f1bde00cad3fc", "width": 320, "height": 175}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=018afecb2358addb0ef361e6db0e3abfa59ff4ed", "width": 640, "height": 350}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4a419a06364430f23db47e24443c356ec63cc3ef", "width": 960, "height": 526}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=739089bae6a0ba18df34b02b8ef8145a9f7b067a", "width": 1080, "height": 592}], "variants": {}, "id": "EIbAEFWYS3HzVZSKuLBYDA8qdb6Kgl7d7hGOqBbvNVw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zhjvcw", "is_robot_indexable": true, "report_reasons": null, "author": "hjkl_ornah", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhjvcw/advanced_dbt_macros/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@kadek/advanced-dbt-macros-c274f1cfa989", "subreddit_subscribers": 82489, "created_utc": 1670649765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a junior (~2 years experience) De and my current work contract ends in April. I was wondering, for those who are in a similar position and will be looking for jobs in the New Year, what are some things you will take in to consideration? any criteria for a role? any advice for looking for roles?", "author_fullname": "t2_a3m6qw38", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Applying or Looking for Jobs in the New Year?? What to think about?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhpew8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670668913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a junior (~2 years experience) De and my current work contract ends in April. I was wondering, for those who are in a similar position and will be looking for jobs in the New Year, what are some things you will take in to consideration? any criteria for a role? any advice for looking for roles?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zhpew8", "is_robot_indexable": true, "report_reasons": null, "author": "sk808mafia", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhpew8/applying_or_looking_for_jobs_in_the_new_year_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhpew8/applying_or_looking_for_jobs_in_the_new_year_what/", "subreddit_subscribers": 82489, "created_utc": 1670668913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wonder how in demand Databricks is. I\u2019m curious cause my company offers trainings in some technologies and Databricks is one of them. However most of the job vacancies I see, at least around me, are not asking for Databricks skills/experience specifically. A lot of them mention Spark, but not Databricks specifically. I\u2019m curious to see how popular it is in this community.\n\n[View Poll](https://www.reddit.com/poll/zhcfid)", "author_fullname": "t2_8dnn00ks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Databricks in demand? How often do you use it at your job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhcfid", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670628728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wonder how in demand Databricks is. I\u2019m curious cause my company offers trainings in some technologies and Databricks is one of them. However most of the job vacancies I see, at least around me, are not asking for Databricks skills/experience specifically. A lot of them mention Spark, but not Databricks specifically. I\u2019m curious to see how popular it is in this community.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/zhcfid\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zhcfid", "is_robot_indexable": true, "report_reasons": null, "author": "Se7enEl11ven", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1670887928052, "options": [{"text": "I use it Daily/Frequently", "id": "20289706"}, {"text": "I use it from time to time/Depends on projects", "id": "20289707"}, {"text": "I don\u2019t use it but already did (Old job or project)", "id": "20289708"}, {"text": "Never used it", "id": "20289709"}, {"text": "Never used it and never heard of it before", "id": "20289710"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 294, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhcfid/is_databricks_in_demand_how_often_do_you_use_it/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/zhcfid/is_databricks_in_demand_how_often_do_you_use_it/", "subreddit_subscribers": 82489, "created_utc": 1670628728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hoping to find someone to hop on a call and look over a take home assignment for a mid-level DE job with me. \n\nTo be clear, I'm not looking for anyone to do any work for me-- just critique the answers I have already written. At my current job nobody writes python, so I don't have any experience writing python in a shared codebase and the conventions that might come along with that. As a result, I'm concerned about my code coming off as amateurish. \n\nIf any generous soul would be willing to help me out for a little bit, it would be much appreciated. Am willing to compensate at your hourly. Feel free to dm or comment. Can be discord/teams/Skype etc.", "author_fullname": "t2_kdj5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an experienced DE to walk through/critique take-home python assignment Saturday morning/afternoon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhm02s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670657136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hoping to find someone to hop on a call and look over a take home assignment for a mid-level DE job with me. &lt;/p&gt;\n\n&lt;p&gt;To be clear, I&amp;#39;m not looking for anyone to do any work for me-- just critique the answers I have already written. At my current job nobody writes python, so I don&amp;#39;t have any experience writing python in a shared codebase and the conventions that might come along with that. As a result, I&amp;#39;m concerned about my code coming off as amateurish. &lt;/p&gt;\n\n&lt;p&gt;If any generous soul would be willing to help me out for a little bit, it would be much appreciated. Am willing to compensate at your hourly. Feel free to dm or comment. Can be discord/teams/Skype etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zhm02s", "is_robot_indexable": true, "report_reasons": null, "author": "Touvejs", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhm02s/looking_for_an_experienced_de_to_walk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhm02s/looking_for_an_experienced_de_to_walk/", "subreddit_subscribers": 82489, "created_utc": 1670657136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nPreviously, our Postgres tables were single monolithic tables. We successfully used AWS DMS to migrate the data in them toward Redshift. 1 to 1 table mappings, no problem. Smiles every day.\n\nNow, we're frowning big time!\n\nFor some of the Postgres tables that are expected to grow quickly, we've implemented partitioning. DMS clearly states that it won't work as expected under these conditions.\n\n[https://docs.aws.amazon.com/dms/latest/userguide/CHAP\\_Source.PostgreSQL.html#CHAP\\_Source.PostgreSQL.Limitations](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.Limitations)\n\nThe root cause of the incompatibilities is that the Postgres WAL entries are no longer associated with the top-level table, but rather ONLY with the partition tables underneath. This has several undesired side-effects, and supports what is said in the AWS documentation.\n\nWhile we've mangled DMS to jankily work here, it is not robust in this scenario.\n\nPlease would you clever folks help me tackle this specific migration problem? If I can provide any more info, I'd be happy to.\n\nEDIT: I should mention what we've tried. We have tried or considered several things.\n\n1. A scheduled Glue job? We're comfortable with Glue, but are attracted to no-code solutions for this. We'd accept Glue, we just haven't yet tried it here.\n2. MSK Connect? We've tried MSK Connect, sending data to S3 rather than Redshift (leaving the S3 -&gt; Redshift problem for later). Seems to work well. It does incremental batches based on our 'updated\\_at' field. There are some concerns about performance and/or whether this is the right approach. \n3. Debezium? Being WAL-based, we feel it will suffer the same fate as DMS.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating Postgres \\w Paritioned Tables to Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhbd7z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670626715.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670626268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Previously, our Postgres tables were single monolithic tables. We successfully used AWS DMS to migrate the data in them toward Redshift. 1 to 1 table mappings, no problem. Smiles every day.&lt;/p&gt;\n\n&lt;p&gt;Now, we&amp;#39;re frowning big time!&lt;/p&gt;\n\n&lt;p&gt;For some of the Postgres tables that are expected to grow quickly, we&amp;#39;ve implemented partitioning. DMS clearly states that it won&amp;#39;t work as expected under these conditions.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.Limitations\"&gt;https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.Limitations&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The root cause of the incompatibilities is that the Postgres WAL entries are no longer associated with the top-level table, but rather ONLY with the partition tables underneath. This has several undesired side-effects, and supports what is said in the AWS documentation.&lt;/p&gt;\n\n&lt;p&gt;While we&amp;#39;ve mangled DMS to jankily work here, it is not robust in this scenario.&lt;/p&gt;\n\n&lt;p&gt;Please would you clever folks help me tackle this specific migration problem? If I can provide any more info, I&amp;#39;d be happy to.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I should mention what we&amp;#39;ve tried. We have tried or considered several things.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A scheduled Glue job? We&amp;#39;re comfortable with Glue, but are attracted to no-code solutions for this. We&amp;#39;d accept Glue, we just haven&amp;#39;t yet tried it here.&lt;/li&gt;\n&lt;li&gt;MSK Connect? We&amp;#39;ve tried MSK Connect, sending data to S3 rather than Redshift (leaving the S3 -&amp;gt; Redshift problem for later). Seems to work well. It does incremental batches based on our &amp;#39;updated_at&amp;#39; field. There are some concerns about performance and/or whether this is the right approach. &lt;/li&gt;\n&lt;li&gt;Debezium? Being WAL-based, we feel it will suffer the same fate as DMS.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zhbd7z", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhbd7z/migrating_postgres_w_paritioned_tables_to_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhbd7z/migrating_postgres_w_paritioned_tables_to_redshift/", "subreddit_subscribers": 82489, "created_utc": 1670626268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wrote down some observations, and would love to hear yours.   \n[https://www.asanverse.com/why-data-projects-get-failed-reverse-etl/](https://www.asanverse.com/why-data-projects-get-failed-reverse-etl/)", "author_fullname": "t2_i2j8bdtn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is a data project delivering valuable insights for users still get failed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zi2cw7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670703499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote down some observations, and would love to hear yours.&lt;br/&gt;\n&lt;a href=\"https://www.asanverse.com/why-data-projects-get-failed-reverse-etl/\"&gt;https://www.asanverse.com/why-data-projects-get-failed-reverse-etl/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?auto=webp&amp;s=1f6570962697e192f874632ce64d36651cddb5f2", "width": 1000, "height": 501}, "resolutions": [{"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=26bb4299318c0ebdf935f8ea381bd5446353da57", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=88152dd1e2af6554dae0e7c06646988dd38cb059", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=622137554b579d28bcb6b349824635bcc1dad1cb", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d81b70acd5107625ae5670d59e1eed6c9d652730", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/GdAle-Wu47BxZ39_HYcMyRK_ytaCuhA4hsHAmPvtGT4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ed5cad063043d95adda5d722bd84e9e8d0104070", "width": 960, "height": 480}], "variants": {}, "id": "IpHXWC29OWlk894t8bJEURrR9ND6_veRIcSz_wmjqak"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zi2cw7", "is_robot_indexable": true, "report_reasons": null, "author": "ubukhary", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zi2cw7/why_is_a_data_project_delivering_valuable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zi2cw7/why_is_a_data_project_delivering_valuable/", "subreddit_subscribers": 82489, "created_utc": 1670703499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone did this swap? Is it doable?", "author_fullname": "t2_54sksh03", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "from data engineering to data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhzww2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670697575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone did this swap? Is it doable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zhzww2", "is_robot_indexable": true, "report_reasons": null, "author": "kiesket", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhzww2/from_data_engineering_to_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhzww2/from_data_engineering_to_data_science/", "subreddit_subscribers": 82489, "created_utc": 1670697575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, \n\nI have been using data factory successfully to move data from cosmos db to sql tables with data flows and JSON flattening options.   \n\n\nHowever I have come across a situation with data in cosmos  that requires some custom handling and I have successfully created the python script to do just that.   \n\n\nI am confused where and how I can run this piece of code to take data from the query running in data flow, pass it to python, flatten it with python, and then write the dataset to sql table, if possible using the sink connector in datafactory?  I am ok to use python to write the table with python if the sink is not possible.   \n\n\nThank you in advance.", "author_fullname": "t2_l1znu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to run python code in either datafactory or databricks to parse JSON data from cosmos db to SQL db in azure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhmgac", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670658790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I have been using data factory successfully to move data from cosmos db to sql tables with data flows and JSON flattening options.   &lt;/p&gt;\n\n&lt;p&gt;However I have come across a situation with data in cosmos  that requires some custom handling and I have successfully created the python script to do just that.   &lt;/p&gt;\n\n&lt;p&gt;I am confused where and how I can run this piece of code to take data from the query running in data flow, pass it to python, flatten it with python, and then write the dataset to sql table, if possible using the sink connector in datafactory?  I am ok to use python to write the table with python if the sink is not possible.   &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zhmgac", "is_robot_indexable": true, "report_reasons": null, "author": "boston101", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhmgac/where_to_run_python_code_in_either_datafactory_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhmgac/where_to_run_python_code_in_either_datafactory_or/", "subreddit_subscribers": 82489, "created_utc": 1670658790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to load some data into a db table and i wanted to know how to handle a situation where some of my partitions didn't get loaded into the table.\n\nShould I do a left anti join with the data in the table and then load the remaining data or is there a better approach available?", "author_fullname": "t2_t1rd5ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle failure of data load by spark job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhlykc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670656978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to load some data into a db table and i wanted to know how to handle a situation where some of my partitions didn&amp;#39;t get loaded into the table.&lt;/p&gt;\n\n&lt;p&gt;Should I do a left anti join with the data in the table and then load the remaining data or is there a better approach available?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zhlykc", "is_robot_indexable": true, "report_reasons": null, "author": "PR0K1NG", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/zhlykc/how_to_handle_failure_of_data_load_by_spark_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhlykc/how_to_handle_failure_of_data_load_by_spark_job/", "subreddit_subscribers": 82489, "created_utc": 1670656978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How we guarantee ACID integration over arbitrary SQL databases without a central monolith", "author_fullname": "t2_1d8tsh6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loosely coupled monoliths and where to find them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_zhpeit", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/P6-74TnPmtfZcjAXAE5hh2KUX61-IsZ7q-Iktr86am8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670668881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "itnext.io", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How we guarantee ACID integration over arbitrary SQL databases without a central monolith&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://itnext.io/loosely-coupled-monoliths-and-where-to-find-them-4004fac8ecc1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fFfbGLE0B-fJf3h9dQwq8asBhjwnQJT4aaQi_3L_b9g.jpg?auto=webp&amp;s=6b7d9871cd44e99c3b04b9fe56685aa54514b404", "width": 957, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/fFfbGLE0B-fJf3h9dQwq8asBhjwnQJT4aaQi_3L_b9g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a987bfc36fa2e7d6cb45ea00d26eb2dbbcfdf23", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/fFfbGLE0B-fJf3h9dQwq8asBhjwnQJT4aaQi_3L_b9g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=24861d0b9575680ac82b004ba70647afa07acb61", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/fFfbGLE0B-fJf3h9dQwq8asBhjwnQJT4aaQi_3L_b9g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a6fc0b52ed8473cc40f264445fb24fce14bf859", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/fFfbGLE0B-fJf3h9dQwq8asBhjwnQJT4aaQi_3L_b9g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=67153255c4d2b33ff7d80cbd2a305df3b954a75a", "width": 640, "height": 481}], "variants": {}, "id": "0cTQ2NWyC01Dq-h8MHCOUI-ikfcBaC2ht4B6s1xf9eY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zhpeit", "is_robot_indexable": true, "report_reasons": null, "author": "andras_gerlits", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhpeit/loosely_coupled_monoliths_and_where_to_find_them/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://itnext.io/loosely-coupled-monoliths-and-where-to-find-them-4004fac8ecc1", "subreddit_subscribers": 82489, "created_utc": 1670668881.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}