{"kind": "Listing", "data": {"after": "t3_zhl168", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/s8llls4you4a1.png?width=460&amp;format=png&amp;auto=webp&amp;s=0e7cccdde22e2da0be5d14dff0e0f7f02d664628\n\nI love the irony of this :D \n\n&amp;#x200B;\n\n(and probably also the meta-paradox of being a jerk by posting this thus violating the very rule I'm citing \ud83d\ude09 )", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dates are hard\u2014we can relate to that, can't we r/dataengineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"s8llls4you4a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 109, "x": 108, "u": "https://preview.redd.it/s8llls4you4a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3f756ff21ba91600cd4d8c7c74f1d322c80d4937"}, {"y": 219, "x": 216, "u": "https://preview.redd.it/s8llls4you4a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=36c3599e24a661f896517cb1bb61cb7490a02724"}, {"y": 324, "x": 320, "u": "https://preview.redd.it/s8llls4you4a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=da1061070f16f5a33564ac4cd46d6d066cff4802"}], "s": {"y": 467, "x": 460, "u": "https://preview.redd.it/s8llls4you4a1.png?width=460&amp;format=png&amp;auto=webp&amp;s=0e7cccdde22e2da0be5d14dff0e0f7f02d664628"}, "id": "s8llls4you4a1"}}, "name": "t3_zgtrnk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 111, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 111, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/lRoOsAy7xdttw5-id-v1nfKCGtkz5CUP62h43Pfxseo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670582282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s8llls4you4a1.png?width=460&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e7cccdde22e2da0be5d14dff0e0f7f02d664628\"&gt;https://preview.redd.it/s8llls4you4a1.png?width=460&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e7cccdde22e2da0be5d14dff0e0f7f02d664628&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I love the irony of this :D &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(and probably also the meta-paradox of being a jerk by posting this thus violating the very rule I&amp;#39;m citing \ud83d\ude09 )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "zgtrnk", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgtrnk/dates_are_hardwe_can_relate_to_that_cant_we/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgtrnk/dates_are_hardwe_can_relate_to_that_cant_we/", "subreddit_subscribers": 82431, "created_utc": 1670582282.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So far I've created a Python script to merge the critical data from each excel into a JSON file so every file it's an object, it takes approximately half a second per excel file so I'm going to parse 7 thousand at a time so I doesn't take the whole day. \nThe thing is that the data isn't clean, so I don't think JSON it's the best file type for this scenario. \nHave you done something similar? Any advice?\n\nEdit: Sorry, Forgot to mention, although the data is stored in excels it is not in tabular. \nEach file it's basically a QA test register, consist of a unique ID, multiple parameters containing part number, date, qty and stuff like that, then it contains the amount of features measured and each feature measure can have multiple samples", "author_fullname": "t2_5ho9t8h4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have over 70,000 excel files that I need to analyze.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zh0asy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670604366.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670599985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So far I&amp;#39;ve created a Python script to merge the critical data from each excel into a JSON file so every file it&amp;#39;s an object, it takes approximately half a second per excel file so I&amp;#39;m going to parse 7 thousand at a time so I doesn&amp;#39;t take the whole day. \nThe thing is that the data isn&amp;#39;t clean, so I don&amp;#39;t think JSON it&amp;#39;s the best file type for this scenario. \nHave you done something similar? Any advice?&lt;/p&gt;\n\n&lt;p&gt;Edit: Sorry, Forgot to mention, although the data is stored in excels it is not in tabular. \nEach file it&amp;#39;s basically a QA test register, consist of a unique ID, multiple parameters containing part number, date, qty and stuff like that, then it contains the amount of features measured and each feature measure can have multiple samples&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zh0asy", "is_robot_indexable": true, "report_reasons": null, "author": "Slayne_S", "discussion_type": null, "num_comments": 62, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh0asy/i_have_over_70000_excel_files_that_i_need_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh0asy/i_have_over_70000_excel_files_that_i_need_to/", "subreddit_subscribers": 82431, "created_utc": 1670599985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This one seems confusing to me as if you don't run create if not exists every time then technically your batch job is not idempotent. On the other hand, it seems inelegant to have complicated table definitions etc in your batch job.\n\nFor context I'm working with pyspark jobs on databricks.", "author_fullname": "t2_18xb5x05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you have create table jobs/pipelines seperate from regularly run update pipelines? Or do you just run create if not exists every time you run your batch jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zh0sb5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670601212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This one seems confusing to me as if you don&amp;#39;t run create if not exists every time then technically your batch job is not idempotent. On the other hand, it seems inelegant to have complicated table definitions etc in your batch job.&lt;/p&gt;\n\n&lt;p&gt;For context I&amp;#39;m working with pyspark jobs on databricks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zh0sb5", "is_robot_indexable": true, "report_reasons": null, "author": "the-data-scientist", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh0sb5/do_you_have_create_table_jobspipelines_seperate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh0sb5/do_you_have_create_table_jobspipelines_seperate/", "subreddit_subscribers": 82431, "created_utc": 1670601212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w2y7e99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacing Looker with Cube and Metabase", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_zgvvsz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/o3_3qur-Hl0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Replacing Looker with Cube and Metabase\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Replacing Looker with Cube and Metabase", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/o3_3qur-Hl0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Replacing Looker with Cube and Metabase\"&gt;&lt;/iframe&gt;", "author_name": "Cube", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/o3_3qur-Hl0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@cube8910"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/o3_3qur-Hl0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Replacing Looker with Cube and Metabase\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/zgvvsz", "height": 200}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YmpGAtrzB9E7h1Vk3tkK6lZ1zlaRbu2MYLbvtljxYPI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670588424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=o3_3qur-Hl0", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hWtyNNl6WciiPeYyQJqCtdWwoHLpx_jAV7nAVzn8GDA.jpg?auto=webp&amp;s=d5aac53f42371f81fbeef23c370b1cbb18895cd9", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/hWtyNNl6WciiPeYyQJqCtdWwoHLpx_jAV7nAVzn8GDA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ccc476f4ed901ef702f24ac7a7aafbaec7313f22", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/hWtyNNl6WciiPeYyQJqCtdWwoHLpx_jAV7nAVzn8GDA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bd4958722e896d207507f1105a5ee64f59292e6c", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/hWtyNNl6WciiPeYyQJqCtdWwoHLpx_jAV7nAVzn8GDA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2db1858a5ef4affb38c8c04bea4d9fdceab50014", "width": 320, "height": 240}], "variants": {}, "id": "CJFTP2Zny9qZBWmH_kAC0Z7hc80p1yvrqY10mRzaE4Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "zgvvsz", "is_robot_indexable": true, "report_reasons": null, "author": "whichalps", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgvvsz/replacing_looker_with_cube_and_metabase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=o3_3qur-Hl0", "subreddit_subscribers": 82431, "created_utc": 1670588424.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Replacing Looker with Cube and Metabase", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/o3_3qur-Hl0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Replacing Looker with Cube and Metabase\"&gt;&lt;/iframe&gt;", "author_name": "Cube", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/o3_3qur-Hl0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@cube8910"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_wqct3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Conversation with chatGPT on Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_zhm66m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/j6QJXqBiJhPLVEgoUN1oygxPn2o_MWUFDLoNQpFWl74.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670657750.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "link.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://link.medium.com/ScfLkXrHDvb", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?auto=webp&amp;s=802152dcad572430c26b31d3441447118d6b7686", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d9933bfc52c090d2b690b0632c57808fada55ed", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9fee743a6a013e2943dd90a7ff0e352d8344726", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33309caadaaa69438579868ea0bfcbd1bcc1b3ac", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=730ff9e726b3520935bfcefcdd17f9a9e704a6ac", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dd6d9af53c22efdd0fa0896d3023265f19330222", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/IkwOdsZ3PlG5moPs0_nCu4EdPJlR2uL7XmwBouJh9Wg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c25ce605074dbe468c8fc1b5d3ce67f36ed0c657", "width": 1080, "height": 607}], "variants": {}, "id": "jKaQlVsP7O4k3Ibxba0Vhjw-0dOfpUpXCJJ_yFlcxew"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zhm66m", "is_robot_indexable": true, "report_reasons": null, "author": "Koushik5586", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhm66m/a_conversation_with_chatgpt_on_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://link.medium.com/ScfLkXrHDvb", "subreddit_subscribers": 82431, "created_utc": 1670657750.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The new way of creating Sensors in Apache Airflow! \ud83e\udef6", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_zh096u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/QYZEqGGgvyRtgAsk1QABGdNW7bECXjuSXvyf1O8eBR0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670599872.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/posts/marclamberti_airflow-apacheairflow-dataengineering-activity-7006984438110670848-d-p7?utm_source=share&amp;utm_medium=member_desktop", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oJknx6cTFRscfGdJIsTMbfqf06Fv5fmQhXj_yp80qOo.jpg?auto=webp&amp;s=5b233c2c1455e8ff462eb90e113da7221296f40f", "width": 1042, "height": 544}, "resolutions": [{"url": "https://external-preview.redd.it/oJknx6cTFRscfGdJIsTMbfqf06Fv5fmQhXj_yp80qOo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a5f9d38fd913415726afe77428ba64ab0708533", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oJknx6cTFRscfGdJIsTMbfqf06Fv5fmQhXj_yp80qOo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=34271b22c2e3dd27275f68a40bcf46b05a5b2b15", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/oJknx6cTFRscfGdJIsTMbfqf06Fv5fmQhXj_yp80qOo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dd385f8fe0654c50c40bf397b220dcb4485c712b", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/oJknx6cTFRscfGdJIsTMbfqf06Fv5fmQhXj_yp80qOo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3abe88e0d983064e21d9abbe70d8a59f1dfbe316", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/oJknx6cTFRscfGdJIsTMbfqf06Fv5fmQhXj_yp80qOo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cdcad97c501f073d584fbf374b6b7220fc593173", "width": 960, "height": 501}], "variants": {}, "id": "INVLTnqVQs_P7yaCQEoPAvOsj4Ol3PZKsrTyuWMcxSk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zh096u", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh096u/the_new_way_of_creating_sensors_in_apache_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/posts/marclamberti_airflow-apacheairflow-dataengineering-activity-7006984438110670848-d-p7?utm_source=share&amp;utm_medium=member_desktop", "subreddit_subscribers": 82431, "created_utc": 1670599872.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey I have been trying to solve this issue for the past couple days. We are looking to bring in DBT and our snowflake accounts are split by environment. So we have 3 accounts (Dev, qa, prod). \n\nDoes anyone use DBT with 3 snowflake accounts? What is your branching strategy? \n\nI want to use trunk based (1 long lived branch) but I can\u2019t figure out how to do that currently. \n\nI would appreciate any help on this!", "author_fullname": "t2_1n3qfa0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT + Snowflake Envs Account level", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zha4fc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670623385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey I have been trying to solve this issue for the past couple days. We are looking to bring in DBT and our snowflake accounts are split by environment. So we have 3 accounts (Dev, qa, prod). &lt;/p&gt;\n\n&lt;p&gt;Does anyone use DBT with 3 snowflake accounts? What is your branching strategy? &lt;/p&gt;\n\n&lt;p&gt;I want to use trunk based (1 long lived branch) but I can\u2019t figure out how to do that currently. &lt;/p&gt;\n\n&lt;p&gt;I would appreciate any help on this!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zha4fc", "is_robot_indexable": true, "report_reasons": null, "author": "Culpgrant21", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zha4fc/dbt_snowflake_envs_account_level/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zha4fc/dbt_snowflake_envs_account_level/", "subreddit_subscribers": 82431, "created_utc": 1670623385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I am in somewhat a tough situation. I have been offered a job offer and contract with all benefits and salary mentioned, but it's contingent on certain criteria, all which I'm fine with except for a reference from my current manager. I don't mind giving reference of my manager, but I feel like there's a risk that if I ask reference from my manager, and give notice thereafter, and if the job offer is rescinded from the new place for some reason, I will be left in a difficult situation where my manager will not trust me thereafter.\n\nHas anyone else experienced such a dilemma? HR from new place says that I can give my notice safely and just ask HR/Manager from old place for reference since all other checks (background, drugs etc.) have been done.", "author_fullname": "t2_l35gwhuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Offer in Writing, but Contingent on Reference From Current Manager (UK)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zh0i52", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670600485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I am in somewhat a tough situation. I have been offered a job offer and contract with all benefits and salary mentioned, but it&amp;#39;s contingent on certain criteria, all which I&amp;#39;m fine with except for a reference from my current manager. I don&amp;#39;t mind giving reference of my manager, but I feel like there&amp;#39;s a risk that if I ask reference from my manager, and give notice thereafter, and if the job offer is rescinded from the new place for some reason, I will be left in a difficult situation where my manager will not trust me thereafter.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else experienced such a dilemma? HR from new place says that I can give my notice safely and just ask HR/Manager from old place for reference since all other checks (background, drugs etc.) have been done.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "zh0i52", "is_robot_indexable": true, "report_reasons": null, "author": "Senior_Anteater4688", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh0i52/job_offer_in_writing_but_contingent_on_reference/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh0i52/job_offer_in_writing_but_contingent_on_reference/", "subreddit_subscribers": 82431, "created_utc": 1670600485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just started my first DE job and am the only DE here. I'm trying to work my way through DE courses to architect something up which I'm currently building as a demo. I'm working with a pretty much non-existent senior DE who doesn't have much technical or programming knowledge but rather is a manager. \n\nI'm here for 3 months now and I'm quite unhappy as I thought I'd have better mentorship and leadership however, it really is a one man band here. I'm unsure as to whether to find a new job or not now. What should I do?", "author_fullname": "t2_tgwctzuo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm a junior/grad, is it normal for me to be the ONLY data engineer in a company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zgvc4r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670586909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just started my first DE job and am the only DE here. I&amp;#39;m trying to work my way through DE courses to architect something up which I&amp;#39;m currently building as a demo. I&amp;#39;m working with a pretty much non-existent senior DE who doesn&amp;#39;t have much technical or programming knowledge but rather is a manager. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m here for 3 months now and I&amp;#39;m quite unhappy as I thought I&amp;#39;d have better mentorship and leadership however, it really is a one man band here. I&amp;#39;m unsure as to whether to find a new job or not now. What should I do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zgvc4r", "is_robot_indexable": true, "report_reasons": null, "author": "Icy_Requirement_3635", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgvc4r/im_a_juniorgrad_is_it_normal_for_me_to_be_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgvc4r/im_a_juniorgrad_is_it_normal_for_me_to_be_the/", "subreddit_subscribers": 82431, "created_utc": 1670586909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "even more so for cdc type of data/streaming data without affecting its real time latency\n\nwhere/how does it fit in your data architecture", "author_fullname": "t2_epjy5nh4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how do you do anonymisation in your organisation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zh4gho", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670610021.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670609819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;even more so for cdc type of data/streaming data without affecting its real time latency&lt;/p&gt;\n\n&lt;p&gt;where/how does it fit in your data architecture&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zh4gho", "is_robot_indexable": true, "report_reasons": null, "author": "Spare-Youth-6874", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh4gho/how_do_you_do_anonymisation_in_your_organisation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh4gho/how_do_you_do_anonymisation_in_your_organisation/", "subreddit_subscribers": 82431, "created_utc": 1670609819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know if its possible to use Synapse Spark Pool with DBT? If so can you share your experience with that set up? I see documentation for using Synapse dedicated and seeverless SQL pool with DBT but not the Spark Pool. All the DBT Spark documentation is for databricks and AWS EMR. Any insight would help", "author_fullname": "t2_l15do6pb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT + Spark (synapse)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zh0whm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670601488.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know if its possible to use Synapse Spark Pool with DBT? If so can you share your experience with that set up? I see documentation for using Synapse dedicated and seeverless SQL pool with DBT but not the Spark Pool. All the DBT Spark documentation is for databricks and AWS EMR. Any insight would help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zh0whm", "is_robot_indexable": true, "report_reasons": null, "author": "Fearless_Strategy_19", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh0whm/dbt_spark_synapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh0whm/dbt_spark_synapse/", "subreddit_subscribers": 82431, "created_utc": 1670601488.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the general consensus on where a production **database** for **Airflow** should be?\n\nI usually start an Airflow Docker container. Airflow rightfully suggests to not use SQLite, but instead switch to Postgres or Mysql. \n\nNow, from an architecture and resilience point of view:\n\n1. Should I install the RDBMS (I choose Postgres) in the same Docker container as Airflow?\n2. or should I put the database in an extra container?", "author_fullname": "t2_136crg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow DB: In the same container or a separate one?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zgy0w1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670594275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the general consensus on where a production &lt;strong&gt;database&lt;/strong&gt; for &lt;strong&gt;Airflow&lt;/strong&gt; should be?&lt;/p&gt;\n\n&lt;p&gt;I usually start an Airflow Docker container. Airflow rightfully suggests to not use SQLite, but instead switch to Postgres or Mysql. &lt;/p&gt;\n\n&lt;p&gt;Now, from an architecture and resilience point of view:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Should I install the RDBMS (I choose Postgres) in the same Docker container as Airflow?&lt;/li&gt;\n&lt;li&gt;or should I put the database in an extra container?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zgy0w1", "is_robot_indexable": true, "report_reasons": null, "author": "Boruroku", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgy0w1/airflow_db_in_the_same_container_or_a_separate_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgy0w1/airflow_db_in_the_same_container_or_a_separate_one/", "subreddit_subscribers": 82431, "created_utc": 1670594275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hoping to find someone to hop on a call and look over a take home assignment for a mid-level DE job with me. \n\nTo be clear, I'm not looking for anyone to do any work for me-- just critique the answers I have already written. At my current job nobody writes python, so I don't have any experience writing python in a shared codebase and the conventions that might come along with that. As a result, I'm concerned about my code coming off as amateurish. \n\nIf any generous soul would be willing to help me out for a little bit, it would be much appreciated. Am willing to compensate at your hourly. Feel free to dm or comment. Can be discord/teams/Skype etc.", "author_fullname": "t2_kdj5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for an experienced DE to walk through/critique take-home python assignment Saturday morning/afternoon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhm02s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670657136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hoping to find someone to hop on a call and look over a take home assignment for a mid-level DE job with me. &lt;/p&gt;\n\n&lt;p&gt;To be clear, I&amp;#39;m not looking for anyone to do any work for me-- just critique the answers I have already written. At my current job nobody writes python, so I don&amp;#39;t have any experience writing python in a shared codebase and the conventions that might come along with that. As a result, I&amp;#39;m concerned about my code coming off as amateurish. &lt;/p&gt;\n\n&lt;p&gt;If any generous soul would be willing to help me out for a little bit, it would be much appreciated. Am willing to compensate at your hourly. Feel free to dm or comment. Can be discord/teams/Skype etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zhm02s", "is_robot_indexable": true, "report_reasons": null, "author": "Touvejs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhm02s/looking_for_an_experienced_de_to_walk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhm02s/looking_for_an_experienced_de_to_walk/", "subreddit_subscribers": 82431, "created_utc": 1670657136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am coming across this term called Shard when I was reading some Docs on Pub/Sub in GCP and wanted to know what exactly is this? I have read some articles which I found on google. But can some one explain this to me in easy terms.   \nThanks in Advance.", "author_fullname": "t2_76h7jr47", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a Shard?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zh4uqs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670610718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am coming across this term called Shard when I was reading some Docs on Pub/Sub in GCP and wanted to know what exactly is this? I have read some articles which I found on google. But can some one explain this to me in easy terms.&lt;br/&gt;\nThanks in Advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zh4uqs", "is_robot_indexable": true, "report_reasons": null, "author": "s1va1209", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh4uqs/what_is_a_shard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh4uqs/what_is_a_shard/", "subreddit_subscribers": 82431, "created_utc": 1670610718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, \n\nI have been using data factory successfully to move data from cosmos db to sql tables with data flows and JSON flattening options.   \n\n\nHowever I have come across a situation with data in cosmos  that requires some custom handling and I have successfully created the python script to do just that.   \n\n\nI am confused where and how I can run this piece of code to take data from the query running in data flow, pass it to python, flatten it with python, and then write the dataset to sql table, if possible using the sink connector in datafactory?  I am ok to use python to write the table with python if the sink is not possible.   \n\n\nThank you in advance.", "author_fullname": "t2_l1znu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to run python code in either datafactory or databricks to parse JSON data from cosmos db to SQL db in azure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhmgac", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670658790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I have been using data factory successfully to move data from cosmos db to sql tables with data flows and JSON flattening options.   &lt;/p&gt;\n\n&lt;p&gt;However I have come across a situation with data in cosmos  that requires some custom handling and I have successfully created the python script to do just that.   &lt;/p&gt;\n\n&lt;p&gt;I am confused where and how I can run this piece of code to take data from the query running in data flow, pass it to python, flatten it with python, and then write the dataset to sql table, if possible using the sink connector in datafactory?  I am ok to use python to write the table with python if the sink is not possible.   &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zhmgac", "is_robot_indexable": true, "report_reasons": null, "author": "boston101", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhmgac/where_to_run_python_code_in_either_datafactory_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhmgac/where_to_run_python_code_in_either_datafactory_or/", "subreddit_subscribers": 82431, "created_utc": 1670658790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to load some data into a db table and i wanted to know how to handle a situation where some of my partitions didn't get loaded into the table.\n\nShould I do a left anti join with the data in the table and then load the remaining data or is there a better approach available?", "author_fullname": "t2_t1rd5ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle failure of data load by spark job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhlykc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670656978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to load some data into a db table and i wanted to know how to handle a situation where some of my partitions didn&amp;#39;t get loaded into the table.&lt;/p&gt;\n\n&lt;p&gt;Should I do a left anti join with the data in the table and then load the remaining data or is there a better approach available?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zhlykc", "is_robot_indexable": true, "report_reasons": null, "author": "PR0K1NG", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/zhlykc/how_to_handle_failure_of_data_load_by_spark_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhlykc/how_to_handle_failure_of_data_load_by_spark_job/", "subreddit_subscribers": 82431, "created_utc": 1670656978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7g8ahtr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advanced DBT Macros", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "name": "t3_zhjvcw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kUnox4hHhEct0IHebUENb9Hb_Bq57dHmF2u2b8AVlOI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670649765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@kadek/advanced-dbt-macros-c274f1cfa989", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?auto=webp&amp;s=319ba75d9c5d15b9ee79ef24fe04b672f5ab2d25", "width": 1200, "height": 658}, "resolutions": [{"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6daed398df2e0a759ca234ad545f82d5dd8bb0e", "width": 108, "height": 59}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6599dc4486e9c8775acfd4240f604832ce65ef0e", "width": 216, "height": 118}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdd29e283e196351479b87c4b93f1bde00cad3fc", "width": 320, "height": 175}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=018afecb2358addb0ef361e6db0e3abfa59ff4ed", "width": 640, "height": 350}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4a419a06364430f23db47e24443c356ec63cc3ef", "width": 960, "height": 526}, {"url": "https://external-preview.redd.it/2stQ0mVA73meCBnM_9uICpGT5EDAsod4g3SUpg5c_SA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=739089bae6a0ba18df34b02b8ef8145a9f7b067a", "width": 1080, "height": 592}], "variants": {}, "id": "EIbAEFWYS3HzVZSKuLBYDA8qdb6Kgl7d7hGOqBbvNVw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zhjvcw", "is_robot_indexable": true, "report_reasons": null, "author": "hjkl_ornah", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhjvcw/advanced_dbt_macros/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@kadek/advanced-dbt-macros-c274f1cfa989", "subreddit_subscribers": 82431, "created_utc": 1670649765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wonder how in demand Databricks is. I\u2019m curious cause my company offers trainings in some technologies and Databricks is one of them. However most of the job vacancies I see, at least around me, are not asking for Databricks skills/experience specifically. A lot of them mention Spark, but not Databricks specifically. I\u2019m curious to see how popular it is in this community.\n\n[View Poll](https://www.reddit.com/poll/zhcfid)", "author_fullname": "t2_8dnn00ks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Databricks in demand? How often do you use it at your job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhcfid", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670628728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wonder how in demand Databricks is. I\u2019m curious cause my company offers trainings in some technologies and Databricks is one of them. However most of the job vacancies I see, at least around me, are not asking for Databricks skills/experience specifically. A lot of them mention Spark, but not Databricks specifically. I\u2019m curious to see how popular it is in this community.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/zhcfid\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zhcfid", "is_robot_indexable": true, "report_reasons": null, "author": "Se7enEl11ven", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1670887928052, "options": [{"text": "I use it Daily/Frequently", "id": "20289706"}, {"text": "I use it from time to time/Depends on projects", "id": "20289707"}, {"text": "I don\u2019t use it but already did (Old job or project)", "id": "20289708"}, {"text": "Never used it", "id": "20289709"}, {"text": "Never used it and never heard of it before", "id": "20289710"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 197, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhcfid/is_databricks_in_demand_how_often_do_you_use_it/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/zhcfid/is_databricks_in_demand_how_often_do_you_use_it/", "subreddit_subscribers": 82431, "created_utc": 1670628728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nPreviously, our Postgres tables were single monolithic tables. We successfully used AWS DMS to migrate the data in them toward Redshift. 1 to 1 table mappings, no problem. Smiles every day.\n\nNow, we're frowning big time!\n\nFor some of the Postgres tables that are expected to grow quickly, we've implemented partitioning. DMS clearly states that it won't work as expected under these conditions.\n\n[https://docs.aws.amazon.com/dms/latest/userguide/CHAP\\_Source.PostgreSQL.html#CHAP\\_Source.PostgreSQL.Limitations](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.Limitations)\n\nThe root cause of the incompatibilities is that the Postgres WAL entries are no longer associated with the top-level table, but rather ONLY with the partition tables underneath. This has several undesired side-effects, and supports what is said in the AWS documentation.\n\nWhile we've mangled DMS to jankily work here, it is not robust in this scenario.\n\nPlease would you clever folks help me tackle this specific migration problem? If I can provide any more info, I'd be happy to.\n\nEDIT: I should mention what we've tried. We have tried or considered several things.\n\n1. A scheduled Glue job? We're comfortable with Glue, but are attracted to no-code solutions for this. We'd accept Glue, we just haven't yet tried it here.\n2. MSK Connect? We've tried MSK Connect, sending data to S3 rather than Redshift (leaving the S3 -&gt; Redshift problem for later). Seems to work well. It does incremental batches based on our 'updated\\_at' field. There are some concerns about performance and/or whether this is the right approach. \n3. Debezium? Being WAL-based, we feel it will suffer the same fate as DMS.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating Postgres \\w Paritioned Tables to Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhbd7z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670626715.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670626268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Previously, our Postgres tables were single monolithic tables. We successfully used AWS DMS to migrate the data in them toward Redshift. 1 to 1 table mappings, no problem. Smiles every day.&lt;/p&gt;\n\n&lt;p&gt;Now, we&amp;#39;re frowning big time!&lt;/p&gt;\n\n&lt;p&gt;For some of the Postgres tables that are expected to grow quickly, we&amp;#39;ve implemented partitioning. DMS clearly states that it won&amp;#39;t work as expected under these conditions.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.Limitations\"&gt;https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.Limitations&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The root cause of the incompatibilities is that the Postgres WAL entries are no longer associated with the top-level table, but rather ONLY with the partition tables underneath. This has several undesired side-effects, and supports what is said in the AWS documentation.&lt;/p&gt;\n\n&lt;p&gt;While we&amp;#39;ve mangled DMS to jankily work here, it is not robust in this scenario.&lt;/p&gt;\n\n&lt;p&gt;Please would you clever folks help me tackle this specific migration problem? If I can provide any more info, I&amp;#39;d be happy to.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I should mention what we&amp;#39;ve tried. We have tried or considered several things.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A scheduled Glue job? We&amp;#39;re comfortable with Glue, but are attracted to no-code solutions for this. We&amp;#39;d accept Glue, we just haven&amp;#39;t yet tried it here.&lt;/li&gt;\n&lt;li&gt;MSK Connect? We&amp;#39;ve tried MSK Connect, sending data to S3 rather than Redshift (leaving the S3 -&amp;gt; Redshift problem for later). Seems to work well. It does incremental batches based on our &amp;#39;updated_at&amp;#39; field. There are some concerns about performance and/or whether this is the right approach. &lt;/li&gt;\n&lt;li&gt;Debezium? Being WAL-based, we feel it will suffer the same fate as DMS.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zhbd7z", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhbd7z/migrating_postgres_w_paritioned_tables_to_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhbd7z/migrating_postgres_w_paritioned_tables_to_redshift/", "subreddit_subscribers": 82431, "created_utc": 1670626268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to understand what\u2019s the best way to troubleshoot and debug data inconsistency in prod or downstream applications. \n\nDefinitely live debugging and tinkering around prod data is not an option, although I\u2019m guilty of having done that on few occasions. \n\nWhat do you folks do? What does the process look like? Reproducing is not exactly possible bcz the state of data keeps changing and you never know which data caused the error. \n\nOne rogue way of dealing with it is blindly rerunning that pipeline hoping and praying it writes clean data this time. \n\nHow do you troubleshoot data issues in prod?", "author_fullname": "t2_8r6amwln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debugging data errors or inconsistencies in downstream dashboards", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zgxm83", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670593177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to understand what\u2019s the best way to troubleshoot and debug data inconsistency in prod or downstream applications. &lt;/p&gt;\n\n&lt;p&gt;Definitely live debugging and tinkering around prod data is not an option, although I\u2019m guilty of having done that on few occasions. &lt;/p&gt;\n\n&lt;p&gt;What do you folks do? What does the process look like? Reproducing is not exactly possible bcz the state of data keeps changing and you never know which data caused the error. &lt;/p&gt;\n\n&lt;p&gt;One rogue way of dealing with it is blindly rerunning that pipeline hoping and praying it writes clean data this time. &lt;/p&gt;\n\n&lt;p&gt;How do you troubleshoot data issues in prod?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zgxm83", "is_robot_indexable": true, "report_reasons": null, "author": "money_noob_007", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgxm83/debugging_data_errors_or_inconsistencies_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgxm83/debugging_data_errors_or_inconsistencies_in/", "subreddit_subscribers": 82431, "created_utc": 1670593177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tutorial: Multi-Table Transactions on the Lakehouse \u2013 Enabled by Dremio Arctic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_zgv89y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/q5-aSKAzjU-CqI2KRGxhX1sFkncEcNbrqUmQ7DhIgR8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670586601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dremio.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dremio.com/blog/multi-table-transactions-on-the-lakehouse-enabled-by-dremio-arctic/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?auto=webp&amp;s=677cfd2acf5f5314a32e08944e0951a1f33653f6", "width": 1201, "height": 629}, "resolutions": [{"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=44822d4e1e6d4168f34ab6113de12db8eb8237fa", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8ca6804f857e3ac1eeb4cdfdefda2979d98a202c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac4453013c6e922f576686e26eaf4f02abc504ae", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fba584a4d0519c1a5d41c1cf6ab8d567abea9fac", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2b91903c24fc515615dda409ebf8c60adcbf5c4", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/-STl7dV3co2QunLN9qv77qAjg6DlJj4RxCYeZzRo9Gw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28a6061daf8a4c19c2cbe3e8970f1b8bdc882434", "width": 1080, "height": 565}], "variants": {}, "id": "LBpbW9OLTwQyp-VamoofyQF1Ff8aV6ZdpWiaa-KyRig"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zgv89y", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgv89y/tutorial_multitable_transactions_on_the_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dremio.com/blog/multi-table-transactions-on-the-lakehouse-enabled-by-dremio-arctic/", "subreddit_subscribers": 82431, "created_utc": 1670586601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long Story Short: Company sent laptop early before joining - Later, I clearly mentioned I am not joining and asked them how to return it. Both on call and mail, but no response.\n\nIt's been almost 2 years, can I start using the laptop? Is there anyway they track it? \n\nPS: It's Windows laptop       \n\n[View Poll](https://www.reddit.com/poll/zgv4a8)", "author_fullname": "t2_8h8kwca0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I Start using Startup Laptop? It's been 2 years and they did not collet it!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zgv4a8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670586289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long Story Short: Company sent laptop early before joining - Later, I clearly mentioned I am not joining and asked them how to return it. Both on call and mail, but no response.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been almost 2 years, can I start using the laptop? Is there anyway they track it? &lt;/p&gt;\n\n&lt;p&gt;PS: It&amp;#39;s Windows laptop       &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/zgv4a8\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zgv4a8", "is_robot_indexable": true, "report_reasons": null, "author": "Rude-Veterinarian-45", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1670845489290, "options": [{"text": "Start Using, its been 2 years", "id": "20279087"}, {"text": "Don't Use it, they can track", "id": "20279088"}, {"text": "Other, please comment", "id": "20279089"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 112, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgv4a8/can_i_start_using_startup_laptop_its_been_2_years/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/zgv4a8/can_i_start_using_startup_laptop_its_been_2_years/", "subreddit_subscribers": 82431, "created_utc": 1670586289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have mostly been a full-stack developer with experience in cloud technology as well. This is my first big work involving data pipeline.\n\nProblem - currently data scientists run ad-hoc sqoop jobs to query data periodically. We need to schedule the same using airflow. We need to store the query results in s3 bucket as it is used for ML mod training.\n\nConstraint- we have to run these airflow jobs to trigger queries with kubeOperator, which I don\u2019t think can handle millions of records that are generated everyday. Much worse if it has to be done monthly. \n\nHow should we go about it? Use the airflow job to remotely run sqoop job? Or use Kafka connect ( have no clue about it) or something else?", "author_fullname": "t2_yn202", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving from ad-hoc sqoop job to Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zgtata", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670580827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have mostly been a full-stack developer with experience in cloud technology as well. This is my first big work involving data pipeline.&lt;/p&gt;\n\n&lt;p&gt;Problem - currently data scientists run ad-hoc sqoop jobs to query data periodically. We need to schedule the same using airflow. We need to store the query results in s3 bucket as it is used for ML mod training.&lt;/p&gt;\n\n&lt;p&gt;Constraint- we have to run these airflow jobs to trigger queries with kubeOperator, which I don\u2019t think can handle millions of records that are generated everyday. Much worse if it has to be done monthly. &lt;/p&gt;\n\n&lt;p&gt;How should we go about it? Use the airflow job to remotely run sqoop job? Or use Kafka connect ( have no clue about it) or something else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zgtata", "is_robot_indexable": true, "report_reasons": null, "author": "litti_wala", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zgtata/moving_from_adhoc_sqoop_job_to_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zgtata/moving_from_adhoc_sqoop_job_to_airflow/", "subreddit_subscribers": 82431, "created_utc": 1670580827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ever wonder what BI tools other teams are using? We've gathered data from over 250 data teams and outlined the most popular BI tools used by Secoda customers in 2022: https://www.secoda.co/blog/top-5-business-intelligence-tools", "author_fullname": "t2_aiinah9q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Top 5 business intelligence tools (based on over 250 data teams)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zh768a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670616241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever wonder what BI tools other teams are using? We&amp;#39;ve gathered data from over 250 data teams and outlined the most popular BI tools used by Secoda customers in 2022: &lt;a href=\"https://www.secoda.co/blog/top-5-business-intelligence-tools\"&gt;https://www.secoda.co/blog/top-5-business-intelligence-tools&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ObfKZ_8zvF5Srb8sORcNGuFAE3Pm1xV-29YAl3ZpLU0.jpg?auto=webp&amp;s=4a7be7b670f7bcda1ada014d20855437305db957", "width": 767, "height": 448}, "resolutions": [{"url": "https://external-preview.redd.it/ObfKZ_8zvF5Srb8sORcNGuFAE3Pm1xV-29YAl3ZpLU0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d26e8292fce31a830851ce27b2ec0ce70efc60c", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/ObfKZ_8zvF5Srb8sORcNGuFAE3Pm1xV-29YAl3ZpLU0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c026311ccad9c2794d7fedd61ef5fe6f7070e499", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/ObfKZ_8zvF5Srb8sORcNGuFAE3Pm1xV-29YAl3ZpLU0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4cbbf6b73c3a26e61840a55fda9ed4a4eb09eda3", "width": 320, "height": 186}, {"url": "https://external-preview.redd.it/ObfKZ_8zvF5Srb8sORcNGuFAE3Pm1xV-29YAl3ZpLU0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=21032ca66176918321a8b632697179c12a1b9849", "width": 640, "height": 373}], "variants": {}, "id": "3xy2BqEaM_UDTGtVbw-A3eDKj-kygQF7HPVI6AuGkz4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zh768a", "is_robot_indexable": true, "report_reasons": null, "author": "secodaHQ", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zh768a/top_5_business_intelligence_tools_based_on_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zh768a/top_5_business_intelligence_tools_based_on_over/", "subreddit_subscribers": 82431, "created_utc": 1670616241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nWhat kind of big data service a new agency could provide?\n\nAs I am looking to know:\n\n- The services that data companies sell\n- Example of the value of the services.\n- The company name to do a research.\n- Finally, the average price of these services.\n\n\nPlease provide any kind of resource, courses, youtube channels or books in order to me to learn more.\n\nThank you so much.", "author_fullname": "t2_372ntpiy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Big Data Services", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zhl168", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670653603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;What kind of big data service a new agency could provide?&lt;/p&gt;\n\n&lt;p&gt;As I am looking to know:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The services that data companies sell&lt;/li&gt;\n&lt;li&gt;Example of the value of the services.&lt;/li&gt;\n&lt;li&gt;The company name to do a research.&lt;/li&gt;\n&lt;li&gt;Finally, the average price of these services.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Please provide any kind of resource, courses, youtube channels or books in order to me to learn more.&lt;/p&gt;\n\n&lt;p&gt;Thank you so much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zhl168", "is_robot_indexable": true, "report_reasons": null, "author": "SaifAlnuaimi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zhl168/big_data_services/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zhl168/big_data_services/", "subreddit_subscribers": 82431, "created_utc": 1670653603.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}