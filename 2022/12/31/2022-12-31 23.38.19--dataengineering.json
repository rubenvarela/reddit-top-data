{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free \"dbt for beginners\" course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_zzvb1o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 78, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 78, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RaeQZ3P2tYb6Uzky98mgMcGSEYnZe_2P2PGi2ZL1_0I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1672497046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataschool.alterclass.school", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataschool.alterclass.school/courses/dbt-for-beginners-352402536678818389", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zEMvJPBk4kdPN433ba1kQoxVwWrm9TOOktRKWDVPNv0.jpg?auto=webp&amp;s=baf90ba8b8331dbe0a54f56970a40fbee254d0d3", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/zEMvJPBk4kdPN433ba1kQoxVwWrm9TOOktRKWDVPNv0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1eee62f1ff2693057acbc26c5838f81bed8afb2", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/zEMvJPBk4kdPN433ba1kQoxVwWrm9TOOktRKWDVPNv0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd688adda11f8d0ed4f29c4540239966db48e283", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/zEMvJPBk4kdPN433ba1kQoxVwWrm9TOOktRKWDVPNv0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2121c2644d5cf365e27dcbf3190bc7d3fc3ea672", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/zEMvJPBk4kdPN433ba1kQoxVwWrm9TOOktRKWDVPNv0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cb3356a792502f6658b7e28089c0d4f8aa1dab65", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/zEMvJPBk4kdPN433ba1kQoxVwWrm9TOOktRKWDVPNv0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d0691ce36096bf99ca7eb3fbea27b6c84f7324b", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/zEMvJPBk4kdPN433ba1kQoxVwWrm9TOOktRKWDVPNv0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=781b594238bb083681f62321b55522d0afc496db", "width": 1080, "height": 607}], "variants": {}, "id": "iwMa9Qo4xqw27g3bFxQt8ELlV6OYhY0ykbzvaZze_gw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zzvb1o", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zzvb1o/free_dbt_for_beginners_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataschool.alterclass.school/courses/dbt-for-beginners-352402536678818389", "subreddit_subscribers": 84767, "created_utc": 1672497046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have quite a lot of experience with GCP but no experience of any other cloud platform. I\u2019m curious about the general opinion on which (of the big three) platforms you prefer to work with. I have considered going into contracting and knowing that GCP has the lowest market share of the three I realize I need to extend my skillset to at least include another cloud platform. \n\nWhich do you think is more fun to work with between AWS and Azure?\nPros and cons?", "author_fullname": "t2_91wtm7y0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Cloud vs Azure vs AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zzqo79", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1672484182.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672480693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have quite a lot of experience with GCP but no experience of any other cloud platform. I\u2019m curious about the general opinion on which (of the big three) platforms you prefer to work with. I have considered going into contracting and knowing that GCP has the lowest market share of the three I realize I need to extend my skillset to at least include another cloud platform. &lt;/p&gt;\n\n&lt;p&gt;Which do you think is more fun to work with between AWS and Azure?\nPros and cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zzqo79", "is_robot_indexable": true, "report_reasons": null, "author": "Gueule-Debois", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zzqo79/google_cloud_vs_azure_vs_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zzqo79/google_cloud_vs_azure_vs_aws/", "subreddit_subscribers": 84767, "created_utc": 1672480693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently heard Elon Musk talk about the Views feature at Twitter on the All-In podcast and how it processes about 5 Million TPS. \n\nDoes anyone have any ideas for how the architecture behind something like this works? \n\nIf they exist, I would love to read any resources covering advanced topics like this.", "author_fullname": "t2_ij7c2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Views feature at Twitter, 5 Million TPS - how?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zzk4ib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672457183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently heard Elon Musk talk about the Views feature at Twitter on the All-In podcast and how it processes about 5 Million TPS. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any ideas for how the architecture behind something like this works? &lt;/p&gt;\n\n&lt;p&gt;If they exist, I would love to read any resources covering advanced topics like this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zzk4ib", "is_robot_indexable": true, "report_reasons": null, "author": "TehMightyDuk", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zzk4ib/views_feature_at_twitter_5_million_tps_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zzk4ib/views_feature_at_twitter_5_million_tps_how/", "subreddit_subscribers": 84767, "created_utc": 1672457183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking to implement Delta format storage on my data lake in s3. Data is generated as parquet files from previous iterations. \n\nMy goal is to eventually run incremental jobs to handle upsert/delete operations from the source stream (which is partitioned hourly, and hence I can these incremental jobs at hourly cadence). \n\nCurrently exploring Delta format operations on AWS EMR by importing open source Delta. [Here](https://github.com/aws-samples/emr-studio-notebook-examples/blob/0842dbb6cef4eb9c0130c04affadf950726b2b83/examples/deltalake-example-notebook-pyspark.ipynb) is an example of what I've been working through as my first steps. I would like to know if someone has done similar implementation and anything to watch out with this path.\n\nAnother obvious way - pick up a vendor like Databricks and go with package solution. I want to vet the available OSS solutions to make an informed choice here. \n\nTY", "author_fullname": "t2_j1zb3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OSS Delta format implementation on AWS EMR for Incremental load jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zzfxdu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1672445434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking to implement Delta format storage on my data lake in s3. Data is generated as parquet files from previous iterations. &lt;/p&gt;\n\n&lt;p&gt;My goal is to eventually run incremental jobs to handle upsert/delete operations from the source stream (which is partitioned hourly, and hence I can these incremental jobs at hourly cadence). &lt;/p&gt;\n\n&lt;p&gt;Currently exploring Delta format operations on AWS EMR by importing open source Delta. &lt;a href=\"https://github.com/aws-samples/emr-studio-notebook-examples/blob/0842dbb6cef4eb9c0130c04affadf950726b2b83/examples/deltalake-example-notebook-pyspark.ipynb\"&gt;Here&lt;/a&gt; is an example of what I&amp;#39;ve been working through as my first steps. I would like to know if someone has done similar implementation and anything to watch out with this path.&lt;/p&gt;\n\n&lt;p&gt;Another obvious way - pick up a vendor like Databricks and go with package solution. I want to vet the available OSS solutions to make an informed choice here. &lt;/p&gt;\n\n&lt;p&gt;TY&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/P_yFb3BZdMGAwgGUW_--rPIl0gF-ot2OdkqDTX5qD9U.jpg?auto=webp&amp;s=8fb8e465da1bd049a52e0b6c7307689ee56ab2f8", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/P_yFb3BZdMGAwgGUW_--rPIl0gF-ot2OdkqDTX5qD9U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b49b29becbc5fa3649914ec382b7014e8c7648a1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/P_yFb3BZdMGAwgGUW_--rPIl0gF-ot2OdkqDTX5qD9U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80ba700cc6854f9097206ee1344de70dbdec93ac", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/P_yFb3BZdMGAwgGUW_--rPIl0gF-ot2OdkqDTX5qD9U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=af9f61cfdb2c338717e6583139240c3e81939505", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/P_yFb3BZdMGAwgGUW_--rPIl0gF-ot2OdkqDTX5qD9U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bbfde569b14241942e7e072105d00677602468d3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/P_yFb3BZdMGAwgGUW_--rPIl0gF-ot2OdkqDTX5qD9U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b9e0d900d8def22cc1d4a71816264f5e4ffa990", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/P_yFb3BZdMGAwgGUW_--rPIl0gF-ot2OdkqDTX5qD9U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=afc8b1307f8a8ecbe1ed9fea4ba55ae45dd98002", "width": 1080, "height": 540}], "variants": {}, "id": "OcYzVm-Iahld81YZjctN_TWTsY-DCy61RAEjnwVj_Fw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "zzfxdu", "is_robot_indexable": true, "report_reasons": null, "author": "abhi5025", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zzfxdu/oss_delta_format_implementation_on_aws_emr_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zzfxdu/oss_delta_format_implementation_on_aws_emr_for/", "subreddit_subscribers": 84767, "created_utc": 1672445434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There is an event by the Seattle Data Guy on 3rd January or 8th.  Anybody here joining it? I wanted to know if we could network on it. I'm looking for Summer 2023 internships as a Data engineer intern or Data Analyst intern", "author_fullname": "t2_iascmrib", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone attending The State of Data Infra 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zznsdq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672469156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There is an event by the Seattle Data Guy on 3rd January or 8th.  Anybody here joining it? I wanted to know if we could network on it. I&amp;#39;m looking for Summer 2023 internships as a Data engineer intern or Data Analyst intern&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zznsdq", "is_robot_indexable": true, "report_reasons": null, "author": "catchereye22", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zznsdq/anyone_attending_the_state_of_data_infra_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zznsdq/anyone_attending_the_state_of_data_infra_2023/", "subreddit_subscribers": 84767, "created_utc": 1672469156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Search as I might, I\u2019m unable to find what I\u2019m looking for. Hoping someone could point me in the right direction.\n\nI want to build a docker-compose for home use to \u2018try out\u2019 a data lakehouse.\n\nI was figuring on using the below stack:\n\n- localstack S3\n- Delta Lake\n- PySpark \n- Presto\n- dbt\n- Airflow\n\nIs this achievable? Is there a component (or more) that I\u2019m missing? I can\u2019t find any guides, or even a docker-compose for Delta Lake. Is anyone aware of anything?\n\nAt work we use S3, Airflow, dbt and Snowflake, hence wanting to keep some of the things I\u2019m familiar with.\n\nAny direction would be appreciated.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying Delta Lake at home", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zzqn5r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672480563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Search as I might, I\u2019m unable to find what I\u2019m looking for. Hoping someone could point me in the right direction.&lt;/p&gt;\n\n&lt;p&gt;I want to build a docker-compose for home use to \u2018try out\u2019 a data lakehouse.&lt;/p&gt;\n\n&lt;p&gt;I was figuring on using the below stack:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;localstack S3&lt;/li&gt;\n&lt;li&gt;Delta Lake&lt;/li&gt;\n&lt;li&gt;PySpark &lt;/li&gt;\n&lt;li&gt;Presto&lt;/li&gt;\n&lt;li&gt;dbt&lt;/li&gt;\n&lt;li&gt;Airflow&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is this achievable? Is there a component (or more) that I\u2019m missing? I can\u2019t find any guides, or even a docker-compose for Delta Lake. Is anyone aware of anything?&lt;/p&gt;\n\n&lt;p&gt;At work we use S3, Airflow, dbt and Snowflake, hence wanting to keep some of the things I\u2019m familiar with.&lt;/p&gt;\n\n&lt;p&gt;Any direction would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zzqn5r", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zzqn5r/trying_delta_lake_at_home/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zzqn5r/trying_delta_lake_at_home/", "subreddit_subscribers": 84767, "created_utc": 1672480563.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, \n\nCurious to hear how people version control custom connectors in Airbyte. Specifically private connectors that aren\u2019t going to be integrated into the source. \n\nUsing the CDK pops it into the integrations directory, which is needed for the requirements pip file. \n\nSo are people\n\n-\tforking Airbyte and building their connectors in the fork. Then just keeping your fork up to date with the main repo\n-\tstoring the connectors elsewhere and keeping a saved environment for dev. VC each connector in its own repo. \n-\tother?\n\nThanks!", "author_fullname": "t2_ahu1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Version controlling private Airbyte connectors", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1000vcf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672512657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;Curious to hear how people version control custom connectors in Airbyte. Specifically private connectors that aren\u2019t going to be integrated into the source. &lt;/p&gt;\n\n&lt;p&gt;Using the CDK pops it into the integrations directory, which is needed for the requirements pip file. &lt;/p&gt;\n\n&lt;p&gt;So are people&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;  forking Airbyte and building their connectors in the fork. Then just keeping your fork up to date with the main repo&lt;/li&gt;\n&lt;li&gt;  storing the connectors elsewhere and keeping a saved environment for dev. VC each connector in its own repo. &lt;/li&gt;\n&lt;li&gt;  other?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1000vcf", "is_robot_indexable": true, "report_reasons": null, "author": "Namur007", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1000vcf/version_controlling_private_airbyte_connectors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1000vcf/version_controlling_private_airbyte_connectors/", "subreddit_subscribers": 84767, "created_utc": 1672512657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We\u2019re looking at potentially using AirByte to transfer data and updates to a Postgres database.  That said, their MSSQL driver says it supports incremental log based updates.\n\nWe want to keep low load on our production database and this seems ideal for our larger tables.\n\nMy dba seems to think we need to add modified time stamps to all tables.  That would be a big lift and not deal with deletion use cases.  I\u2019d like to be as non-invasive as possible.\n\nDoes anybody have experience with this or other tools that could minimize impact on load and structure changes?\n\nTIA", "author_fullname": "t2_kaqsa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AirByte - Incremental MSSQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zzrptl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1672495888.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672484677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We\u2019re looking at potentially using AirByte to transfer data and updates to a Postgres database.  That said, their MSSQL driver says it supports incremental log based updates.&lt;/p&gt;\n\n&lt;p&gt;We want to keep low load on our production database and this seems ideal for our larger tables.&lt;/p&gt;\n\n&lt;p&gt;My dba seems to think we need to add modified time stamps to all tables.  That would be a big lift and not deal with deletion use cases.  I\u2019d like to be as non-invasive as possible.&lt;/p&gt;\n\n&lt;p&gt;Does anybody have experience with this or other tools that could minimize impact on load and structure changes?&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zzrptl", "is_robot_indexable": true, "report_reasons": null, "author": "vaporofnuance", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zzrptl/airbyte_incremental_mssql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zzrptl/airbyte_incremental_mssql/", "subreddit_subscribers": 84767, "created_utc": 1672484677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, I'm struggling with airflow at this moment. Currently we have millions of data points and we provide different metrics per customer on that points. And as for now only one Dag run which triggers my Django management command (using k8s operator)  but the problem is now that dag run around 6-7 hours daily not to mention i run one dag every 30 minutes to sync data to druid too. I want to do it per customer like one Dag (one pod) per customer but I'm not sure if so many dag run possible. Someone help with good solution", "author_fullname": "t2_akoo6us4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Designing help needed for airflow pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_10043k7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672522005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;m struggling with airflow at this moment. Currently we have millions of data points and we provide different metrics per customer on that points. And as for now only one Dag run which triggers my Django management command (using k8s operator)  but the problem is now that dag run around 6-7 hours daily not to mention i run one dag every 30 minutes to sync data to druid too. I want to do it per customer like one Dag (one pod) per customer but I&amp;#39;m not sure if so many dag run possible. Someone help with good solution&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "10043k7", "is_robot_indexable": true, "report_reasons": null, "author": "Gullible-Proof1629", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/10043k7/designing_help_needed_for_airflow_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/10043k7/designing_help_needed_for_airflow_pipeline/", "subreddit_subscribers": 84767, "created_utc": 1672522005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone here made this change before or knows someone who has?\n\nIf so, did you do a bootcamp? Were you self-taught primarily? Or something else?\n\nAlso how easy was it to get a job afterwards and did you take a pay cut to do it?\n\nSome context: I've got three years experience and did a web dev bootcamp to start with", "author_fullname": "t2_ox67j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how easy is it to switch from software engineering with a web focus to data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1003u2b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672521217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone here made this change before or knows someone who has?&lt;/p&gt;\n\n&lt;p&gt;If so, did you do a bootcamp? Were you self-taught primarily? Or something else?&lt;/p&gt;\n\n&lt;p&gt;Also how easy was it to get a job afterwards and did you take a pay cut to do it?&lt;/p&gt;\n\n&lt;p&gt;Some context: I&amp;#39;ve got three years experience and did a web dev bootcamp to start with&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1003u2b", "is_robot_indexable": true, "report_reasons": null, "author": "atomicwombat00", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1003u2b/how_easy_is_it_to_switch_from_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1003u2b/how_easy_is_it_to_switch_from_software/", "subreddit_subscribers": 84767, "created_utc": 1672521217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have an on-prem SQL Server 2019 instance (\\~50M rows, 10K daily inserts) used for Read/Write workloads. We want to offload Read-Only workloads to ASMI using the [link](https://techcommunity.microsoft.com/t5/azure-sql-blog/managed-instance-link-connecting-sql-server-to-azure-reimagined/ba-p/2911614) feature. Has anyone tried it and willing to share their experiences?", "author_fullname": "t2_42hc4int", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure SQL Managed Instance (ASMI) link feature for data replication", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zzmti9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1672465780.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have an on-prem SQL Server 2019 instance (~50M rows, 10K daily inserts) used for Read/Write workloads. We want to offload Read-Only workloads to ASMI using the &lt;a href=\"https://techcommunity.microsoft.com/t5/azure-sql-blog/managed-instance-link-connecting-sql-server-to-azure-reimagined/ba-p/2911614\"&gt;link&lt;/a&gt; feature. Has anyone tried it and willing to share their experiences?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tUeSOUlUmpNXBu6c-i70dzjHR19HvU46iw6JzngJr0g.jpg?auto=webp&amp;s=73f6eb2517c027c61307db8c20e4ea4cea6620d5", "width": 2427, "height": 1152}, "resolutions": [{"url": "https://external-preview.redd.it/tUeSOUlUmpNXBu6c-i70dzjHR19HvU46iw6JzngJr0g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f6b295507f6f42560a281a613b6499154ad95a4", "width": 108, "height": 51}, {"url": "https://external-preview.redd.it/tUeSOUlUmpNXBu6c-i70dzjHR19HvU46iw6JzngJr0g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3739c8791cbca8a27074da501ff289f1d4b6dd7b", "width": 216, "height": 102}, {"url": "https://external-preview.redd.it/tUeSOUlUmpNXBu6c-i70dzjHR19HvU46iw6JzngJr0g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=59f3a455649eb97234908dd89d5c7e032e072221", "width": 320, "height": 151}, {"url": "https://external-preview.redd.it/tUeSOUlUmpNXBu6c-i70dzjHR19HvU46iw6JzngJr0g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2990e8949ed437bc975ac922563fa748d7c4e33f", "width": 640, "height": 303}, {"url": "https://external-preview.redd.it/tUeSOUlUmpNXBu6c-i70dzjHR19HvU46iw6JzngJr0g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4a8251b6d493663f2e8b94dd9c7c1f94f235651e", "width": 960, "height": 455}, {"url": "https://external-preview.redd.it/tUeSOUlUmpNXBu6c-i70dzjHR19HvU46iw6JzngJr0g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f043f50bb0b7cb2090de7389917106f98bd07a0", "width": 1080, "height": 512}], "variants": {}, "id": "-vRmsq1GYk4mNHkQcqNcgCvxa-tXhUBPrsJLqqwjzYw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zzmti9", "is_robot_indexable": true, "report_reasons": null, "author": "vanillacap", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zzmti9/azure_sql_managed_instance_asmi_link_feature_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zzmti9/azure_sql_managed_instance_asmi_link_feature_for/", "subreddit_subscribers": 84767, "created_utc": 1672465780.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, \n\nI am trying to implement concurrent.futures.ProcessPoolExecutor() in an AWS Glue Spark job and I could really use some help with understanding specifically max\\_workers and if this plays nice with Glue.  \n\nShould the max\\_workers value I pass in ultimately be the number of glue workers multiplied by the number of processors on each glue worker?  Is concurrent.futures.ProcessPoolExecutor even smart enough to recognize all of the 120 processors in the Glue cluster? \n\nAny help would be monumentally appreciated. I need to make an API call for every ID in a list, and given the average length of the generated list every day, I calculated that this process would take 24 hours to run with a regular for loop. I split the list into smaller lists based on the max\\_workers value, and I'm wanting to process these lists asynchronously. Just need to know if I'm on the right track.\n\nThanks!!\n\ncpus\\_per\\_glue\\_worker=4\n\nnumber\\_of\\_glue\\_workers=30\n\nnumber\\_of\\_workers = number\\_of\\_glue\\_workers \\* cpus\\_per\\_glue\\_worker\n\nwith concurrent.futures.ProcessPoolExecutor(max\\_workers=number\\_of\\_workers) as executor:\n\nresults = \\[executor.submit(\\*functionname\\*, i) for i in range(120)\\]", "author_fullname": "t2_rwmd2gxn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Multiprocessing (concurrent.futures.ProcessPoolExecutor()) in AWS Glue Spark Job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zzikxv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672452707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, &lt;/p&gt;\n\n&lt;p&gt;I am trying to implement concurrent.futures.ProcessPoolExecutor() in an AWS Glue Spark job and I could really use some help with understanding specifically max_workers and if this plays nice with Glue.  &lt;/p&gt;\n\n&lt;p&gt;Should the max_workers value I pass in ultimately be the number of glue workers multiplied by the number of processors on each glue worker?  Is concurrent.futures.ProcessPoolExecutor even smart enough to recognize all of the 120 processors in the Glue cluster? &lt;/p&gt;\n\n&lt;p&gt;Any help would be monumentally appreciated. I need to make an API call for every ID in a list, and given the average length of the generated list every day, I calculated that this process would take 24 hours to run with a regular for loop. I split the list into smaller lists based on the max_workers value, and I&amp;#39;m wanting to process these lists asynchronously. Just need to know if I&amp;#39;m on the right track.&lt;/p&gt;\n\n&lt;p&gt;Thanks!!&lt;/p&gt;\n\n&lt;p&gt;cpus_per_glue_worker=4&lt;/p&gt;\n\n&lt;p&gt;number_of_glue_workers=30&lt;/p&gt;\n\n&lt;p&gt;number_of_workers = number_of_glue_workers * cpus_per_glue_worker&lt;/p&gt;\n\n&lt;p&gt;with concurrent.futures.ProcessPoolExecutor(max_workers=number_of_workers) as executor:&lt;/p&gt;\n\n&lt;p&gt;results = [executor.submit(*functionname*, i) for i in range(120)]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zzikxv", "is_robot_indexable": true, "report_reasons": null, "author": "quiet_storm_09", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zzikxv/using_multiprocessing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zzikxv/using_multiprocessing/", "subreddit_subscribers": 84767, "created_utc": 1672452707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello\n\nI have some experience with Spark sometime ago where 2.0 was just being released. I moved onto different tech stack since then and lost touch with it. \n\n3.x seems to be the current stable version and I feel too rusty with my outdated knowledge. Is there any primer that you can point me to pick up basics in 3.x.\n\nI am looking for some starting point tbh and finding it difficult as internet is flooded with too many shallow resources. \n\nTIA.", "author_fullname": "t2_j1zb3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Online resources - Spark Refresher", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zzg2gm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672445797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I have some experience with Spark sometime ago where 2.0 was just being released. I moved onto different tech stack since then and lost touch with it. &lt;/p&gt;\n\n&lt;p&gt;3.x seems to be the current stable version and I feel too rusty with my outdated knowledge. Is there any primer that you can point me to pick up basics in 3.x.&lt;/p&gt;\n\n&lt;p&gt;I am looking for some starting point tbh and finding it difficult as internet is flooded with too many shallow resources. &lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zzg2gm", "is_robot_indexable": true, "report_reasons": null, "author": "abhi5025", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zzg2gm/online_resources_spark_refresher/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zzg2gm/online_resources_spark_refresher/", "subreddit_subscribers": 84767, "created_utc": 1672445797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello \nI need your help. \nI'm new to Pyspark and I need to write a script that moves several files.\n I would like to test if a file exists before moving it. \nI'm working on Azure and the files are in S2 storage. \nI saw that there was an OS module that allows you to do this but it seems that it only works if the file is local. \nDo you have a solution to perform this existence test of a file in S2?\n\nThks", "author_fullname": "t2_j68sac68", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pyspark Test if file exists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zzfr0s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1672444988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello \nI need your help. \nI&amp;#39;m new to Pyspark and I need to write a script that moves several files.\n I would like to test if a file exists before moving it. \nI&amp;#39;m working on Azure and the files are in S2 storage. \nI saw that there was an OS module that allows you to do this but it seems that it only works if the file is local. \nDo you have a solution to perform this existence test of a file in S2?&lt;/p&gt;\n\n&lt;p&gt;Thks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zzfr0s", "is_robot_indexable": true, "report_reasons": null, "author": "Playful-Sprinkles-27", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zzfr0s/pyspark_test_if_file_exists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zzfr0s/pyspark_test_if_file_exists/", "subreddit_subscribers": 84767, "created_utc": 1672444988.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}