{"kind": "Listing", "data": {"after": "t3_zk4g75", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1omlki7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just accidentally nuked ~90% of my video library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 90, "top_awarded_type": null, "hide_score": false, "name": "t3_zkc7jv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 601, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 601, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/JxQNCfnKe8WfA3OrEpx3B-VGJQFmz3-txa9-rY2TVb8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670880757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/guh9oj5gaj5a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/guh9oj5gaj5a1.png?auto=webp&amp;s=8d4162e08a0eb95f5808b839c879287c8e88657b", "width": 1493, "height": 964}, "resolutions": [{"url": "https://preview.redd.it/guh9oj5gaj5a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a655b2982fb80e95c8937d57dbff1c0f6250643b", "width": 108, "height": 69}, {"url": "https://preview.redd.it/guh9oj5gaj5a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb90753a98ae5f98cf4bd30f283a1775d58d6713", "width": 216, "height": 139}, {"url": "https://preview.redd.it/guh9oj5gaj5a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=86d23aeddbc14ffa850acde5b6ac9d8121f1d801", "width": 320, "height": 206}, {"url": "https://preview.redd.it/guh9oj5gaj5a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cb2e5841e1858156c88d669693728d61de69e089", "width": 640, "height": 413}, {"url": "https://preview.redd.it/guh9oj5gaj5a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e190fc6ad72cc5bb8f0a33ba04a7f2745fdecb0f", "width": 960, "height": 619}, {"url": "https://preview.redd.it/guh9oj5gaj5a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1070499d138a01f4b1555806200fc7f3442ad16", "width": 1080, "height": 697}], "variants": {}, "id": "-IQY7NkOwJilESbG22s_hQOlk1Wwg1s1gimRet7UtX0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkc7jv", "is_robot_indexable": true, "report_reasons": null, "author": "randombystander3001", "discussion_type": null, "num_comments": 258, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkc7jv/just_accidentally_nuked_90_of_my_video_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/guh9oj5gaj5a1.png", "subreddit_subscribers": 659013, "created_utc": 1670880757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_dc1bf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Huge thanks to Seagate and u/Seagate_Surfer for running the IronWolf Pro SSD giveaway! It has found a good home and replaced an ancient OCZ relic.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"smerxp99hi5a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ce7ce1a7531f48d50128c88f41834c439eb9a04"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=51d99f23267c6296f35f8718cf654cd1c2b3ce9c"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0dc449aad132695ef3c2d51ca76ac2b2bd908f56"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e120c9ec9ce3417efc0118987747a20595b26801"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=71b3726ea0c0dcb0eb21b3b6152df599a2b22872"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e534075c1930e4020f4004d7d66656851b5e36cc"}], "s": {"y": 3024, "x": 4032, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=574037522ec30d587ec0df7fb2f90260813c73f9"}, "id": "smerxp99hi5a1"}, "b1p98rf8hi5a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 138, "x": 108, "u": "https://preview.redd.it/b1p98rf8hi5a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=73d95b83319554632043d28d74d88b36d8de89e7"}, {"y": 276, "x": 216, "u": "https://preview.redd.it/b1p98rf8hi5a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=53cf0884e3b5b62a2cc91526f37cf2bd374ccfab"}, {"y": 409, "x": 320, "u": "https://preview.redd.it/b1p98rf8hi5a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=baa20e2c7a50427d52715404f44eaebbdd58e7c8"}, {"y": 819, "x": 640, "u": "https://preview.redd.it/b1p98rf8hi5a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae4fd9db311173e99eb50045f8672311005d030c"}], "s": {"y": 922, "x": 720, "u": "https://preview.redd.it/b1p98rf8hi5a1.jpg?width=720&amp;format=pjpg&amp;auto=webp&amp;s=0fc5ab769725df96e84e8ef7491832bbddb8ad8f"}, "id": "b1p98rf8hi5a1"}, "61tian8ahi5a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 39, "x": 108, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2d0d6a0ade073c64da315099ed910a935af0a2c5"}, {"y": 78, "x": 216, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f4ba353b79b466d87e1b3855c36789a3befad87a"}, {"y": 116, "x": 320, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a0d549cd84767d024d4e56452a115ec9e36ddca"}, {"y": 233, "x": 640, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5a3e0a9422be67e363394c72f3d96222c3a8c43"}, {"y": 350, "x": 960, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7df140ab2076ff3ba8047c1ea980751bfbdaf8a5"}, {"y": 394, "x": 1080, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=37e7d0103352e989bf5f3f846fef429d3b14da67"}], "s": {"y": 469, "x": 1284, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=1284&amp;format=png&amp;auto=webp&amp;s=60a087c520ed31b30679540bb204a1e271ea363c"}, "id": "61tian8ahi5a1"}}, "name": "t3_zk7aa0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 307, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "b1p98rf8hi5a1", "id": 218525898}, {"media_id": "smerxp99hi5a1", "id": 218525899}, {"media_id": "61tian8ahi5a1", "id": 218525900}]}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 307, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/lvphIk5c9FViHQG02pQlJNYkKTllgXKlxAERzuVVKDU.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670870209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zk7aa0", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "74TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zk7aa0", "is_robot_indexable": true, "report_reasons": null, "author": "Clawz114", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zk7aa0/huge_thanks_to_seagate_and_useagate_surfer_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zk7aa0", "subreddit_subscribers": 659013, "created_utc": 1670870209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "because date hoarding is quite an expensive hobby.", "author_fullname": "t2_lo8e4f55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who are you by profession?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk62zn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670867644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;because date hoarding is quite an expensive hobby.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "russian military ship, go to hell", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zk62zn", "is_robot_indexable": true, "report_reasons": null, "author": "kovach_ua", "discussion_type": null, "num_comments": 86, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zk62zn/who_are_you_by_profession/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk62zn/who_are_you_by_profession/", "subreddit_subscribers": 659013, "created_utc": 1670867644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\nAs I frequently see interesting threads on Reddit, and I want to get them offline to 1) find them again easily, and 2) preserve them in case messages get removed, I made myself some time ago a Python script to \"download\" threads off Reddit.This script does not *only* download the thread, but generates a nice HTML file, so it can be opened in a browser and the thread navigated around conveniently. [Here is an example of such HTML file](https://htmlpreview.github.io/?https://github.com/Ailothaen/RedditArchiver/blob/main/github/example.html).\n\nRecently, I told myself it would be better to have a web frontend for that tooling, since I am sometimes on the go and do not have the script and/or the Python interpreter on my machine.\n\nThis therefore led to RedditArchive, a Flask self-hosted app to archive and download Reddit threads (screenshots available in the README):[https://github.com/Ailothaen/RedditArchiver](https://github.com/Ailothaen/RedditArchiver)\n\nYou can install it on a small server of yours, such as a Raspberry Pi or a VPS. Installations instructions are provided if you want to try it on.\n\nIf you do not want to deal with the hassle of setting a web server up, worry not! I also made the original script available here:[https://github.com/Ailothaen/RedditArchiver-standalone](https://github.com/Ailothaen/RedditArchiver-standalone)\n\nDo not hesitate to comment and make suggestions \u2013 I have ideas for further features, but that's probably for another time. \ud83e\udd89", "author_fullname": "t2_ozmwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Made myself Python tooling to download threads off Reddit (available in Web UI and standalone script)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk6491", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670867716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;As I frequently see interesting threads on Reddit, and I want to get them offline to 1) find them again easily, and 2) preserve them in case messages get removed, I made myself some time ago a Python script to &amp;quot;download&amp;quot; threads off Reddit.This script does not &lt;em&gt;only&lt;/em&gt; download the thread, but generates a nice HTML file, so it can be opened in a browser and the thread navigated around conveniently. &lt;a href=\"https://htmlpreview.github.io/?https://github.com/Ailothaen/RedditArchiver/blob/main/github/example.html\"&gt;Here is an example of such HTML file&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Recently, I told myself it would be better to have a web frontend for that tooling, since I am sometimes on the go and do not have the script and/or the Python interpreter on my machine.&lt;/p&gt;\n\n&lt;p&gt;This therefore led to RedditArchive, a Flask self-hosted app to archive and download Reddit threads (screenshots available in the README):&lt;a href=\"https://github.com/Ailothaen/RedditArchiver\"&gt;https://github.com/Ailothaen/RedditArchiver&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You can install it on a small server of yours, such as a Raspberry Pi or a VPS. Installations instructions are provided if you want to try it on.&lt;/p&gt;\n\n&lt;p&gt;If you do not want to deal with the hassle of setting a web server up, worry not! I also made the original script available here:&lt;a href=\"https://github.com/Ailothaen/RedditArchiver-standalone\"&gt;https://github.com/Ailothaen/RedditArchiver-standalone&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Do not hesitate to comment and make suggestions \u2013 I have ideas for further features, but that&amp;#39;s probably for another time. \ud83e\udd89&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk6491", "is_robot_indexable": true, "report_reasons": null, "author": "Ailothaen", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk6491/made_myself_python_tooling_to_download_threads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk6491/made_myself_python_tooling_to_download_threads/", "subreddit_subscribers": 659013, "created_utc": 1670867716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My apologies if this topic does not belong to this group, but I ran out of options and this is my last resource. \n\nThere is in **BBC Sounds** a podcast called **Night Tracks**. Each episode is a beautiful collection of classical and rare experimental music. The episodes are only available for certain amount of time and then they are removed from the website. \n\nI want to collect every single episode but I haven't found any way to download them. I've tried with multiple ad-ons, extensions, and software but it is just imposible. \n\nPerhaps someone here knows a way. I would deeply appreciate any help.", "author_fullname": "t2_tzaj22sa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A datahoarding attempt that has proven to be almost impossible", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkdqq7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670883990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My apologies if this topic does not belong to this group, but I ran out of options and this is my last resource. &lt;/p&gt;\n\n&lt;p&gt;There is in &lt;strong&gt;BBC Sounds&lt;/strong&gt; a podcast called &lt;strong&gt;Night Tracks&lt;/strong&gt;. Each episode is a beautiful collection of classical and rare experimental music. The episodes are only available for certain amount of time and then they are removed from the website. &lt;/p&gt;\n\n&lt;p&gt;I want to collect every single episode but I haven&amp;#39;t found any way to download them. I&amp;#39;ve tried with multiple ad-ons, extensions, and software but it is just imposible. &lt;/p&gt;\n\n&lt;p&gt;Perhaps someone here knows a way. I would deeply appreciate any help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkdqq7", "is_robot_indexable": true, "report_reasons": null, "author": "Melancholic-Beast", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zkdqq7/a_datahoarding_attempt_that_has_proven_to_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkdqq7/a_datahoarding_attempt_that_has_proven_to_be/", "subreddit_subscribers": 659013, "created_utc": 1670883990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for something dedicated but less expensive like this for my Windows PC, don't need to have Raid:\n\n[https://eshop.macsales.com/item/OWC/SSDACL4M208T/#customer-reviews](https://eshop.macsales.com/item/OWC/SSDACL4M208T/#customer-reviews)\n\nThanks!", "author_fullname": "t2_3zj1o3uc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any dedicated PCIe 4.0 NVMe quad adapters?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk7y1j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670871565.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for something dedicated but less expensive like this for my Windows PC, don&amp;#39;t need to have Raid:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eshop.macsales.com/item/OWC/SSDACL4M208T/#customer-reviews\"&gt;https://eshop.macsales.com/item/OWC/SSDACL4M208T/#customer-reviews&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk7y1j", "is_robot_indexable": true, "report_reasons": null, "author": "MarkGeraz", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk7y1j/any_dedicated_pcie_40_nvme_quad_adapters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk7y1j/any_dedicated_pcie_40_nvme_quad_adapters/", "subreddit_subscribers": 659013, "created_utc": 1670871565.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi \n\nI'm trying to find an online backup solution for my veracrypt container that is around 300GB and growing. Ideally, I want a provider that doesn't require you to split the large file in parts. I'm not certain, but my current understanding is that I cannot split up the container into separate parts...\n\n&amp;#x200B;\n\nI don't need any syncing. I intend on uploading the entire container once per week. \n\n&amp;#x200B;\n\nAny suggestions would be much appreciated. ", "author_fullname": "t2_u07vnc7p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud solution for huge files that cannot be split up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk77qt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670870051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to find an online backup solution for my veracrypt container that is around 300GB and growing. Ideally, I want a provider that doesn&amp;#39;t require you to split the large file in parts. I&amp;#39;m not certain, but my current understanding is that I cannot split up the container into separate parts...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t need any syncing. I intend on uploading the entire container once per week. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any suggestions would be much appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk77qt", "is_robot_indexable": true, "report_reasons": null, "author": "user44566829", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk77qt/cloud_solution_for_huge_files_that_cannot_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk77qt/cloud_solution_for_huge_files_that_cannot_be/", "subreddit_subscribers": 659013, "created_utc": 1670870051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to download the entire SCP wiki so I can browse it offline, but WITHOUT download the comment sections. Is there a software that can do this? How would I limit the software to only download this wiki and any pages closely related to it, without following any possible links to other wikis and downloading those?", "author_fullname": "t2_50fpwuu0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download an entire wiki?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zkt9hc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670930243.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to download the entire SCP wiki so I can browse it offline, but WITHOUT download the comment sections. Is there a software that can do this? How would I limit the software to only download this wiki and any pages closely related to it, without following any possible links to other wikis and downloading those?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkt9hc", "is_robot_indexable": true, "report_reasons": null, "author": "Voldy256", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkt9hc/how_to_download_an_entire_wiki/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkt9hc/how_to_download_an_entire_wiki/", "subreddit_subscribers": 659013, "created_utc": 1670930243.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_oj0pl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Turkish movies which are hard to find on torrent are being uploaded with English subtitles to Youtube. Go ahead and back them up.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zkbvih", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/wZznEwWjQxzEBKDpk0qBMND9lriEWFKgKWT3vWB22p8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670880033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/@moviturkinternational/videos", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ru_AMlstSs-CGi-1_U-UXoek4nQGAg_xK8luucv6hFc.jpg?auto=webp&amp;s=bd24de6703c0cea3f6934e6e3e87ec331933771a", "width": 900, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/ru_AMlstSs-CGi-1_U-UXoek4nQGAg_xK8luucv6hFc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d43f81079e3aa103d4993d570555a39fc14ff42e", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ru_AMlstSs-CGi-1_U-UXoek4nQGAg_xK8luucv6hFc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a538f9a9f896238f3e2aeaccd161890a4dd20c66", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/ru_AMlstSs-CGi-1_U-UXoek4nQGAg_xK8luucv6hFc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e0c6b70089562eda047670f5b50d0629c79f684c", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/ru_AMlstSs-CGi-1_U-UXoek4nQGAg_xK8luucv6hFc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=84131d91db097765f56c2ea83cf9b746c56b741e", "width": 640, "height": 640}], "variants": {}, "id": "1QOSAck5v15D_7FRM65FyafUWQnJojwQvqpO6xJFrpM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkbvih", "is_robot_indexable": true, "report_reasons": null, "author": "Sacrer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkbvih/turkish_movies_which_are_hard_to_find_on_torrent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/@moviturkinternational/videos", "subreddit_subscribers": 659013, "created_utc": 1670880033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, apologies for asking a question I know has been asked a million times. In fact I saw what I'm pretty sure was the answer to my Q a couple weeks ago but have searched and searched and unfortunately can't find it again (shoulda archived it, heh.)\n\nI'm just starting to become a datahoarder and picked up a Synology NAS. My main motivation was the huge collection of tutorials and other knowledge I've saved on YouTube that may up and disappear one day.\n\n**First Goal:** I'm looking for really simple no-code (or very low code - I code all day long, and don't want to have to write/maintain a bunch of this stuff too) way to automatically archive anything I add to a specific YouTube playlist to my Synology NAS. I'd ideally like it to save thumbnails and descriptions as well.\n\n**Second Goal:** Some sort of easy interface for browsing, searching, and watching those archived videos, so I'm not just using crummy Windows search to try and find a video in the future.\n\nDoes anyone have suggestions that can address 1 or 2, or both?\n\nThanks in advance!", "author_fullname": "t2_mdffa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple and comprehensive approach to YouTube archiving + browsing/searching?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk98yu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670874356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, apologies for asking a question I know has been asked a million times. In fact I saw what I&amp;#39;m pretty sure was the answer to my Q a couple weeks ago but have searched and searched and unfortunately can&amp;#39;t find it again (shoulda archived it, heh.)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just starting to become a datahoarder and picked up a Synology NAS. My main motivation was the huge collection of tutorials and other knowledge I&amp;#39;ve saved on YouTube that may up and disappear one day.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;First Goal:&lt;/strong&gt; I&amp;#39;m looking for really simple no-code (or very low code - I code all day long, and don&amp;#39;t want to have to write/maintain a bunch of this stuff too) way to automatically archive anything I add to a specific YouTube playlist to my Synology NAS. I&amp;#39;d ideally like it to save thumbnails and descriptions as well.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Second Goal:&lt;/strong&gt; Some sort of easy interface for browsing, searching, and watching those archived videos, so I&amp;#39;m not just using crummy Windows search to try and find a video in the future.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have suggestions that can address 1 or 2, or both?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk98yu", "is_robot_indexable": true, "report_reasons": null, "author": "turn-down-for-what", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk98yu/simple_and_comprehensive_approach_to_youtube/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk98yu/simple_and_comprehensive_approach_to_youtube/", "subreddit_subscribers": 659013, "created_utc": 1670874356.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://www.amazon.com/dp/B0BGYV6B9V?psc=1\n\nA non-\"renewed\" disk drive at a reasonable price! I'm completely unfamiliar with the company however, are these bad signs? Do you guys have any thoughts on this company or this disk in particular? Any recommendations for me? I've been looking on diskprices.com and I'm kind of hung up on buying new/renewed, more drives, lower storage OR less drives, higher storage each... I'm looking to increase my storage capacity for my main rig PC, it's only got a 1TB NVME and an external USB HDD with 4TB. My goal is to hoard the entire Z-Library archive (23TB) and to maybe host my own cloud storage, but until then I'll just piece it together one at a time until a drive fills up or my PC runs out of space. \ninb4 read the wiki thank you thank you", "author_fullname": "t2_eflrm5nv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WDD (Max Digital Data?) Thoughts on increasing storage for main rig PC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zka5fj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670876336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.amazon.com/dp/B0BGYV6B9V?psc=1\"&gt;https://www.amazon.com/dp/B0BGYV6B9V?psc=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A non-&amp;quot;renewed&amp;quot; disk drive at a reasonable price! I&amp;#39;m completely unfamiliar with the company however, are these bad signs? Do you guys have any thoughts on this company or this disk in particular? Any recommendations for me? I&amp;#39;ve been looking on diskprices.com and I&amp;#39;m kind of hung up on buying new/renewed, more drives, lower storage OR less drives, higher storage each... I&amp;#39;m looking to increase my storage capacity for my main rig PC, it&amp;#39;s only got a 1TB NVME and an external USB HDD with 4TB. My goal is to hoard the entire Z-Library archive (23TB) and to maybe host my own cloud storage, but until then I&amp;#39;ll just piece it together one at a time until a drive fills up or my PC runs out of space. \ninb4 read the wiki thank you thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zka5fj", "is_robot_indexable": true, "report_reasons": null, "author": "Goberoberto", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zka5fj/wdd_max_digital_data_thoughts_on_increasing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zka5fj/wdd_max_digital_data_thoughts_on_increasing/", "subreddit_subscribers": 659013, "created_utc": 1670876336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\nI have around 500gb of backup files ranging from 5gb to 150gb. The full backups are the large ones and contained in one large file. \n\nI am wondering what is the best way to upload the files to cloud (using gdrive now). Any kind of tools that can resume upload if connection got disconnected? Using Linux on my server. \n\nAppreciate any suggestions. Thank you.", "author_fullname": "t2_b88a0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upload large files to gdrive dilemma", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkk3ev", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670899655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I have around 500gb of backup files ranging from 5gb to 150gb. The full backups are the large ones and contained in one large file. &lt;/p&gt;\n\n&lt;p&gt;I am wondering what is the best way to upload the files to cloud (using gdrive now). Any kind of tools that can resume upload if connection got disconnected? Using Linux on my server. &lt;/p&gt;\n\n&lt;p&gt;Appreciate any suggestions. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkk3ev", "is_robot_indexable": true, "report_reasons": null, "author": "abubin", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkk3ev/upload_large_files_to_gdrive_dilemma/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkk3ev/upload_large_files_to_gdrive_dilemma/", "subreddit_subscribers": 659013, "created_utc": 1670899655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I generally lurk and rarely posts here.\n\n&amp;#x200B;\n\nI saw the Western Digital sale post during Black Friday for the Red Pro NAS drives so I jumped on the chance. Bought 2, and then realized I needed more so bought 2 more. The first 2 drives came perfectly fine, passed SMART and had no problems with stress tests.  \n\n\nThe second batch had huge problems. One, they delivered only ONE drive instead of the two I ordered. Next, the one drive that did arrive was dead on arrival. SMART wasn't even working when I plugged it in. So I tried RMAing it,  but that requires registering it. So I tried registering the drive and lo and behold, I get   \n\"Sorry! Product registration failed, please try later. (STATCODE108)\"  \n\n\nCustomer support is just running me around the ringer. They keep promising me updates with no updates given after their promised deadline of \"24-48 hours.\" I have gone through both their chat system, garnering me the generic   \n(\"Please allow me to inform you that your issue has already been forwarde to our team and they are looking iinto the issue.\")   \nand calls - what else can I do?  \n\n\nI registered the previous 2 drives just fine. All drives were bought directly from the WD online store.", "author_fullname": "t2_lprcf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Western Digital RMA Help - Red PRO NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zke3o5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670884737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I generally lurk and rarely posts here.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I saw the Western Digital sale post during Black Friday for the Red Pro NAS drives so I jumped on the chance. Bought 2, and then realized I needed more so bought 2 more. The first 2 drives came perfectly fine, passed SMART and had no problems with stress tests.  &lt;/p&gt;\n\n&lt;p&gt;The second batch had huge problems. One, they delivered only ONE drive instead of the two I ordered. Next, the one drive that did arrive was dead on arrival. SMART wasn&amp;#39;t even working when I plugged it in. So I tried RMAing it,  but that requires registering it. So I tried registering the drive and lo and behold, I get&lt;br/&gt;\n&amp;quot;Sorry! Product registration failed, please try later. (STATCODE108)&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;Customer support is just running me around the ringer. They keep promising me updates with no updates given after their promised deadline of &amp;quot;24-48 hours.&amp;quot; I have gone through both their chat system, garnering me the generic&lt;br/&gt;\n(&amp;quot;Please allow me to inform you that your issue has already been forwarde to our team and they are looking iinto the issue.&amp;quot;)&lt;br/&gt;\nand calls - what else can I do?  &lt;/p&gt;\n\n&lt;p&gt;I registered the previous 2 drives just fine. All drives were bought directly from the WD online store.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zke3o5", "is_robot_indexable": true, "report_reasons": null, "author": "RockyX123", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zke3o5/western_digital_rma_help_red_pro_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zke3o5/western_digital_rma_help_red_pro_nas/", "subreddit_subscribers": 659013, "created_utc": 1670884737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If an SSD has been formatted can data still be covered from the drive and if so what free software can i use to try and retrieve the data ?", "author_fullname": "t2_g3pttvkl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data recovery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zka49j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670876261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If an SSD has been formatted can data still be covered from the drive and if so what free software can i use to try and retrieve the data ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zka49j", "is_robot_indexable": true, "report_reasons": null, "author": "CumsockFinder", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zka49j/data_recovery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zka49j/data_recovery/", "subreddit_subscribers": 659013, "created_utc": 1670876261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i've been using a dead discord channel for random occasional file uploads for like a year, and would like to back them up. anybody know how I could extract the direct attachment urls from the entire channel without manual labor?\n\nthank", "author_fullname": "t2_fa63c16m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "quick way to grab direct links for discord uploads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk7un4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.47, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670871354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;ve been using a dead discord channel for random occasional file uploads for like a year, and would like to back them up. anybody know how I could extract the direct attachment urls from the entire channel without manual labor?&lt;/p&gt;\n\n&lt;p&gt;thank&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk7un4", "is_robot_indexable": true, "report_reasons": null, "author": "pbdrizz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk7un4/quick_way_to_grab_direct_links_for_discord_uploads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk7un4/quick_way_to_grab_direct_links_for_discord_uploads/", "subreddit_subscribers": 659013, "created_utc": 1670871354.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nIm on windows, with 2 samba shares that are on different hard drives but same unraid server.\n\nI copy from one to another, does it copy directly on unraid or it goes all the way through my PC and slowed down by network?\n\nCan i just use my Windows 11with SpeedCommander to manage files on unraid or I must use something like Krusader to do it locally?\n\nThanks", "author_fullname": "t2_onbly768", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does Remote Copy/Move works?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkkmyh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670901118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Im on windows, with 2 samba shares that are on different hard drives but same unraid server.&lt;/p&gt;\n\n&lt;p&gt;I copy from one to another, does it copy directly on unraid or it goes all the way through my PC and slowed down by network?&lt;/p&gt;\n\n&lt;p&gt;Can i just use my Windows 11with SpeedCommander to manage files on unraid or I must use something like Krusader to do it locally?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkkmyh", "is_robot_indexable": true, "report_reasons": null, "author": "-Hexenhammer-", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkkmyh/how_does_remote_copymove_works/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkkmyh/how_does_remote_copymove_works/", "subreddit_subscribers": 659013, "created_utc": 1670901118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just backed up my 2TB drive (yes baby numbers I know) to a bucket using copy. I'm new to all this so my question is how do I make it so that next time I need to back up it just uploads new items and not the full 2 TB again? \n\nThanks in advance", "author_fullname": "t2_5rmrxudp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "quick question about Rclone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkkltu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670901035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just backed up my 2TB drive (yes baby numbers I know) to a bucket using copy. I&amp;#39;m new to all this so my question is how do I make it so that next time I need to back up it just uploads new items and not the full 2 TB again? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkkltu", "is_robot_indexable": true, "report_reasons": null, "author": "TroothBeToldPodcast", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkkltu/quick_question_about_rclone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkkltu/quick_question_about_rclone/", "subreddit_subscribers": 659013, "created_utc": 1670901035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I used DVDFab to try and backup my Nick scene it dvd and i noticed it ripped extremely quickly. It even says its a large file like a dvd iso should be. Upon booting the iso file the intro screens worked fine but when i select play game it either crashes the video player, or restarts the iso from the start. \n\nIt seems most of the disc doesn\u2019t copy as it should at all, and i cant find any information online on how to fully rip these interactive dvd games. Any ideas?", "author_fullname": "t2_6j6wnh52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iso backups of \u201cscene it?\u201d Discs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkeoig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670886019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used DVDFab to try and backup my Nick scene it dvd and i noticed it ripped extremely quickly. It even says its a large file like a dvd iso should be. Upon booting the iso file the intro screens worked fine but when i select play game it either crashes the video player, or restarts the iso from the start. &lt;/p&gt;\n\n&lt;p&gt;It seems most of the disc doesn\u2019t copy as it should at all, and i cant find any information online on how to fully rip these interactive dvd games. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkeoig", "is_robot_indexable": true, "report_reasons": null, "author": "Slonkweed", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkeoig/iso_backups_of_scene_it_discs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkeoig/iso_backups_of_scene_it_discs/", "subreddit_subscribers": 659013, "created_utc": 1670886019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I just bought a ULTRASTAR DC HC310 4TB to use for a backup of everything that's important to me and I'm looking for advices and tips on how to store it.\n\nI did read different opinions... Someone say to store it powered off in an antistatic bag and wrapped in pluriball. Others instead say that having it powered off corrupt faster the data and also the lubricant on the moving parts dries out so that when you power it on again it will put the drive under extreme friction.\n\nI have not enough knowledge about this to take a decision on my own so I'm here asking for help. \n\nAlso any other advice will be welcome (such as what to choose when formatting it or anything else that I might not know)\n\nThank you to everyone who will help me !", "author_fullname": "t2_14b9rj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need help with my first backup of everything that's important to me", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk7hm7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670871022.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670870622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just bought a ULTRASTAR DC HC310 4TB to use for a backup of everything that&amp;#39;s important to me and I&amp;#39;m looking for advices and tips on how to store it.&lt;/p&gt;\n\n&lt;p&gt;I did read different opinions... Someone say to store it powered off in an antistatic bag and wrapped in pluriball. Others instead say that having it powered off corrupt faster the data and also the lubricant on the moving parts dries out so that when you power it on again it will put the drive under extreme friction.&lt;/p&gt;\n\n&lt;p&gt;I have not enough knowledge about this to take a decision on my own so I&amp;#39;m here asking for help. &lt;/p&gt;\n\n&lt;p&gt;Also any other advice will be welcome (such as what to choose when formatting it or anything else that I might not know)&lt;/p&gt;\n\n&lt;p&gt;Thank you to everyone who will help me !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk7hm7", "is_robot_indexable": true, "report_reasons": null, "author": "Cris257", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk7hm7/i_need_help_with_my_first_backup_of_everything/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk7hm7/i_need_help_with_my_first_backup_of_everything/", "subreddit_subscribers": 659013, "created_utc": 1670870622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need help understanding sata power connector. My psu rated at 500w has 1 ribbon with 5 sata power connectors on it, and another with 3 molex and 2 sata. Label says 5v at 15a and 12v at 41a.\n\nI'm connecting 2x molex into a hot swap drive bay that houses 3x hdd, and a sata on the same ribbon to another hdd.\n\nThe other ribbon has 2 sata connector attached to a drive bay housing 4x2.5\" (vendor says it still works if I only connect 1 sata connector). Another sata connector is attached to an ssd directly.\n\nQuestion: what is the max power draw my setup can support when fully populated? Assume the 3xhdd are the same make/model and the same for 4x2.5\". 2.5\" are all ssds.", "author_fullname": "t2_ratqygnj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drives per rail...and drive bays", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkn6ps", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670908411.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help understanding sata power connector. My psu rated at 500w has 1 ribbon with 5 sata power connectors on it, and another with 3 molex and 2 sata. Label says 5v at 15a and 12v at 41a.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m connecting 2x molex into a hot swap drive bay that houses 3x hdd, and a sata on the same ribbon to another hdd.&lt;/p&gt;\n\n&lt;p&gt;The other ribbon has 2 sata connector attached to a drive bay housing 4x2.5&amp;quot; (vendor says it still works if I only connect 1 sata connector). Another sata connector is attached to an ssd directly.&lt;/p&gt;\n\n&lt;p&gt;Question: what is the max power draw my setup can support when fully populated? Assume the 3xhdd are the same make/model and the same for 4x2.5&amp;quot;. 2.5&amp;quot; are all ssds.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkn6ps", "is_robot_indexable": true, "report_reasons": null, "author": "lmux", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkn6ps/drives_per_railand_drive_bays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkn6ps/drives_per_railand_drive_bays/", "subreddit_subscribers": 659013, "created_utc": 1670908411.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got an old LaCie d2 Quadra with a missing PSU that I want to salvage the data from. According to the specs of a \"compatible\" (and expensive) PSU on eBay, it requires a 4-pin DIN with Pin 1 (+5V), Pin 2 (+12V), Pin 3 (GND), Pin 4 (GND).\n\n[Would this 5.5mm barrel jack to DIN work?](https://mesg.ebay.co.uk/mesgweb/ViewMessageDetail/0/All/165468186278) It's got the correct polarities, but 12V on Pin 1, not 5V. Normally, I'd say \"no, incompatible\", but I'm unfamiliar with the DIN standard, and got the 5V idea from another 3rd party PSU, so was wondering if anyone had any experience with this and could advise? It would save me having to shell out for an expensive PSU or IDE adapter, and be compatible with my pre-existing pile of 12V DC 5.5mm jack HDD PSUs.", "author_fullname": "t2_4ibusga", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Question] PSU to salvage data from an old LaCie d2 Quadra?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkj86f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670897364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got an old LaCie d2 Quadra with a missing PSU that I want to salvage the data from. According to the specs of a &amp;quot;compatible&amp;quot; (and expensive) PSU on eBay, it requires a 4-pin DIN with Pin 1 (+5V), Pin 2 (+12V), Pin 3 (GND), Pin 4 (GND).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://mesg.ebay.co.uk/mesgweb/ViewMessageDetail/0/All/165468186278\"&gt;Would this 5.5mm barrel jack to DIN work?&lt;/a&gt; It&amp;#39;s got the correct polarities, but 12V on Pin 1, not 5V. Normally, I&amp;#39;d say &amp;quot;no, incompatible&amp;quot;, but I&amp;#39;m unfamiliar with the DIN standard, and got the 5V idea from another 3rd party PSU, so was wondering if anyone had any experience with this and could advise? It would save me having to shell out for an expensive PSU or IDE adapter, and be compatible with my pre-existing pile of 12V DC 5.5mm jack HDD PSUs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkj86f", "is_robot_indexable": true, "report_reasons": null, "author": "Darth_Agnon", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkj86f/question_psu_to_salvage_data_from_an_old_lacie_d2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkj86f/question_psu_to_salvage_data_from_an_old_lacie_d2/", "subreddit_subscribers": 659013, "created_utc": 1670897364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "first: i didnt grow up with dvds so im a noob.\n\ni'm trying to rip a video from DVD. but all of the dvd ripping softwares i've tried gave error at exactly 40th minute of the video. i think its becouse VTS\\_013.VOB file. so i copied other files with DVD Decrypter and skipped VTS\\_013.VOB to manually copy it with file explorer. but it also gave error. i can run VTS\\_013.VOB with vlc no issues.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n    [00:58:00] hb_init: starting libhb thread\n    \n     # Starting Scan ...\n    \n    [00:58:00] CPU:\n    [00:58:00]  - logical processor count: 8\n    [00:58:00] Intel Quick Sync Video support: no\n    [00:58:00] hb_scan: path=D:\\VIDEO_TS\\VTS_01_3.VOB, title_index=0\n    udfread ERROR: ECMA 167 Volume Recognition failed\n    src/libbluray/disc/disc.c:333: failed opening UDF image D:\\VIDEO_TS\\VTS_01_3.VOB\n    src/libbluray/disc/disc.c:437: error opening file BDMV\\index.bdmv\n    src/libbluray/disc/disc.c:437: error opening file BDMV\\BACKUP\\index.bdmv\n    src/libbluray/bluray.c:2646: nav_get_title_list(D:\\VIDEO_TS\\VTS_01_3.VOB\\) failed\n    [00:58:00] bd: not a bd - trying as a stream/file instead\n    libdvdread: DVDOpenFileUDF:UDFFindFile /VIDEO_TS/VIDEO_TS.IFO failed\n    libdvdnav: vm: vm: failed to read VIDEO_TS.IFO\n    [00:58:00] dvd: not a dvd - trying as a stream/file instead\n    [00:58:00] file is MPEG Program Stream\n    [00:58:11] Probing 1 unknown stream\n    [00:58:11]     Probe: Found stream mpegvideo. stream id 0xe0-0x0\n    [00:58:11] Found the following streams\n    [00:58:11]     Video Streams :\n    [00:58:11]       0xe0-0x0 type mpegvideo (0x2)\n    [00:58:11]     Audio Streams :\n    [00:58:11]       0xc0-0x0 type MPEG2 (0x4)\n    [00:58:11]     Subtitle Streams :\n    [00:58:11]     Other Streams :\n    [00:58:11] stream id 0xc0 (type 0x4 substream 0x0) audio 0xc0\n    hb_ps_read_packet: error (32)\n    hb_ps_read_packet: error (32)\n    hb_ps_read_packet: error (32)\n    hb_ps_read_packet: error (32)\n    Last error repeated 21 times\n    hb_ps_read_packet: error (32)\n    hb_ps_read_packet: error (32)\n    Last error repeated 313 times\n    hb_ps_read_packet: error (32)\n    hb_ps_read_packet: error (32)\n    Last error repeated 127 times\n    hb_ps_read_packet: error (32)\n    hb_ps_read_packet: error (32)\n    Last error repeated 48 times\n    hb_ps_read_packet: error (32)\n    hb_ps_read_packet: error (32)\n    Last error repeated 7 times\n    hb_ps_read_packet: error (32)\n    hb_ps_read_packet: error (32)\n    Last error repeated 3 times\n    hb_ps_read_packet: error (32)\n    \n\n&amp;#x200B;\n\n[translation: 1 terminated event.       file cant be read from disk?](https://preview.redd.it/uolzdhn3gj5a1.png?width=482&amp;format=png&amp;auto=webp&amp;s=2c3a0d92c14407040395a2d1ab10ddea440c04ae)\n\nhttps://preview.redd.it/4mqvskn3gj5a1.png?width=1251&amp;format=png&amp;auto=webp&amp;s=6bc6d711fa7af55d717f745dc8e27c9a0b59f4e9", "author_fullname": "t2_aayl4x9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "i cant copy a particular file from DVD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "media_metadata": {"uolzdhn3gj5a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/uolzdhn3gj5a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0c33a66ff02e8f24efbad16442f8e5a9e4f59c52"}, {"y": 111, "x": 216, "u": "https://preview.redd.it/uolzdhn3gj5a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4986fc33c31ca2c14295d7632bcd74f7da1b39a7"}, {"y": 165, "x": 320, "u": "https://preview.redd.it/uolzdhn3gj5a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1716d37016a3b411c31afe3cd5b7716b2f22a460"}], "s": {"y": 249, "x": 482, "u": "https://preview.redd.it/uolzdhn3gj5a1.png?width=482&amp;format=png&amp;auto=webp&amp;s=2c3a0d92c14407040395a2d1ab10ddea440c04ae"}, "id": "uolzdhn3gj5a1"}, "4mqvskn3gj5a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 84, "x": 108, "u": "https://preview.redd.it/4mqvskn3gj5a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ee4617465face5ed49179fe9302f1f6dfbad718"}, {"y": 168, "x": 216, "u": "https://preview.redd.it/4mqvskn3gj5a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d30bebd77962f6ce731c6bb42bb2dc976ba87c61"}, {"y": 249, "x": 320, "u": "https://preview.redd.it/4mqvskn3gj5a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d2f8b9eb07e0ec0cb6b669700684b9676083344"}, {"y": 499, "x": 640, "u": "https://preview.redd.it/4mqvskn3gj5a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=48672b9fe6bbf816dea237c1e75c14916f482672"}, {"y": 749, "x": 960, "u": "https://preview.redd.it/4mqvskn3gj5a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e4c41ce160e00684b3bb2830e621bdbe3da18065"}, {"y": 843, "x": 1080, "u": "https://preview.redd.it/4mqvskn3gj5a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eb80f33fbe8400f23b40ce02036e45642ca3d325"}], "s": {"y": 977, "x": 1251, "u": "https://preview.redd.it/4mqvskn3gj5a1.png?width=1251&amp;format=png&amp;auto=webp&amp;s=6bc6d711fa7af55d717f745dc8e27c9a0b59f4e9"}, "id": "4mqvskn3gj5a1"}}, "name": "t3_zkcprp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KFPah0Jablz2ZT_B3PlzCfK2aTl6zzA_VTr5JMH651M.jpg", "edited": 1670882392.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670881838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;first: i didnt grow up with dvds so im a noob.&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;m trying to rip a video from DVD. but all of the dvd ripping softwares i&amp;#39;ve tried gave error at exactly 40th minute of the video. i think its becouse VTS_013.VOB file. so i copied other files with DVD Decrypter and skipped VTS_013.VOB to manually copy it with file explorer. but it also gave error. i can run VTS_013.VOB with vlc no issues.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[00:58:00] hb_init: starting libhb thread\n\n # Starting Scan ...\n\n[00:58:00] CPU:\n[00:58:00]  - logical processor count: 8\n[00:58:00] Intel Quick Sync Video support: no\n[00:58:00] hb_scan: path=D:\\VIDEO_TS\\VTS_01_3.VOB, title_index=0\nudfread ERROR: ECMA 167 Volume Recognition failed\nsrc/libbluray/disc/disc.c:333: failed opening UDF image D:\\VIDEO_TS\\VTS_01_3.VOB\nsrc/libbluray/disc/disc.c:437: error opening file BDMV\\index.bdmv\nsrc/libbluray/disc/disc.c:437: error opening file BDMV\\BACKUP\\index.bdmv\nsrc/libbluray/bluray.c:2646: nav_get_title_list(D:\\VIDEO_TS\\VTS_01_3.VOB\\) failed\n[00:58:00] bd: not a bd - trying as a stream/file instead\nlibdvdread: DVDOpenFileUDF:UDFFindFile /VIDEO_TS/VIDEO_TS.IFO failed\nlibdvdnav: vm: vm: failed to read VIDEO_TS.IFO\n[00:58:00] dvd: not a dvd - trying as a stream/file instead\n[00:58:00] file is MPEG Program Stream\n[00:58:11] Probing 1 unknown stream\n[00:58:11]     Probe: Found stream mpegvideo. stream id 0xe0-0x0\n[00:58:11] Found the following streams\n[00:58:11]     Video Streams :\n[00:58:11]       0xe0-0x0 type mpegvideo (0x2)\n[00:58:11]     Audio Streams :\n[00:58:11]       0xc0-0x0 type MPEG2 (0x4)\n[00:58:11]     Subtitle Streams :\n[00:58:11]     Other Streams :\n[00:58:11] stream id 0xc0 (type 0x4 substream 0x0) audio 0xc0\nhb_ps_read_packet: error (32)\nhb_ps_read_packet: error (32)\nhb_ps_read_packet: error (32)\nhb_ps_read_packet: error (32)\nLast error repeated 21 times\nhb_ps_read_packet: error (32)\nhb_ps_read_packet: error (32)\nLast error repeated 313 times\nhb_ps_read_packet: error (32)\nhb_ps_read_packet: error (32)\nLast error repeated 127 times\nhb_ps_read_packet: error (32)\nhb_ps_read_packet: error (32)\nLast error repeated 48 times\nhb_ps_read_packet: error (32)\nhb_ps_read_packet: error (32)\nLast error repeated 7 times\nhb_ps_read_packet: error (32)\nhb_ps_read_packet: error (32)\nLast error repeated 3 times\nhb_ps_read_packet: error (32)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uolzdhn3gj5a1.png?width=482&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c3a0d92c14407040395a2d1ab10ddea440c04ae\"&gt;translation: 1 terminated event.       file cant be read from disk?&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4mqvskn3gj5a1.png?width=1251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bc6d711fa7af55d717f745dc8e27c9a0b59f4e9\"&gt;https://preview.redd.it/4mqvskn3gj5a1.png?width=1251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bc6d711fa7af55d717f745dc8e27c9a0b59f4e9&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkcprp", "is_robot_indexable": true, "report_reasons": null, "author": "0xdieaster", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkcprp/i_cant_copy_a_particular_file_from_dvd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkcprp/i_cant_copy_a_particular_file_from_dvd/", "subreddit_subscribers": 659013, "created_utc": 1670881838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am a moron and have some 15.36tb used enterprise ssd's in U.2 format lying around. What's the best case for them? They get kinda hot so I'd like a fan, replaceable ideal so I can swap it with a moctua\n\nI don't care about throughput that much but I'd like more than 2 drives stored", "author_fullname": "t2_7p2i6djo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best U.2 enclosure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk788r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670870082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a moron and have some 15.36tb used enterprise ssd&amp;#39;s in U.2 format lying around. What&amp;#39;s the best case for them? They get kinda hot so I&amp;#39;d like a fan, replaceable ideal so I can swap it with a moctua&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t care about throughput that much but I&amp;#39;d like more than 2 drives stored&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk788r", "is_robot_indexable": true, "report_reasons": null, "author": "Spirited-Guidance-91", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk788r/best_u2_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk788r/best_u2_enclosure/", "subreddit_subscribers": 659013, "created_utc": 1670870082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Long time lurker first time posting. Also English is not my first Language so please bear with me. I currently have Synology NAS with 2 bays with 2 4TB hard drives. I want to expand because I'm quickly approaching 7TB. I have my previous desktop computer that has about 8 drive bays that I would like to expand over time. I would like to get some opinions from the experts before I begin throwing time and money at this project.\n\nWhat OS/software should I use to manage/initialize my NAS? -- so far I have heard of TrueNAS, FreeNAS, ZFS and Unraid. Note:not even sure if I know what i am talking about here or if there are differences i am not aware of.\n\nIs expanding my \"pools\" for future additional drives going to be an issue? I don't want to be moving around several TB.\n\nI also read that my drives would have to be matching in size or I won't be able to use the drive to their fullest extent. I was thinking about going to 12-14 TB drives right away but maybe I should by smaller capacity drives to start out? It just seems that in the future my lower capacity drives with hold me back.\n\nThoughts about backups - just looking for some suggestions? I read about snapshots but I'm not sure that applies here. I am familiar with the 3-2-1 rule but with the TB amounts of data I have I can't reasonably buy duplicates of all drives just to have 1 to 1 backups (if that makes sense) but maybe there is no way around this.\n\nI started off my journey with my synology NAS just to see if it would work and now that I have become a little more serious about my data-hoarding I would like a better setup with more storage as well as implement best practices. I see this as the next logical step before I get something that is rack mounted. Let me know all of your thoughts as I am sure I have missed something or have not thought a position through. Thank you for your help!", "author_fullname": "t2_ui7yc7hu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upgrading NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk6efk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670868275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long time lurker first time posting. Also English is not my first Language so please bear with me. I currently have Synology NAS with 2 bays with 2 4TB hard drives. I want to expand because I&amp;#39;m quickly approaching 7TB. I have my previous desktop computer that has about 8 drive bays that I would like to expand over time. I would like to get some opinions from the experts before I begin throwing time and money at this project.&lt;/p&gt;\n\n&lt;p&gt;What OS/software should I use to manage/initialize my NAS? -- so far I have heard of TrueNAS, FreeNAS, ZFS and Unraid. Note:not even sure if I know what i am talking about here or if there are differences i am not aware of.&lt;/p&gt;\n\n&lt;p&gt;Is expanding my &amp;quot;pools&amp;quot; for future additional drives going to be an issue? I don&amp;#39;t want to be moving around several TB.&lt;/p&gt;\n\n&lt;p&gt;I also read that my drives would have to be matching in size or I won&amp;#39;t be able to use the drive to their fullest extent. I was thinking about going to 12-14 TB drives right away but maybe I should by smaller capacity drives to start out? It just seems that in the future my lower capacity drives with hold me back.&lt;/p&gt;\n\n&lt;p&gt;Thoughts about backups - just looking for some suggestions? I read about snapshots but I&amp;#39;m not sure that applies here. I am familiar with the 3-2-1 rule but with the TB amounts of data I have I can&amp;#39;t reasonably buy duplicates of all drives just to have 1 to 1 backups (if that makes sense) but maybe there is no way around this.&lt;/p&gt;\n\n&lt;p&gt;I started off my journey with my synology NAS just to see if it would work and now that I have become a little more serious about my data-hoarding I would like a better setup with more storage as well as implement best practices. I see this as the next logical step before I get something that is rack mounted. Let me know all of your thoughts as I am sure I have missed something or have not thought a position through. Thank you for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zk6efk", "is_robot_indexable": true, "report_reasons": null, "author": "TheKingMongo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk6efk/upgrading_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk6efk/upgrading_nas/", "subreddit_subscribers": 659013, "created_utc": 1670868275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/57p1d5l0zh5a1.jpg?width=1212&amp;format=pjpg&amp;auto=webp&amp;s=06439c41fe91a67d125fb543db66df7914e6b5fb\n\n&amp;#x200B;\n\nFor background, I am quite new to this.  I am a photographer and have been using simple external hard drives to edit and view my photos from.  A few months ago, I purchased a 10TB WD Elements desktop hard drive, and the pursuit of having faster read/write speeds (without using SSDs) and joining this subreddit has only heightened that \"passion\" (or addiction?).\n\nI recently acquired 2x WD Gold 12TB and I have them in the OWC Gemini Dual Thunderbolt 3 enclosure.  I have a large amount of photos in which I'd like to use the two drives to store and edit from (I have the appropriate backup system just in case).  I would have never thought to go the Raid 0 route but this subreddit triggered my curiosity so I went ahead and purchased 2x WD Golds to see what the differences were with the read/write speeds between a single drive versus two in Raid 0.  As you can see, the numbers with the two drives double in the read department with the \"write\" under Q8T1 increase by more than 90%.  The problem I'm focused on Q1T1 where the write speed actually decreased from 178mbs down to 135mbs.  My question is:  am I doing something wrong or is this normal?   I also performed this test under Blackmagic and it didn't note the dip write speed.  I didn't expect this because the two drives are the exact same in specs.  I have not put photos/data on either drives yet as I wanted to perform these tests before deciding on migrating over to this approach and whether or not I want to pursue the Raid 0 or just the single disk option.\n\nLastly, did performed a real world test where I transferred about 6gbs of photos from a V60 SD card to the Raid 0 setup.  I don't have the screenshot but what happened was I saw a consistent copy speed of around 220mbs but then in the middle of the transfer process, the speed literally dipped down to ZERO before shooting back up to 220mbs.  Again, is this normal given that the two drives are exactly the same?  Should I look for anything as it relates to testing to look for specific values that may provide insight on this bottleneck?\n\nUPDATE:  I'm an idiot.  Second, it's resolved.  I went back and properly set the Raid dial on the device to \"Raid 0\" and the system immediately recognized it as a single volume without me having to tell Windows disk management if it should be spanned, striped, etc.  The original issue was that I had the Raid dial set to \"JBOD\" (hardware) and I was telling Windows (software) that I wanted striped drives  therefore creating a conflict. With this fix, my read and write speeds are over 500mbs.  Very happy.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/tqmqktlkji5a1.png?width=1206&amp;format=png&amp;auto=webp&amp;s=69f3ddb94c92111f240ef293e696ee8f4bf134b6\n\nhttps://preview.redd.it/4wn0lqlkji5a1.png?width=1253&amp;format=png&amp;auto=webp&amp;s=b2e0510419a58301a5e9e53abcb55494f6e08503", "author_fullname": "t2_exy97", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the write speed for Raid 0 normal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 83, "top_awarded_type": null, "hide_score": false, "media_metadata": {"57p1d5l0zh5a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 64, "x": 108, "u": "https://preview.redd.it/57p1d5l0zh5a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f273337846fef45ecb26a3f3fe595be12ac3189"}, {"y": 128, "x": 216, "u": "https://preview.redd.it/57p1d5l0zh5a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec943272db2128ba6f5bfe2622febf44456f22f1"}, {"y": 190, "x": 320, "u": "https://preview.redd.it/57p1d5l0zh5a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aec05db5f283032a4010066510e4df4925bb03db"}, {"y": 381, "x": 640, "u": "https://preview.redd.it/57p1d5l0zh5a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2dc5dc7e507e3f1ba0a082d3a96cca885b760030"}, {"y": 571, "x": 960, "u": "https://preview.redd.it/57p1d5l0zh5a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=59e0b7ab9cff4acdeca7b6a7d47c31b9d8c6cae6"}, {"y": 643, "x": 1080, "u": "https://preview.redd.it/57p1d5l0zh5a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=543994bd8e432c061321c419dd1b317fb2a64759"}], "s": {"y": 722, "x": 1212, "u": "https://preview.redd.it/57p1d5l0zh5a1.jpg?width=1212&amp;format=pjpg&amp;auto=webp&amp;s=06439c41fe91a67d125fb543db66df7914e6b5fb"}, "id": "57p1d5l0zh5a1"}, "4wn0lqlkji5a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 111, "x": 108, "u": "https://preview.redd.it/4wn0lqlkji5a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8217eeb447e6b893bf09fe6396bfbccec4fb2329"}, {"y": 223, "x": 216, "u": "https://preview.redd.it/4wn0lqlkji5a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc24522fc32af7c04f4df6fa688b904913051777"}, {"y": 330, "x": 320, "u": "https://preview.redd.it/4wn0lqlkji5a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=48569ffc0e39da8911c2cf0ff27fddb2eb20a896"}, {"y": 660, "x": 640, "u": "https://preview.redd.it/4wn0lqlkji5a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dcc912fb9f2198fbb3a905b91cbfb6ada1880324"}, {"y": 991, "x": 960, "u": "https://preview.redd.it/4wn0lqlkji5a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=36473f6434942bd67a478f6976301c3d8985b903"}, {"y": 1115, "x": 1080, "u": "https://preview.redd.it/4wn0lqlkji5a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9f8e465940d0d2cde9f9027bf6e90885277dbe3b"}], "s": {"y": 1294, "x": 1253, "u": "https://preview.redd.it/4wn0lqlkji5a1.png?width=1253&amp;format=png&amp;auto=webp&amp;s=b2e0510419a58301a5e9e53abcb55494f6e08503"}, "id": "4wn0lqlkji5a1"}, "tqmqktlkji5a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 78, "x": 108, "u": "https://preview.redd.it/tqmqktlkji5a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1facf2995f538e253b9064370263c28ce609e360"}, {"y": 156, "x": 216, "u": "https://preview.redd.it/tqmqktlkji5a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e40f9c3a8e2285dfe28b350cc5ec2afd3d2348e8"}, {"y": 231, "x": 320, "u": "https://preview.redd.it/tqmqktlkji5a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bef5f1f0f443520d62cf2f2b1ae3a1046c86d904"}, {"y": 463, "x": 640, "u": "https://preview.redd.it/tqmqktlkji5a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f858f529a19a3b6d0b129501d02d8d724f2178e5"}, {"y": 695, "x": 960, "u": "https://preview.redd.it/tqmqktlkji5a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f68a6520b3ffbcde1c401521aa7af7fc092fbb8a"}, {"y": 782, "x": 1080, "u": "https://preview.redd.it/tqmqktlkji5a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=97759ebb816c9a65be4eca260746fe74d1bae23d"}], "s": {"y": 874, "x": 1206, "u": "https://preview.redd.it/tqmqktlkji5a1.png?width=1206&amp;format=png&amp;auto=webp&amp;s=69f3ddb94c92111f240ef293e696ee8f4bf134b6"}, "id": "tqmqktlkji5a1"}}, "name": "t3_zk4g75", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/dQYNjz_Wv3E2FhlbVNEjFXQSbNEfL4LKeSwNGu2iTmw.jpg", "edited": 1670871220.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670864076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/57p1d5l0zh5a1.jpg?width=1212&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=06439c41fe91a67d125fb543db66df7914e6b5fb\"&gt;https://preview.redd.it/57p1d5l0zh5a1.jpg?width=1212&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=06439c41fe91a67d125fb543db66df7914e6b5fb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;For background, I am quite new to this.  I am a photographer and have been using simple external hard drives to edit and view my photos from.  A few months ago, I purchased a 10TB WD Elements desktop hard drive, and the pursuit of having faster read/write speeds (without using SSDs) and joining this subreddit has only heightened that &amp;quot;passion&amp;quot; (or addiction?).&lt;/p&gt;\n\n&lt;p&gt;I recently acquired 2x WD Gold 12TB and I have them in the OWC Gemini Dual Thunderbolt 3 enclosure.  I have a large amount of photos in which I&amp;#39;d like to use the two drives to store and edit from (I have the appropriate backup system just in case).  I would have never thought to go the Raid 0 route but this subreddit triggered my curiosity so I went ahead and purchased 2x WD Golds to see what the differences were with the read/write speeds between a single drive versus two in Raid 0.  As you can see, the numbers with the two drives double in the read department with the &amp;quot;write&amp;quot; under Q8T1 increase by more than 90%.  The problem I&amp;#39;m focused on Q1T1 where the write speed actually decreased from 178mbs down to 135mbs.  My question is:  am I doing something wrong or is this normal?   I also performed this test under Blackmagic and it didn&amp;#39;t note the dip write speed.  I didn&amp;#39;t expect this because the two drives are the exact same in specs.  I have not put photos/data on either drives yet as I wanted to perform these tests before deciding on migrating over to this approach and whether or not I want to pursue the Raid 0 or just the single disk option.&lt;/p&gt;\n\n&lt;p&gt;Lastly, did performed a real world test where I transferred about 6gbs of photos from a V60 SD card to the Raid 0 setup.  I don&amp;#39;t have the screenshot but what happened was I saw a consistent copy speed of around 220mbs but then in the middle of the transfer process, the speed literally dipped down to ZERO before shooting back up to 220mbs.  Again, is this normal given that the two drives are exactly the same?  Should I look for anything as it relates to testing to look for specific values that may provide insight on this bottleneck?&lt;/p&gt;\n\n&lt;p&gt;UPDATE:  I&amp;#39;m an idiot.  Second, it&amp;#39;s resolved.  I went back and properly set the Raid dial on the device to &amp;quot;Raid 0&amp;quot; and the system immediately recognized it as a single volume without me having to tell Windows disk management if it should be spanned, striped, etc.  The original issue was that I had the Raid dial set to &amp;quot;JBOD&amp;quot; (hardware) and I was telling Windows (software) that I wanted striped drives  therefore creating a conflict. With this fix, my read and write speeds are over 500mbs.  Very happy.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tqmqktlkji5a1.png?width=1206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69f3ddb94c92111f240ef293e696ee8f4bf134b6\"&gt;https://preview.redd.it/tqmqktlkji5a1.png?width=1206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69f3ddb94c92111f240ef293e696ee8f4bf134b6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4wn0lqlkji5a1.png?width=1253&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2e0510419a58301a5e9e53abcb55494f6e08503\"&gt;https://preview.redd.it/4wn0lqlkji5a1.png?width=1253&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2e0510419a58301a5e9e53abcb55494f6e08503&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk4g75", "is_robot_indexable": true, "report_reasons": null, "author": "chijrt", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk4g75/is_the_write_speed_for_raid_0_normal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk4g75/is_the_write_speed_for_raid_0_normal/", "subreddit_subscribers": 659013, "created_utc": 1670864076.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}