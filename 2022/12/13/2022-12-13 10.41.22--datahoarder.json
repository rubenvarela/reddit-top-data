{"kind": "Listing", "data": {"after": "t3_zkn59c", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1omlki7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just accidentally nuked ~90% of my video library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 90, "top_awarded_type": null, "hide_score": false, "name": "t3_zkc7jv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 482, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 482, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/JxQNCfnKe8WfA3OrEpx3B-VGJQFmz3-txa9-rY2TVb8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670880757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/guh9oj5gaj5a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/guh9oj5gaj5a1.png?auto=webp&amp;s=8d4162e08a0eb95f5808b839c879287c8e88657b", "width": 1493, "height": 964}, "resolutions": [{"url": "https://preview.redd.it/guh9oj5gaj5a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a655b2982fb80e95c8937d57dbff1c0f6250643b", "width": 108, "height": 69}, {"url": "https://preview.redd.it/guh9oj5gaj5a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb90753a98ae5f98cf4bd30f283a1775d58d6713", "width": 216, "height": 139}, {"url": "https://preview.redd.it/guh9oj5gaj5a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=86d23aeddbc14ffa850acde5b6ac9d8121f1d801", "width": 320, "height": 206}, {"url": "https://preview.redd.it/guh9oj5gaj5a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cb2e5841e1858156c88d669693728d61de69e089", "width": 640, "height": 413}, {"url": "https://preview.redd.it/guh9oj5gaj5a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e190fc6ad72cc5bb8f0a33ba04a7f2745fdecb0f", "width": 960, "height": 619}, {"url": "https://preview.redd.it/guh9oj5gaj5a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1070499d138a01f4b1555806200fc7f3442ad16", "width": 1080, "height": 697}], "variants": {}, "id": "-IQY7NkOwJilESbG22s_hQOlk1Wwg1s1gimRet7UtX0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkc7jv", "is_robot_indexable": true, "report_reasons": null, "author": "randombystander3001", "discussion_type": null, "num_comments": 212, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkc7jv/just_accidentally_nuked_90_of_my_video_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/guh9oj5gaj5a1.png", "subreddit_subscribers": 658997, "created_utc": 1670880757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_dc1bf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Huge thanks to Seagate and u/Seagate_Surfer for running the IronWolf Pro SSD giveaway! It has found a good home and replaced an ancient OCZ relic.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"smerxp99hi5a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ce7ce1a7531f48d50128c88f41834c439eb9a04"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=51d99f23267c6296f35f8718cf654cd1c2b3ce9c"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0dc449aad132695ef3c2d51ca76ac2b2bd908f56"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e120c9ec9ce3417efc0118987747a20595b26801"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=71b3726ea0c0dcb0eb21b3b6152df599a2b22872"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e534075c1930e4020f4004d7d66656851b5e36cc"}], "s": {"y": 3024, "x": 4032, "u": "https://preview.redd.it/smerxp99hi5a1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=574037522ec30d587ec0df7fb2f90260813c73f9"}, "id": "smerxp99hi5a1"}, "b1p98rf8hi5a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 138, "x": 108, "u": "https://preview.redd.it/b1p98rf8hi5a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=73d95b83319554632043d28d74d88b36d8de89e7"}, {"y": 276, "x": 216, "u": "https://preview.redd.it/b1p98rf8hi5a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=53cf0884e3b5b62a2cc91526f37cf2bd374ccfab"}, {"y": 409, "x": 320, "u": "https://preview.redd.it/b1p98rf8hi5a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=baa20e2c7a50427d52715404f44eaebbdd58e7c8"}, {"y": 819, "x": 640, "u": "https://preview.redd.it/b1p98rf8hi5a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae4fd9db311173e99eb50045f8672311005d030c"}], "s": {"y": 922, "x": 720, "u": "https://preview.redd.it/b1p98rf8hi5a1.jpg?width=720&amp;format=pjpg&amp;auto=webp&amp;s=0fc5ab769725df96e84e8ef7491832bbddb8ad8f"}, "id": "b1p98rf8hi5a1"}, "61tian8ahi5a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 39, "x": 108, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2d0d6a0ade073c64da315099ed910a935af0a2c5"}, {"y": 78, "x": 216, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f4ba353b79b466d87e1b3855c36789a3befad87a"}, {"y": 116, "x": 320, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a0d549cd84767d024d4e56452a115ec9e36ddca"}, {"y": 233, "x": 640, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5a3e0a9422be67e363394c72f3d96222c3a8c43"}, {"y": 350, "x": 960, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7df140ab2076ff3ba8047c1ea980751bfbdaf8a5"}, {"y": 394, "x": 1080, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=37e7d0103352e989bf5f3f846fef429d3b14da67"}], "s": {"y": 469, "x": 1284, "u": "https://preview.redd.it/61tian8ahi5a1.png?width=1284&amp;format=png&amp;auto=webp&amp;s=60a087c520ed31b30679540bb204a1e271ea363c"}, "id": "61tian8ahi5a1"}}, "name": "t3_zk7aa0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 297, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "b1p98rf8hi5a1", "id": 218525898}, {"media_id": "smerxp99hi5a1", "id": 218525899}, {"media_id": "61tian8ahi5a1", "id": 218525900}]}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 297, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/lvphIk5c9FViHQG02pQlJNYkKTllgXKlxAERzuVVKDU.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670870209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zk7aa0", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "74TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zk7aa0", "is_robot_indexable": true, "report_reasons": null, "author": "Clawz114", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zk7aa0/huge_thanks_to_seagate_and_useagate_surfer_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zk7aa0", "subreddit_subscribers": 658997, "created_utc": 1670870209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "because date hoarding is quite an expensive hobby.", "author_fullname": "t2_lo8e4f55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who are you by profession?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk62zn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670867644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;because date hoarding is quite an expensive hobby.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "russian military ship, go to hell", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zk62zn", "is_robot_indexable": true, "report_reasons": null, "author": "kovach_ua", "discussion_type": null, "num_comments": 83, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zk62zn/who_are_you_by_profession/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk62zn/who_are_you_by_profession/", "subreddit_subscribers": 658997, "created_utc": 1670867644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\nAs I frequently see interesting threads on Reddit, and I want to get them offline to 1) find them again easily, and 2) preserve them in case messages get removed, I made myself some time ago a Python script to \"download\" threads off Reddit.This script does not *only* download the thread, but generates a nice HTML file, so it can be opened in a browser and the thread navigated around conveniently. [Here is an example of such HTML file](https://htmlpreview.github.io/?https://github.com/Ailothaen/RedditArchiver/blob/main/github/example.html).\n\nRecently, I told myself it would be better to have a web frontend for that tooling, since I am sometimes on the go and do not have the script and/or the Python interpreter on my machine.\n\nThis therefore led to RedditArchive, a Flask self-hosted app to archive and download Reddit threads (screenshots available in the README):[https://github.com/Ailothaen/RedditArchiver](https://github.com/Ailothaen/RedditArchiver)\n\nYou can install it on a small server of yours, such as a Raspberry Pi or a VPS. Installations instructions are provided if you want to try it on.\n\nIf you do not want to deal with the hassle of setting a web server up, worry not! I also made the original script available here:[https://github.com/Ailothaen/RedditArchiver-standalone](https://github.com/Ailothaen/RedditArchiver-standalone)\n\nDo not hesitate to comment and make suggestions \u2013 I have ideas for further features, but that's probably for another time. \ud83e\udd89", "author_fullname": "t2_ozmwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Made myself Python tooling to download threads off Reddit (available in Web UI and standalone script)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk6491", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670867716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;As I frequently see interesting threads on Reddit, and I want to get them offline to 1) find them again easily, and 2) preserve them in case messages get removed, I made myself some time ago a Python script to &amp;quot;download&amp;quot; threads off Reddit.This script does not &lt;em&gt;only&lt;/em&gt; download the thread, but generates a nice HTML file, so it can be opened in a browser and the thread navigated around conveniently. &lt;a href=\"https://htmlpreview.github.io/?https://github.com/Ailothaen/RedditArchiver/blob/main/github/example.html\"&gt;Here is an example of such HTML file&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Recently, I told myself it would be better to have a web frontend for that tooling, since I am sometimes on the go and do not have the script and/or the Python interpreter on my machine.&lt;/p&gt;\n\n&lt;p&gt;This therefore led to RedditArchive, a Flask self-hosted app to archive and download Reddit threads (screenshots available in the README):&lt;a href=\"https://github.com/Ailothaen/RedditArchiver\"&gt;https://github.com/Ailothaen/RedditArchiver&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You can install it on a small server of yours, such as a Raspberry Pi or a VPS. Installations instructions are provided if you want to try it on.&lt;/p&gt;\n\n&lt;p&gt;If you do not want to deal with the hassle of setting a web server up, worry not! I also made the original script available here:&lt;a href=\"https://github.com/Ailothaen/RedditArchiver-standalone\"&gt;https://github.com/Ailothaen/RedditArchiver-standalone&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Do not hesitate to comment and make suggestions \u2013 I have ideas for further features, but that&amp;#39;s probably for another time. \ud83e\udd89&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk6491", "is_robot_indexable": true, "report_reasons": null, "author": "Ailothaen", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk6491/made_myself_python_tooling_to_download_threads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk6491/made_myself_python_tooling_to_download_threads/", "subreddit_subscribers": 658997, "created_utc": 1670867716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My apologies if this topic does not belong to this group, but I ran out of options and this is my last resource. \n\nThere is in **BBC Sounds** a podcast called **Night Tracks**. Each episode is a beautiful collection of classical and rare experimental music. The episodes are only available for certain amount of time and then they are removed from the website. \n\nI want to collect every single episode but I haven't found any way to download them. I've tried with multiple ad-ons, extensions, and software but it is just imposible. \n\nPerhaps someone here knows a way. I would deeply appreciate any help.", "author_fullname": "t2_tzaj22sa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A datahoarding attempt that has proven to be almost impossible", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkdqq7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670883990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My apologies if this topic does not belong to this group, but I ran out of options and this is my last resource. &lt;/p&gt;\n\n&lt;p&gt;There is in &lt;strong&gt;BBC Sounds&lt;/strong&gt; a podcast called &lt;strong&gt;Night Tracks&lt;/strong&gt;. Each episode is a beautiful collection of classical and rare experimental music. The episodes are only available for certain amount of time and then they are removed from the website. &lt;/p&gt;\n\n&lt;p&gt;I want to collect every single episode but I haven&amp;#39;t found any way to download them. I&amp;#39;ve tried with multiple ad-ons, extensions, and software but it is just imposible. &lt;/p&gt;\n\n&lt;p&gt;Perhaps someone here knows a way. I would deeply appreciate any help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkdqq7", "is_robot_indexable": true, "report_reasons": null, "author": "Melancholic-Beast", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zkdqq7/a_datahoarding_attempt_that_has_proven_to_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkdqq7/a_datahoarding_attempt_that_has_proven_to_be/", "subreddit_subscribers": 658997, "created_utc": 1670883990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for something dedicated but less expensive like this for my Windows PC, don't need to have Raid:\n\n[https://eshop.macsales.com/item/OWC/SSDACL4M208T/#customer-reviews](https://eshop.macsales.com/item/OWC/SSDACL4M208T/#customer-reviews)\n\nThanks!", "author_fullname": "t2_3zj1o3uc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any dedicated PCIe 4.0 NVMe quad adapters?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk7y1j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670871565.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for something dedicated but less expensive like this for my Windows PC, don&amp;#39;t need to have Raid:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eshop.macsales.com/item/OWC/SSDACL4M208T/#customer-reviews\"&gt;https://eshop.macsales.com/item/OWC/SSDACL4M208T/#customer-reviews&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk7y1j", "is_robot_indexable": true, "report_reasons": null, "author": "MarkGeraz", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk7y1j/any_dedicated_pcie_40_nvme_quad_adapters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk7y1j/any_dedicated_pcie_40_nvme_quad_adapters/", "subreddit_subscribers": 658997, "created_utc": 1670871565.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello data hoarders please spread some knowledge\n\nI thought I'd burn all my saved up cash during a recession and got myself a decent starter NAS - DS720+, which had some \"new\", read; suspectedly S.M.A.R.T wiped disks from 2014, included.\n\nMy idea was to run my current linux.iso server with the NAS mounted in NFS4.1 and just smack all the sweet linux iso's right onto there.\n\nQuickly realized I'm getting bottlenecked bigtime, despite running the newest storage tech in RAID 0 for double the speed etc (I kid, don't hurt me). I know Raid0 is stupid but I'll take any performance wins as this will only host data I don't care about losing - not much of a data hoarder, eh.\n\nAnyways I'm getting **cache overloaded** to infinity and beyond.\n\nWith some linux.iso testing I did manage to max my gigabit line with the **old drives**. When dowloading from a single source.\n\nThereafter I decided I'd eat oatmeal for a few months and got myself a pair of 18TB Toshiba disks. I assume this was a very bad purchase as I'm now getting even worse cache overload.\n\nEdit: tried a brand new iso download with the **new disks** and It also goes full speed when there is only one seed.\n\nWhenever there multiple seeds / files at once at once it halts to a stop - normal for HDD's I suppose - lesson learned.  I feel there should be SOME way to tweak it for a decent download speed. Be it qbit settings or NFS config.\n\nThis is downloading roughly at 25MiB/s - *qbittorent-nox*\n\n    Performance statistics\n    Write cache overload:\t87.50%\n    Read cache overload:\t0%\n    Queued I/O jobs:\t4602\n    Average time in queue:\t2667 ms\n    Total queued size:\t0 B\n\nI've tried plenty of recommended qbittorent-nox settings with next to no difference in performance. As you can tell I'm very novice dealing with storage tech - which makes this a perfect opportunity to learn!\n\nTL;DR - Just facts below\n\nBasically I'm hosting a NFS4.1 share on the Synology and have mounted it as a disk, where qbitt-nox is saving directly to.\n\n* *1000/1000 Net - with VPN I don't expect full gigabit to ever work (testing without VPN)*\n* *Synology and Server is connected to eachother with static routes*\n* *NFS Latency on Synology spikes when the cache / queue gets too big*\n* *Moving files to from Server to NAS goes full speed*\n* *Moving files from Desktop over Wifi-&gt;Switch-&gt;NAS  almost full speed*\n* *Worse /  same performance with newer drives*\n\n**Drives**\n\n    Old drives: hus724040ale641 \n    New drives: Toshiba MG09ACA18TE\n\n**Server ( Network basically the same on NAS**\n\n    sudo mount -t nfs4 10.21.0.200:/volume1/share /mnt/synology -o async\n    [ Mount ] \n    10.21.0.200:/volume1/share on /mnt/synology type nfs4 (rw,relatime,vers=4.1,rsize=131072,wsize=131072,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.21.0.201,local_lock=none,addr=10.21.0.200)\n    \n    /etc/netplan/00-installer-config.yaml\n        ens19:\n          addresses:\n            - 10.21.0.201/31\n          routes:\n            - to: 10.21.0.200\n              via: 10.21.0.200\n\n**Some nfsiostats - running the same download as performance example above**\n\n               ops/s       rpc bklog\n            1889.751           0.000\n    \n    read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)  avg queue (ms)          errors\n                    1669.134        7589.243           4.547        0 (0.0%)           0.353           0.403           0.031        0 (0.0%)\n    write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)  avg queue (ms)          errors\n                     212.920       26649.617         125.163        0 (0.0%)          10.306         838.922         828.588        0 (0.0%)\n\nGreatly appreciate any input, even if it's telling me I'm stupid \ud83d\ude05\n\nAnything to Tweak in qbittorrent?\n\nAny NFS settings, mount arg i should be using?\n\nNetwork config problems?", "author_fullname": "t2_nomuasx5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synology NFS4.1 NetShare headaches", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkaexa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670878880.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670876916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello data hoarders please spread some knowledge&lt;/p&gt;\n\n&lt;p&gt;I thought I&amp;#39;d burn all my saved up cash during a recession and got myself a decent starter NAS - DS720+, which had some &amp;quot;new&amp;quot;, read; suspectedly S.M.A.R.T wiped disks from 2014, included.&lt;/p&gt;\n\n&lt;p&gt;My idea was to run my current linux.iso server with the NAS mounted in NFS4.1 and just smack all the sweet linux iso&amp;#39;s right onto there.&lt;/p&gt;\n\n&lt;p&gt;Quickly realized I&amp;#39;m getting bottlenecked bigtime, despite running the newest storage tech in RAID 0 for double the speed etc (I kid, don&amp;#39;t hurt me). I know Raid0 is stupid but I&amp;#39;ll take any performance wins as this will only host data I don&amp;#39;t care about losing - not much of a data hoarder, eh.&lt;/p&gt;\n\n&lt;p&gt;Anyways I&amp;#39;m getting &lt;strong&gt;cache overloaded&lt;/strong&gt; to infinity and beyond.&lt;/p&gt;\n\n&lt;p&gt;With some linux.iso testing I did manage to max my gigabit line with the &lt;strong&gt;old drives&lt;/strong&gt;. When dowloading from a single source.&lt;/p&gt;\n\n&lt;p&gt;Thereafter I decided I&amp;#39;d eat oatmeal for a few months and got myself a pair of 18TB Toshiba disks. I assume this was a very bad purchase as I&amp;#39;m now getting even worse cache overload.&lt;/p&gt;\n\n&lt;p&gt;Edit: tried a brand new iso download with the &lt;strong&gt;new disks&lt;/strong&gt; and It also goes full speed when there is only one seed.&lt;/p&gt;\n\n&lt;p&gt;Whenever there multiple seeds / files at once at once it halts to a stop - normal for HDD&amp;#39;s I suppose - lesson learned.  I feel there should be SOME way to tweak it for a decent download speed. Be it qbit settings or NFS config.&lt;/p&gt;\n\n&lt;p&gt;This is downloading roughly at 25MiB/s - &lt;em&gt;qbittorent-nox&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Performance statistics\nWrite cache overload:   87.50%\nRead cache overload:    0%\nQueued I/O jobs:    4602\nAverage time in queue:  2667 ms\nTotal queued size:  0 B\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;ve tried plenty of recommended qbittorent-nox settings with next to no difference in performance. As you can tell I&amp;#39;m very novice dealing with storage tech - which makes this a perfect opportunity to learn!&lt;/p&gt;\n\n&lt;p&gt;TL;DR - Just facts below&lt;/p&gt;\n\n&lt;p&gt;Basically I&amp;#39;m hosting a NFS4.1 share on the Synology and have mounted it as a disk, where qbitt-nox is saving directly to.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;1000/1000 Net - with VPN I don&amp;#39;t expect full gigabit to ever work (testing without VPN)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Synology and Server is connected to eachother with static routes&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;NFS Latency on Synology spikes when the cache / queue gets too big&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Moving files to from Server to NAS goes full speed&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Moving files from Desktop over Wifi-&amp;gt;Switch-&amp;gt;NAS  almost full speed&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Worse /  same performance with newer drives&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Drives&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Old drives: hus724040ale641 \nNew drives: Toshiba MG09ACA18TE\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Server ( Network basically the same on NAS&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sudo mount -t nfs4 10.21.0.200:/volume1/share /mnt/synology -o async\n[ Mount ] \n10.21.0.200:/volume1/share on /mnt/synology type nfs4 (rw,relatime,vers=4.1,rsize=131072,wsize=131072,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.21.0.201,local_lock=none,addr=10.21.0.200)\n\n/etc/netplan/00-installer-config.yaml\n    ens19:\n      addresses:\n        - 10.21.0.201/31\n      routes:\n        - to: 10.21.0.200\n          via: 10.21.0.200\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Some nfsiostats - running the same download as performance example above&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;           ops/s       rpc bklog\n        1889.751           0.000\n\nread:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)  avg queue (ms)          errors\n                1669.134        7589.243           4.547        0 (0.0%)           0.353           0.403           0.031        0 (0.0%)\nwrite:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)  avg queue (ms)          errors\n                 212.920       26649.617         125.163        0 (0.0%)          10.306         838.922         828.588        0 (0.0%)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Greatly appreciate any input, even if it&amp;#39;s telling me I&amp;#39;m stupid \ud83d\ude05&lt;/p&gt;\n\n&lt;p&gt;Anything to Tweak in qbittorrent?&lt;/p&gt;\n\n&lt;p&gt;Any NFS settings, mount arg i should be using?&lt;/p&gt;\n\n&lt;p&gt;Network config problems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkaexa", "is_robot_indexable": true, "report_reasons": null, "author": "mnklakej", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkaexa/synology_nfs41_netshare_headaches/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkaexa/synology_nfs41_netshare_headaches/", "subreddit_subscribers": 658997, "created_utc": 1670876916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi \n\nI'm trying to find an online backup solution for my veracrypt container that is around 300GB and growing. Ideally, I want a provider that doesn't require you to split the large file in parts. I'm not certain, but my current understanding is that I cannot split up the container into separate parts...\n\n&amp;#x200B;\n\nI don't need any syncing. I intend on uploading the entire container once per week. \n\n&amp;#x200B;\n\nAny suggestions would be much appreciated. ", "author_fullname": "t2_u07vnc7p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud solution for huge files that cannot be split up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk77qt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670870051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to find an online backup solution for my veracrypt container that is around 300GB and growing. Ideally, I want a provider that doesn&amp;#39;t require you to split the large file in parts. I&amp;#39;m not certain, but my current understanding is that I cannot split up the container into separate parts...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t need any syncing. I intend on uploading the entire container once per week. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any suggestions would be much appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk77qt", "is_robot_indexable": true, "report_reasons": null, "author": "user44566829", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk77qt/cloud_solution_for_huge_files_that_cannot_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk77qt/cloud_solution_for_huge_files_that_cannot_be/", "subreddit_subscribers": 658997, "created_utc": 1670870051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_oj0pl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Turkish movies which are hard to find on torrent are being uploaded with English subtitles to Youtube. Go ahead and back them up.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zkbvih", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/wZznEwWjQxzEBKDpk0qBMND9lriEWFKgKWT3vWB22p8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670880033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/@moviturkinternational/videos", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ru_AMlstSs-CGi-1_U-UXoek4nQGAg_xK8luucv6hFc.jpg?auto=webp&amp;s=bd24de6703c0cea3f6934e6e3e87ec331933771a", "width": 900, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/ru_AMlstSs-CGi-1_U-UXoek4nQGAg_xK8luucv6hFc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d43f81079e3aa103d4993d570555a39fc14ff42e", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ru_AMlstSs-CGi-1_U-UXoek4nQGAg_xK8luucv6hFc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a538f9a9f896238f3e2aeaccd161890a4dd20c66", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/ru_AMlstSs-CGi-1_U-UXoek4nQGAg_xK8luucv6hFc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e0c6b70089562eda047670f5b50d0629c79f684c", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/ru_AMlstSs-CGi-1_U-UXoek4nQGAg_xK8luucv6hFc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=84131d91db097765f56c2ea83cf9b746c56b741e", "width": 640, "height": 640}], "variants": {}, "id": "1QOSAck5v15D_7FRM65FyafUWQnJojwQvqpO6xJFrpM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkbvih", "is_robot_indexable": true, "report_reasons": null, "author": "Sacrer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkbvih/turkish_movies_which_are_hard_to_find_on_torrent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/@moviturkinternational/videos", "subreddit_subscribers": 658997, "created_utc": 1670880033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, apologies for asking a question I know has been asked a million times. In fact I saw what I'm pretty sure was the answer to my Q a couple weeks ago but have searched and searched and unfortunately can't find it again (shoulda archived it, heh.)\n\nI'm just starting to become a datahoarder and picked up a Synology NAS. My main motivation was the huge collection of tutorials and other knowledge I've saved on YouTube that may up and disappear one day.\n\n**First Goal:** I'm looking for really simple no-code (or very low code - I code all day long, and don't want to have to write/maintain a bunch of this stuff too) way to automatically archive anything I add to a specific YouTube playlist to my Synology NAS. I'd ideally like it to save thumbnails and descriptions as well.\n\n**Second Goal:** Some sort of easy interface for browsing, searching, and watching those archived videos, so I'm not just using crummy Windows search to try and find a video in the future.\n\nDoes anyone have suggestions that can address 1 or 2, or both?\n\nThanks in advance!", "author_fullname": "t2_mdffa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple and comprehensive approach to YouTube archiving + browsing/searching?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk98yu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670874356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, apologies for asking a question I know has been asked a million times. In fact I saw what I&amp;#39;m pretty sure was the answer to my Q a couple weeks ago but have searched and searched and unfortunately can&amp;#39;t find it again (shoulda archived it, heh.)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just starting to become a datahoarder and picked up a Synology NAS. My main motivation was the huge collection of tutorials and other knowledge I&amp;#39;ve saved on YouTube that may up and disappear one day.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;First Goal:&lt;/strong&gt; I&amp;#39;m looking for really simple no-code (or very low code - I code all day long, and don&amp;#39;t want to have to write/maintain a bunch of this stuff too) way to automatically archive anything I add to a specific YouTube playlist to my Synology NAS. I&amp;#39;d ideally like it to save thumbnails and descriptions as well.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Second Goal:&lt;/strong&gt; Some sort of easy interface for browsing, searching, and watching those archived videos, so I&amp;#39;m not just using crummy Windows search to try and find a video in the future.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have suggestions that can address 1 or 2, or both?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk98yu", "is_robot_indexable": true, "report_reasons": null, "author": "turn-down-for-what", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk98yu/simple_and_comprehensive_approach_to_youtube/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk98yu/simple_and_comprehensive_approach_to_youtube/", "subreddit_subscribers": 658997, "created_utc": 1670874356.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\nI have around 500gb of backup files ranging from 5gb to 150gb. The full backups are the large ones and contained in one large file. \n\nI am wondering what is the best way to upload the files to cloud (using gdrive now). Any kind of tools that can resume upload if connection got disconnected? Using Linux on my server. \n\nAppreciate any suggestions. Thank you.", "author_fullname": "t2_b88a0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upload large files to gdrive dilemma", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkk3ev", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670899655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I have around 500gb of backup files ranging from 5gb to 150gb. The full backups are the large ones and contained in one large file. &lt;/p&gt;\n\n&lt;p&gt;I am wondering what is the best way to upload the files to cloud (using gdrive now). Any kind of tools that can resume upload if connection got disconnected? Using Linux on my server. &lt;/p&gt;\n\n&lt;p&gt;Appreciate any suggestions. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkk3ev", "is_robot_indexable": true, "report_reasons": null, "author": "abubin", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkk3ev/upload_large_files_to_gdrive_dilemma/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkk3ev/upload_large_files_to_gdrive_dilemma/", "subreddit_subscribers": 658997, "created_utc": 1670899655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I generally lurk and rarely posts here.\n\n&amp;#x200B;\n\nI saw the Western Digital sale post during Black Friday for the Red Pro NAS drives so I jumped on the chance. Bought 2, and then realized I needed more so bought 2 more. The first 2 drives came perfectly fine, passed SMART and had no problems with stress tests.  \n\n\nThe second batch had huge problems. One, they delivered only ONE drive instead of the two I ordered. Next, the one drive that did arrive was dead on arrival. SMART wasn't even working when I plugged it in. So I tried RMAing it,  but that requires registering it. So I tried registering the drive and lo and behold, I get   \n\"Sorry! Product registration failed, please try later. (STATCODE108)\"  \n\n\nCustomer support is just running me around the ringer. They keep promising me updates with no updates given after their promised deadline of \"24-48 hours.\" I have gone through both their chat system, garnering me the generic   \n(\"Please allow me to inform you that your issue has already been forwarde to our team and they are looking iinto the issue.\")   \nand calls - what else can I do?  \n\n\nI registered the previous 2 drives just fine. All drives were bought directly from the WD online store.", "author_fullname": "t2_lprcf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Western Digital RMA Help - Red PRO NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zke3o5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670884737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I generally lurk and rarely posts here.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I saw the Western Digital sale post during Black Friday for the Red Pro NAS drives so I jumped on the chance. Bought 2, and then realized I needed more so bought 2 more. The first 2 drives came perfectly fine, passed SMART and had no problems with stress tests.  &lt;/p&gt;\n\n&lt;p&gt;The second batch had huge problems. One, they delivered only ONE drive instead of the two I ordered. Next, the one drive that did arrive was dead on arrival. SMART wasn&amp;#39;t even working when I plugged it in. So I tried RMAing it,  but that requires registering it. So I tried registering the drive and lo and behold, I get&lt;br/&gt;\n&amp;quot;Sorry! Product registration failed, please try later. (STATCODE108)&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;Customer support is just running me around the ringer. They keep promising me updates with no updates given after their promised deadline of &amp;quot;24-48 hours.&amp;quot; I have gone through both their chat system, garnering me the generic&lt;br/&gt;\n(&amp;quot;Please allow me to inform you that your issue has already been forwarde to our team and they are looking iinto the issue.&amp;quot;)&lt;br/&gt;\nand calls - what else can I do?  &lt;/p&gt;\n\n&lt;p&gt;I registered the previous 2 drives just fine. All drives were bought directly from the WD online store.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zke3o5", "is_robot_indexable": true, "report_reasons": null, "author": "RockyX123", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zke3o5/western_digital_rma_help_red_pro_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zke3o5/western_digital_rma_help_red_pro_nas/", "subreddit_subscribers": 658997, "created_utc": 1670884737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://www.amazon.com/dp/B0BGYV6B9V?psc=1\n\nA non-\"renewed\" disk drive at a reasonable price! I'm completely unfamiliar with the company however, are these bad signs? Do you guys have any thoughts on this company or this disk in particular? Any recommendations for me? I've been looking on diskprices.com and I'm kind of hung up on buying new/renewed, more drives, lower storage OR less drives, higher storage each... I'm looking to increase my storage capacity for my main rig PC, it's only got a 1TB NVME and an external USB HDD with 4TB. My goal is to hoard the entire Z-Library archive (23TB) and to maybe host my own cloud storage, but until then I'll just piece it together one at a time until a drive fills up or my PC runs out of space. \ninb4 read the wiki thank you thank you", "author_fullname": "t2_eflrm5nv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WDD (Max Digital Data?) Thoughts on increasing storage for main rig PC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zka5fj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670876336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.amazon.com/dp/B0BGYV6B9V?psc=1\"&gt;https://www.amazon.com/dp/B0BGYV6B9V?psc=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A non-&amp;quot;renewed&amp;quot; disk drive at a reasonable price! I&amp;#39;m completely unfamiliar with the company however, are these bad signs? Do you guys have any thoughts on this company or this disk in particular? Any recommendations for me? I&amp;#39;ve been looking on diskprices.com and I&amp;#39;m kind of hung up on buying new/renewed, more drives, lower storage OR less drives, higher storage each... I&amp;#39;m looking to increase my storage capacity for my main rig PC, it&amp;#39;s only got a 1TB NVME and an external USB HDD with 4TB. My goal is to hoard the entire Z-Library archive (23TB) and to maybe host my own cloud storage, but until then I&amp;#39;ll just piece it together one at a time until a drive fills up or my PC runs out of space. \ninb4 read the wiki thank you thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zka5fj", "is_robot_indexable": true, "report_reasons": null, "author": "Goberoberto", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zka5fj/wdd_max_digital_data_thoughts_on_increasing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zka5fj/wdd_max_digital_data_thoughts_on_increasing/", "subreddit_subscribers": 658997, "created_utc": 1670876336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This pissed me off so much that she wanted so much for something so worthless. I just wanted to see what\u2019s on it you think some guys txt document is worth 20 bucks? Why does it cost so much guys?!!", "author_fullname": "t2_7kwz7oq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I was asked 20 dollars for a 4 megabyte drive at an estate sale\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zkryod", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670925185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This pissed me off so much that she wanted so much for something so worthless. I just wanted to see what\u2019s on it you think some guys txt document is worth 20 bucks? Why does it cost so much guys?!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkryod", "is_robot_indexable": true, "report_reasons": null, "author": "Cantfrickingthink", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkryod/i_was_asked_20_dollars_for_a_4_megabyte_drive_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkryod/i_was_asked_20_dollars_for_a_4_megabyte_drive_at/", "subreddit_subscribers": 658997, "created_utc": 1670925185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nIm on windows, with 2 samba shares that are on different hard drives but same unraid server.\n\nI copy from one to another, does it copy directly on unraid or it goes all the way through my PC and slowed down by network?\n\nCan i just use my Windows 11with SpeedCommander to manage files on unraid or I must use something like Krusader to do it locally?\n\nThanks", "author_fullname": "t2_onbly768", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does Remote Copy/Move works?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkkmyh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670901118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Im on windows, with 2 samba shares that are on different hard drives but same unraid server.&lt;/p&gt;\n\n&lt;p&gt;I copy from one to another, does it copy directly on unraid or it goes all the way through my PC and slowed down by network?&lt;/p&gt;\n\n&lt;p&gt;Can i just use my Windows 11with SpeedCommander to manage files on unraid or I must use something like Krusader to do it locally?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkkmyh", "is_robot_indexable": true, "report_reasons": null, "author": "-Hexenhammer-", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkkmyh/how_does_remote_copymove_works/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkkmyh/how_does_remote_copymove_works/", "subreddit_subscribers": 658997, "created_utc": 1670901118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just backed up my 2TB drive (yes baby numbers I know) to a bucket using copy. I'm new to all this so my question is how do I make it so that next time I need to back up it just uploads new items and not the full 2 TB again? \n\nThanks in advance", "author_fullname": "t2_5rmrxudp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "quick question about Rclone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkkltu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670901035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just backed up my 2TB drive (yes baby numbers I know) to a bucket using copy. I&amp;#39;m new to all this so my question is how do I make it so that next time I need to back up it just uploads new items and not the full 2 TB again? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkkltu", "is_robot_indexable": true, "report_reasons": null, "author": "TroothBeToldPodcast", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkkltu/quick_question_about_rclone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkkltu/quick_question_about_rclone/", "subreddit_subscribers": 658997, "created_utc": 1670901035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I used DVDFab to try and backup my Nick scene it dvd and i noticed it ripped extremely quickly. It even says its a large file like a dvd iso should be. Upon booting the iso file the intro screens worked fine but when i select play game it either crashes the video player, or restarts the iso from the start. \n\nIt seems most of the disc doesn\u2019t copy as it should at all, and i cant find any information online on how to fully rip these interactive dvd games. Any ideas?", "author_fullname": "t2_6j6wnh52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iso backups of \u201cscene it?\u201d Discs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkeoig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670886019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used DVDFab to try and backup my Nick scene it dvd and i noticed it ripped extremely quickly. It even says its a large file like a dvd iso should be. Upon booting the iso file the intro screens worked fine but when i select play game it either crashes the video player, or restarts the iso from the start. &lt;/p&gt;\n\n&lt;p&gt;It seems most of the disc doesn\u2019t copy as it should at all, and i cant find any information online on how to fully rip these interactive dvd games. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkeoig", "is_robot_indexable": true, "report_reasons": null, "author": "Slonkweed", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkeoig/iso_backups_of_scene_it_discs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkeoig/iso_backups_of_scene_it_discs/", "subreddit_subscribers": 658997, "created_utc": 1670886019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If an SSD has been formatted can data still be covered from the drive and if so what free software can i use to try and retrieve the data ?", "author_fullname": "t2_g3pttvkl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data recovery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zka49j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670876261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If an SSD has been formatted can data still be covered from the drive and if so what free software can i use to try and retrieve the data ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zka49j", "is_robot_indexable": true, "report_reasons": null, "author": "CumsockFinder", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zka49j/data_recovery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zka49j/data_recovery/", "subreddit_subscribers": 658997, "created_utc": 1670876261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i've been using a dead discord channel for random occasional file uploads for like a year, and would like to back them up. anybody know how I could extract the direct attachment urls from the entire channel without manual labor?\n\nthank", "author_fullname": "t2_fa63c16m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "quick way to grab direct links for discord uploads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk7un4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670871354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;ve been using a dead discord channel for random occasional file uploads for like a year, and would like to back them up. anybody know how I could extract the direct attachment urls from the entire channel without manual labor?&lt;/p&gt;\n\n&lt;p&gt;thank&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk7un4", "is_robot_indexable": true, "report_reasons": null, "author": "pbdrizz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk7un4/quick_way_to_grab_direct_links_for_discord_uploads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk7un4/quick_way_to_grab_direct_links_for_discord_uploads/", "subreddit_subscribers": 658997, "created_utc": 1670871354.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I just bought a ULTRASTAR DC HC310 4TB to use for a backup of everything that's important to me and I'm looking for advices and tips on how to store it.\n\nI did read different opinions... Someone say to store it powered off in an antistatic bag and wrapped in pluriball. Others instead say that having it powered off corrupt faster the data and also the lubricant on the moving parts dries out so that when you power it on again it will put the drive under extreme friction.\n\nI have not enough knowledge about this to take a decision on my own so I'm here asking for help. \n\nAlso any other advice will be welcome (such as what to choose when formatting it or anything else that I might not know)\n\nThank you to everyone who will help me !", "author_fullname": "t2_14b9rj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need help with my first backup of everything that's important to me", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk7hm7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.47, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670871022.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670870622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just bought a ULTRASTAR DC HC310 4TB to use for a backup of everything that&amp;#39;s important to me and I&amp;#39;m looking for advices and tips on how to store it.&lt;/p&gt;\n\n&lt;p&gt;I did read different opinions... Someone say to store it powered off in an antistatic bag and wrapped in pluriball. Others instead say that having it powered off corrupt faster the data and also the lubricant on the moving parts dries out so that when you power it on again it will put the drive under extreme friction.&lt;/p&gt;\n\n&lt;p&gt;I have not enough knowledge about this to take a decision on my own so I&amp;#39;m here asking for help. &lt;/p&gt;\n\n&lt;p&gt;Also any other advice will be welcome (such as what to choose when formatting it or anything else that I might not know)&lt;/p&gt;\n\n&lt;p&gt;Thank you to everyone who will help me !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk7hm7", "is_robot_indexable": true, "report_reasons": null, "author": "Cris257", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk7hm7/i_need_help_with_my_first_backup_of_everything/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk7hm7/i_need_help_with_my_first_backup_of_everything/", "subreddit_subscribers": 658997, "created_utc": 1670870622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need help understanding sata power connector. My psu rated at 500w has 1 ribbon with 5 sata power connectors on it, and another with 3 molex and 2 sata. Label says 5v at 15a and 12v at 41a.\n\nI'm connecting 2x molex into a hot swap drive bay that houses 3x hdd, and a sata on the same ribbon to another hdd.\n\nThe other ribbon has 2 sata connector attached to a drive bay housing 4x2.5\" (vendor says it still works if I only connect 1 sata connector). Another sata connector is attached to an ssd directly.\n\nQuestion: what is the max power draw my setup can support when fully populated? Assume the 3xhdd are the same make/model and the same for 4x2.5\". 2.5\" are all ssds.", "author_fullname": "t2_ratqygnj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drives per rail...and drive bays", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkn6ps", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670908411.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help understanding sata power connector. My psu rated at 500w has 1 ribbon with 5 sata power connectors on it, and another with 3 molex and 2 sata. Label says 5v at 15a and 12v at 41a.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m connecting 2x molex into a hot swap drive bay that houses 3x hdd, and a sata on the same ribbon to another hdd.&lt;/p&gt;\n\n&lt;p&gt;The other ribbon has 2 sata connector attached to a drive bay housing 4x2.5&amp;quot; (vendor says it still works if I only connect 1 sata connector). Another sata connector is attached to an ssd directly.&lt;/p&gt;\n\n&lt;p&gt;Question: what is the max power draw my setup can support when fully populated? Assume the 3xhdd are the same make/model and the same for 4x2.5&amp;quot;. 2.5&amp;quot; are all ssds.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkn6ps", "is_robot_indexable": true, "report_reasons": null, "author": "lmux", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkn6ps/drives_per_railand_drive_bays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkn6ps/drives_per_railand_drive_bays/", "subreddit_subscribers": 658997, "created_utc": 1670908411.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got an old LaCie d2 Quadra with a missing PSU that I want to salvage the data from. According to the specs of a \"compatible\" (and expensive) PSU on eBay, it requires a 4-pin DIN with Pin 1 (+5V), Pin 2 (+12V), Pin 3 (GND), Pin 4 (GND).\n\n[Would this 5.5mm barrel jack to DIN work?](https://mesg.ebay.co.uk/mesgweb/ViewMessageDetail/0/All/165468186278) It's got the correct polarities, but 12V on Pin 1, not 5V. Normally, I'd say \"no, incompatible\", but I'm unfamiliar with the DIN standard, and got the 5V idea from another 3rd party PSU, so was wondering if anyone had any experience with this and could advise? It would save me having to shell out for an expensive PSU or IDE adapter, and be compatible with my pre-existing pile of 12V DC 5.5mm jack HDD PSUs.", "author_fullname": "t2_4ibusga", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Question] PSU to salvage data from an old LaCie d2 Quadra?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkj86f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670897364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got an old LaCie d2 Quadra with a missing PSU that I want to salvage the data from. According to the specs of a &amp;quot;compatible&amp;quot; (and expensive) PSU on eBay, it requires a 4-pin DIN with Pin 1 (+5V), Pin 2 (+12V), Pin 3 (GND), Pin 4 (GND).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://mesg.ebay.co.uk/mesgweb/ViewMessageDetail/0/All/165468186278\"&gt;Would this 5.5mm barrel jack to DIN work?&lt;/a&gt; It&amp;#39;s got the correct polarities, but 12V on Pin 1, not 5V. Normally, I&amp;#39;d say &amp;quot;no, incompatible&amp;quot;, but I&amp;#39;m unfamiliar with the DIN standard, and got the 5V idea from another 3rd party PSU, so was wondering if anyone had any experience with this and could advise? It would save me having to shell out for an expensive PSU or IDE adapter, and be compatible with my pre-existing pile of 12V DC 5.5mm jack HDD PSUs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkj86f", "is_robot_indexable": true, "report_reasons": null, "author": "Darth_Agnon", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkj86f/question_psu_to_salvage_data_from_an_old_lacie_d2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkj86f/question_psu_to_salvage_data_from_an_old_lacie_d2/", "subreddit_subscribers": 658997, "created_utc": 1670897364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am a moron and have some 15.36tb used enterprise ssd's in U.2 format lying around. What's the best case for them? They get kinda hot so I'd like a fan, replaceable ideal so I can swap it with a moctua\n\nI don't care about throughput that much but I'd like more than 2 drives stored", "author_fullname": "t2_7p2i6djo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best U.2 enclosure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk788r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670870082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a moron and have some 15.36tb used enterprise ssd&amp;#39;s in U.2 format lying around. What&amp;#39;s the best case for them? They get kinda hot so I&amp;#39;d like a fan, replaceable ideal so I can swap it with a moctua&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t care about throughput that much but I&amp;#39;d like more than 2 drives stored&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zk788r", "is_robot_indexable": true, "report_reasons": null, "author": "Spirited-Guidance-91", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk788r/best_u2_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk788r/best_u2_enclosure/", "subreddit_subscribers": 658997, "created_utc": 1670870082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Long time lurker first time posting. Also English is not my first Language so please bear with me. I currently have Synology NAS with 2 bays with 2 4TB hard drives. I want to expand because I'm quickly approaching 7TB. I have my previous desktop computer that has about 8 drive bays that I would like to expand over time. I would like to get some opinions from the experts before I begin throwing time and money at this project.\n\nWhat OS/software should I use to manage/initialize my NAS? -- so far I have heard of TrueNAS, FreeNAS, ZFS and Unraid. Note:not even sure if I know what i am talking about here or if there are differences i am not aware of.\n\nIs expanding my \"pools\" for future additional drives going to be an issue? I don't want to be moving around several TB.\n\nI also read that my drives would have to be matching in size or I won't be able to use the drive to their fullest extent. I was thinking about going to 12-14 TB drives right away but maybe I should by smaller capacity drives to start out? It just seems that in the future my lower capacity drives with hold me back.\n\nThoughts about backups - just looking for some suggestions? I read about snapshots but I'm not sure that applies here. I am familiar with the 3-2-1 rule but with the TB amounts of data I have I can't reasonably buy duplicates of all drives just to have 1 to 1 backups (if that makes sense) but maybe there is no way around this.\n\nI started off my journey with my synology NAS just to see if it would work and now that I have become a little more serious about my data-hoarding I would like a better setup with more storage as well as implement best practices. I see this as the next logical step before I get something that is rack mounted. Let me know all of your thoughts as I am sure I have missed something or have not thought a position through. Thank you for your help!", "author_fullname": "t2_ui7yc7hu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upgrading NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zk6efk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670868275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long time lurker first time posting. Also English is not my first Language so please bear with me. I currently have Synology NAS with 2 bays with 2 4TB hard drives. I want to expand because I&amp;#39;m quickly approaching 7TB. I have my previous desktop computer that has about 8 drive bays that I would like to expand over time. I would like to get some opinions from the experts before I begin throwing time and money at this project.&lt;/p&gt;\n\n&lt;p&gt;What OS/software should I use to manage/initialize my NAS? -- so far I have heard of TrueNAS, FreeNAS, ZFS and Unraid. Note:not even sure if I know what i am talking about here or if there are differences i am not aware of.&lt;/p&gt;\n\n&lt;p&gt;Is expanding my &amp;quot;pools&amp;quot; for future additional drives going to be an issue? I don&amp;#39;t want to be moving around several TB.&lt;/p&gt;\n\n&lt;p&gt;I also read that my drives would have to be matching in size or I won&amp;#39;t be able to use the drive to their fullest extent. I was thinking about going to 12-14 TB drives right away but maybe I should by smaller capacity drives to start out? It just seems that in the future my lower capacity drives with hold me back.&lt;/p&gt;\n\n&lt;p&gt;Thoughts about backups - just looking for some suggestions? I read about snapshots but I&amp;#39;m not sure that applies here. I am familiar with the 3-2-1 rule but with the TB amounts of data I have I can&amp;#39;t reasonably buy duplicates of all drives just to have 1 to 1 backups (if that makes sense) but maybe there is no way around this.&lt;/p&gt;\n\n&lt;p&gt;I started off my journey with my synology NAS just to see if it would work and now that I have become a little more serious about my data-hoarding I would like a better setup with more storage as well as implement best practices. I see this as the next logical step before I get something that is rack mounted. Let me know all of your thoughts as I am sure I have missed something or have not thought a position through. Thank you for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zk6efk", "is_robot_indexable": true, "report_reasons": null, "author": "TheKingMongo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zk6efk/upgrading_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zk6efk/upgrading_nas/", "subreddit_subscribers": 658997, "created_utc": 1670868275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have about 50TB of data I hoarded cuz I have OCD and I document everything I do in several ways, I store everything in many 8TB harddrives but it\u2019s becoming a problem.\n\nThere are way too many already, I\u2019m afraid of drive failures and each one is quite expensive so buying another one for each one for backups is not an option..\n\nAnd Idk about cloud services cuz I want my data to stay with me and not put it in someone else's hands, and I'm not\nlooking for monthly payments..\n\nIs there a better way for me to save everything?\n\nOther devices?\n\nMaybe compress everything somehow?\n\nI\u2019ve been suggested making copies on blueray discs, but they don\u2019t have enough space.\n\nWhat do I do?\n\nThank you!:)", "author_fullname": "t2_1aqkxvx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to store a large amount of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkn59c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670908284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have about 50TB of data I hoarded cuz I have OCD and I document everything I do in several ways, I store everything in many 8TB harddrives but it\u2019s becoming a problem.&lt;/p&gt;\n\n&lt;p&gt;There are way too many already, I\u2019m afraid of drive failures and each one is quite expensive so buying another one for each one for backups is not an option..&lt;/p&gt;\n\n&lt;p&gt;And Idk about cloud services cuz I want my data to stay with me and not put it in someone else&amp;#39;s hands, and I&amp;#39;m not\nlooking for monthly payments..&lt;/p&gt;\n\n&lt;p&gt;Is there a better way for me to save everything?&lt;/p&gt;\n\n&lt;p&gt;Other devices?&lt;/p&gt;\n\n&lt;p&gt;Maybe compress everything somehow?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been suggested making copies on blueray discs, but they don\u2019t have enough space.&lt;/p&gt;\n\n&lt;p&gt;What do I do?&lt;/p&gt;\n\n&lt;p&gt;Thank you!:)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkn59c", "is_robot_indexable": true, "report_reasons": null, "author": "baryaakov555", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkn59c/how_to_store_a_large_amount_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkn59c/how_to_store_a_large_amount_of_data/", "subreddit_subscribers": 658997, "created_utc": 1670908284.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}