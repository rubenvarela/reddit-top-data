{"kind": "Listing", "data": {"after": "t3_zlex78", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://www.tomshardware.com/news/backblaze-expects-one-cent-per-gb-hdds-by-2025](https://www.tomshardware.com/news/backblaze-expects-one-cent-per-gb-hdds-by-2025)\n\n&amp;#x200B;\n\nLet's hope inflation, crypto, wars, and mother nature don't interfere with this prediction.", "author_fullname": "t2_i1nd3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backblaze Expects $0.01 per GB HDs by 2025", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zlc8bz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 295, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 295, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670978094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.tomshardware.com/news/backblaze-expects-one-cent-per-gb-hdds-by-2025\"&gt;https://www.tomshardware.com/news/backblaze-expects-one-cent-per-gb-hdds-by-2025&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s hope inflation, crypto, wars, and mother nature don&amp;#39;t interfere with this prediction.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BrJKiUC6_NdpNa7rDiEX0UOCCOyac2z4fxB0ZliO5ko.jpg?auto=webp&amp;v=enabled&amp;s=a581ae7e1f8711a67dbed39fec91a03b26d7feca", "width": 1200, "height": 677}, "resolutions": [{"url": "https://external-preview.redd.it/BrJKiUC6_NdpNa7rDiEX0UOCCOyac2z4fxB0ZliO5ko.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=508107b95bbd650277f7e0f371f049fc2768212e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/BrJKiUC6_NdpNa7rDiEX0UOCCOyac2z4fxB0ZliO5ko.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cca146430610e0028291d778788daa8902fac3c8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/BrJKiUC6_NdpNa7rDiEX0UOCCOyac2z4fxB0ZliO5ko.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e5af6df105bbfdc8f41b4274f80f9f9e80ee5d4b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/BrJKiUC6_NdpNa7rDiEX0UOCCOyac2z4fxB0ZliO5ko.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f573c8263cd918980ab89a1f1b6d0031614b9d7c", "width": 640, "height": 361}, {"url": "https://external-preview.redd.it/BrJKiUC6_NdpNa7rDiEX0UOCCOyac2z4fxB0ZliO5ko.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4003777274fdc895d374869c768f29e71630d651", "width": 960, "height": 541}, {"url": "https://external-preview.redd.it/BrJKiUC6_NdpNa7rDiEX0UOCCOyac2z4fxB0ZliO5ko.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4f080c9ff41581dd5f0ac09ce8c0e038f348c71", "width": 1080, "height": 609}], "variants": {}, "id": "rAR_o-x11H5fKuMMdCsuwT568-KDIBh141380hjhT9U"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlc8bz", "is_robot_indexable": true, "report_reasons": null, "author": "wbs3333", "discussion_type": null, "num_comments": 99, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlc8bz/backblaze_expects_001_per_gb_hds_by_2025/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlc8bz/backblaze_expects_001_per_gb_hds_by_2025/", "subreddit_subscribers": 659172, "created_utc": 1670978094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What type of power cable is this?  Thanks.", "author_fullname": "t2_21fbzay8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Need help identifying the right power cable to buy for a long lost hard drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"wt26ppnjlp5a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/wt26ppnjlp5a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54835895ee94f4b6294df44cdb45d818864a8883"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/wt26ppnjlp5a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dae344e2f11b90cdb748a8193c18d8f289fe5658"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/wt26ppnjlp5a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bcce42bc7d81be0f5e279628ec7688953908da5d"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/wt26ppnjlp5a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc481cf66dba3a20d49a81b8399263ba72ebb70d"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/wt26ppnjlp5a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5687418f2a9488bad4fc96ea4a42e1e560dcfe4c"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/wt26ppnjlp5a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a0961cd7e6777f3e5e45e6caa69927a743ff1e0"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/wt26ppnjlp5a1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=2fac85227ca5429d227c92e1d20ec04530ad3054"}, "id": "wt26ppnjlp5a1"}, "nn7j3pnjlp5a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/nn7j3pnjlp5a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07edf98458aeba2e9d9f34c901efc07dbd75753b"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/nn7j3pnjlp5a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=329dde1cb8a211c26d0bb520c00070d6c1ac6265"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/nn7j3pnjlp5a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=391948e0bb3f1637ad9efdb6ecfcdcddc5179c4f"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/nn7j3pnjlp5a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ed677b706f4aed208c5e0756898f6ddb183c8e0"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/nn7j3pnjlp5a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eddb174155dc639cd3caa355dd3a8306227983e1"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/nn7j3pnjlp5a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c45b7ebaa3004d963baa3400526eea1c0801b933"}], "s": {"y": 2180, "x": 1635, "u": "https://preview.redd.it/nn7j3pnjlp5a1.jpg?width=1635&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=07c4773db351c0bfd539d35106d1e0b392da706f"}, "id": "nn7j3pnjlp5a1"}}, "name": "t3_zl31gw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 165, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "wt26ppnjlp5a1", "id": 218868773}, {"media_id": "nn7j3pnjlp5a1", "id": 218868774}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 165, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/kBnojedM5Fy-9Z4DvBxIXAqtHm70-cjSB18Da0Mlol8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670956330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What type of power cable is this?  Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zl31gw", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zl31gw", "is_robot_indexable": true, "report_reasons": null, "author": "chufenschmirtz", "discussion_type": null, "num_comments": 72, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zl31gw/need_help_identifying_the_right_power_cable_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zl31gw", "subreddit_subscribers": 659172, "created_utc": 1670956330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_aiim8w8f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Read Error Rate is marked as Red, and i've been facing difficulty to run operations on this HDD. Any suggestions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_zkz0qj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YsZdl0E0uqareUBhMbhZQK3qyWU0hMDh7GDT6catmwc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670946951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/7pgy7i0bto5a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/7pgy7i0bto5a1.png?auto=webp&amp;v=enabled&amp;s=904a1acdc2465afe328ed96b50e8d0bdbc0f90ce", "width": 684, "height": 415}, "resolutions": [{"url": "https://preview.redd.it/7pgy7i0bto5a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=538b6081444bbaa5133237db8136f2b32d9fd63f", "width": 108, "height": 65}, {"url": "https://preview.redd.it/7pgy7i0bto5a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e138b4416ce8e90140db527e162b6b2af19b0de", "width": 216, "height": 131}, {"url": "https://preview.redd.it/7pgy7i0bto5a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=567a36e3ed09140a8ba82f7b123ca6d53d3dbf6d", "width": 320, "height": 194}, {"url": "https://preview.redd.it/7pgy7i0bto5a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90e62ad1304659d75da8521c579eb9788847c506", "width": 640, "height": 388}], "variants": {}, "id": "v9gE7Uaj3jwJyz0ftC9NArHwZG6d3nxB28-ZmYQSbAw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkz0qj", "is_robot_indexable": true, "report_reasons": null, "author": "V0idH3art", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkz0qj/read_error_rate_is_marked_as_red_and_ive_been/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/7pgy7i0bto5a1.png", "subreddit_subscribers": 659172, "created_utc": 1670946951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just wondering if anyone else is using this method. I just got my Pixel 1 in from ebay.\n\nI plan to keep and host all family videos / photos on my server, and have backups on and off site of that server.  \n\nPlanning on using something like syncthing to get pics from our phones to the pixel, and then from the pixel to our \"family\" google account.\n\nQuestions:\n\n1. What is the best way to get pictures from our phones (android and iphone) to my server? Syncthing? Some other app im not thinking of?\n\n2. Is it really unlimited full res for the life of the phone? I used the \"reduced quality\" for a long time and was never disappointed with the quality of the pictures saved - should I just use that to not piss google off?\n\n3. Any other tips to get this to work flawlessly? (auto delete from phone after it uploads to google, etc)", "author_fullname": "t2_6q742al1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using a Pixel 1 for another backup location of pictures/videos - advice needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zl2tsh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670955829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wondering if anyone else is using this method. I just got my Pixel 1 in from ebay.&lt;/p&gt;\n\n&lt;p&gt;I plan to keep and host all family videos / photos on my server, and have backups on and off site of that server.  &lt;/p&gt;\n\n&lt;p&gt;Planning on using something like syncthing to get pics from our phones to the pixel, and then from the pixel to our &amp;quot;family&amp;quot; google account.&lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;What is the best way to get pictures from our phones (android and iphone) to my server? Syncthing? Some other app im not thinking of?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is it really unlimited full res for the life of the phone? I used the &amp;quot;reduced quality&amp;quot; for a long time and was never disappointed with the quality of the pictures saved - should I just use that to not piss google off?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Any other tips to get this to work flawlessly? (auto delete from phone after it uploads to google, etc)&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zl2tsh", "is_robot_indexable": true, "report_reasons": null, "author": "bringo24", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zl2tsh/using_a_pixel_1_for_another_backup_location_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zl2tsh/using_a_pixel_1_for_another_backup_location_of/", "subreddit_subscribers": 659172, "created_utc": 1670955829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I asked this in /r/webdev and they suggested you folks could help me more:\n\nI have a niche website that currently has about a dozen TB of storage. As the website has started to grow, the data has started to become an issue....the hard drive I use to hold the files is no longer going to be able to hold it all. I'm estimating the storage will be 25-50TB, with about 50-100GB in daily bandwidth, at a minimum. So I need solutions with unlimited traffic.\n\nI am cheap, so using cloud providers like Azure, AWS is not really an option. I have noticed that Hetzner has cheap servers, with large hard drives (and empty slots for future growth). The issue I now see is the practical question of...how do I merge the hard drives while maintaining some level of protection against disk failure? If I have 3 HDDs pooled in RAID-0, and then 1 disk fails, well I'm screwed... At this point I'm thinking perhaps the only solution is just getting a server with double the space I need, then use RAID-5. However, I wanted to ask around to see if there is a better idea that I hadn't considered?\n\nI have a 3Gbps parallel connection at home (dynamic IP), so I pondered the idea of buying a used server and hosting it in my house and then just have a cheap compute server hosted in a data centre, so whenever the user wanted the data, the compute server would pull from my server, and the data centre server would serve the user. My issue there is complexity in coding this, and managing this.\n\nOr I just buy a used server and host it entirely from my home. Just one server to do it all, compute and storage. I'm not sure about the dynamic IP. I guess I could use something like a Dynamic DNS service, and then point it to cloudflare to give me extra protection. I guess that could work?\n\nI should clarify that I am asking ways to host a website with a large data set for cheap, in a relatively safe manner. I'll perform backups, etc. routinely so that isn't the main issue at hand. I really just need to figure out how to server a website with a 50TB data set (not all at one go) to users for cheap. \n\nI'm planning on doing a full re-write of the website, so feel free to suggest any idea. I'm all ears to suggestions. If I'm in the wrong sub, please let me know what sub is better to ask this in. Thank you!", "author_fullname": "t2_glyyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hosting a website with dozens of terabytes of storage. how to keep the data safe for cheap. how to pool the multiple disks, or other options available?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zl8v24", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670970260.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670969963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I asked this in &lt;a href=\"/r/webdev\"&gt;/r/webdev&lt;/a&gt; and they suggested you folks could help me more:&lt;/p&gt;\n\n&lt;p&gt;I have a niche website that currently has about a dozen TB of storage. As the website has started to grow, the data has started to become an issue....the hard drive I use to hold the files is no longer going to be able to hold it all. I&amp;#39;m estimating the storage will be 25-50TB, with about 50-100GB in daily bandwidth, at a minimum. So I need solutions with unlimited traffic.&lt;/p&gt;\n\n&lt;p&gt;I am cheap, so using cloud providers like Azure, AWS is not really an option. I have noticed that Hetzner has cheap servers, with large hard drives (and empty slots for future growth). The issue I now see is the practical question of...how do I merge the hard drives while maintaining some level of protection against disk failure? If I have 3 HDDs pooled in RAID-0, and then 1 disk fails, well I&amp;#39;m screwed... At this point I&amp;#39;m thinking perhaps the only solution is just getting a server with double the space I need, then use RAID-5. However, I wanted to ask around to see if there is a better idea that I hadn&amp;#39;t considered?&lt;/p&gt;\n\n&lt;p&gt;I have a 3Gbps parallel connection at home (dynamic IP), so I pondered the idea of buying a used server and hosting it in my house and then just have a cheap compute server hosted in a data centre, so whenever the user wanted the data, the compute server would pull from my server, and the data centre server would serve the user. My issue there is complexity in coding this, and managing this.&lt;/p&gt;\n\n&lt;p&gt;Or I just buy a used server and host it entirely from my home. Just one server to do it all, compute and storage. I&amp;#39;m not sure about the dynamic IP. I guess I could use something like a Dynamic DNS service, and then point it to cloudflare to give me extra protection. I guess that could work?&lt;/p&gt;\n\n&lt;p&gt;I should clarify that I am asking ways to host a website with a large data set for cheap, in a relatively safe manner. I&amp;#39;ll perform backups, etc. routinely so that isn&amp;#39;t the main issue at hand. I really just need to figure out how to server a website with a 50TB data set (not all at one go) to users for cheap. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning on doing a full re-write of the website, so feel free to suggest any idea. I&amp;#39;m all ears to suggestions. If I&amp;#39;m in the wrong sub, please let me know what sub is better to ask this in. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zl8v24", "is_robot_indexable": true, "report_reasons": null, "author": "SkittleMeWonder", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zl8v24/hosting_a_website_with_dozens_of_terabytes_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zl8v24/hosting_a_website_with_dozens_of_terabytes_of/", "subreddit_subscribers": 659172, "created_utc": 1670969963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey. So I just moved into a new place and I'm close enough to some railroad tracks that the entire house vibrates occasionally when a train goes by. How is this going to affect the two 18TB platter drives in my server?\n\nThanks.", "author_fullname": "t2_2xo0f1ss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trains and hard drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zlt2ew", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671029731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey. So I just moved into a new place and I&amp;#39;m close enough to some railroad tracks that the entire house vibrates occasionally when a train goes by. How is this going to affect the two 18TB platter drives in my server?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlt2ew", "is_robot_indexable": true, "report_reasons": null, "author": "Huecuva", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlt2ew/trains_and_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlt2ew/trains_and_hard_drives/", "subreddit_subscribers": 659172, "created_utc": 1671029731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Everything is up to date and authorized, cookies and cache has been cleared, etc. \n\nWe get this error - https://i.imgur.com/KVZEwfW.png when getting a .acsm from archive.org and trying to download/view via Adobe Digital Editions. \n\nAnyone else having this issue?", "author_fullname": "t2_g17q6hnm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone else having issues downloading from archive.org? Since this morning, we've been getting an error in Adobe Digital Editions that prevents download.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zkz6py", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670947409.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everything is up to date and authorized, cookies and cache has been cleared, etc. &lt;/p&gt;\n\n&lt;p&gt;We get this error - &lt;a href=\"https://i.imgur.com/KVZEwfW.png\"&gt;https://i.imgur.com/KVZEwfW.png&lt;/a&gt; when getting a .acsm from archive.org and trying to download/view via Adobe Digital Editions. &lt;/p&gt;\n\n&lt;p&gt;Anyone else having this issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/r_T9gs_zFLqBSGoM1rz1nvBsfEYOeONQfkG-8f2IRDQ.png?auto=webp&amp;v=enabled&amp;s=34dea8060d311f4ec5a5de86fb78b28af8af2864", "width": 439, "height": 158}, "resolutions": [{"url": "https://external-preview.redd.it/r_T9gs_zFLqBSGoM1rz1nvBsfEYOeONQfkG-8f2IRDQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2381c24b387df99576271f8e779264c37ae1808", "width": 108, "height": 38}, {"url": "https://external-preview.redd.it/r_T9gs_zFLqBSGoM1rz1nvBsfEYOeONQfkG-8f2IRDQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=590da8e893a2a78e83a293c35cb2fcd07221d411", "width": 216, "height": 77}, {"url": "https://external-preview.redd.it/r_T9gs_zFLqBSGoM1rz1nvBsfEYOeONQfkG-8f2IRDQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b930e711dd5bbe5698f140d1760327ec1ac2be79", "width": 320, "height": 115}], "variants": {}, "id": "LOiRBrtwQOnOJT2_d6nj7phF7MYmBiDgvt0Xr-IR7iE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zkz6py", "is_robot_indexable": true, "report_reasons": null, "author": "QuestPirate", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zkz6py/is_anyone_else_having_issues_downloading_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zkz6py/is_anyone_else_having_issues_downloading_from/", "subreddit_subscribers": 659172, "created_utc": 1670947409.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_7gg7qbe8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apologies for any mistakes, im quite new to this, my power went out and when my pc came back on I noticed my hard drive was a lot louder so I checked Crystal Disk Info and saw this, is this something to be concerned about and I should start backing up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zlc3qs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JtZjWOnD0jLPlfJEOU7RSr_1PFeiKK3q1jp16Vynvjw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670977772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/evvlk97adr5a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/evvlk97adr5a1.png?auto=webp&amp;v=enabled&amp;s=fcebe0cb66dc734320d6c64c73d2ba15d5f1e6d3", "width": 669, "height": 689}, "resolutions": [{"url": "https://preview.redd.it/evvlk97adr5a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=863091528ebe6e504468b4bcfeec184b53653319", "width": 108, "height": 111}, {"url": "https://preview.redd.it/evvlk97adr5a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d623df68e611f9ea0e2137892f055b4a80f71256", "width": 216, "height": 222}, {"url": "https://preview.redd.it/evvlk97adr5a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e9806ab959a2e099af6f0ad810657a7d9a33dd45", "width": 320, "height": 329}, {"url": "https://preview.redd.it/evvlk97adr5a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a62326836b8bc1f3ea34cc2d63a3d5fa1187e3d", "width": 640, "height": 659}], "variants": {}, "id": "nDaDkGV7vUxV2dfXTj548-SUZvNrbJ4ObkP7i2kgYVA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlc3qs", "is_robot_indexable": true, "report_reasons": null, "author": "Riley79", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlc3qs/apologies_for_any_mistakes_im_quite_new_to_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/evvlk97adr5a1.png", "subreddit_subscribers": 659172, "created_utc": 1670977772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Intel is discontinuing Optane memory, and there are some good deals to be had on smaller Optane M.2. modules.    I saw a YouTube video that says Optane modules are great for ZFS metadata storage.\n\nI don't have a ZFS server, but plan to build a modest one sometime in the future - so should I grab some Optane modules now before they are gone for good?\n\nBackground: Optane solid state memory is great for write caching because unlike standard SSD storage it really won't wear out in any reasonable amount of time.", "author_fullname": "t2_9gvsknn6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optane memory - buy now for a later ZFS build?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zl8c02", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670968769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Intel is discontinuing Optane memory, and there are some good deals to be had on smaller Optane M.2. modules.    I saw a YouTube video that says Optane modules are great for ZFS metadata storage.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have a ZFS server, but plan to build a modest one sometime in the future - so should I grab some Optane modules now before they are gone for good?&lt;/p&gt;\n\n&lt;p&gt;Background: Optane solid state memory is great for write caching because unlike standard SSD storage it really won&amp;#39;t wear out in any reasonable amount of time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zl8c02", "is_robot_indexable": true, "report_reasons": null, "author": "CarlGustav2", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zl8c02/optane_memory_buy_now_for_a_later_zfs_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zl8c02/optane_memory_buy_now_for_a_later_zfs_build/", "subreddit_subscribers": 659172, "created_utc": 1670968769.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,  \nI recently asked about downloading iCloud photos - You can find my post here.\n\nAs I am now using gimme-iphoto for regularly downloading all of my photos, I found that there's certain metadata missing from certain files... What I found missing:\n\n* **Any** geolocation data - missing from any photo or video\n* **Date taken** \\- missing from any screenshot or imported picture\n\nI have also tried using a Mac and exporting the photos (by hand), but this information is also missing, whereas if I look at the photos' info boxes, even screenshots have dates attached to them.\n\nIs there any solution to download / export the files with this missing data?", "author_fullname": "t2_siud3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download all of my iCloud photos with metadata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zl65th", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670963717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;br/&gt;\nI recently asked about downloading iCloud photos - You can find my post here.&lt;/p&gt;\n\n&lt;p&gt;As I am now using gimme-iphoto for regularly downloading all of my photos, I found that there&amp;#39;s certain metadata missing from certain files... What I found missing:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Any&lt;/strong&gt; geolocation data - missing from any photo or video&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Date taken&lt;/strong&gt; - missing from any screenshot or imported picture&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have also tried using a Mac and exporting the photos (by hand), but this information is also missing, whereas if I look at the photos&amp;#39; info boxes, even screenshots have dates attached to them.&lt;/p&gt;\n\n&lt;p&gt;Is there any solution to download / export the files with this missing data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zl65th", "is_robot_indexable": true, "report_reasons": null, "author": "KRider92", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zl65th/download_all_of_my_icloud_photos_with_metadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zl65th/download_all_of_my_icloud_photos_with_metadata/", "subreddit_subscribers": 659172, "created_utc": 1670963717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am interested in building my first file server. Which OS would you suggest for a semi tech savvy user? So far I know about TrueNAS Core and Unraid. Looking for easiest to install and manage.", "author_fullname": "t2_usun3qeu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OS Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zl64s0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670963651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am interested in building my first file server. Which OS would you suggest for a semi tech savvy user? So far I know about TrueNAS Core and Unraid. Looking for easiest to install and manage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zl64s0", "is_robot_indexable": true, "report_reasons": null, "author": "PuzzleheadedTennis23", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zl64s0/os_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zl64s0/os_suggestions/", "subreddit_subscribers": 659172, "created_utc": 1670963651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4grpu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Since 2018, the total volume of data across the world has nearly doubled in size every two yrs; it currently sits at ~94T gigabytes and is projected to reach 175T by 2025", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 68, "top_awarded_type": null, "hide_score": true, "name": "t3_zlrw0l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vzA3YoVza5B49kKGBL5jLiD0CczqLWwVHpjFTnp73hE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671026927.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "theconversation.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://theconversation.com/the-worlds-data-explained-how-much-were-producing-and-where-its-all-stored-159964", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UyPlvt-4ueMd5L2HwRHfpzK_7wzIDZHZzt6HY_ELgnQ.jpg?auto=webp&amp;v=enabled&amp;s=877db3e109f769e922c2cec01a73cbf24c97f6ad", "width": 1356, "height": 668}, "resolutions": [{"url": "https://external-preview.redd.it/UyPlvt-4ueMd5L2HwRHfpzK_7wzIDZHZzt6HY_ELgnQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=872108a8e1600b3b8297d7dce2f77ea376fb4a61", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/UyPlvt-4ueMd5L2HwRHfpzK_7wzIDZHZzt6HY_ELgnQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2eb4e566c1563e27a995a4797a4559a84183ac3f", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/UyPlvt-4ueMd5L2HwRHfpzK_7wzIDZHZzt6HY_ELgnQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95c42e73f3e90c59d95aadc0ed93983c2b8f8cf5", "width": 320, "height": 157}, {"url": "https://external-preview.redd.it/UyPlvt-4ueMd5L2HwRHfpzK_7wzIDZHZzt6HY_ELgnQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5fe6862a3fc73be575d7ad6fe68c7c7ea1663ec0", "width": 640, "height": 315}, {"url": "https://external-preview.redd.it/UyPlvt-4ueMd5L2HwRHfpzK_7wzIDZHZzt6HY_ELgnQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bce52a8caa8bd15d61aa934610737b23845378aa", "width": 960, "height": 472}, {"url": "https://external-preview.redd.it/UyPlvt-4ueMd5L2HwRHfpzK_7wzIDZHZzt6HY_ELgnQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e321d2ef0bae545987db3fae5a7f22a104fbc1eb", "width": 1080, "height": 532}], "variants": {}, "id": "kJOn4GKKX5wBwR9-7aPYQd9SoqZAZrw7FRI7tU3qYlA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlrw0l", "is_robot_indexable": true, "report_reasons": null, "author": "yourdp", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlrw0l/since_2018_the_total_volume_of_data_across_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://theconversation.com/the-worlds-data-explained-how-much-were-producing-and-where-its-all-stored-159964", "subreddit_subscribers": 659172, "created_utc": 1671026927.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, im not an expert on anything so feel free to tell me if any of this is a dumb idea.  \nI work mostly from a laptop and i work off external storage. My local server is sorted but sometimes i take my projects on the go with an external ssd.  \n\n\n**The important part:** would it be stupid (or suboptimal) to buy a usb-c SSD enclosure and just swap them? I tend to access only one project at a time and I feel safer with them beings stored cold. Plus upgradability would be easy and I can just buy more storage when I need it. Can something happen to the data when its stored \"in a box\", would you buy a m.2 enclosure or 2.5. Are there big differences in enclosures themselves?  \n\n\nThanks in advance", "author_fullname": "t2_11fewf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice on SSD enclosures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zlru01", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671026784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, im not an expert on anything so feel free to tell me if any of this is a dumb idea.&lt;br/&gt;\nI work mostly from a laptop and i work off external storage. My local server is sorted but sometimes i take my projects on the go with an external ssd.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The important part:&lt;/strong&gt; would it be stupid (or suboptimal) to buy a usb-c SSD enclosure and just swap them? I tend to access only one project at a time and I feel safer with them beings stored cold. Plus upgradability would be easy and I can just buy more storage when I need it. Can something happen to the data when its stored &amp;quot;in a box&amp;quot;, would you buy a m.2 enclosure or 2.5. Are there big differences in enclosures themselves?  &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlru01", "is_robot_indexable": true, "report_reasons": null, "author": "vesko26", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlru01/looking_for_advice_on_ssd_enclosures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlru01/looking_for_advice_on_ssd_enclosures/", "subreddit_subscribers": 659172, "created_utc": 1671026784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I plan to buy a new set of drives for my future NAS.\n\nNow high capacity drives vary in prices and here is what I can have:\n\nSeagate Exos X20 - 20TB - 369$\n\nSeagate IronWolfPro - 20TB - 500$\n\nWD Red Pro - 20TB - 539$\n\nWD Gold - 20TB - 499  \n\n\nI don't really know the differences between all those drives what would you recommend me ?  \nThanks !", "author_fullname": "t2_1gnbd4qq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which HDD for NAS (between those listed below) ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zlqt3k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671024124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I plan to buy a new set of drives for my future NAS.&lt;/p&gt;\n\n&lt;p&gt;Now high capacity drives vary in prices and here is what I can have:&lt;/p&gt;\n\n&lt;p&gt;Seagate Exos X20 - 20TB - 369$&lt;/p&gt;\n\n&lt;p&gt;Seagate IronWolfPro - 20TB - 500$&lt;/p&gt;\n\n&lt;p&gt;WD Red Pro - 20TB - 539$&lt;/p&gt;\n\n&lt;p&gt;WD Gold - 20TB - 499  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t really know the differences between all those drives what would you recommend me ?&lt;br/&gt;\nThanks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlqt3k", "is_robot_indexable": true, "report_reasons": null, "author": "SirSirae", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlqt3k/which_hdd_for_nas_between_those_listed_below/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlqt3k/which_hdd_for_nas_between_those_listed_below/", "subreddit_subscribers": 659172, "created_utc": 1671024124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I have about 29 TB of data from over the years that i would like to transfer from my drobo to my new drivepool/snapraid setup. I have a lot of family photos, family videos, documents, school work, data files from all my previous machines (not OS files just data) and so many other things including Linux iso's. I have the drobo connected via usb c to usb 3 A connector, while the new drives are inside the same pc the drobo is connected too. I am copying all the data from drobo to the internal drives goes about 30 mb/s, i tried teracopy but took one day for 3 TB. Any smarter ways to transfer the data between the two drives? Maybe usb c to usb c cable to make it go 100 mb/s (which is the fastest it goes, that i've seen from the drobo). Any advice would be greatly appreciated!", "author_fullname": "t2_3hr19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transfer data from drobo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zlizw3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670997064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have about 29 TB of data from over the years that i would like to transfer from my drobo to my new drivepool/snapraid setup. I have a lot of family photos, family videos, documents, school work, data files from all my previous machines (not OS files just data) and so many other things including Linux iso&amp;#39;s. I have the drobo connected via usb c to usb 3 A connector, while the new drives are inside the same pc the drobo is connected too. I am copying all the data from drobo to the internal drives goes about 30 mb/s, i tried teracopy but took one day for 3 TB. Any smarter ways to transfer the data between the two drives? Maybe usb c to usb c cable to make it go 100 mb/s (which is the fastest it goes, that i&amp;#39;ve seen from the drobo). Any advice would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlizw3", "is_robot_indexable": true, "report_reasons": null, "author": "xelu01", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlizw3/transfer_data_from_drobo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlizw3/transfer_data_from_drobo/", "subreddit_subscribers": 659172, "created_utc": 1670997064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I work for a small business that uses MyCloud for our in-office network; can anyone suggest an alternative for our network that also allows remote access? I'm not especially good with computers or anything like that, so I'm hoping for something that will be relatively easy to implement and that doesn't require me to set up a VPN or anything. Hope this is the right subreddit!", "author_fullname": "t2_13sug0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MyCloud Alternative - remote access?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zlaq5d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670974320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a small business that uses MyCloud for our in-office network; can anyone suggest an alternative for our network that also allows remote access? I&amp;#39;m not especially good with computers or anything like that, so I&amp;#39;m hoping for something that will be relatively easy to implement and that doesn&amp;#39;t require me to set up a VPN or anything. Hope this is the right subreddit!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlaq5d", "is_robot_indexable": true, "report_reasons": null, "author": "normopathy", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlaq5d/mycloud_alternative_remote_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlaq5d/mycloud_alternative_remote_access/", "subreddit_subscribers": 659172, "created_utc": 1670974320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I work for a small company that uses MyCloud, which doesn't really have any options for remote access. Can anyone suggest a better option for an in-office network, that can also be accessed remotely? I don't really know anything about IT/computers, so I'm hoping for something that is relatively simple to set up, without having to create a VPN or anything like that. Thanks!", "author_fullname": "t2_uykrwwro", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to MyCloud - remote access?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zlak2f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670973923.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a small company that uses MyCloud, which doesn&amp;#39;t really have any options for remote access. Can anyone suggest a better option for an in-office network, that can also be accessed remotely? I don&amp;#39;t really know anything about IT/computers, so I&amp;#39;m hoping for something that is relatively simple to set up, without having to create a VPN or anything like that. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlak2f", "is_robot_indexable": true, "report_reasons": null, "author": "BluebirdDry1173", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlak2f/alternative_to_mycloud_remote_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlak2f/alternative_to_mycloud_remote_access/", "subreddit_subscribers": 659172, "created_utc": 1670973923.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My ssd doesn't delete files that contain  sensitive info properly, because I can just use recuva to restore the file\n\nWhat I'd love to be able to do is use some kind of tool or process to be able to delete these files contents, but keep the actual file there as a placeholder, kinda like a corrupt file\n\nThat way, when I do delete the   files, after they've already been corrupted\n\n if recuva tries to recover the files, it will just be placeholder filed without any useful data contained in the file itself , \n\nIs there any software that would let you do this?", "author_fullname": "t2_c3mfpp7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any software to corrupt files so that they can't be used? Not actually deleting them, just corrupting them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zlsopl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671028827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My ssd doesn&amp;#39;t delete files that contain  sensitive info properly, because I can just use recuva to restore the file&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;d love to be able to do is use some kind of tool or process to be able to delete these files contents, but keep the actual file there as a placeholder, kinda like a corrupt file&lt;/p&gt;\n\n&lt;p&gt;That way, when I do delete the   files, after they&amp;#39;ve already been corrupted&lt;/p&gt;\n\n&lt;p&gt;if recuva tries to recover the files, it will just be placeholder filed without any useful data contained in the file itself , &lt;/p&gt;\n\n&lt;p&gt;Is there any software that would let you do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zlsopl", "is_robot_indexable": true, "report_reasons": null, "author": "computerstuffs", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlsopl/any_software_to_corrupt_files_so_that_they_cant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlsopl/any_software_to_corrupt_files_so_that_they_cant/", "subreddit_subscribers": 659172, "created_utc": 1671028827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://www.coursera.org/learn/html-css-javascript-for-web-developers\n\nI'm enrolled in the course. What's the way to download it? I'm not understanding syntax to download it. Please help.\n\nhttps://github.com/coursera-dl/coursera-dl\n\n\nI tried this command:\n\ncoursera-dl -u email -p password html-css-javascript-for-web-developers\n\nBut it's throwing some errors.\n\n\n    \n    \n    Traceback (most recent call last):\n      File \"C:\\Users\\inspiron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\coursera\\cookies.py\", line 148, in login\n        r.raise_for_status()\n      File \"C:\\Users\\inspiron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\models.py\", line 1021, in raise_for_status\n        raise HTTPError(http_error_msg, response=self)\n    requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://api.coursera.org/api/login/v3\n    \n    During handling of the above exception, another exception occurred:\n    \n    Traceback (most recent call last):\n      File \"&lt;frozen runpy&gt;\", line 198, in _run_module_as_main\n      File \"&lt;frozen runpy&gt;\", line 88, in _run_code\n      File \"C:\\Users\\inspiron\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\coursera-dl.exe\\__main__.py\", line 7, in &lt;module&gt;\n      File \"C:\\Users\\inspiron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\coursera\\coursera_dl.py\", line 239, in main\n        login(session, args.username, args.password)\n      File \"C:\\Users\\inspiron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\coursera\\cookies.py\", line 155, in login\n        raise AuthenticationFailed('Cannot login on coursera.org: %s' % e)\n    coursera.cookies.AuthenticationFailed: Cannot login on coursera.org: 400 Client Error: Bad Request for url: https://api.coursera.org/api/login/v3", "author_fullname": "t2_sng65k2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've coursera-dl installed how do I download this course?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zlqib3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671023293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.coursera.org/learn/html-css-javascript-for-web-developers\"&gt;https://www.coursera.org/learn/html-css-javascript-for-web-developers&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m enrolled in the course. What&amp;#39;s the way to download it? I&amp;#39;m not understanding syntax to download it. Please help.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/coursera-dl/coursera-dl\"&gt;https://github.com/coursera-dl/coursera-dl&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I tried this command:&lt;/p&gt;\n\n&lt;p&gt;coursera-dl -u email -p password html-css-javascript-for-web-developers&lt;/p&gt;\n\n&lt;p&gt;But it&amp;#39;s throwing some errors.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):\n  File &amp;quot;C:\\Users\\inspiron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\coursera\\cookies.py&amp;quot;, line 148, in login\n    r.raise_for_status()\n  File &amp;quot;C:\\Users\\inspiron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\models.py&amp;quot;, line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://api.coursera.org/api/login/v3\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\n  File &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\n  File &amp;quot;C:\\Users\\inspiron\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\coursera-dl.exe\\__main__.py&amp;quot;, line 7, in &amp;lt;module&amp;gt;\n  File &amp;quot;C:\\Users\\inspiron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\coursera\\coursera_dl.py&amp;quot;, line 239, in main\n    login(session, args.username, args.password)\n  File &amp;quot;C:\\Users\\inspiron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\coursera\\cookies.py&amp;quot;, line 155, in login\n    raise AuthenticationFailed(&amp;#39;Cannot login on coursera.org: %s&amp;#39; % e)\ncoursera.cookies.AuthenticationFailed: Cannot login on coursera.org: 400 Client Error: Bad Request for url: https://api.coursera.org/api/login/v3\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q_mKpqIeyxzTruB6EUKnfYt7vR1ZTdQVKPC0lp52hx4.jpg?auto=webp&amp;v=enabled&amp;s=77667bfd0c7830caf17018574f345ca2104f4c07", "width": 1772, "height": 928}, "resolutions": [{"url": "https://external-preview.redd.it/q_mKpqIeyxzTruB6EUKnfYt7vR1ZTdQVKPC0lp52hx4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14ed26129ba49c6b1d93f7401e3af4f0509a4ea3", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/q_mKpqIeyxzTruB6EUKnfYt7vR1ZTdQVKPC0lp52hx4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc930359415b2ef7aeab37386065c1f71c9383d4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/q_mKpqIeyxzTruB6EUKnfYt7vR1ZTdQVKPC0lp52hx4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de66095f0ccb986225032a1fd7cf36fccd6cfd8a", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/q_mKpqIeyxzTruB6EUKnfYt7vR1ZTdQVKPC0lp52hx4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd9b58665b8fd23700c37aa3a11e5ac1ec73701f", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/q_mKpqIeyxzTruB6EUKnfYt7vR1ZTdQVKPC0lp52hx4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8de79761ac49ef18856820654c29a756400b074", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/q_mKpqIeyxzTruB6EUKnfYt7vR1ZTdQVKPC0lp52hx4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ebffce4cde58462254e7d7b33e019e8b5dc660a", "width": 1080, "height": 565}], "variants": {}, "id": "q_QhPSlUQ2VQT36O0YJV79KDbr7P3O6PiRdCh7yg0eQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlqib3", "is_robot_indexable": true, "report_reasons": null, "author": "HippolytosJocasta", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlqib3/ive_courseradl_installed_how_do_i_download_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlqib3/ive_courseradl_installed_how_do_i_download_this/", "subreddit_subscribers": 659172, "created_utc": 1671023293.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought a new Western Digital My Passport 1TB HDD 2627 last week. I tried to run the quick test on WD utilities and it completed with no problems, but the \"complete drive test\" was just stuck at 90% for at least 3 to 4 hours straight, I just didn't have that time to wait so I cancelled the test and ran chkdsk /f /r and came with no problems or bad sectors. Should I just be at ease with the chkdsk test and not worry that I should run the \"complete drive test\" again?", "author_fullname": "t2_3ndttd6i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with external drive.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zlqbn1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671022788.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a new Western Digital My Passport 1TB HDD 2627 last week. I tried to run the quick test on WD utilities and it completed with no problems, but the &amp;quot;complete drive test&amp;quot; was just stuck at 90% for at least 3 to 4 hours straight, I just didn&amp;#39;t have that time to wait so I cancelled the test and ran chkdsk /f /r and came with no problems or bad sectors. Should I just be at ease with the chkdsk test and not worry that I should run the &amp;quot;complete drive test&amp;quot; again?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlqbn1", "is_robot_indexable": true, "report_reasons": null, "author": "MinhajBEHz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlqbn1/need_help_with_external_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlqbn1/need_help_with_external_drive/", "subreddit_subscribers": 659172, "created_utc": 1671022788.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi! I got an email from clara.io saying that their service will be permanently shut down on december 31, 2022. It\u2019s a JavaScript based application and I\u2019d like to archive it as I find it really cool.\n\nQuestions:\n1. Is it even possible to archive a JS based site?\n2. How to do it? (I never have done something like this so ELI5)\n\nThank you for your replies in advance!", "author_fullname": "t2_hw27avzu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Clara.io archival", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zlp9th", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671019648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I got an email from clara.io saying that their service will be permanently shut down on december 31, 2022. It\u2019s a JavaScript based application and I\u2019d like to archive it as I find it really cool.&lt;/p&gt;\n\n&lt;p&gt;Questions:\n1. Is it even possible to archive a JS based site?\n2. How to do it? (I never have done something like this so ELI5)&lt;/p&gt;\n\n&lt;p&gt;Thank you for your replies in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlp9th", "is_robot_indexable": true, "report_reasons": null, "author": "AssOverflow12", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlp9th/claraio_archival/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlp9th/claraio_archival/", "subreddit_subscribers": 659172, "created_utc": 1671019648.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys, I have some Reddit posts that I would like to save forever. I mean, I saved them on Reddit account, but the users can to delete the post or the comments, so I will lose the deleted content. I tried to \"CTRL + s\" on Chrome, it download the HTML with its folder, but it is not working properly. For example, If I click on a text marked as spoiler, it will not show the text, also I can't expand comments (and probably there are other problems).\n\nIs there a way to download the entire page with its CSS and Javascript? Maybe an online tool where I past the link of the topic and it download it completely with all CSS and JS so that I can to show spoiler text and expand comments.\n\n&amp;#x200B;\n\nAlso, I would like to do this on Stack Overflow too, because also on Stack Overflow \"CTRL + S\" saves the page, but it will not works properly", "author_fullname": "t2_2sh9g5iz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download a Reddit post with all its comments?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zlopml", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671017838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I have some Reddit posts that I would like to save forever. I mean, I saved them on Reddit account, but the users can to delete the post or the comments, so I will lose the deleted content. I tried to &amp;quot;CTRL + s&amp;quot; on Chrome, it download the HTML with its folder, but it is not working properly. For example, If I click on a text marked as spoiler, it will not show the text, also I can&amp;#39;t expand comments (and probably there are other problems).&lt;/p&gt;\n\n&lt;p&gt;Is there a way to download the entire page with its CSS and Javascript? Maybe an online tool where I past the link of the topic and it download it completely with all CSS and JS so that I can to show spoiler text and expand comments.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Also, I would like to do this on Stack Overflow too, because also on Stack Overflow &amp;quot;CTRL + S&amp;quot; saves the page, but it will not works properly&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlopml", "is_robot_indexable": true, "report_reasons": null, "author": "secon25", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlopml/how_to_download_a_reddit_post_with_all_its/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlopml/how_to_download_a_reddit_post_with_all_its/", "subreddit_subscribers": 659172, "created_utc": 1671017838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, recently I\u2019ve been archiving a lot of videos but I keep encountering this problem..soft subs! I was wondering what program I can use to download the video with the subtitles..I got CleverGet because after reading around I was under the impression it could do that with YouTube videos but apparently not. I see a site called DownSub has been mentioned in the past but is that site safe? Is it one of those sites littered with pop up ads?", "author_fullname": "t2_f0wgvm5i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Saving videos with subtitles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zlilm2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670995768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, recently I\u2019ve been archiving a lot of videos but I keep encountering this problem..soft subs! I was wondering what program I can use to download the video with the subtitles..I got CleverGet because after reading around I was under the impression it could do that with YouTube videos but apparently not. I see a site called DownSub has been mentioned in the past but is that site safe? Is it one of those sites littered with pop up ads?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlilm2", "is_robot_indexable": true, "report_reasons": null, "author": "aroundforthefetus", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlilm2/saving_videos_with_subtitles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlilm2/saving_videos_with_subtitles/", "subreddit_subscribers": 659172, "created_utc": 1670995768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Like title says I have a 8TB WD RED NAS drive with few bad sectors that I wanted to clone to a good 8TB WD RED NAS with clonezilla on a linux system. The WD Red with bad sectors is still in warranty so I was going to rma it after.  \n\n\nI tried to use an old like 20 yr old emachine which boots fine if a hdd with windows installed is the boot drive but when I tried to run ubuntu it said something about a bios error. Like it didnt like ubuntu or something...honestly I don't really care which linux system I use. If i can't get that emachine to work, I either need to buy a cheap gpu for like an 8yr old amd system or just a pre-built office desktop I guess.  \n\n\nCan anyone recommend anything? I was using IDE to sata converters on the emachine but I dont think its going to work with ubuntu not sure if any other linux systems would work for whatever reason either. As long as I can get the clone done because theres like a small amount of personal data that I really do want to recover rest is just stuff I can easily get back.   \n\n\nBut I'm not sure whether to get a old office desktop or a gpu for my 8yr old pc. I'm only wanting to spend around 200$. I figure soon I would make my 8year old pc my server. So a GPU could be on the table, but was hoping for some advice. I just really want to get the clone done. Any help would be appreciated.", "author_fullname": "t2_68nd22zh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Could use some help need a cheap system to do a wd red 8tb to wd red 8tb clone with bad sectors", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zlhtpi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670993375.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like title says I have a 8TB WD RED NAS drive with few bad sectors that I wanted to clone to a good 8TB WD RED NAS with clonezilla on a linux system. The WD Red with bad sectors is still in warranty so I was going to rma it after.  &lt;/p&gt;\n\n&lt;p&gt;I tried to use an old like 20 yr old emachine which boots fine if a hdd with windows installed is the boot drive but when I tried to run ubuntu it said something about a bios error. Like it didnt like ubuntu or something...honestly I don&amp;#39;t really care which linux system I use. If i can&amp;#39;t get that emachine to work, I either need to buy a cheap gpu for like an 8yr old amd system or just a pre-built office desktop I guess.  &lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend anything? I was using IDE to sata converters on the emachine but I dont think its going to work with ubuntu not sure if any other linux systems would work for whatever reason either. As long as I can get the clone done because theres like a small amount of personal data that I really do want to recover rest is just stuff I can easily get back.   &lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m not sure whether to get a old office desktop or a gpu for my 8yr old pc. I&amp;#39;m only wanting to spend around 200$. I figure soon I would make my 8year old pc my server. So a GPU could be on the table, but was hoping for some advice. I just really want to get the clone done. Any help would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zlhtpi", "is_robot_indexable": true, "report_reasons": null, "author": "DroopyOregon", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlhtpi/could_use_some_help_need_a_cheap_system_to_do_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlhtpi/could_use_some_help_need_a_cheap_system_to_do_a/", "subreddit_subscribers": 659172, "created_utc": 1670993375.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I plan on getting two 10 tb seagate ironwolf drives and maybe later a lower capacity drive for Home security. What raid should I use?", "author_fullname": "t2_tbwt3an0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Raid should I use for 2 HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zlex78", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "raid", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670985078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I plan on getting two 10 tb seagate ironwolf drives and maybe later a lower capacity drive for Home security. What raid should I use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zlex78", "is_robot_indexable": true, "report_reasons": null, "author": "DescriptionTrue8326", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zlex78/what_raid_should_i_use_for_2_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zlex78/what_raid_should_i_use_for_2_hdd/", "subreddit_subscribers": 659172, "created_utc": 1670985078.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}