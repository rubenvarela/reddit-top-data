{"kind": "Listing", "data": {"after": "t3_zoci44", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7tk61jlx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Offend a data scientist in one tweet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 133, "top_awarded_type": null, "hide_score": false, "name": "t3_zo5bwf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 1665, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 1665, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BSxLRIlq1njHZP5Z_wamCF9V9SfkDjm9KyuGwG8B2oM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671280215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/t7n4hi55uh6a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?auto=webp&amp;s=bac6161094a9a79489f52437166b8dcb7f64d3b6", "width": 1170, "height": 1113}, "resolutions": [{"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0808045bdcd76aa0c585fab3f629abbd08b9eb4a", "width": 108, "height": 102}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d062ac3047031d4f2a8bb9edd39a1395fa6b71b", "width": 216, "height": 205}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=960fe5dbb627fe1c085be0b1f0cd2cc981a065fc", "width": 320, "height": 304}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=103f470deb4cdda1eb9ce38819483792ba3ea0b8", "width": 640, "height": 608}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7628380a74c4952d35e2d439d930c7d9b3fa6cf9", "width": 960, "height": 913}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=41fbefb81a833b413faf3f777ba8756ea5939573", "width": 1080, "height": 1027}], "variants": {}, "id": "cF2H-4KHwkY2zala2RSzU8criZ8swOdmyibBo-iHicE"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo5bwf", "is_robot_indexable": true, "report_reasons": null, "author": "datasciencepro", "discussion_type": null, "num_comments": 165, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo5bwf/offend_a_data_scientist_in_one_tweet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/t7n4hi55uh6a1.jpg", "subreddit_subscribers": 827987, "created_utc": 1671280215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_f900b52f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zo5cll", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 467, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 467, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/s0ynn7G8nXbTR_uXt90gEhSZHUL0mLo7H8bhVRTMixY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_1": 1}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671280285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/49ppdolcuh6a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?auto=webp&amp;s=a7a64c1a98fd1a74d3e4355ebccf288513ca89bb", "width": 1080, "height": 1920}, "resolutions": [{"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=30310c228dfd08348c71abb5fb5527956974f114", "width": 108, "height": 192}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=913abe79dc9b0a53473b7089eccc385ca0a071e2", "width": 216, "height": 384}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d9eb5d70911cce318caf1b6c5c8eae9f1ebd5b46", "width": 320, "height": 568}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=12e7b553a6788b9cf808a1629bbe8594e5c4e1e4", "width": 640, "height": 1137}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=afbf3adf5602589e4b4e263d6ff360a7a427131d", "width": 960, "height": 1706}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf90a16c987c9f4074158f0ba3347e2f0e27f4f5", "width": 1080, "height": 1920}], "variants": {}, "id": "W-uSuaiYuKbO5KZaKtG_pZhr1FKoXou0FiZEo6YQpfY"}], "enabled": true}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 100, "id": "gid_1", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Shows the Silver Award... and that's it.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Silver", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo5cll", "is_robot_indexable": true, "report_reasons": null, "author": "DwightScott69", "discussion_type": null, "num_comments": 143, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo5cll/thoughts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/49ppdolcuh6a1.jpg", "subreddit_subscribers": 827987, "created_utc": 1671280285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I\u2019ve previously worked as a Data Scientist mainly with NLP and most recently I\u2019ve been working as a Data Engineer. I will be joining a new team as a Data Scientist in energy sector, specifically HVAC. What kind of topics should I prepare myself from technical point of view, my Python, SQL skills are above average or average.", "author_fullname": "t2_90pyvsip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone with data scientist experience in energy sector?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zohi08", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671314410.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I\u2019ve previously worked as a Data Scientist mainly with NLP and most recently I\u2019ve been working as a Data Engineer. I will be joining a new team as a Data Scientist in energy sector, specifically HVAC. What kind of topics should I prepare myself from technical point of view, my Python, SQL skills are above average or average.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zohi08", "is_robot_indexable": true, "report_reasons": null, "author": "Gagan_Ku2905", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zohi08/anyone_with_data_scientist_experience_in_energy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zohi08/anyone_with_data_scientist_experience_in_energy/", "subreddit_subscribers": 827987, "created_utc": 1671314410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What are some common functions/transformations you run on a dataset before training and what libraries do you use, or is it a custom script?", "author_fullname": "t2_6i7on", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Common dataset prep operations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo2tb9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671269962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some common functions/transformations you run on a dataset before training and what libraries do you use, or is it a custom script?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo2tb9", "is_robot_indexable": true, "report_reasons": null, "author": "jy2k", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo2tb9/common_dataset_prep_operations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo2tb9/common_dataset_prep_operations/", "subreddit_subscribers": 827987, "created_utc": 1671269962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For context, I'm currently finishing my bachelor's degree in electrical engineering and I just completed my minor in data science (i.e. I finished the last course required to satisfy the minor's requirements).  I found I like the data science stuff significantly more than EE, but I'm too far along to even consider switching majors at this point.  Hence, I'm trying to self-teach additional data science skills and I know being to use SQL and work with databases (something none of my DS courses covered unfortunately) in particular is a vital skill to have if I have any hope of getting a job in DS. \n\nI posted previously about this and I got a ton of responses with people recommending so many different learning platforms and several different API's and DBMS's that I'm a little unsure where to start.  I started just reading about what databases even _are_ so I can have a clear mental model in my head, but now I'm struggling to decide how to actually get started with SQL itself.  \n\nThe easiest thing (and hence what I'm tempted to do) would probably be to use one of the Python API's people recommended, just because I already have some experience using Python for data cleaning, exploration, and analysis, and I have Python fully set-up on my system already (and getting everything set up to use _any_ new programming language is typically a pain).  But is that a good idea, seeing as this will be the first time I've used SQL?  Will it it hurt me later on if I get used to just using Python to call SQL rather than learning how to use it directly?  Like, would prospective employers be less likely to higher me if I only have experience using SQL via Python, or will there be things I can't do through the API?  Or am I just completely overthinking this and it doesn't really matter whether I use SQL directly or indirectly?", "author_fullname": "t2_16ojfv6p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm attempting to self-teach SQL. If I already know already know Python, should I start by using a Python API for SQL or would that handicap me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zon2g1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671328853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context, I&amp;#39;m currently finishing my bachelor&amp;#39;s degree in electrical engineering and I just completed my minor in data science (i.e. I finished the last course required to satisfy the minor&amp;#39;s requirements).  I found I like the data science stuff significantly more than EE, but I&amp;#39;m too far along to even consider switching majors at this point.  Hence, I&amp;#39;m trying to self-teach additional data science skills and I know being to use SQL and work with databases (something none of my DS courses covered unfortunately) in particular is a vital skill to have if I have any hope of getting a job in DS. &lt;/p&gt;\n\n&lt;p&gt;I posted previously about this and I got a ton of responses with people recommending so many different learning platforms and several different API&amp;#39;s and DBMS&amp;#39;s that I&amp;#39;m a little unsure where to start.  I started just reading about what databases even &lt;em&gt;are&lt;/em&gt; so I can have a clear mental model in my head, but now I&amp;#39;m struggling to decide how to actually get started with SQL itself.  &lt;/p&gt;\n\n&lt;p&gt;The easiest thing (and hence what I&amp;#39;m tempted to do) would probably be to use one of the Python API&amp;#39;s people recommended, just because I already have some experience using Python for data cleaning, exploration, and analysis, and I have Python fully set-up on my system already (and getting everything set up to use &lt;em&gt;any&lt;/em&gt; new programming language is typically a pain).  But is that a good idea, seeing as this will be the first time I&amp;#39;ve used SQL?  Will it it hurt me later on if I get used to just using Python to call SQL rather than learning how to use it directly?  Like, would prospective employers be less likely to higher me if I only have experience using SQL via Python, or will there be things I can&amp;#39;t do through the API?  Or am I just completely overthinking this and it doesn&amp;#39;t really matter whether I use SQL directly or indirectly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zon2g1", "is_robot_indexable": true, "report_reasons": null, "author": "dcfan105", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zon2g1/im_attempting_to_selfteach_sql_if_i_already_know/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zon2g1/im_attempting_to_selfteach_sql_if_i_already_know/", "subreddit_subscribers": 827987, "created_utc": 1671328853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve seen companies hiring for all 4 at once at a single location so I\u2019m curious how the responsibilities would be divided. I feel there still lacks universal definitions for these roles.", "author_fullname": "t2_524iawau", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If a company has a Data Engineer, Analyst, and a ML Engineer, what does the Data Scientist do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo99yd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671292687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve seen companies hiring for all 4 at once at a single location so I\u2019m curious how the responsibilities would be divided. I feel there still lacks universal definitions for these roles.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo99yd", "is_robot_indexable": true, "report_reasons": null, "author": "bassabyss", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo99yd/if_a_company_has_a_data_engineer_analyst_and_a_ml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo99yd/if_a_company_has_a_data_engineer_analyst_and_a_ml/", "subreddit_subscribers": 827987, "created_utc": 1671292687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_lck7n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Becoming a better programmer is twice as hard as just writing code. This is how I will sum up my learning of the past 30 years.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_zomtw5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2nMUqCSCP8aQRQB4QROhtnh0218LmurcEun03fdsUOI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671328151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "link.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://link.medium.com/9i9BBxu1Nvb", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?auto=webp&amp;s=4e38a91c86986c5fb593b6dd325f56fab6f68f47", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1da8588eee88de02d759267fe6be2b0c39216814", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=57d81a4a7abd09f4177018e245b1a7b22a52c39d", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ddcbe6e6b5137a38107cd6821bf7ba34e08b846", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=900b04e19b8ec42294770880b325da4742626500", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ab157542147f7a54674b3bedd8dc9edf2e3692f8", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a1176e703b1f9bead39076a66735df27b23a039", "width": 1080, "height": 720}], "variants": {}, "id": "XH0z2WesZmlFmuitxqj7OA-8eBZPyjuDh8kVOEqUXmU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zomtw5", "is_robot_indexable": true, "report_reasons": null, "author": "ishwarjha", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zomtw5/becoming_a_better_programmer_is_twice_as_hard_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://link.medium.com/9i9BBxu1Nvb", "subreddit_subscribers": 827987, "created_utc": 1671328151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m looking for some data on p&amp;c insurance losses (claim/no-claim; claim size; peril; date; address). Does anyone know if there is a public data base with this?", "author_fullname": "t2_91itiala", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "P&amp;C Claim Data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zom9b9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671326434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking for some data on p&amp;amp;c insurance losses (claim/no-claim; claim size; peril; date; address). Does anyone know if there is a public data base with this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zom9b9", "is_robot_indexable": true, "report_reasons": null, "author": "MGeeeeeezy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zom9b9/pc_claim_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zom9b9/pc_claim_data/", "subreddit_subscribers": 827987, "created_utc": 1671326434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm curious what folks in larger companies have found useful for working around/with encrypted data within a database. Not passwords but other types of controlled info \u2013 device checkins, friend lists, etc.\n\nDoes your security team have a process/tool in place to access the decryption key(s) from your code? Are there other limits on what you can do/can't do?", "author_fullname": "t2_5z2ths3h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your workflow for decrypting PII fields from a database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zodofx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671304493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious what folks in larger companies have found useful for working around/with encrypted data within a database. Not passwords but other types of controlled info \u2013 device checkins, friend lists, etc.&lt;/p&gt;\n\n&lt;p&gt;Does your security team have a process/tool in place to access the decryption key(s) from your code? Are there other limits on what you can do/can&amp;#39;t do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zodofx", "is_robot_indexable": true, "report_reasons": null, "author": "rszumski", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zodofx/what_is_your_workflow_for_decrypting_pii_fields/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zodofx/what_is_your_workflow_for_decrypting_pii_fields/", "subreddit_subscribers": 827987, "created_utc": 1671304493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is there a place to find portfolio examples? How do people usually attach a portfolio to a resume? What do these portfolios look like? How are they optimized? \n\nI'm about to start my final year of a Bachelors in Data Science and want to start trying to find a full / part-time (not just seasonal) internship/job. I have seen a handful of the hiring managers in this sub say they do check the portfolios, and sometimes they do more damage than good. I was hoping to find some examples of great portfolios to compare to mine. \n\nTIA", "author_fullname": "t2_15elnh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Portfolio Examples", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zos2gd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671345201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a place to find portfolio examples? How do people usually attach a portfolio to a resume? What do these portfolios look like? How are they optimized? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m about to start my final year of a Bachelors in Data Science and want to start trying to find a full / part-time (not just seasonal) internship/job. I have seen a handful of the hiring managers in this sub say they do check the portfolios, and sometimes they do more damage than good. I was hoping to find some examples of great portfolios to compare to mine. &lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zos2gd", "is_robot_indexable": true, "report_reasons": null, "author": "RunescapeJoe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zos2gd/portfolio_examples/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zos2gd/portfolio_examples/", "subreddit_subscribers": 827987, "created_utc": 1671345201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Current neuroscience major. Would it be viable to apply to data science masters programs with this degree? I was thinking of adding a CS minor too.", "author_fullname": "t2_wpbwvz9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Neuroscience major -&gt; data science masters?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoryyn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671344846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Current neuroscience major. Would it be viable to apply to data science masters programs with this degree? I was thinking of adding a CS minor too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoryyn", "is_robot_indexable": true, "report_reasons": null, "author": "qsauce6", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoryyn/neuroscience_major_data_science_masters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoryyn/neuroscience_major_data_science_masters/", "subreddit_subscribers": 827987, "created_utc": 1671344846.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "question is a bit confusing, but let me explain. I study data science at university, but it seems to be a lot more data analysis than actual data science (for now, at least) but I still want to end up in the field of data science and have an internship under my belt before I graduate (I'm a junior rn). So what are some concepts I should know, and how well should I know them?\n\nFor example, within machine learning (or other concepts), which models should I know and how well should I know them?\n\nLet me know if I'm still confusing lol.. and any other advice is much appreciated ! - sincerely, a struggling college student.", "author_fullname": "t2_55jsie1i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "weird, open-ended question: How good at DS do I need to be to get a DS internship?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zorihl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671343191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;question is a bit confusing, but let me explain. I study data science at university, but it seems to be a lot more data analysis than actual data science (for now, at least) but I still want to end up in the field of data science and have an internship under my belt before I graduate (I&amp;#39;m a junior rn). So what are some concepts I should know, and how well should I know them?&lt;/p&gt;\n\n&lt;p&gt;For example, within machine learning (or other concepts), which models should I know and how well should I know them?&lt;/p&gt;\n\n&lt;p&gt;Let me know if I&amp;#39;m still confusing lol.. and any other advice is much appreciated ! - sincerely, a struggling college student.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zorihl", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway_jfkdhsmdns", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zorihl/weird_openended_question_how_good_at_ds_do_i_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zorihl/weird_openended_question_how_good_at_ds_do_i_need/", "subreddit_subscribers": 827987, "created_utc": 1671343191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys,\nFrom your experience, what models would you consider for product assortment/range optimisation for FMCG retail sector\n( i.e getting the right product range in the right store/ product placement).\n\nI\u2019ve seen some use XGBoost for product placement, but curious to know what other models work well.\n\nI know we have to take product substitution into consideration so Customer Decision Tree Model is already taken care of.", "author_fullname": "t2_idusno00", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Product assortment/range optimisation for FMCG", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zolerx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671324227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,\nFrom your experience, what models would you consider for product assortment/range optimisation for FMCG retail sector\n( i.e getting the right product range in the right store/ product placement).&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen some use XGBoost for product placement, but curious to know what other models work well.&lt;/p&gt;\n\n&lt;p&gt;I know we have to take product substitution into consideration so Customer Decision Tree Model is already taken care of.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zolerx", "is_robot_indexable": true, "report_reasons": null, "author": "Maria_Adel", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zolerx/product_assortmentrange_optimisation_for_fmcg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zolerx/product_assortmentrange_optimisation_for_fmcg/", "subreddit_subscribers": 827987, "created_utc": 1671324227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Working on a time series classification project. I\u2019m going through the sktime library and reading each and every classifiers summary about the method that\u2019s being used under the hood to fit the model, and I\u2019m basing my choices based on how long it may take to train. For example, half of them are distance based (nearest neighbors) methods, and I know for my dataset a distance matrix (and most data in general) just takes a stupid amount of time to compute. Or some do Shapelet discovery, which is traversing the entire subsequence space and computing distances between the subsequence and original series and using those subsequences (shapelets) to predict, which also, takes a long time. So I\u2019m just avoiding using them for others (interval based)\n\nDoes anyone else do this? Is this a bad practice?", "author_fullname": "t2_i69qgpqa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching models because a model takes too long to train", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zogrsk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671312539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a time series classification project. I\u2019m going through the sktime library and reading each and every classifiers summary about the method that\u2019s being used under the hood to fit the model, and I\u2019m basing my choices based on how long it may take to train. For example, half of them are distance based (nearest neighbors) methods, and I know for my dataset a distance matrix (and most data in general) just takes a stupid amount of time to compute. Or some do Shapelet discovery, which is traversing the entire subsequence space and computing distances between the subsequence and original series and using those subsequences (shapelets) to predict, which also, takes a long time. So I\u2019m just avoiding using them for others (interval based)&lt;/p&gt;\n\n&lt;p&gt;Does anyone else do this? Is this a bad practice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zogrsk", "is_robot_indexable": true, "report_reasons": null, "author": "AdFew4357", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zogrsk/switching_models_because_a_model_takes_too_long/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zogrsk/switching_models_because_a_model_takes_too_long/", "subreddit_subscribers": 827987, "created_utc": 1671312539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello scientists,\n\nI have some basic data, more like a very basic personal project, and I want to find a way how to visualize it in a nice way. A three circle Venn diagram is not enough, I will need something like a \"oval flower model\" with multiple intertwining and mutually intersecting fields.\n\nThe basic tools online and in Excel seem not to be self adjusting based on how much text I want to put into the image (some fields require a lot of it). So I would like to find an online tool which can create a beautiful visual representation for me and which would automatically adjust the size of the diagram fields. \n\nI am not proficient yet in advanced tools or programming languages like R or Python, so I would like something simple.\n\nI am sure such a thing exist, I just suck a bit at finding it. It seems very easy, something that would look like this but that would automatically adjust the size and shape:\n\n[https://slidemodel.com/templates/4-set-venn-diagram-for-powerpoint/](https://slidemodel.com/templates/4-set-venn-diagram-for-powerpoint/)", "author_fullname": "t2_pm1g85e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Self-adjusting Venn and other diagram TOOL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zofz2p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671310504.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello scientists,&lt;/p&gt;\n\n&lt;p&gt;I have some basic data, more like a very basic personal project, and I want to find a way how to visualize it in a nice way. A three circle Venn diagram is not enough, I will need something like a &amp;quot;oval flower model&amp;quot; with multiple intertwining and mutually intersecting fields.&lt;/p&gt;\n\n&lt;p&gt;The basic tools online and in Excel seem not to be self adjusting based on how much text I want to put into the image (some fields require a lot of it). So I would like to find an online tool which can create a beautiful visual representation for me and which would automatically adjust the size of the diagram fields. &lt;/p&gt;\n\n&lt;p&gt;I am not proficient yet in advanced tools or programming languages like R or Python, so I would like something simple.&lt;/p&gt;\n\n&lt;p&gt;I am sure such a thing exist, I just suck a bit at finding it. It seems very easy, something that would look like this but that would automatically adjust the size and shape:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://slidemodel.com/templates/4-set-venn-diagram-for-powerpoint/\"&gt;https://slidemodel.com/templates/4-set-venn-diagram-for-powerpoint/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?auto=webp&amp;s=b8c436d0f16f68aeff3f032511de8c58efd15670", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e789ac40ee9c21361d89a6fbec1ca0b013ff3582", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=056a01229ae1d234fac7fcd7729803138e5f7372", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50f6d2f4f9b8bd285b7baa8c2693c1da7c6e0687", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=687fa2608a2b4bad7abcd89b9b81f683c5e6ea5f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2bc1dcb1e475f1203f6f0e0dcfaf36862d372525", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9f34f65b6861adc750282491554af75ea2c5c6a1", "width": 1080, "height": 607}], "variants": {}, "id": "pENjWJRqsa-GYfZ9DXYXCN3hnLoi0pZG0UcoOiIeSQM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zofz2p", "is_robot_indexable": true, "report_reasons": null, "author": "Sheetmusicman94", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zofz2p/selfadjusting_venn_and_other_diagram_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zofz2p/selfadjusting_venn_and_other_diagram_tool/", "subreddit_subscribers": 827987, "created_utc": 1671310504.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My sibling has a career in data science and machine learning and they are really into it, it is their hobby too. Any ideas what I cam get them?\n\nThey won\u2019t like knick knacks like a mug with an AI generated dog, or a t shirt. They would just throw that out. What would be a great gift is something (not a book) that allows them to explore, learn, or experiment with ai more.", "author_fullname": "t2_qgco0kmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What can I get my sibling who is an AI data scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoagmh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671295838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My sibling has a career in data science and machine learning and they are really into it, it is their hobby too. Any ideas what I cam get them?&lt;/p&gt;\n\n&lt;p&gt;They won\u2019t like knick knacks like a mug with an AI generated dog, or a t shirt. They would just throw that out. What would be a great gift is something (not a book) that allows them to explore, learn, or experiment with ai more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoagmh", "is_robot_indexable": true, "report_reasons": null, "author": "CoupleConsistent5378", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoagmh/what_can_i_get_my_sibling_who_is_an_ai_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoagmh/what_can_i_get_my_sibling_who_is_an_ai_data/", "subreddit_subscribers": 827987, "created_utc": 1671295838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to set up an unsupervised learning project for anomaly detection and I'm learning along the way. Since I haven't got any previous experience with unsupervised experiments, I have what is probably a very basic question with regard to setting up my experiment: is the basic approach for unsupervised learning broadly comparable to that of supervised learning?\n\nMy intent is to go with a data split of 70:15:15 (train/test/holdout) and do an EDA on the data. I want to check how meaningful certain features are, whether they correlate etc.. Then I'll drop features and/or engineer derived features and see how they cluster and whether PCA will result in any insight with regard to feature relevance?\n\nSince I'll likely use the insight of the PCA to inform the feature engineering, I think a holdout data set is necessary - at least in theory. However, since anomalies are both \\_anomalous\\_ and sparse, I'm not really sure whether this can actually be practically implemented.\n\nAny pointers to best practices or or design philosophies would be much appreciate by this unsupervised newbie. ;)", "author_fullname": "t2_4j7ujk5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I also use a train/test/holdout split in unsupervised learning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoaan4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671295389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to set up an unsupervised learning project for anomaly detection and I&amp;#39;m learning along the way. Since I haven&amp;#39;t got any previous experience with unsupervised experiments, I have what is probably a very basic question with regard to setting up my experiment: is the basic approach for unsupervised learning broadly comparable to that of supervised learning?&lt;/p&gt;\n\n&lt;p&gt;My intent is to go with a data split of 70:15:15 (train/test/holdout) and do an EDA on the data. I want to check how meaningful certain features are, whether they correlate etc.. Then I&amp;#39;ll drop features and/or engineer derived features and see how they cluster and whether PCA will result in any insight with regard to feature relevance?&lt;/p&gt;\n\n&lt;p&gt;Since I&amp;#39;ll likely use the insight of the PCA to inform the feature engineering, I think a holdout data set is necessary - at least in theory. However, since anomalies are both _anomalous_ and sparse, I&amp;#39;m not really sure whether this can actually be practically implemented.&lt;/p&gt;\n\n&lt;p&gt;Any pointers to best practices or or design philosophies would be much appreciate by this unsupervised newbie. ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoaan4", "is_robot_indexable": true, "report_reasons": null, "author": "norfkens2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoaan4/do_i_also_use_a_traintestholdout_split_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoaan4/do_i_also_use_a_traintestholdout_split_in/", "subreddit_subscribers": 827987, "created_utc": 1671295389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Check out out, would you use this?\n\nhttps://github.com/aws-samples/distributed-compute-on-aws-with-cross-regional-dask", "author_fullname": "t2_v1f56kco", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cross Regional Dask on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo8v5e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671291531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check out out, would you use this?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/aws-samples/distributed-compute-on-aws-with-cross-regional-dask\"&gt;https://github.com/aws-samples/distributed-compute-on-aws-with-cross-regional-dask&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?auto=webp&amp;s=2007921aefa32104b16120fdcda67f25052c18d3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0d2363e91de15e7251f6ad01a45ba9d7cd5b36c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef2d65fc0163c885d031bf3b3742c1c90e71a245", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c83d83b48aaaca832314c54b7e208d9414432ff", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8ef536a4b30cac8de2b8c1256e74a37aece58c8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=23049cb7c3f55d216d707f36a48e1fda6e7a758f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02a1f497a8851982103795c3df6c1b2e3029757e", "width": 1080, "height": 540}], "variants": {}, "id": "d7SCmwvPs0_VsnLQQ01R-H3jpt4LRVP6Zmmmk5Icd0c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo8v5e", "is_robot_indexable": true, "report_reasons": null, "author": "oconpa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo8v5e/cross_regional_dask_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo8v5e/cross_regional_dask_on_aws/", "subreddit_subscribers": 827987, "created_utc": 1671291531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,  \nCurrently I am researching extreme learning machines, and - after I calculated the beta weights  - I would like to optimize the alpha weights (similarly with the Moore-Penrose pseudo inverse) in order to receive better accuracy.  \nDuring this backward calculation, I use the inverse of the previously applied activation function. In case of the inverse sigmoid and tanh (logit and arctan), I receive NaN values, although with the inverse leaky ReLU, on certain datasets, the method yields better accuracy. Unfortunately, in most cases, the accuracy drops (sometimes significantly) with these new optimized alpha weights. What do you think, what is the reason for this?", "author_fullname": "t2_kq8l2zbu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inverse activation function", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo7wjc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671288767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;br/&gt;\nCurrently I am researching extreme learning machines, and - after I calculated the beta weights  - I would like to optimize the alpha weights (similarly with the Moore-Penrose pseudo inverse) in order to receive better accuracy.&lt;br/&gt;\nDuring this backward calculation, I use the inverse of the previously applied activation function. In case of the inverse sigmoid and tanh (logit and arctan), I receive NaN values, although with the inverse leaky ReLU, on certain datasets, the method yields better accuracy. Unfortunately, in most cases, the accuracy drops (sometimes significantly) with these new optimized alpha weights. What do you think, what is the reason for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo7wjc", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical-Plane3730", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo7wjc/inverse_activation_function/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo7wjc/inverse_activation_function/", "subreddit_subscribers": 827987, "created_utc": 1671288767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to do a new project in R, where I'm working with a lot of data (about 6GB of text) my inefficient code is tying me down a lot and was hoping maybe you guys had some tricks.\n\nI'm supposed to load in large .txt files, each file should be split into shorter parts consisting of 10 words each, they are then put into my data frame with each word being put into one of 10 columns. \n\nThe problem is that my code is slow, like really slow. It takes around 20 minutes to load the first 100000 rows of text into my dataframe, which is less than 1% of my data. I have also noticed that code slows down the further it takes to run, it takes about a second to load in the first 3000 rows, and 15 seconds for the first 10000 rows.   \n\nWhat is the main bottleneck slowing me down? Should I just rewrite the code entirely in another language like python?\n\nMy current code below:\n\n    #setup\n    folder &lt;- \"C:/Users/A_tiny/\"\n    \n    # Get a list of files in the folder\n    files &lt;- list.files(folder)\n    \n    # Create an empty data frame to hold the indexed words\n    df &lt;- data.frame(word1 = character(), word2 = character(), word3 = character(), word4 = character(),\n    word5 = character(), word6 = character(), word7 = character(), word8 = character(),\n    word9 = character(), word10 = character(), index = integer())\n    \n    for (file in files) {\n    # Check if the file is a .epub.txt file\n    if (grepl(\".epub.txt$\", file)) {\n    # Read the .epub.txt file into a string variable\n    epub_txt &lt;- readLines(paste0(folder, file))\n    \n    # Split the string into a vector of words\n    words &lt;- unlist(strsplit(epub_txt, \" \"))\n    \n    # Loop through the vector of words and add the next 10 words to the data frame, skipping 10 words at a time\n    for (i in seq(1, length(words), by=10)) {\n      # Create a data frame with the next 10 words and the index\n      df_temp &lt;- data.frame(word1 = words[i], word2 = words[i+1], word3 = words[i+2], word4 = words[i+3],\n                            word5 = words[i+4], word6 = words[i+5], word7 = words[i+6], word8 = words[i+7],\n                            word9 = words[i+8], word10 = words[i+9], index = i)\n      # Add the data frame to the main data frame\n      df &lt;- rbind(df, df_temp)\n    }\n      }\n    }", "author_fullname": "t2_gx7hn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The best approach for rewriting my code more efficiently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo4p8z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671277772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to do a new project in R, where I&amp;#39;m working with a lot of data (about 6GB of text) my inefficient code is tying me down a lot and was hoping maybe you guys had some tricks.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m supposed to load in large .txt files, each file should be split into shorter parts consisting of 10 words each, they are then put into my data frame with each word being put into one of 10 columns. &lt;/p&gt;\n\n&lt;p&gt;The problem is that my code is slow, like really slow. It takes around 20 minutes to load the first 100000 rows of text into my dataframe, which is less than 1% of my data. I have also noticed that code slows down the further it takes to run, it takes about a second to load in the first 3000 rows, and 15 seconds for the first 10000 rows.   &lt;/p&gt;\n\n&lt;p&gt;What is the main bottleneck slowing me down? Should I just rewrite the code entirely in another language like python?&lt;/p&gt;\n\n&lt;p&gt;My current code below:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#setup\nfolder &amp;lt;- &amp;quot;C:/Users/A_tiny/&amp;quot;\n\n# Get a list of files in the folder\nfiles &amp;lt;- list.files(folder)\n\n# Create an empty data frame to hold the indexed words\ndf &amp;lt;- data.frame(word1 = character(), word2 = character(), word3 = character(), word4 = character(),\nword5 = character(), word6 = character(), word7 = character(), word8 = character(),\nword9 = character(), word10 = character(), index = integer())\n\nfor (file in files) {\n# Check if the file is a .epub.txt file\nif (grepl(&amp;quot;.epub.txt$&amp;quot;, file)) {\n# Read the .epub.txt file into a string variable\nepub_txt &amp;lt;- readLines(paste0(folder, file))\n\n# Split the string into a vector of words\nwords &amp;lt;- unlist(strsplit(epub_txt, &amp;quot; &amp;quot;))\n\n# Loop through the vector of words and add the next 10 words to the data frame, skipping 10 words at a time\nfor (i in seq(1, length(words), by=10)) {\n  # Create a data frame with the next 10 words and the index\n  df_temp &amp;lt;- data.frame(word1 = words[i], word2 = words[i+1], word3 = words[i+2], word4 = words[i+3],\n                        word5 = words[i+4], word6 = words[i+5], word7 = words[i+6], word8 = words[i+7],\n                        word9 = words[i+8], word10 = words[i+9], index = i)\n  # Add the data frame to the main data frame\n  df &amp;lt;- rbind(df, df_temp)\n}\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo4p8z", "is_robot_indexable": true, "report_reasons": null, "author": "_Just7_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo4p8z/the_best_approach_for_rewriting_my_code_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo4p8z/the_best_approach_for_rewriting_my_code_more/", "subreddit_subscribers": 827987, "created_utc": 1671277772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am at company A an Associate Director DS since 1.8 years, with no guarantee to move to director anytime soon. I have been offered a position of Principal DS at company B with 13% salary increase, sign-in bonus and payout of 70% of my unvested stocks value. Pay wise it is an upgrade.", "author_fullname": "t2_6kg9d73t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a downgrade going from Associate Director DS to Principal DS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoctli", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671302228.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am at company A an Associate Director DS since 1.8 years, with no guarantee to move to director anytime soon. I have been offered a position of Principal DS at company B with 13% salary increase, sign-in bonus and payout of 70% of my unvested stocks value. Pay wise it is an upgrade.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoctli", "is_robot_indexable": true, "report_reasons": null, "author": "IllustratorInfamous1", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoctli/is_it_a_downgrade_going_from_associate_director/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoctli/is_it_a_downgrade_going_from_associate_director/", "subreddit_subscribers": 827987, "created_utc": 1671302228.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone, does anyone know if chatGPT has been trained on Kaggle projects? If so, it should already be pretty good at a lot of DS stuff, right?\n\nIf not, I think it's only a matter of time before they will include that, which could create a very powerful DS personal assistant.\n\nI guess it could be challenging to train it on large datasets specifically, but I'm sure there are some smart ways to make that part more efficient, like only using a sample of each data set. Plus, there is the legal question if Kaggle would allow openAI to use their data.\n\nDo you guys have any thoughts?", "author_fullname": "t2_12hyas", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Was ChatGPT trained on Kaggle and other DS coding platforms?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo2pj1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671269496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, does anyone know if chatGPT has been trained on Kaggle projects? If so, it should already be pretty good at a lot of DS stuff, right?&lt;/p&gt;\n\n&lt;p&gt;If not, I think it&amp;#39;s only a matter of time before they will include that, which could create a very powerful DS personal assistant.&lt;/p&gt;\n\n&lt;p&gt;I guess it could be challenging to train it on large datasets specifically, but I&amp;#39;m sure there are some smart ways to make that part more efficient, like only using a sample of each data set. Plus, there is the legal question if Kaggle would allow openAI to use their data.&lt;/p&gt;\n\n&lt;p&gt;Do you guys have any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo2pj1", "is_robot_indexable": true, "report_reasons": null, "author": "ikke89", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo2pj1/was_chatgpt_trained_on_kaggle_and_other_ds_coding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo2pj1/was_chatgpt_trained_on_kaggle_and_other_ds_coding/", "subreddit_subscribers": 827987, "created_utc": 1671269496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7da18ldj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "What do you think about ChatGPT? To me, it's interesting. I'm new to programming, so I d like to know what you guys think about this. Ignore that Activate windows watermark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 66, "top_awarded_type": null, "hide_score": true, "media_metadata": {"rbxnz3ur0m6a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 51, "x": 108, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33dea72aa634b10ba69aab1291d3911edb89a90b"}, {"y": 103, "x": 216, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=99137500383ab9750749f5bfc7b32bf7cb2b00fc"}, {"y": 152, "x": 320, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=27bf8ea1d25b3942a94f127f76cdf790c6114429"}, {"y": 305, "x": 640, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e18c2ccb2f75ec6ebcb1c387fcdddc623e33e8c"}, {"y": 458, "x": 960, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bcecfbc2c291bfd8efaeaca82c639e6ee7af46fe"}, {"y": 515, "x": 1080, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4c162274d8827eb86f8ab61726c826f094e76242"}], "s": {"y": 651, "x": 1364, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=1364&amp;format=png&amp;auto=webp&amp;s=af5c668e8597dcdc660eae915653af3b7c0b0987"}, "id": "rbxnz3ur0m6a1"}, "gf0yjwtr0m6a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6fec6936e2ce01c780704dd39e0dbcc123d81ff"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e0ab4ba0b0ecd61c0254e84e103c928d8fb889b"}, {"y": 154, "x": 320, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f1f2d099940f5c8b97b32fd53e1bfeaba37717f"}, {"y": 308, "x": 640, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed1baceaba7483280ad2d9c35c9c39c9f6b687a6"}, {"y": 463, "x": 960, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55e3579fc161af2b068cc8461c08dd195d9daf5b"}, {"y": 521, "x": 1080, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1dbf37712aa01b438cb4fa417a3731ae4a322409"}], "s": {"y": 659, "x": 1366, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=1366&amp;format=png&amp;auto=webp&amp;s=c436a25d418e50e1d3683e8b6e2222278e5e5efb"}, "id": "gf0yjwtr0m6a1"}}, "name": "t3_zot4ny", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "rbxnz3ur0m6a1", "id": 220367381}, {"media_id": "gf0yjwtr0m6a1", "id": 220367382}]}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/MTKUc86iRVElQIxSB8QRxNtH8xfBJnlARxuoIa4MFIQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671349377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zot4ny", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": null, "id": "zot4ny", "is_robot_indexable": true, "report_reasons": null, "author": "deadlyb0y", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zot4ny/what_do_you_think_about_chatgpt_to_me_its/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zot4ny", "subreddit_subscribers": 827987, "created_utc": 1671349377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone!\n\nI psuedo joined the tech layoff getting put into a free agency to apply anywhere by my company and have been joining the mad job rush getting into things. I thought to share some experiential learning I had going on that roughly equates to normal good practice, but is also a good reminder to always consider ourselves expendable and a free agent regardless of if we're safe or not. I'm getting this quality checked on multiple communities and I'd love to hear your feedback.\n\n1. If your resume stands out, they have a specific need, and you fill it beautifully which you\u2019ll understand based on the job description, you can expect to get through the hiring process quickly.\n2. If it\u2019s a not sure, the initial contact and scheduling of the interview will go quickly and the process can be very robust based on the size of the company. Even a small company coming from big names can be concerned about the people brought in.\n3. The more towards early venture capital you are, the harder and more specific you can expect the hiring process to be because you\u2019ll need to have a very specific skillset to help bootstrap the employer. If it is something bigger like FAANG, they\u2019ll be more inclusive of different backgrounds and start off on the easier foot with something that would represent a leetcode question .\n4. Whoever it is though, they will hug to their process, so just give them the room they need to feel they\u2019re making the right decision as they will do the same for you. Keep applying in the meantime.\n5. Move towards the companies/positions that are incredibly fascinated with you as that\u2019s the level you\u2019re probably at for the moment.\n6. Understand which interviews you enjoyed the most and prioritize learning in that direction throughout the year as that\u2019s where the next step of your career probably is.\n7. See what you could chain into next based on the positions you get and relative volume of demand in the market using job boards like LinkedIn as you start to get a feel for how in demand that job role is.\n8. Get a feel too for how this current position prepares you for your next position. Maybe it lets you consolidate 3 resumes down to 1 for instance and be very specific on the skills you prep throughout the year in wait for the new role.\n9. At a certain point in the search, you\u2019ll start to notice some leads are consistently souring. Use that understanding to get a sense of how competitive the market is, but also what skills you lack so that you can grab an appropriate position to be successful in the future.\n10. Have fun with the process. It can be overall nailbiting a lot of the time, but everyone is working to everyone\u2019s elses\u2019 success. Keep up the momentum, but give each other room!\n11. As the leads start to wind down, feel free to get started closing the gap towards the next role. The quicker you grow, the faster they grow and the quicker the project will be done. Things might get a bit rambunctious, but focus on getting to the finish line and adding the completed tasks to a brag doc.\n12. You can have all the education in the world on a specific topic and the recruiters will still turn you down for unknown reasons once talking with a hiring manager. Once you get good, partner up with people in teams and push your skills to the next level, thereby alleviating any concerns had on their end. Keep in mind what the recruiter says about your experience as you will not get feedback past that.", "author_fullname": "t2_3mkgr19p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rules of the road navigating the tech layoffs and hiring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zopial", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671336423.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;I psuedo joined the tech layoff getting put into a free agency to apply anywhere by my company and have been joining the mad job rush getting into things. I thought to share some experiential learning I had going on that roughly equates to normal good practice, but is also a good reminder to always consider ourselves expendable and a free agent regardless of if we&amp;#39;re safe or not. I&amp;#39;m getting this quality checked on multiple communities and I&amp;#39;d love to hear your feedback.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;If your resume stands out, they have a specific need, and you fill it beautifully which you\u2019ll understand based on the job description, you can expect to get through the hiring process quickly.&lt;/li&gt;\n&lt;li&gt;If it\u2019s a not sure, the initial contact and scheduling of the interview will go quickly and the process can be very robust based on the size of the company. Even a small company coming from big names can be concerned about the people brought in.&lt;/li&gt;\n&lt;li&gt;The more towards early venture capital you are, the harder and more specific you can expect the hiring process to be because you\u2019ll need to have a very specific skillset to help bootstrap the employer. If it is something bigger like FAANG, they\u2019ll be more inclusive of different backgrounds and start off on the easier foot with something that would represent a leetcode question .&lt;/li&gt;\n&lt;li&gt;Whoever it is though, they will hug to their process, so just give them the room they need to feel they\u2019re making the right decision as they will do the same for you. Keep applying in the meantime.&lt;/li&gt;\n&lt;li&gt;Move towards the companies/positions that are incredibly fascinated with you as that\u2019s the level you\u2019re probably at for the moment.&lt;/li&gt;\n&lt;li&gt;Understand which interviews you enjoyed the most and prioritize learning in that direction throughout the year as that\u2019s where the next step of your career probably is.&lt;/li&gt;\n&lt;li&gt;See what you could chain into next based on the positions you get and relative volume of demand in the market using job boards like LinkedIn as you start to get a feel for how in demand that job role is.&lt;/li&gt;\n&lt;li&gt;Get a feel too for how this current position prepares you for your next position. Maybe it lets you consolidate 3 resumes down to 1 for instance and be very specific on the skills you prep throughout the year in wait for the new role.&lt;/li&gt;\n&lt;li&gt;At a certain point in the search, you\u2019ll start to notice some leads are consistently souring. Use that understanding to get a sense of how competitive the market is, but also what skills you lack so that you can grab an appropriate position to be successful in the future.&lt;/li&gt;\n&lt;li&gt;Have fun with the process. It can be overall nailbiting a lot of the time, but everyone is working to everyone\u2019s elses\u2019 success. Keep up the momentum, but give each other room!&lt;/li&gt;\n&lt;li&gt;As the leads start to wind down, feel free to get started closing the gap towards the next role. The quicker you grow, the faster they grow and the quicker the project will be done. Things might get a bit rambunctious, but focus on getting to the finish line and adding the completed tasks to a brag doc.&lt;/li&gt;\n&lt;li&gt;You can have all the education in the world on a specific topic and the recruiters will still turn you down for unknown reasons once talking with a hiring manager. Once you get good, partner up with people in teams and push your skills to the next level, thereby alleviating any concerns had on their end. Keep in mind what the recruiter says about your experience as you will not get feedback past that.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "zopial", "is_robot_indexable": true, "report_reasons": null, "author": "messerb5467", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zopial/rules_of_the_road_navigating_the_tech_layoffs_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zopial/rules_of_the_road_navigating_the_tech_layoffs_and/", "subreddit_subscribers": 827987, "created_utc": 1671336423.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_unl5xgdi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I had so much fun playing around with data from the World Cup to fit a random forrest model to predict who will win this weekends games! So far the model predicted todays accurately!!! Made a YouTube video sharing the code predictions here.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_zoci44", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/atuCThw4DVg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Using Python to predict who will win the 2022 FIFA World Cup\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Using Python to predict who will win the 2022 FIFA World Cup", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/atuCThw4DVg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Using Python to predict who will win the 2022 FIFA World Cup\"&gt;&lt;/iframe&gt;", "author_name": "Marlene Mhangami", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/atuCThw4DVg/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@marlenezw"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/atuCThw4DVg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Using Python to predict who will win the 2022 FIFA World Cup\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/zoci44", "height": 200}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/yvSG-GGPmOerfVkG4Ba-1g6Qn24X3JE3bQjhYxr-dM0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671301386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://youtube.com/watch?v=atuCThw4DVg&amp;feature=share", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nGHfhiT27yDFRcjotaHD-HrtvhmxtJas6S6fnQMxEAE.jpg?auto=webp&amp;s=33be6ebceb007d9376d88e2c9d3ceb2a823cf37c", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/nGHfhiT27yDFRcjotaHD-HrtvhmxtJas6S6fnQMxEAE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e458ecef93fdd18af037a69368c1e19ab6908ad8", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/nGHfhiT27yDFRcjotaHD-HrtvhmxtJas6S6fnQMxEAE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f1f9237f0c90a51053293fbe7702b6fb1955afc", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/nGHfhiT27yDFRcjotaHD-HrtvhmxtJas6S6fnQMxEAE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e4b52592d42b76155e9a64532ba301866666d15", "width": 320, "height": 240}], "variants": {}, "id": "0vHmlkeYPqAx4JMOn_G3acKBLqUQrMU7RnhcJ4MKyN0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoci44", "is_robot_indexable": true, "report_reasons": null, "author": "Python-programer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoci44/i_had_so_much_fun_playing_around_with_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtube.com/watch?v=atuCThw4DVg&amp;feature=share", "subreddit_subscribers": 827987, "created_utc": 1671301386.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Using Python to predict who will win the 2022 FIFA World Cup", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/atuCThw4DVg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Using Python to predict who will win the 2022 FIFA World Cup\"&gt;&lt;/iframe&gt;", "author_name": "Marlene Mhangami", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/atuCThw4DVg/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@marlenezw"}}, "is_video": false}}], "before": null}}