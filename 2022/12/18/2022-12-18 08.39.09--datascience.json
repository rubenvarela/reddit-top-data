{"kind": "Listing", "data": {"after": "t3_zo2pj1", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7tk61jlx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Offend a data scientist in one tweet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 133, "top_awarded_type": null, "hide_score": false, "name": "t3_zo5bwf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 1630, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 1630, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BSxLRIlq1njHZP5Z_wamCF9V9SfkDjm9KyuGwG8B2oM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671280215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/t7n4hi55uh6a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?auto=webp&amp;s=bac6161094a9a79489f52437166b8dcb7f64d3b6", "width": 1170, "height": 1113}, "resolutions": [{"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0808045bdcd76aa0c585fab3f629abbd08b9eb4a", "width": 108, "height": 102}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d062ac3047031d4f2a8bb9edd39a1395fa6b71b", "width": 216, "height": 205}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=960fe5dbb627fe1c085be0b1f0cd2cc981a065fc", "width": 320, "height": 304}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=103f470deb4cdda1eb9ce38819483792ba3ea0b8", "width": 640, "height": 608}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7628380a74c4952d35e2d439d930c7d9b3fa6cf9", "width": 960, "height": 913}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=41fbefb81a833b413faf3f777ba8756ea5939573", "width": 1080, "height": 1027}], "variants": {}, "id": "cF2H-4KHwkY2zala2RSzU8criZ8swOdmyibBo-iHicE"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo5bwf", "is_robot_indexable": true, "report_reasons": null, "author": "datasciencepro", "discussion_type": null, "num_comments": 162, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo5bwf/offend_a_data_scientist_in_one_tweet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/t7n4hi55uh6a1.jpg", "subreddit_subscribers": 827971, "created_utc": 1671280215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_f900b52f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zo5cll", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 449, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 449, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/s0ynn7G8nXbTR_uXt90gEhSZHUL0mLo7H8bhVRTMixY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_1": 1}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671280285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/49ppdolcuh6a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?auto=webp&amp;s=a7a64c1a98fd1a74d3e4355ebccf288513ca89bb", "width": 1080, "height": 1920}, "resolutions": [{"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=30310c228dfd08348c71abb5fb5527956974f114", "width": 108, "height": 192}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=913abe79dc9b0a53473b7089eccc385ca0a071e2", "width": 216, "height": 384}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d9eb5d70911cce318caf1b6c5c8eae9f1ebd5b46", "width": 320, "height": 568}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=12e7b553a6788b9cf808a1629bbe8594e5c4e1e4", "width": 640, "height": 1137}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=afbf3adf5602589e4b4e263d6ff360a7a427131d", "width": 960, "height": 1706}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf90a16c987c9f4074158f0ba3347e2f0e27f4f5", "width": 1080, "height": 1920}], "variants": {}, "id": "W-uSuaiYuKbO5KZaKtG_pZhr1FKoXou0FiZEo6YQpfY"}], "enabled": true}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 100, "id": "gid_1", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Shows the Silver Award... and that's it.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Silver", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo5cll", "is_robot_indexable": true, "report_reasons": null, "author": "DwightScott69", "discussion_type": null, "num_comments": 139, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo5cll/thoughts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/49ppdolcuh6a1.jpg", "subreddit_subscribers": 827971, "created_utc": 1671280285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For context, I'm currently finishing my bachelor's degree in electrical engineering and I just completed my minor in data science (i.e. I finished the last course required to satisfy the minor's requirements).  I found I like the data science stuff significantly more than EE, but I'm too far along to even consider switching majors at this point.  Hence, I'm trying to self-teach additional data science skills and I know being to use SQL and work with databases (something none of my DS courses covered unfortunately) in particular is a vital skill to have if I have any hope of getting a job in DS. \n\nI posted previously about this and I got a ton of responses with people recommending so many different learning platforms and several different API's and DBMS's that I'm a little unsure where to start.  I started just reading about what databases even _are_ so I can have a clear mental model in my head, but now I'm struggling to decide how to actually get started with SQL itself.  \n\nThe easiest thing (and hence what I'm tempted to do) would probably be to use one of the Python API's people recommended, just because I already have some experience using Python for data cleaning, exploration, and analysis, and I have Python fully set-up on my system already (and getting everything set up to use _any_ new programming language is typically a pain).  But is that a good idea, seeing as this will be the first time I've used SQL?  Will it it hurt me later on if I get used to just using Python to call SQL rather than learning how to use it directly?  Like, would prospective employers be less likely to higher me if I only have experience using SQL via Python, or will there be things I can't do through the API?  Or am I just completely overthinking this and it doesn't really matter whether I use SQL directly or indirectly?", "author_fullname": "t2_16ojfv6p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm attempting to self-teach SQL. If I already know already know Python, should I start by using a Python API for SQL or would that handicap me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zon2g1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671328853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context, I&amp;#39;m currently finishing my bachelor&amp;#39;s degree in electrical engineering and I just completed my minor in data science (i.e. I finished the last course required to satisfy the minor&amp;#39;s requirements).  I found I like the data science stuff significantly more than EE, but I&amp;#39;m too far along to even consider switching majors at this point.  Hence, I&amp;#39;m trying to self-teach additional data science skills and I know being to use SQL and work with databases (something none of my DS courses covered unfortunately) in particular is a vital skill to have if I have any hope of getting a job in DS. &lt;/p&gt;\n\n&lt;p&gt;I posted previously about this and I got a ton of responses with people recommending so many different learning platforms and several different API&amp;#39;s and DBMS&amp;#39;s that I&amp;#39;m a little unsure where to start.  I started just reading about what databases even &lt;em&gt;are&lt;/em&gt; so I can have a clear mental model in my head, but now I&amp;#39;m struggling to decide how to actually get started with SQL itself.  &lt;/p&gt;\n\n&lt;p&gt;The easiest thing (and hence what I&amp;#39;m tempted to do) would probably be to use one of the Python API&amp;#39;s people recommended, just because I already have some experience using Python for data cleaning, exploration, and analysis, and I have Python fully set-up on my system already (and getting everything set up to use &lt;em&gt;any&lt;/em&gt; new programming language is typically a pain).  But is that a good idea, seeing as this will be the first time I&amp;#39;ve used SQL?  Will it it hurt me later on if I get used to just using Python to call SQL rather than learning how to use it directly?  Like, would prospective employers be less likely to higher me if I only have experience using SQL via Python, or will there be things I can&amp;#39;t do through the API?  Or am I just completely overthinking this and it doesn&amp;#39;t really matter whether I use SQL directly or indirectly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zon2g1", "is_robot_indexable": true, "report_reasons": null, "author": "dcfan105", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zon2g1/im_attempting_to_selfteach_sql_if_i_already_know/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zon2g1/im_attempting_to_selfteach_sql_if_i_already_know/", "subreddit_subscribers": 827971, "created_utc": 1671328853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I\u2019ve previously worked as a Data Scientist mainly with NLP and most recently I\u2019ve been working as a Data Engineer. I will be joining a new team as a Data Scientist in energy sector, specifically HVAC. What kind of topics should I prepare myself from technical point of view, my Python, SQL skills are above average or average.", "author_fullname": "t2_90pyvsip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone with data scientist experience in energy sector?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zohi08", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671314410.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I\u2019ve previously worked as a Data Scientist mainly with NLP and most recently I\u2019ve been working as a Data Engineer. I will be joining a new team as a Data Scientist in energy sector, specifically HVAC. What kind of topics should I prepare myself from technical point of view, my Python, SQL skills are above average or average.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zohi08", "is_robot_indexable": true, "report_reasons": null, "author": "Gagan_Ku2905", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zohi08/anyone_with_data_scientist_experience_in_energy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zohi08/anyone_with_data_scientist_experience_in_energy/", "subreddit_subscribers": 827971, "created_utc": 1671314410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What are some common functions/transformations you run on a dataset before training and what libraries do you use, or is it a custom script?", "author_fullname": "t2_6i7on", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Common dataset prep operations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo2tb9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671269962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some common functions/transformations you run on a dataset before training and what libraries do you use, or is it a custom script?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo2tb9", "is_robot_indexable": true, "report_reasons": null, "author": "jy2k", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo2tb9/common_dataset_prep_operations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo2tb9/common_dataset_prep_operations/", "subreddit_subscribers": 827971, "created_utc": 1671269962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve seen companies hiring for all 4 at once at a single location so I\u2019m curious how the responsibilities would be divided. I feel there still lacks universal definitions for these roles.", "author_fullname": "t2_524iawau", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If a company has a Data Engineer, Analyst, and a ML Engineer, what does the Data Scientist do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo99yd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671292687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve seen companies hiring for all 4 at once at a single location so I\u2019m curious how the responsibilities would be divided. I feel there still lacks universal definitions for these roles.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo99yd", "is_robot_indexable": true, "report_reasons": null, "author": "bassabyss", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo99yd/if_a_company_has_a_data_engineer_analyst_and_a_ml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo99yd/if_a_company_has_a_data_engineer_analyst_and_a_ml/", "subreddit_subscribers": 827971, "created_utc": 1671292687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "question is a bit confusing, but let me explain. I study data science at university, but it seems to be a lot more data analysis than actual data science (for now, at least) but I still want to end up in the field of data science and have an internship under my belt before I graduate (I'm a junior rn). So what are some concepts I should know, and how well should I know them?\n\nFor example, within machine learning (or other concepts), which models should I know and how well should I know them?\n\nLet me know if I'm still confusing lol.. and any other advice is much appreciated ! - sincerely, a struggling college student.", "author_fullname": "t2_55jsie1i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "weird, open-ended question: How good at DS do I need to be to get a DS internship?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zorihl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671343191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;question is a bit confusing, but let me explain. I study data science at university, but it seems to be a lot more data analysis than actual data science (for now, at least) but I still want to end up in the field of data science and have an internship under my belt before I graduate (I&amp;#39;m a junior rn). So what are some concepts I should know, and how well should I know them?&lt;/p&gt;\n\n&lt;p&gt;For example, within machine learning (or other concepts), which models should I know and how well should I know them?&lt;/p&gt;\n\n&lt;p&gt;Let me know if I&amp;#39;m still confusing lol.. and any other advice is much appreciated ! - sincerely, a struggling college student.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zorihl", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway_jfkdhsmdns", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zorihl/weird_openended_question_how_good_at_ds_do_i_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zorihl/weird_openended_question_how_good_at_ds_do_i_need/", "subreddit_subscribers": 827971, "created_utc": 1671343191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m looking for some data on p&amp;c insurance losses (claim/no-claim; claim size; peril; date; address). Does anyone know if there is a public data base with this?", "author_fullname": "t2_91itiala", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "P&amp;C Claim Data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zom9b9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671326434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking for some data on p&amp;amp;c insurance losses (claim/no-claim; claim size; peril; date; address). Does anyone know if there is a public data base with this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zom9b9", "is_robot_indexable": true, "report_reasons": null, "author": "MGeeeeeezy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zom9b9/pc_claim_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zom9b9/pc_claim_data/", "subreddit_subscribers": 827971, "created_utc": 1671326434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm curious what folks in larger companies have found useful for working around/with encrypted data within a database. Not passwords but other types of controlled info \u2013 device checkins, friend lists, etc.\n\nDoes your security team have a process/tool in place to access the decryption key(s) from your code? Are there other limits on what you can do/can't do?", "author_fullname": "t2_5z2ths3h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your workflow for decrypting PII fields from a database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zodofx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671304493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious what folks in larger companies have found useful for working around/with encrypted data within a database. Not passwords but other types of controlled info \u2013 device checkins, friend lists, etc.&lt;/p&gt;\n\n&lt;p&gt;Does your security team have a process/tool in place to access the decryption key(s) from your code? Are there other limits on what you can do/can&amp;#39;t do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zodofx", "is_robot_indexable": true, "report_reasons": null, "author": "rszumski", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zodofx/what_is_your_workflow_for_decrypting_pii_fields/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zodofx/what_is_your_workflow_for_decrypting_pii_fields/", "subreddit_subscribers": 827971, "created_utc": 1671304493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My sibling has a career in data science and machine learning and they are really into it, it is their hobby too. Any ideas what I cam get them?\n\nThey won\u2019t like knick knacks like a mug with an AI generated dog, or a t shirt. They would just throw that out. What would be a great gift is something (not a book) that allows them to explore, learn, or experiment with ai more.", "author_fullname": "t2_qgco0kmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What can I get my sibling who is an AI data scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoagmh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671295838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My sibling has a career in data science and machine learning and they are really into it, it is their hobby too. Any ideas what I cam get them?&lt;/p&gt;\n\n&lt;p&gt;They won\u2019t like knick knacks like a mug with an AI generated dog, or a t shirt. They would just throw that out. What would be a great gift is something (not a book) that allows them to explore, learn, or experiment with ai more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoagmh", "is_robot_indexable": true, "report_reasons": null, "author": "CoupleConsistent5378", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoagmh/what_can_i_get_my_sibling_who_is_an_ai_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoagmh/what_can_i_get_my_sibling_who_is_an_ai_data/", "subreddit_subscribers": 827971, "created_utc": 1671295838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_2doc5c82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transformers for Machine Learning (2022)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_zotbpl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/qt64heEh8DXHjsk_fZ31AZdZ52Tg6Kl_kOLNxWkH7a8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671350175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cuty.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://cuty.io/pQLyTvMxHr", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?auto=webp&amp;s=cadc6355acccda8a915c9a27230a40bd0ad25087", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=688d3d9620e71e28c73184125b67407ff00d5e6f", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c90a5ca5e086aa4f555a1f719cf8fd71238ce205", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6dcec763df9a593f9aafc67959a1b32ac25b50d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3234d9e6a4f5ea26bf8daf2a79a2f859f4df27b", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=668604c1ca449f34b7c56acb5ad924183e918169", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16145ef42ec3e79256b058e582d8ff6a74099617", "width": 1080, "height": 567}], "variants": {}, "id": "Y2JI1Q-WUrM7tt6rce3htGGH9cvN7fkQISE5mrc8-FU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zotbpl", "is_robot_indexable": true, "report_reasons": null, "author": "HusseinSheikho", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zotbpl/transformers_for_machine_learning_2022/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://cuty.io/pQLyTvMxHr", "subreddit_subscribers": 827971, "created_utc": 1671350175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_2doc5c82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Lead in Data Science (2022)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_zot2q7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/S4RNPbJM5P7gUax7qodXtZnTZoxiqOrCY6B26dzM180.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671349167.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cuty.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://cuty.io/AWuvYAj0sH", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?auto=webp&amp;s=cadc6355acccda8a915c9a27230a40bd0ad25087", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=688d3d9620e71e28c73184125b67407ff00d5e6f", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c90a5ca5e086aa4f555a1f719cf8fd71238ce205", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6dcec763df9a593f9aafc67959a1b32ac25b50d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3234d9e6a4f5ea26bf8daf2a79a2f859f4df27b", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=668604c1ca449f34b7c56acb5ad924183e918169", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oQ0Rn4qhp7ZNWXarEfbG_ihLOJ-5d5cZRFEJWSJ46hc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16145ef42ec3e79256b058e582d8ff6a74099617", "width": 1080, "height": 567}], "variants": {}, "id": "Y2JI1Q-WUrM7tt6rce3htGGH9cvN7fkQISE5mrc8-FU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zot2q7", "is_robot_indexable": true, "report_reasons": null, "author": "HusseinSheikho", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zot2q7/how_to_lead_in_data_science_2022/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://cuty.io/AWuvYAj0sH", "subreddit_subscribers": 827971, "created_utc": 1671349167.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is there a place to find portfolio examples? How do people usually attach a portfolio to a resume? What do these portfolios look like? How are they optimized? \n\nI'm about to start my final year of a Bachelors in Data Science and want to start trying to find a full / part-time (not just seasonal) internship/job. I have seen a handful of the hiring managers in this sub say they do check the portfolios, and sometimes they do more damage than good. I was hoping to find some examples of great portfolios to compare to mine. \n\nTIA", "author_fullname": "t2_15elnh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Portfolio Examples", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zos2gd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671345201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a place to find portfolio examples? How do people usually attach a portfolio to a resume? What do these portfolios look like? How are they optimized? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m about to start my final year of a Bachelors in Data Science and want to start trying to find a full / part-time (not just seasonal) internship/job. I have seen a handful of the hiring managers in this sub say they do check the portfolios, and sometimes they do more damage than good. I was hoping to find some examples of great portfolios to compare to mine. &lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zos2gd", "is_robot_indexable": true, "report_reasons": null, "author": "RunescapeJoe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zos2gd/portfolio_examples/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zos2gd/portfolio_examples/", "subreddit_subscribers": 827971, "created_utc": 1671345201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Current neuroscience major. Would it be viable to apply to data science masters programs with this degree? I was thinking of adding a CS minor too.", "author_fullname": "t2_wpbwvz9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Neuroscience major -&gt; data science masters?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoryyn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671344846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Current neuroscience major. Would it be viable to apply to data science masters programs with this degree? I was thinking of adding a CS minor too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoryyn", "is_robot_indexable": true, "report_reasons": null, "author": "qsauce6", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoryyn/neuroscience_major_data_science_masters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoryyn/neuroscience_major_data_science_masters/", "subreddit_subscribers": 827971, "created_utc": 1671344846.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_lck7n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Becoming a better programmer is twice as hard as just writing code. This is how I will sum up my learning of the past 30 years.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_zomtw5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2nMUqCSCP8aQRQB4QROhtnh0218LmurcEun03fdsUOI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671328151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "link.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://link.medium.com/9i9BBxu1Nvb", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?auto=webp&amp;s=4e38a91c86986c5fb593b6dd325f56fab6f68f47", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1da8588eee88de02d759267fe6be2b0c39216814", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=57d81a4a7abd09f4177018e245b1a7b22a52c39d", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ddcbe6e6b5137a38107cd6821bf7ba34e08b846", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=900b04e19b8ec42294770880b325da4742626500", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ab157542147f7a54674b3bedd8dc9edf2e3692f8", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a1176e703b1f9bead39076a66735df27b23a039", "width": 1080, "height": 720}], "variants": {}, "id": "XH0z2WesZmlFmuitxqj7OA-8eBZPyjuDh8kVOEqUXmU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zomtw5", "is_robot_indexable": true, "report_reasons": null, "author": "ishwarjha", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zomtw5/becoming_a_better_programmer_is_twice_as_hard_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://link.medium.com/9i9BBxu1Nvb", "subreddit_subscribers": 827971, "created_utc": 1671328151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys,\nFrom your experience, what models would you consider for product assortment/range optimisation for FMCG retail sector\n( i.e getting the right product range in the right store/ product placement).\n\nI\u2019ve seen some use XGBoost for product placement, but curious to know what other models work well.\n\nI know we have to take product substitution into consideration so Customer Decision Tree Model is already taken care of.", "author_fullname": "t2_idusno00", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Product assortment/range optimisation for FMCG", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zolerx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671324227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,\nFrom your experience, what models would you consider for product assortment/range optimisation for FMCG retail sector\n( i.e getting the right product range in the right store/ product placement).&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen some use XGBoost for product placement, but curious to know what other models work well.&lt;/p&gt;\n\n&lt;p&gt;I know we have to take product substitution into consideration so Customer Decision Tree Model is already taken care of.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zolerx", "is_robot_indexable": true, "report_reasons": null, "author": "Maria_Adel", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zolerx/product_assortmentrange_optimisation_for_fmcg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zolerx/product_assortmentrange_optimisation_for_fmcg/", "subreddit_subscribers": 827971, "created_utc": 1671324227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Working on a time series classification project. I\u2019m going through the sktime library and reading each and every classifiers summary about the method that\u2019s being used under the hood to fit the model, and I\u2019m basing my choices based on how long it may take to train. For example, half of them are distance based (nearest neighbors) methods, and I know for my dataset a distance matrix (and most data in general) just takes a stupid amount of time to compute. Or some do Shapelet discovery, which is traversing the entire subsequence space and computing distances between the subsequence and original series and using those subsequences (shapelets) to predict, which also, takes a long time. So I\u2019m just avoiding using them for others (interval based)\n\nDoes anyone else do this? Is this a bad practice?", "author_fullname": "t2_i69qgpqa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching models because a model takes too long to train", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zogrsk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671312539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a time series classification project. I\u2019m going through the sktime library and reading each and every classifiers summary about the method that\u2019s being used under the hood to fit the model, and I\u2019m basing my choices based on how long it may take to train. For example, half of them are distance based (nearest neighbors) methods, and I know for my dataset a distance matrix (and most data in general) just takes a stupid amount of time to compute. Or some do Shapelet discovery, which is traversing the entire subsequence space and computing distances between the subsequence and original series and using those subsequences (shapelets) to predict, which also, takes a long time. So I\u2019m just avoiding using them for others (interval based)&lt;/p&gt;\n\n&lt;p&gt;Does anyone else do this? Is this a bad practice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zogrsk", "is_robot_indexable": true, "report_reasons": null, "author": "AdFew4357", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zogrsk/switching_models_because_a_model_takes_too_long/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zogrsk/switching_models_because_a_model_takes_too_long/", "subreddit_subscribers": 827971, "created_utc": 1671312539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello scientists,\n\nI have some basic data, more like a very basic personal project, and I want to find a way how to visualize it in a nice way. A three circle Venn diagram is not enough, I will need something like a \"oval flower model\" with multiple intertwining and mutually intersecting fields.\n\nThe basic tools online and in Excel seem not to be self adjusting based on how much text I want to put into the image (some fields require a lot of it). So I would like to find an online tool which can create a beautiful visual representation for me and which would automatically adjust the size of the diagram fields. \n\nI am not proficient yet in advanced tools or programming languages like R or Python, so I would like something simple.\n\nI am sure such a thing exist, I just suck a bit at finding it. It seems very easy, something that would look like this but that would automatically adjust the size and shape:\n\n[https://slidemodel.com/templates/4-set-venn-diagram-for-powerpoint/](https://slidemodel.com/templates/4-set-venn-diagram-for-powerpoint/)", "author_fullname": "t2_pm1g85e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Self-adjusting Venn and other diagram TOOL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zofz2p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671310504.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello scientists,&lt;/p&gt;\n\n&lt;p&gt;I have some basic data, more like a very basic personal project, and I want to find a way how to visualize it in a nice way. A three circle Venn diagram is not enough, I will need something like a &amp;quot;oval flower model&amp;quot; with multiple intertwining and mutually intersecting fields.&lt;/p&gt;\n\n&lt;p&gt;The basic tools online and in Excel seem not to be self adjusting based on how much text I want to put into the image (some fields require a lot of it). So I would like to find an online tool which can create a beautiful visual representation for me and which would automatically adjust the size of the diagram fields. &lt;/p&gt;\n\n&lt;p&gt;I am not proficient yet in advanced tools or programming languages like R or Python, so I would like something simple.&lt;/p&gt;\n\n&lt;p&gt;I am sure such a thing exist, I just suck a bit at finding it. It seems very easy, something that would look like this but that would automatically adjust the size and shape:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://slidemodel.com/templates/4-set-venn-diagram-for-powerpoint/\"&gt;https://slidemodel.com/templates/4-set-venn-diagram-for-powerpoint/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?auto=webp&amp;s=b8c436d0f16f68aeff3f032511de8c58efd15670", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e789ac40ee9c21361d89a6fbec1ca0b013ff3582", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=056a01229ae1d234fac7fcd7729803138e5f7372", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50f6d2f4f9b8bd285b7baa8c2693c1da7c6e0687", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=687fa2608a2b4bad7abcd89b9b81f683c5e6ea5f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2bc1dcb1e475f1203f6f0e0dcfaf36862d372525", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9f34f65b6861adc750282491554af75ea2c5c6a1", "width": 1080, "height": 607}], "variants": {}, "id": "pENjWJRqsa-GYfZ9DXYXCN3hnLoi0pZG0UcoOiIeSQM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zofz2p", "is_robot_indexable": true, "report_reasons": null, "author": "Sheetmusicman94", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zofz2p/selfadjusting_venn_and_other_diagram_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zofz2p/selfadjusting_venn_and_other_diagram_tool/", "subreddit_subscribers": 827971, "created_utc": 1671310504.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to set up an unsupervised learning project for anomaly detection and I'm learning along the way. Since I haven't got any previous experience with unsupervised experiments, I have what is probably a very basic question with regard to setting up my experiment: is the basic approach for unsupervised learning broadly comparable to that of supervised learning?\n\nMy intent is to go with a data split of 70:15:15 (train/test/holdout) and do an EDA on the data. I want to check how meaningful certain features are, whether they correlate etc.. Then I'll drop features and/or engineer derived features and see how they cluster and whether PCA will result in any insight with regard to feature relevance?\n\nSince I'll likely use the insight of the PCA to inform the feature engineering, I think a holdout data set is necessary - at least in theory. However, since anomalies are both \\_anomalous\\_ and sparse, I'm not really sure whether this can actually be practically implemented.\n\nAny pointers to best practices or or design philosophies would be much appreciate by this unsupervised newbie. ;)", "author_fullname": "t2_4j7ujk5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I also use a train/test/holdout split in unsupervised learning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoaan4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671295389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to set up an unsupervised learning project for anomaly detection and I&amp;#39;m learning along the way. Since I haven&amp;#39;t got any previous experience with unsupervised experiments, I have what is probably a very basic question with regard to setting up my experiment: is the basic approach for unsupervised learning broadly comparable to that of supervised learning?&lt;/p&gt;\n\n&lt;p&gt;My intent is to go with a data split of 70:15:15 (train/test/holdout) and do an EDA on the data. I want to check how meaningful certain features are, whether they correlate etc.. Then I&amp;#39;ll drop features and/or engineer derived features and see how they cluster and whether PCA will result in any insight with regard to feature relevance?&lt;/p&gt;\n\n&lt;p&gt;Since I&amp;#39;ll likely use the insight of the PCA to inform the feature engineering, I think a holdout data set is necessary - at least in theory. However, since anomalies are both _anomalous_ and sparse, I&amp;#39;m not really sure whether this can actually be practically implemented.&lt;/p&gt;\n\n&lt;p&gt;Any pointers to best practices or or design philosophies would be much appreciate by this unsupervised newbie. ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoaan4", "is_robot_indexable": true, "report_reasons": null, "author": "norfkens2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoaan4/do_i_also_use_a_traintestholdout_split_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoaan4/do_i_also_use_a_traintestholdout_split_in/", "subreddit_subscribers": 827971, "created_utc": 1671295389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Check out out, would you use this?\n\nhttps://github.com/aws-samples/distributed-compute-on-aws-with-cross-regional-dask", "author_fullname": "t2_v1f56kco", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cross Regional Dask on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo8v5e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671291531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check out out, would you use this?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/aws-samples/distributed-compute-on-aws-with-cross-regional-dask\"&gt;https://github.com/aws-samples/distributed-compute-on-aws-with-cross-regional-dask&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?auto=webp&amp;s=2007921aefa32104b16120fdcda67f25052c18d3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0d2363e91de15e7251f6ad01a45ba9d7cd5b36c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef2d65fc0163c885d031bf3b3742c1c90e71a245", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c83d83b48aaaca832314c54b7e208d9414432ff", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8ef536a4b30cac8de2b8c1256e74a37aece58c8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=23049cb7c3f55d216d707f36a48e1fda6e7a758f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02a1f497a8851982103795c3df6c1b2e3029757e", "width": 1080, "height": 540}], "variants": {}, "id": "d7SCmwvPs0_VsnLQQ01R-H3jpt4LRVP6Zmmmk5Icd0c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo8v5e", "is_robot_indexable": true, "report_reasons": null, "author": "oconpa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo8v5e/cross_regional_dask_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo8v5e/cross_regional_dask_on_aws/", "subreddit_subscribers": 827971, "created_utc": 1671291531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,  \nCurrently I am researching extreme learning machines, and - after I calculated the beta weights  - I would like to optimize the alpha weights (similarly with the Moore-Penrose pseudo inverse) in order to receive better accuracy.  \nDuring this backward calculation, I use the inverse of the previously applied activation function. In case of the inverse sigmoid and tanh (logit and arctan), I receive NaN values, although with the inverse leaky ReLU, on certain datasets, the method yields better accuracy. Unfortunately, in most cases, the accuracy drops (sometimes significantly) with these new optimized alpha weights. What do you think, what is the reason for this?", "author_fullname": "t2_kq8l2zbu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inverse activation function", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo7wjc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671288767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;br/&gt;\nCurrently I am researching extreme learning machines, and - after I calculated the beta weights  - I would like to optimize the alpha weights (similarly with the Moore-Penrose pseudo inverse) in order to receive better accuracy.&lt;br/&gt;\nDuring this backward calculation, I use the inverse of the previously applied activation function. In case of the inverse sigmoid and tanh (logit and arctan), I receive NaN values, although with the inverse leaky ReLU, on certain datasets, the method yields better accuracy. Unfortunately, in most cases, the accuracy drops (sometimes significantly) with these new optimized alpha weights. What do you think, what is the reason for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo7wjc", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical-Plane3730", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo7wjc/inverse_activation_function/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo7wjc/inverse_activation_function/", "subreddit_subscribers": 827971, "created_utc": 1671288767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to do a new project in R, where I'm working with a lot of data (about 6GB of text) my inefficient code is tying me down a lot and was hoping maybe you guys had some tricks.\n\nI'm supposed to load in large .txt files, each file should be split into shorter parts consisting of 10 words each, they are then put into my data frame with each word being put into one of 10 columns. \n\nThe problem is that my code is slow, like really slow. It takes around 20 minutes to load the first 100000 rows of text into my dataframe, which is less than 1% of my data. I have also noticed that code slows down the further it takes to run, it takes about a second to load in the first 3000 rows, and 15 seconds for the first 10000 rows.   \n\nWhat is the main bottleneck slowing me down? Should I just rewrite the code entirely in another language like python?\n\nMy current code below:\n\n    #setup\n    folder &lt;- \"C:/Users/A_tiny/\"\n    \n    # Get a list of files in the folder\n    files &lt;- list.files(folder)\n    \n    # Create an empty data frame to hold the indexed words\n    df &lt;- data.frame(word1 = character(), word2 = character(), word3 = character(), word4 = character(),\n    word5 = character(), word6 = character(), word7 = character(), word8 = character(),\n    word9 = character(), word10 = character(), index = integer())\n    \n    for (file in files) {\n    # Check if the file is a .epub.txt file\n    if (grepl(\".epub.txt$\", file)) {\n    # Read the .epub.txt file into a string variable\n    epub_txt &lt;- readLines(paste0(folder, file))\n    \n    # Split the string into a vector of words\n    words &lt;- unlist(strsplit(epub_txt, \" \"))\n    \n    # Loop through the vector of words and add the next 10 words to the data frame, skipping 10 words at a time\n    for (i in seq(1, length(words), by=10)) {\n      # Create a data frame with the next 10 words and the index\n      df_temp &lt;- data.frame(word1 = words[i], word2 = words[i+1], word3 = words[i+2], word4 = words[i+3],\n                            word5 = words[i+4], word6 = words[i+5], word7 = words[i+6], word8 = words[i+7],\n                            word9 = words[i+8], word10 = words[i+9], index = i)\n      # Add the data frame to the main data frame\n      df &lt;- rbind(df, df_temp)\n    }\n      }\n    }", "author_fullname": "t2_gx7hn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The best approach for rewriting my code more efficiently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo4p8z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671277772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to do a new project in R, where I&amp;#39;m working with a lot of data (about 6GB of text) my inefficient code is tying me down a lot and was hoping maybe you guys had some tricks.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m supposed to load in large .txt files, each file should be split into shorter parts consisting of 10 words each, they are then put into my data frame with each word being put into one of 10 columns. &lt;/p&gt;\n\n&lt;p&gt;The problem is that my code is slow, like really slow. It takes around 20 minutes to load the first 100000 rows of text into my dataframe, which is less than 1% of my data. I have also noticed that code slows down the further it takes to run, it takes about a second to load in the first 3000 rows, and 15 seconds for the first 10000 rows.   &lt;/p&gt;\n\n&lt;p&gt;What is the main bottleneck slowing me down? Should I just rewrite the code entirely in another language like python?&lt;/p&gt;\n\n&lt;p&gt;My current code below:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#setup\nfolder &amp;lt;- &amp;quot;C:/Users/A_tiny/&amp;quot;\n\n# Get a list of files in the folder\nfiles &amp;lt;- list.files(folder)\n\n# Create an empty data frame to hold the indexed words\ndf &amp;lt;- data.frame(word1 = character(), word2 = character(), word3 = character(), word4 = character(),\nword5 = character(), word6 = character(), word7 = character(), word8 = character(),\nword9 = character(), word10 = character(), index = integer())\n\nfor (file in files) {\n# Check if the file is a .epub.txt file\nif (grepl(&amp;quot;.epub.txt$&amp;quot;, file)) {\n# Read the .epub.txt file into a string variable\nepub_txt &amp;lt;- readLines(paste0(folder, file))\n\n# Split the string into a vector of words\nwords &amp;lt;- unlist(strsplit(epub_txt, &amp;quot; &amp;quot;))\n\n# Loop through the vector of words and add the next 10 words to the data frame, skipping 10 words at a time\nfor (i in seq(1, length(words), by=10)) {\n  # Create a data frame with the next 10 words and the index\n  df_temp &amp;lt;- data.frame(word1 = words[i], word2 = words[i+1], word3 = words[i+2], word4 = words[i+3],\n                        word5 = words[i+4], word6 = words[i+5], word7 = words[i+6], word8 = words[i+7],\n                        word9 = words[i+8], word10 = words[i+9], index = i)\n  # Add the data frame to the main data frame\n  df &amp;lt;- rbind(df, df_temp)\n}\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo4p8z", "is_robot_indexable": true, "report_reasons": null, "author": "_Just7_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo4p8z/the_best_approach_for_rewriting_my_code_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo4p8z/the_best_approach_for_rewriting_my_code_more/", "subreddit_subscribers": 827971, "created_utc": 1671277772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7da18ldj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "What do you think about ChatGPT? To me, it's interesting. I'm new to programming, so I d like to know what you guys think about this. Ignore that Activate windows watermark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 66, "top_awarded_type": null, "hide_score": true, "media_metadata": {"rbxnz3ur0m6a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 51, "x": 108, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33dea72aa634b10ba69aab1291d3911edb89a90b"}, {"y": 103, "x": 216, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=99137500383ab9750749f5bfc7b32bf7cb2b00fc"}, {"y": 152, "x": 320, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=27bf8ea1d25b3942a94f127f76cdf790c6114429"}, {"y": 305, "x": 640, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e18c2ccb2f75ec6ebcb1c387fcdddc623e33e8c"}, {"y": 458, "x": 960, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bcecfbc2c291bfd8efaeaca82c639e6ee7af46fe"}, {"y": 515, "x": 1080, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4c162274d8827eb86f8ab61726c826f094e76242"}], "s": {"y": 651, "x": 1364, "u": "https://preview.redd.it/rbxnz3ur0m6a1.png?width=1364&amp;format=png&amp;auto=webp&amp;s=af5c668e8597dcdc660eae915653af3b7c0b0987"}, "id": "rbxnz3ur0m6a1"}, "gf0yjwtr0m6a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6fec6936e2ce01c780704dd39e0dbcc123d81ff"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e0ab4ba0b0ecd61c0254e84e103c928d8fb889b"}, {"y": 154, "x": 320, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f1f2d099940f5c8b97b32fd53e1bfeaba37717f"}, {"y": 308, "x": 640, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed1baceaba7483280ad2d9c35c9c39c9f6b687a6"}, {"y": 463, "x": 960, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55e3579fc161af2b068cc8461c08dd195d9daf5b"}, {"y": 521, "x": 1080, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1dbf37712aa01b438cb4fa417a3731ae4a322409"}], "s": {"y": 659, "x": 1366, "u": "https://preview.redd.it/gf0yjwtr0m6a1.png?width=1366&amp;format=png&amp;auto=webp&amp;s=c436a25d418e50e1d3683e8b6e2222278e5e5efb"}, "id": "gf0yjwtr0m6a1"}}, "name": "t3_zot4ny", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "rbxnz3ur0m6a1", "id": 220367381}, {"media_id": "gf0yjwtr0m6a1", "id": 220367382}]}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/MTKUc86iRVElQIxSB8QRxNtH8xfBJnlARxuoIa4MFIQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671349377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zot4ny", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": null, "id": "zot4ny", "is_robot_indexable": true, "report_reasons": null, "author": "deadlyb0y", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zot4ny/what_do_you_think_about_chatgpt_to_me_its/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zot4ny", "subreddit_subscribers": 827971, "created_utc": 1671349377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am at company A an Associate Director DS since 1.8 years, with no guarantee to move to director anytime soon. I have been offered a position of Principal DS at company B with 13% salary increase, sign-in bonus and payout of 70% of my unvested stocks value. Pay wise it is an upgrade.", "author_fullname": "t2_6kg9d73t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a downgrade going from Associate Director DS to Principal DS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoctli", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671302228.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am at company A an Associate Director DS since 1.8 years, with no guarantee to move to director anytime soon. I have been offered a position of Principal DS at company B with 13% salary increase, sign-in bonus and payout of 70% of my unvested stocks value. Pay wise it is an upgrade.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoctli", "is_robot_indexable": true, "report_reasons": null, "author": "IllustratorInfamous1", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoctli/is_it_a_downgrade_going_from_associate_director/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoctli/is_it_a_downgrade_going_from_associate_director/", "subreddit_subscribers": 827971, "created_utc": 1671302228.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone, does anyone know if chatGPT has been trained on Kaggle projects? If so, it should already be pretty good at a lot of DS stuff, right?\n\nIf not, I think it's only a matter of time before they will include that, which could create a very powerful DS personal assistant.\n\nI guess it could be challenging to train it on large datasets specifically, but I'm sure there are some smart ways to make that part more efficient, like only using a sample of each data set. Plus, there is the legal question if Kaggle would allow openAI to use their data.\n\nDo you guys have any thoughts?", "author_fullname": "t2_12hyas", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Was ChatGPT trained on Kaggle and other DS coding platforms?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo2pj1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671269496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, does anyone know if chatGPT has been trained on Kaggle projects? If so, it should already be pretty good at a lot of DS stuff, right?&lt;/p&gt;\n\n&lt;p&gt;If not, I think it&amp;#39;s only a matter of time before they will include that, which could create a very powerful DS personal assistant.&lt;/p&gt;\n\n&lt;p&gt;I guess it could be challenging to train it on large datasets specifically, but I&amp;#39;m sure there are some smart ways to make that part more efficient, like only using a sample of each data set. Plus, there is the legal question if Kaggle would allow openAI to use their data.&lt;/p&gt;\n\n&lt;p&gt;Do you guys have any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo2pj1", "is_robot_indexable": true, "report_reasons": null, "author": "ikke89", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo2pj1/was_chatgpt_trained_on_kaggle_and_other_ds_coding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo2pj1/was_chatgpt_trained_on_kaggle_and_other_ds_coding/", "subreddit_subscribers": 827971, "created_utc": 1671269496.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}