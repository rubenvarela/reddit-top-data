{"kind": "Listing", "data": {"after": "t3_zoctli", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7tk61jlx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Offend a data scientist in one tweet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 133, "top_awarded_type": null, "hide_score": false, "name": "t3_zo5bwf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 1514, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 1514, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BSxLRIlq1njHZP5Z_wamCF9V9SfkDjm9KyuGwG8B2oM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671280215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/t7n4hi55uh6a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?auto=webp&amp;s=bac6161094a9a79489f52437166b8dcb7f64d3b6", "width": 1170, "height": 1113}, "resolutions": [{"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0808045bdcd76aa0c585fab3f629abbd08b9eb4a", "width": 108, "height": 102}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d062ac3047031d4f2a8bb9edd39a1395fa6b71b", "width": 216, "height": 205}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=960fe5dbb627fe1c085be0b1f0cd2cc981a065fc", "width": 320, "height": 304}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=103f470deb4cdda1eb9ce38819483792ba3ea0b8", "width": 640, "height": 608}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7628380a74c4952d35e2d439d930c7d9b3fa6cf9", "width": 960, "height": 913}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=41fbefb81a833b413faf3f777ba8756ea5939573", "width": 1080, "height": 1027}], "variants": {}, "id": "cF2H-4KHwkY2zala2RSzU8criZ8swOdmyibBo-iHicE"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo5bwf", "is_robot_indexable": true, "report_reasons": null, "author": "datasciencepro", "discussion_type": null, "num_comments": 160, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo5bwf/offend_a_data_scientist_in_one_tweet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/t7n4hi55uh6a1.jpg", "subreddit_subscribers": 827935, "created_utc": 1671280215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_f900b52f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zo5cll", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 384, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 384, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/s0ynn7G8nXbTR_uXt90gEhSZHUL0mLo7H8bhVRTMixY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_1": 1}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671280285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/49ppdolcuh6a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?auto=webp&amp;s=a7a64c1a98fd1a74d3e4355ebccf288513ca89bb", "width": 1080, "height": 1920}, "resolutions": [{"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=30310c228dfd08348c71abb5fb5527956974f114", "width": 108, "height": 192}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=913abe79dc9b0a53473b7089eccc385ca0a071e2", "width": 216, "height": 384}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d9eb5d70911cce318caf1b6c5c8eae9f1ebd5b46", "width": 320, "height": 568}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=12e7b553a6788b9cf808a1629bbe8594e5c4e1e4", "width": 640, "height": 1137}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=afbf3adf5602589e4b4e263d6ff360a7a427131d", "width": 960, "height": 1706}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf90a16c987c9f4074158f0ba3347e2f0e27f4f5", "width": 1080, "height": 1920}], "variants": {}, "id": "W-uSuaiYuKbO5KZaKtG_pZhr1FKoXou0FiZEo6YQpfY"}], "enabled": true}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 100, "id": "gid_1", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Shows the Silver Award... and that's it.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Silver", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo5cll", "is_robot_indexable": true, "report_reasons": null, "author": "DwightScott69", "discussion_type": null, "num_comments": 123, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo5cll/thoughts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/49ppdolcuh6a1.jpg", "subreddit_subscribers": 827935, "created_utc": 1671280285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m 27 - through my 20s I\u2019ve had some industry changes, education pivots etc - but I finally found a bit of a niche in data science, if you consider data viz a vertical. \n\nI was hired as a data viz specialist this summer - finally fully getting away from my biz management background. I\u2019m also a CS grad student (on and off), which also contributes to imposter syndrome (I struggle a bit). Anyways, in many ways, I\u2019m still sort of entry level. \n\nI work in an agile environment with 3-4 data engineers, my boss is an ex sr analyst/engineer, and some salesforce folks. I just feel\u2026stupid. They make these amazing things with data happen in queries, I just make it pretty dashboards and do some side admin work (notifications/comms). I get good feedback, I think I\u2019m liked, and I\u2019m the only data viz person remaining - I was an internal hire and we\u2019ve tried 3 consultants to assist with data viz but were all canned for poor performance. So in a way, I am finding some stride in that I\u2019m *the* person creating part of the experience for the end user. \n\nBut then sometimes I feel capped. I\u2019m not in SQL a lot. I have experience in Data Viz tools (Tableau, Power BI), but am mostly using a Salesforce equivalent. I do some light pseudo-SAQL coding (which I\u2019m not sure is even great) but that\u2019s it. I mean, a lot of my queries are sort of trial and error to get the desired output. It works, I\u2019m not always fully sure why, I\u2019m sure there\u2019s a way to do what I did in 5 less lines. But I don\u2019t know. I also manage the downstream dataflows, but that\u2019s mostly simple joins and reporting to DE when a job fails. \n\nIn some ways I\u2019m learning\u2026I think. But in others, I\u2019m wondering if there\u2019s growth in data viz. I love it. But it feels like I\u2019m taking all the true work of the smart people and just doing the work they don\u2019t have bandwidth too. Kind of like a designer and not a core data person. I could be wrong. Most of the time my boss has most of the ideas and I use them. I go back to other dashboards and tweak what worked there and move it to mine. I just feel dumb.\n\nAnd then, what prompted this, is when I worked with the DE team the last few weeks. My only \u201cdata viz\u201d equivalent is another consultant with like, 20 years experience, but he hates data viz and mostly hides in Salesforce tools and data management work. But even then, he knows a lot. We sync with data engineering on topics like data masking, AWS keys, object creation, schemas in tables\u2026I\u2019m just so lost. It\u2019s above me. Sometimes I try to absorb as much as I can, others I just need not to learn something and overwhelm myself because I\u2019m at peak capacity myself. \n\nBut we talk, they solve it. I didn\u2019t even fully understand what buckets are in AWS - dev, prod, etc. I feel like I\u2019m quiet in meetings and let the smart people talk it out. Because I\u2019m out of SQL, I feel like that skill isn\u2019t as sharp. And we\u2019re hiring another core analyst, so my focus remains on viz. and don\u2019t get me wrong - I love viz. I think it\u2019s sort of my niche - but I do want opportunity to do more, earn more, and more so contribute to my team and be useful. Right now it just feels like I\u2019m running alongside everyone trying to keep up, but not fully \u201cgetting\u201d these concepts of data work we\u2019re doing. And obviously I don\u2019t want to ask and sound stupid during meetings. And my boss has limited bandwidth and can\u2019t just teach me things with all this time he doesn\u2019t have. Even I. SQL, I do have the database but have no idea what to do with it. I was told to look into ROW_NUMBER, NOLOCK, etc - but still feel dumbfounded by simple concepts. \n\nThis is mostly a vent. I just feel like a kinda not smart person benefitting from the work of great people, and being unable to understand these concepts. Yes I\u2019m new in a way, but I want to do more and help more. And I\u2019m just unsure what I\u2019m doing is right or if I get all this feedback because honestly, the only skill I feel I have is just following direction", "author_fullname": "t2_ukrr8abp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I work in Data Viz in my team and am feeling bigtime imposter syndrome around my boss and DEs/DAs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znzgsr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671256429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m 27 - through my 20s I\u2019ve had some industry changes, education pivots etc - but I finally found a bit of a niche in data science, if you consider data viz a vertical. &lt;/p&gt;\n\n&lt;p&gt;I was hired as a data viz specialist this summer - finally fully getting away from my biz management background. I\u2019m also a CS grad student (on and off), which also contributes to imposter syndrome (I struggle a bit). Anyways, in many ways, I\u2019m still sort of entry level. &lt;/p&gt;\n\n&lt;p&gt;I work in an agile environment with 3-4 data engineers, my boss is an ex sr analyst/engineer, and some salesforce folks. I just feel\u2026stupid. They make these amazing things with data happen in queries, I just make it pretty dashboards and do some side admin work (notifications/comms). I get good feedback, I think I\u2019m liked, and I\u2019m the only data viz person remaining - I was an internal hire and we\u2019ve tried 3 consultants to assist with data viz but were all canned for poor performance. So in a way, I am finding some stride in that I\u2019m &lt;em&gt;the&lt;/em&gt; person creating part of the experience for the end user. &lt;/p&gt;\n\n&lt;p&gt;But then sometimes I feel capped. I\u2019m not in SQL a lot. I have experience in Data Viz tools (Tableau, Power BI), but am mostly using a Salesforce equivalent. I do some light pseudo-SAQL coding (which I\u2019m not sure is even great) but that\u2019s it. I mean, a lot of my queries are sort of trial and error to get the desired output. It works, I\u2019m not always fully sure why, I\u2019m sure there\u2019s a way to do what I did in 5 less lines. But I don\u2019t know. I also manage the downstream dataflows, but that\u2019s mostly simple joins and reporting to DE when a job fails. &lt;/p&gt;\n\n&lt;p&gt;In some ways I\u2019m learning\u2026I think. But in others, I\u2019m wondering if there\u2019s growth in data viz. I love it. But it feels like I\u2019m taking all the true work of the smart people and just doing the work they don\u2019t have bandwidth too. Kind of like a designer and not a core data person. I could be wrong. Most of the time my boss has most of the ideas and I use them. I go back to other dashboards and tweak what worked there and move it to mine. I just feel dumb.&lt;/p&gt;\n\n&lt;p&gt;And then, what prompted this, is when I worked with the DE team the last few weeks. My only \u201cdata viz\u201d equivalent is another consultant with like, 20 years experience, but he hates data viz and mostly hides in Salesforce tools and data management work. But even then, he knows a lot. We sync with data engineering on topics like data masking, AWS keys, object creation, schemas in tables\u2026I\u2019m just so lost. It\u2019s above me. Sometimes I try to absorb as much as I can, others I just need not to learn something and overwhelm myself because I\u2019m at peak capacity myself. &lt;/p&gt;\n\n&lt;p&gt;But we talk, they solve it. I didn\u2019t even fully understand what buckets are in AWS - dev, prod, etc. I feel like I\u2019m quiet in meetings and let the smart people talk it out. Because I\u2019m out of SQL, I feel like that skill isn\u2019t as sharp. And we\u2019re hiring another core analyst, so my focus remains on viz. and don\u2019t get me wrong - I love viz. I think it\u2019s sort of my niche - but I do want opportunity to do more, earn more, and more so contribute to my team and be useful. Right now it just feels like I\u2019m running alongside everyone trying to keep up, but not fully \u201cgetting\u201d these concepts of data work we\u2019re doing. And obviously I don\u2019t want to ask and sound stupid during meetings. And my boss has limited bandwidth and can\u2019t just teach me things with all this time he doesn\u2019t have. Even I. SQL, I do have the database but have no idea what to do with it. I was told to look into ROW_NUMBER, NOLOCK, etc - but still feel dumbfounded by simple concepts. &lt;/p&gt;\n\n&lt;p&gt;This is mostly a vent. I just feel like a kinda not smart person benefitting from the work of great people, and being unable to understand these concepts. Yes I\u2019m new in a way, but I want to do more and help more. And I\u2019m just unsure what I\u2019m doing is right or if I get all this feedback because honestly, the only skill I feel I have is just following direction&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znzgsr", "is_robot_indexable": true, "report_reasons": null, "author": "hshzhsnnahsbs", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znzgsr/i_work_in_data_viz_in_my_team_and_am_feeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znzgsr/i_work_in_data_viz_in_my_team_and_am_feeling/", "subreddit_subscribers": 827935, "created_utc": 1671256429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What are some common functions/transformations you run on a dataset before training and what libraries do you use, or is it a custom script?", "author_fullname": "t2_6i7on", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Common dataset prep operations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo2tb9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671269962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some common functions/transformations you run on a dataset before training and what libraries do you use, or is it a custom script?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo2tb9", "is_robot_indexable": true, "report_reasons": null, "author": "jy2k", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo2tb9/common_dataset_prep_operations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo2tb9/common_dataset_prep_operations/", "subreddit_subscribers": 827935, "created_utc": 1671269962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For context, I'm currently finishing my bachelor's degree in electrical engineering and I just completed my minor in data science (i.e. I finished the last course required to satisfy the minor's requirements).  I found I like the data science stuff significantly more than EE, but I'm too far along to even consider switching majors at this point.  Hence, I'm trying to self-teach additional data science skills and I know being to use SQL and work with databases (something none of my DS courses covered unfortunately) in particular is a vital skill to have if I have any hope of getting a job in DS. \n\nI posted previously about this and I got a ton of responses with people recommending so many different learning platforms and several different API's and DBMS's that I'm a little unsure where to start.  I started just reading about what databases even _are_ so I can have a clear mental model in my head, but now I'm struggling to decide how to actually get started with SQL itself.  \n\nThe easiest thing (and hence what I'm tempted to do) would probably be to use one of the Python API's people recommended, just because I already have some experience using Python for data cleaning, exploration, and analysis, and I have Python fully set-up on my system already (and getting everything set up to use _any_ new programming language is typically a pain).  But is that a good idea, seeing as this will be the first time I've used SQL?  Will it it hurt me later on if I get used to just using Python to call SQL rather than learning how to use it directly?  Like, would prospective employers be less likely to higher me if I only have experience using SQL via Python, or will there be things I can't do through the API?  Or am I just completely overthinking this and it doesn't really matter whether I use SQL directly or indirectly?", "author_fullname": "t2_16ojfv6p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm attempting to self-teach SQL. If I already know already know Python, should I start by using a Python API for SQL or would that handicap me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zon2g1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671328853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context, I&amp;#39;m currently finishing my bachelor&amp;#39;s degree in electrical engineering and I just completed my minor in data science (i.e. I finished the last course required to satisfy the minor&amp;#39;s requirements).  I found I like the data science stuff significantly more than EE, but I&amp;#39;m too far along to even consider switching majors at this point.  Hence, I&amp;#39;m trying to self-teach additional data science skills and I know being to use SQL and work with databases (something none of my DS courses covered unfortunately) in particular is a vital skill to have if I have any hope of getting a job in DS. &lt;/p&gt;\n\n&lt;p&gt;I posted previously about this and I got a ton of responses with people recommending so many different learning platforms and several different API&amp;#39;s and DBMS&amp;#39;s that I&amp;#39;m a little unsure where to start.  I started just reading about what databases even &lt;em&gt;are&lt;/em&gt; so I can have a clear mental model in my head, but now I&amp;#39;m struggling to decide how to actually get started with SQL itself.  &lt;/p&gt;\n\n&lt;p&gt;The easiest thing (and hence what I&amp;#39;m tempted to do) would probably be to use one of the Python API&amp;#39;s people recommended, just because I already have some experience using Python for data cleaning, exploration, and analysis, and I have Python fully set-up on my system already (and getting everything set up to use &lt;em&gt;any&lt;/em&gt; new programming language is typically a pain).  But is that a good idea, seeing as this will be the first time I&amp;#39;ve used SQL?  Will it it hurt me later on if I get used to just using Python to call SQL rather than learning how to use it directly?  Like, would prospective employers be less likely to higher me if I only have experience using SQL via Python, or will there be things I can&amp;#39;t do through the API?  Or am I just completely overthinking this and it doesn&amp;#39;t really matter whether I use SQL directly or indirectly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zon2g1", "is_robot_indexable": true, "report_reasons": null, "author": "dcfan105", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zon2g1/im_attempting_to_selfteach_sql_if_i_already_know/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zon2g1/im_attempting_to_selfteach_sql_if_i_already_know/", "subreddit_subscribers": 827935, "created_utc": 1671328853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m looking for some data on p&amp;c insurance losses (claim/no-claim; claim size; peril; date; address). Does anyone know if there is a public data base with this?", "author_fullname": "t2_91itiala", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "P&amp;C Claim Data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zom9b9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671326434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking for some data on p&amp;amp;c insurance losses (claim/no-claim; claim size; peril; date; address). Does anyone know if there is a public data base with this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zom9b9", "is_robot_indexable": true, "report_reasons": null, "author": "MGeeeeeezy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zom9b9/pc_claim_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zom9b9/pc_claim_data/", "subreddit_subscribers": 827935, "created_utc": 1671326434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone,\n\nIm not an english native speaker. Although, I can speak english but not on a professional level (my opinion). I've always been seeking for jobs that are more on the technical side so I dont have to communicate much in english. Recently, I've grown an interest in becoming a data analyst but afraid that my english is not efficient enough. Can anyone tell me if excellent oral and writing skill will be required? thanks!", "author_fullname": "t2_7hvcstxn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you need be fluent in English to be a data analyst?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoivvn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671317957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Im not an english native speaker. Although, I can speak english but not on a professional level (my opinion). I&amp;#39;ve always been seeking for jobs that are more on the technical side so I dont have to communicate much in english. Recently, I&amp;#39;ve grown an interest in becoming a data analyst but afraid that my english is not efficient enough. Can anyone tell me if excellent oral and writing skill will be required? thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoivvn", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable_Bug_3102", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoivvn/do_you_need_be_fluent_in_english_to_be_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoivvn/do_you_need_be_fluent_in_english_to_be_a_data/", "subreddit_subscribers": 827935, "created_utc": 1671317957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I\u2019ve previously worked as a Data Scientist mainly with NLP and most recently I\u2019ve been working as a Data Engineer. I will be joining a new team as a Data Scientist in energy sector, specifically HVAC. What kind of topics should I prepare myself from technical point of view, my Python, SQL skills are above average or average.", "author_fullname": "t2_90pyvsip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone with data scientist experience in energy sector?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zohi08", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671314410.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I\u2019ve previously worked as a Data Scientist mainly with NLP and most recently I\u2019ve been working as a Data Engineer. I will be joining a new team as a Data Scientist in energy sector, specifically HVAC. What kind of topics should I prepare myself from technical point of view, my Python, SQL skills are above average or average.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zohi08", "is_robot_indexable": true, "report_reasons": null, "author": "Gagan_Ku2905", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zohi08/anyone_with_data_scientist_experience_in_energy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zohi08/anyone_with_data_scientist_experience_in_energy/", "subreddit_subscribers": 827935, "created_utc": 1671314410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm curious what folks in larger companies have found useful for working around/with encrypted data within a database. Not passwords but other types of controlled info \u2013 device checkins, friend lists, etc.\n\nDoes your security team have a process/tool in place to access the decryption key(s) from your code? Are there other limits on what you can do/can't do?", "author_fullname": "t2_5z2ths3h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your workflow for decrypting PII fields from a database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zodofx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671304493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious what folks in larger companies have found useful for working around/with encrypted data within a database. Not passwords but other types of controlled info \u2013 device checkins, friend lists, etc.&lt;/p&gt;\n\n&lt;p&gt;Does your security team have a process/tool in place to access the decryption key(s) from your code? Are there other limits on what you can do/can&amp;#39;t do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zodofx", "is_robot_indexable": true, "report_reasons": null, "author": "rszumski", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zodofx/what_is_your_workflow_for_decrypting_pii_fields/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zodofx/what_is_your_workflow_for_decrypting_pii_fields/", "subreddit_subscribers": 827935, "created_utc": 1671304493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My sibling has a career in data science and machine learning and they are really into it, it is their hobby too. Any ideas what I cam get them?\n\nThey won\u2019t like knick knacks like a mug with an AI generated dog, or a t shirt. They would just throw that out. What would be a great gift is something (not a book) that allows them to explore, learn, or experiment with ai more.", "author_fullname": "t2_qgco0kmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What can I get my sibling who is an AI data scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoagmh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671295838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My sibling has a career in data science and machine learning and they are really into it, it is their hobby too. Any ideas what I cam get them?&lt;/p&gt;\n\n&lt;p&gt;They won\u2019t like knick knacks like a mug with an AI generated dog, or a t shirt. They would just throw that out. What would be a great gift is something (not a book) that allows them to explore, learn, or experiment with ai more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoagmh", "is_robot_indexable": true, "report_reasons": null, "author": "CoupleConsistent5378", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoagmh/what_can_i_get_my_sibling_who_is_an_ai_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoagmh/what_can_i_get_my_sibling_who_is_an_ai_data/", "subreddit_subscribers": 827935, "created_utc": 1671295838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve seen companies hiring for all 4 at once at a single location so I\u2019m curious how the responsibilities would be divided. I feel there still lacks universal definitions for these roles.", "author_fullname": "t2_524iawau", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If a company has a Data Engineer, Analyst, and a ML Engineer, what does the Data Scientist do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo99yd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671292687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve seen companies hiring for all 4 at once at a single location so I\u2019m curious how the responsibilities would be divided. I feel there still lacks universal definitions for these roles.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo99yd", "is_robot_indexable": true, "report_reasons": null, "author": "bassabyss", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo99yd/if_a_company_has_a_data_engineer_analyst_and_a_ml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo99yd/if_a_company_has_a_data_engineer_analyst_and_a_ml/", "subreddit_subscribers": 827935, "created_utc": 1671292687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone!\n\nI psuedo joined the tech layoff getting put into a free agency to apply anywhere by my company and have been joining the mad job rush getting into things. I thought to share some experiential learning I had going on that roughly equates to normal good practice, but is also a good reminder to always consider ourselves expendable and a free agent regardless of if we're safe or not. I'm getting this quality checked on multiple communities and I'd love to hear your feedback.\n\n1. If your resume stands out, they have a specific need, and you fill it beautifully which you\u2019ll understand based on the job description, you can expect to get through the hiring process quickly.\n2. If it\u2019s a not sure, the initial contact and scheduling of the interview will go quickly and the process can be very robust based on the size of the company. Even a small company coming from big names can be concerned about the people brought in.\n3. The more towards early venture capital you are, the harder and more specific you can expect the hiring process to be because you\u2019ll need to have a very specific skillset to help bootstrap the employer. If it is something bigger like FAANG, they\u2019ll be more inclusive of different backgrounds and start off on the easier foot with something that would represent a leetcode question .\n4. Whoever it is though, they will hug to their process, so just give them the room they need to feel they\u2019re making the right decision as they will do the same for you. Keep applying in the meantime.\n5. Move towards the companies/positions that are incredibly fascinated with you as that\u2019s the level you\u2019re probably at for the moment.\n6. Understand which interviews you enjoyed the most and prioritize learning in that direction throughout the year as that\u2019s where the next step of your career probably is.\n7. See what you could chain into next based on the positions you get and relative volume of demand in the market using job boards like LinkedIn as you start to get a feel for how in demand that job role is.\n8. Get a feel too for how this current position prepares you for your next position. Maybe it lets you consolidate 3 resumes down to 1 for instance and be very specific on the skills you prep throughout the year in wait for the new role.\n9. At a certain point in the search, you\u2019ll start to notice some leads are consistently souring. Use that understanding to get a sense of how competitive the market is, but also what skills you lack so that you can grab an appropriate position to be successful in the future.\n10. Have fun with the process. It can be overall nailbiting a lot of the time, but everyone is working to everyone\u2019s elses\u2019 success. Keep up the momentum, but give each other room!\n11. As the leads start to wind down, feel free to get started closing the gap towards the next role. The quicker you grow, the faster they grow and the quicker the project will be done. Things might get a bit rambunctious, but focus on getting to the finish line and adding the completed tasks to a brag doc.\n12. You can have all the education in the world on a specific topic and the recruiters will still turn you down for unknown reasons once talking with a hiring manager. Once you get good, partner up with people in teams and push your skills to the next level, thereby alleviating any concerns had on their end. Keep in mind what the recruiter says about your experience as you will not get feedback past that.", "author_fullname": "t2_3mkgr19p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rules of the road navigating the tech layoffs and hiring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zopial", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671336423.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;I psuedo joined the tech layoff getting put into a free agency to apply anywhere by my company and have been joining the mad job rush getting into things. I thought to share some experiential learning I had going on that roughly equates to normal good practice, but is also a good reminder to always consider ourselves expendable and a free agent regardless of if we&amp;#39;re safe or not. I&amp;#39;m getting this quality checked on multiple communities and I&amp;#39;d love to hear your feedback.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;If your resume stands out, they have a specific need, and you fill it beautifully which you\u2019ll understand based on the job description, you can expect to get through the hiring process quickly.&lt;/li&gt;\n&lt;li&gt;If it\u2019s a not sure, the initial contact and scheduling of the interview will go quickly and the process can be very robust based on the size of the company. Even a small company coming from big names can be concerned about the people brought in.&lt;/li&gt;\n&lt;li&gt;The more towards early venture capital you are, the harder and more specific you can expect the hiring process to be because you\u2019ll need to have a very specific skillset to help bootstrap the employer. If it is something bigger like FAANG, they\u2019ll be more inclusive of different backgrounds and start off on the easier foot with something that would represent a leetcode question .&lt;/li&gt;\n&lt;li&gt;Whoever it is though, they will hug to their process, so just give them the room they need to feel they\u2019re making the right decision as they will do the same for you. Keep applying in the meantime.&lt;/li&gt;\n&lt;li&gt;Move towards the companies/positions that are incredibly fascinated with you as that\u2019s the level you\u2019re probably at for the moment.&lt;/li&gt;\n&lt;li&gt;Understand which interviews you enjoyed the most and prioritize learning in that direction throughout the year as that\u2019s where the next step of your career probably is.&lt;/li&gt;\n&lt;li&gt;See what you could chain into next based on the positions you get and relative volume of demand in the market using job boards like LinkedIn as you start to get a feel for how in demand that job role is.&lt;/li&gt;\n&lt;li&gt;Get a feel too for how this current position prepares you for your next position. Maybe it lets you consolidate 3 resumes down to 1 for instance and be very specific on the skills you prep throughout the year in wait for the new role.&lt;/li&gt;\n&lt;li&gt;At a certain point in the search, you\u2019ll start to notice some leads are consistently souring. Use that understanding to get a sense of how competitive the market is, but also what skills you lack so that you can grab an appropriate position to be successful in the future.&lt;/li&gt;\n&lt;li&gt;Have fun with the process. It can be overall nailbiting a lot of the time, but everyone is working to everyone\u2019s elses\u2019 success. Keep up the momentum, but give each other room!&lt;/li&gt;\n&lt;li&gt;As the leads start to wind down, feel free to get started closing the gap towards the next role. The quicker you grow, the faster they grow and the quicker the project will be done. Things might get a bit rambunctious, but focus on getting to the finish line and adding the completed tasks to a brag doc.&lt;/li&gt;\n&lt;li&gt;You can have all the education in the world on a specific topic and the recruiters will still turn you down for unknown reasons once talking with a hiring manager. Once you get good, partner up with people in teams and push your skills to the next level, thereby alleviating any concerns had on their end. Keep in mind what the recruiter says about your experience as you will not get feedback past that.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "zopial", "is_robot_indexable": true, "report_reasons": null, "author": "messerb5467", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zopial/rules_of_the_road_navigating_the_tech_layoffs_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zopial/rules_of_the_road_navigating_the_tech_layoffs_and/", "subreddit_subscribers": 827935, "created_utc": 1671336423.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_lck7n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Becoming a better programmer is twice as hard as just writing code. This is how I will sum up my learning of the past 30 years.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_zomtw5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2nMUqCSCP8aQRQB4QROhtnh0218LmurcEun03fdsUOI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671328151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "link.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://link.medium.com/9i9BBxu1Nvb", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?auto=webp&amp;s=4e38a91c86986c5fb593b6dd325f56fab6f68f47", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1da8588eee88de02d759267fe6be2b0c39216814", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=57d81a4a7abd09f4177018e245b1a7b22a52c39d", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ddcbe6e6b5137a38107cd6821bf7ba34e08b846", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=900b04e19b8ec42294770880b325da4742626500", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ab157542147f7a54674b3bedd8dc9edf2e3692f8", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/jton6NIMlIf7gTDrOjaZU9aVSbCeTzxZ7G10hD-wtk0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a1176e703b1f9bead39076a66735df27b23a039", "width": 1080, "height": 720}], "variants": {}, "id": "XH0z2WesZmlFmuitxqj7OA-8eBZPyjuDh8kVOEqUXmU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zomtw5", "is_robot_indexable": true, "report_reasons": null, "author": "ishwarjha", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zomtw5/becoming_a_better_programmer_is_twice_as_hard_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://link.medium.com/9i9BBxu1Nvb", "subreddit_subscribers": 827935, "created_utc": 1671328151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys,\nFrom your experience, what models would you consider for product assortment/range optimisation for FMCG retail sector\n( i.e getting the right product range in the right store/ product placement).\n\nI\u2019ve seen some use XGBoost for product placement, but curious to know what other models work well.\n\nI know we have to take product substitution into consideration so Customer Decision Tree Model is already taken care of.", "author_fullname": "t2_idusno00", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Product assortment/range optimisation for FMCG", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zolerx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671324227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,\nFrom your experience, what models would you consider for product assortment/range optimisation for FMCG retail sector\n( i.e getting the right product range in the right store/ product placement).&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen some use XGBoost for product placement, but curious to know what other models work well.&lt;/p&gt;\n\n&lt;p&gt;I know we have to take product substitution into consideration so Customer Decision Tree Model is already taken care of.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zolerx", "is_robot_indexable": true, "report_reasons": null, "author": "Maria_Adel", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zolerx/product_assortmentrange_optimisation_for_fmcg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zolerx/product_assortmentrange_optimisation_for_fmcg/", "subreddit_subscribers": 827935, "created_utc": 1671324227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys!\n\nJust looking for some guidance. I have a degree in liberal arts, whack degree I know; I spent way too much time goofing around in college and when graduation time came, it was either get the liberal arts degree or spend another 3ish semesters studying. I ended up taking the liberal arts degree and bouncing, probably should\u2019ve stayed in school but ohh whale, but I\u2019m reaching out because I\u2019ve been playing with new ideas. \n\nI would really like a to get a job in data analytics or data science. I\u2019m aware that a degree makes getting a job in that area 100x more easy and I\u2019m willing to go back to school. I have two options right now: I can go to a Purdue sister school, Purdue Fort Wayne, and do in person classes. Or I can do an online bachelors program at IUPUI, a sister school of Indiana University. Housing would not be an issue for either since I live 10-15 minutes from Purdue Fort Wayne and iupui classes are online. Price is similar.\n\nI\u2019m just worried about workplaces not valuing an online degree.\n\nAny input is appreciated.\n\nThanks in advance.", "author_fullname": "t2_2uqteg03", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career choices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoihog", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671316922.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys!&lt;/p&gt;\n\n&lt;p&gt;Just looking for some guidance. I have a degree in liberal arts, whack degree I know; I spent way too much time goofing around in college and when graduation time came, it was either get the liberal arts degree or spend another 3ish semesters studying. I ended up taking the liberal arts degree and bouncing, probably should\u2019ve stayed in school but ohh whale, but I\u2019m reaching out because I\u2019ve been playing with new ideas. &lt;/p&gt;\n\n&lt;p&gt;I would really like a to get a job in data analytics or data science. I\u2019m aware that a degree makes getting a job in that area 100x more easy and I\u2019m willing to go back to school. I have two options right now: I can go to a Purdue sister school, Purdue Fort Wayne, and do in person classes. Or I can do an online bachelors program at IUPUI, a sister school of Indiana University. Housing would not be an issue for either since I live 10-15 minutes from Purdue Fort Wayne and iupui classes are online. Price is similar.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m just worried about workplaces not valuing an online degree.&lt;/p&gt;\n\n&lt;p&gt;Any input is appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoihog", "is_robot_indexable": true, "report_reasons": null, "author": "c_a_a_07", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoihog/career_choices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoihog/career_choices/", "subreddit_subscribers": 827935, "created_utc": 1671316922.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Working on a time series classification project. I\u2019m going through the sktime library and reading each and every classifiers summary about the method that\u2019s being used under the hood to fit the model, and I\u2019m basing my choices based on how long it may take to train. For example, half of them are distance based (nearest neighbors) methods, and I know for my dataset a distance matrix (and most data in general) just takes a stupid amount of time to compute. Or some do Shapelet discovery, which is traversing the entire subsequence space and computing distances between the subsequence and original series and using those subsequences (shapelets) to predict, which also, takes a long time. So I\u2019m just avoiding using them for others (interval based)\n\nDoes anyone else do this? Is this a bad practice?", "author_fullname": "t2_i69qgpqa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching models because a model takes too long to train", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zogrsk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671312539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a time series classification project. I\u2019m going through the sktime library and reading each and every classifiers summary about the method that\u2019s being used under the hood to fit the model, and I\u2019m basing my choices based on how long it may take to train. For example, half of them are distance based (nearest neighbors) methods, and I know for my dataset a distance matrix (and most data in general) just takes a stupid amount of time to compute. Or some do Shapelet discovery, which is traversing the entire subsequence space and computing distances between the subsequence and original series and using those subsequences (shapelets) to predict, which also, takes a long time. So I\u2019m just avoiding using them for others (interval based)&lt;/p&gt;\n\n&lt;p&gt;Does anyone else do this? Is this a bad practice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zogrsk", "is_robot_indexable": true, "report_reasons": null, "author": "AdFew4357", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zogrsk/switching_models_because_a_model_takes_too_long/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zogrsk/switching_models_because_a_model_takes_too_long/", "subreddit_subscribers": 827935, "created_utc": 1671312539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello scientists,\n\nI have some basic data, more like a very basic personal project, and I want to find a way how to visualize it in a nice way. A three circle Venn diagram is not enough, I will need something like a \"oval flower model\" with multiple intertwining and mutually intersecting fields.\n\nThe basic tools online and in Excel seem not to be self adjusting based on how much text I want to put into the image (some fields require a lot of it). So I would like to find an online tool which can create a beautiful visual representation for me and which would automatically adjust the size of the diagram fields. \n\nI am not proficient yet in advanced tools or programming languages like R or Python, so I would like something simple.\n\nI am sure such a thing exist, I just suck a bit at finding it. It seems very easy, something that would look like this but that would automatically adjust the size and shape:\n\n[https://slidemodel.com/templates/4-set-venn-diagram-for-powerpoint/](https://slidemodel.com/templates/4-set-venn-diagram-for-powerpoint/)", "author_fullname": "t2_pm1g85e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Self-adjusting Venn and other diagram TOOL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zofz2p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671310504.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello scientists,&lt;/p&gt;\n\n&lt;p&gt;I have some basic data, more like a very basic personal project, and I want to find a way how to visualize it in a nice way. A three circle Venn diagram is not enough, I will need something like a &amp;quot;oval flower model&amp;quot; with multiple intertwining and mutually intersecting fields.&lt;/p&gt;\n\n&lt;p&gt;The basic tools online and in Excel seem not to be self adjusting based on how much text I want to put into the image (some fields require a lot of it). So I would like to find an online tool which can create a beautiful visual representation for me and which would automatically adjust the size of the diagram fields. &lt;/p&gt;\n\n&lt;p&gt;I am not proficient yet in advanced tools or programming languages like R or Python, so I would like something simple.&lt;/p&gt;\n\n&lt;p&gt;I am sure such a thing exist, I just suck a bit at finding it. It seems very easy, something that would look like this but that would automatically adjust the size and shape:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://slidemodel.com/templates/4-set-venn-diagram-for-powerpoint/\"&gt;https://slidemodel.com/templates/4-set-venn-diagram-for-powerpoint/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?auto=webp&amp;s=b8c436d0f16f68aeff3f032511de8c58efd15670", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e789ac40ee9c21361d89a6fbec1ca0b013ff3582", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=056a01229ae1d234fac7fcd7729803138e5f7372", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=50f6d2f4f9b8bd285b7baa8c2693c1da7c6e0687", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=687fa2608a2b4bad7abcd89b9b81f683c5e6ea5f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2bc1dcb1e475f1203f6f0e0dcfaf36862d372525", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/_yB6IRhQcU0sOg6AtAVgUSdvvmuwRQnC2L9NLd132yo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9f34f65b6861adc750282491554af75ea2c5c6a1", "width": 1080, "height": 607}], "variants": {}, "id": "pENjWJRqsa-GYfZ9DXYXCN3hnLoi0pZG0UcoOiIeSQM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zofz2p", "is_robot_indexable": true, "report_reasons": null, "author": "Sheetmusicman94", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zofz2p/selfadjusting_venn_and_other_diagram_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zofz2p/selfadjusting_venn_and_other_diagram_tool/", "subreddit_subscribers": 827935, "created_utc": 1671310504.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to set up an unsupervised learning project for anomaly detection and I'm learning along the way. Since I haven't got any previous experience with unsupervised experiments, I have what is probably a very basic question with regard to setting up my experiment: is the basic approach for unsupervised learning broadly comparable to that of supervised learning?\n\nMy intent is to go with a data split of 70:15:15 (train/test/holdout) and do an EDA on the data. I want to check how meaningful certain features are, whether they correlate etc.. Then I'll drop features and/or engineer derived features and see how they cluster and whether PCA will result in any insight with regard to feature relevance?\n\nSince I'll likely use the insight of the PCA to inform the feature engineering, I think a holdout data set is necessary - at least in theory. However, since anomalies are both \\_anomalous\\_ and sparse, I'm not really sure whether this can actually be practically implemented.\n\nAny pointers to best practices or or design philosophies would be much appreciate by this unsupervised newbie. ;)", "author_fullname": "t2_4j7ujk5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I also use a train/test/holdout split in unsupervised learning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoaan4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671295389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to set up an unsupervised learning project for anomaly detection and I&amp;#39;m learning along the way. Since I haven&amp;#39;t got any previous experience with unsupervised experiments, I have what is probably a very basic question with regard to setting up my experiment: is the basic approach for unsupervised learning broadly comparable to that of supervised learning?&lt;/p&gt;\n\n&lt;p&gt;My intent is to go with a data split of 70:15:15 (train/test/holdout) and do an EDA on the data. I want to check how meaningful certain features are, whether they correlate etc.. Then I&amp;#39;ll drop features and/or engineer derived features and see how they cluster and whether PCA will result in any insight with regard to feature relevance?&lt;/p&gt;\n\n&lt;p&gt;Since I&amp;#39;ll likely use the insight of the PCA to inform the feature engineering, I think a holdout data set is necessary - at least in theory. However, since anomalies are both _anomalous_ and sparse, I&amp;#39;m not really sure whether this can actually be practically implemented.&lt;/p&gt;\n\n&lt;p&gt;Any pointers to best practices or or design philosophies would be much appreciate by this unsupervised newbie. ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoaan4", "is_robot_indexable": true, "report_reasons": null, "author": "norfkens2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoaan4/do_i_also_use_a_traintestholdout_split_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoaan4/do_i_also_use_a_traintestholdout_split_in/", "subreddit_subscribers": 827935, "created_utc": 1671295389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Check out out, would you use this?\n\nhttps://github.com/aws-samples/distributed-compute-on-aws-with-cross-regional-dask", "author_fullname": "t2_v1f56kco", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cross Regional Dask on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo8v5e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671291531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check out out, would you use this?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/aws-samples/distributed-compute-on-aws-with-cross-regional-dask\"&gt;https://github.com/aws-samples/distributed-compute-on-aws-with-cross-regional-dask&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?auto=webp&amp;s=2007921aefa32104b16120fdcda67f25052c18d3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0d2363e91de15e7251f6ad01a45ba9d7cd5b36c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef2d65fc0163c885d031bf3b3742c1c90e71a245", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c83d83b48aaaca832314c54b7e208d9414432ff", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8ef536a4b30cac8de2b8c1256e74a37aece58c8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=23049cb7c3f55d216d707f36a48e1fda6e7a758f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02a1f497a8851982103795c3df6c1b2e3029757e", "width": 1080, "height": 540}], "variants": {}, "id": "d7SCmwvPs0_VsnLQQ01R-H3jpt4LRVP6Zmmmk5Icd0c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo8v5e", "is_robot_indexable": true, "report_reasons": null, "author": "oconpa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo8v5e/cross_regional_dask_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo8v5e/cross_regional_dask_on_aws/", "subreddit_subscribers": 827935, "created_utc": 1671291531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,  \nCurrently I am researching extreme learning machines, and - after I calculated the beta weights  - I would like to optimize the alpha weights (similarly with the Moore-Penrose pseudo inverse) in order to receive better accuracy.  \nDuring this backward calculation, I use the inverse of the previously applied activation function. In case of the inverse sigmoid and tanh (logit and arctan), I receive NaN values, although with the inverse leaky ReLU, on certain datasets, the method yields better accuracy. Unfortunately, in most cases, the accuracy drops (sometimes significantly) with these new optimized alpha weights. What do you think, what is the reason for this?", "author_fullname": "t2_kq8l2zbu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inverse activation function", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo7wjc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671288767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;br/&gt;\nCurrently I am researching extreme learning machines, and - after I calculated the beta weights  - I would like to optimize the alpha weights (similarly with the Moore-Penrose pseudo inverse) in order to receive better accuracy.&lt;br/&gt;\nDuring this backward calculation, I use the inverse of the previously applied activation function. In case of the inverse sigmoid and tanh (logit and arctan), I receive NaN values, although with the inverse leaky ReLU, on certain datasets, the method yields better accuracy. Unfortunately, in most cases, the accuracy drops (sometimes significantly) with these new optimized alpha weights. What do you think, what is the reason for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo7wjc", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical-Plane3730", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo7wjc/inverse_activation_function/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo7wjc/inverse_activation_function/", "subreddit_subscribers": 827935, "created_utc": 1671288767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi community! I am a senior in college with Mathematics major. I am based in NYC. I have an avid internet in pursuing a career in Data Science. In the past I have participated in a year long data science fellowship run by my University. I have also participated in a Data Science summer school run by a major technology company. I have been persistently applying to related roles but I haven\u2019t had offers or fair amount of interviews either (I am in a student visa currently). Most of the Data Science jobs ask for at least Master\u2019s degree or fair amount of years of experience. Now my concern is that if I should pursue a MS in Data Science.", "author_fullname": "t2_3lz0pnks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Master\u2019s degree?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo5ma2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671281284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi community! I am a senior in college with Mathematics major. I am based in NYC. I have an avid internet in pursuing a career in Data Science. In the past I have participated in a year long data science fellowship run by my University. I have also participated in a Data Science summer school run by a major technology company. I have been persistently applying to related roles but I haven\u2019t had offers or fair amount of interviews either (I am in a student visa currently). Most of the Data Science jobs ask for at least Master\u2019s degree or fair amount of years of experience. Now my concern is that if I should pursue a MS in Data Science.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo5ma2", "is_robot_indexable": true, "report_reasons": null, "author": "m30aru", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo5ma2/masters_degree/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo5ma2/masters_degree/", "subreddit_subscribers": 827935, "created_utc": 1671281284.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to do a new project in R, where I'm working with a lot of data (about 6GB of text) my inefficient code is tying me down a lot and was hoping maybe you guys had some tricks.\n\nI'm supposed to load in large .txt files, each file should be split into shorter parts consisting of 10 words each, they are then put into my data frame with each word being put into one of 10 columns. \n\nThe problem is that my code is slow, like really slow. It takes around 20 minutes to load the first 100000 rows of text into my dataframe, which is less than 1% of my data. I have also noticed that code slows down the further it takes to run, it takes about a second to load in the first 3000 rows, and 15 seconds for the first 10000 rows.   \n\nWhat is the main bottleneck slowing me down? Should I just rewrite the code entirely in another language like python?\n\nMy current code below:\n\n    #setup\n    folder &lt;- \"C:/Users/A_tiny/\"\n    \n    # Get a list of files in the folder\n    files &lt;- list.files(folder)\n    \n    # Create an empty data frame to hold the indexed words\n    df &lt;- data.frame(word1 = character(), word2 = character(), word3 = character(), word4 = character(),\n    word5 = character(), word6 = character(), word7 = character(), word8 = character(),\n    word9 = character(), word10 = character(), index = integer())\n    \n    for (file in files) {\n    # Check if the file is a .epub.txt file\n    if (grepl(\".epub.txt$\", file)) {\n    # Read the .epub.txt file into a string variable\n    epub_txt &lt;- readLines(paste0(folder, file))\n    \n    # Split the string into a vector of words\n    words &lt;- unlist(strsplit(epub_txt, \" \"))\n    \n    # Loop through the vector of words and add the next 10 words to the data frame, skipping 10 words at a time\n    for (i in seq(1, length(words), by=10)) {\n      # Create a data frame with the next 10 words and the index\n      df_temp &lt;- data.frame(word1 = words[i], word2 = words[i+1], word3 = words[i+2], word4 = words[i+3],\n                            word5 = words[i+4], word6 = words[i+5], word7 = words[i+6], word8 = words[i+7],\n                            word9 = words[i+8], word10 = words[i+9], index = i)\n      # Add the data frame to the main data frame\n      df &lt;- rbind(df, df_temp)\n    }\n      }\n    }", "author_fullname": "t2_gx7hn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The best approach for rewriting my code more efficiently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo4p8z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671277772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to do a new project in R, where I&amp;#39;m working with a lot of data (about 6GB of text) my inefficient code is tying me down a lot and was hoping maybe you guys had some tricks.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m supposed to load in large .txt files, each file should be split into shorter parts consisting of 10 words each, they are then put into my data frame with each word being put into one of 10 columns. &lt;/p&gt;\n\n&lt;p&gt;The problem is that my code is slow, like really slow. It takes around 20 minutes to load the first 100000 rows of text into my dataframe, which is less than 1% of my data. I have also noticed that code slows down the further it takes to run, it takes about a second to load in the first 3000 rows, and 15 seconds for the first 10000 rows.   &lt;/p&gt;\n\n&lt;p&gt;What is the main bottleneck slowing me down? Should I just rewrite the code entirely in another language like python?&lt;/p&gt;\n\n&lt;p&gt;My current code below:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#setup\nfolder &amp;lt;- &amp;quot;C:/Users/A_tiny/&amp;quot;\n\n# Get a list of files in the folder\nfiles &amp;lt;- list.files(folder)\n\n# Create an empty data frame to hold the indexed words\ndf &amp;lt;- data.frame(word1 = character(), word2 = character(), word3 = character(), word4 = character(),\nword5 = character(), word6 = character(), word7 = character(), word8 = character(),\nword9 = character(), word10 = character(), index = integer())\n\nfor (file in files) {\n# Check if the file is a .epub.txt file\nif (grepl(&amp;quot;.epub.txt$&amp;quot;, file)) {\n# Read the .epub.txt file into a string variable\nepub_txt &amp;lt;- readLines(paste0(folder, file))\n\n# Split the string into a vector of words\nwords &amp;lt;- unlist(strsplit(epub_txt, &amp;quot; &amp;quot;))\n\n# Loop through the vector of words and add the next 10 words to the data frame, skipping 10 words at a time\nfor (i in seq(1, length(words), by=10)) {\n  # Create a data frame with the next 10 words and the index\n  df_temp &amp;lt;- data.frame(word1 = words[i], word2 = words[i+1], word3 = words[i+2], word4 = words[i+3],\n                        word5 = words[i+4], word6 = words[i+5], word7 = words[i+6], word8 = words[i+7],\n                        word9 = words[i+8], word10 = words[i+9], index = i)\n  # Add the data frame to the main data frame\n  df &amp;lt;- rbind(df, df_temp)\n}\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo4p8z", "is_robot_indexable": true, "report_reasons": null, "author": "_Just7_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo4p8z/the_best_approach_for_rewriting_my_code_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo4p8z/the_best_approach_for_rewriting_my_code_more/", "subreddit_subscribers": 827935, "created_utc": 1671277772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_895cqxfg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Network Graphs of all Goals &amp; Goal Chances created by WC2022 finalists, visualized.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_zo16rm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/i6VMGDsopWj6_KTX59CBPBDTTP79_O1tnicjhjnaaXo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671263108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "wcxgnetworks.streamlit.app", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://wcxgnetworks.streamlit.app/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?auto=webp&amp;s=c1002e8b9aefe041811736d6a215bb038eb7aa8b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a90792efc4e26380ce1f4033e8c0c517d804810d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=78a5ebc886dd56810d6c36026f6e6552966517a4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba6d7ad1934366ab73f23c0cae3d61a24560f20c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ab934a1101b6c1e200b46b117784bdc9b7529dcb", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=798f95e131655041677300c64511006ddde07b99", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3f05f99a6150e14c8df1b370cc580e98b0bb350c", "width": 1080, "height": 567}], "variants": {}, "id": "PIe-3OpMNP61TpZzSjrFl3aA2rFm7yVHhU-B_BoSKhk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo16rm", "is_robot_indexable": true, "report_reasons": null, "author": "noimgonnalie", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo16rm/network_graphs_of_all_goals_goal_chances_created/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://wcxgnetworks.streamlit.app/", "subreddit_subscribers": 827935, "created_utc": 1671263108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have noticed two human *skills* that strongly correlates with ability to have a distinctly good \"common sense\". One is being into data science (statistics, computation, math and understanding). The second is reading A LOT. I don't have enough experience with the latter, so i wonder if my reasoning has some merit? What is behind this and what are other types of *skills* you have noticed being related to a very good common sense?", "author_fullname": "t2_ehomo9o8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question to BIG readers (fiction, romance or other)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zon3e4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671328928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have noticed two human &lt;em&gt;skills&lt;/em&gt; that strongly correlates with ability to have a distinctly good &amp;quot;common sense&amp;quot;. One is being into data science (statistics, computation, math and understanding). The second is reading A LOT. I don&amp;#39;t have enough experience with the latter, so i wonder if my reasoning has some merit? What is behind this and what are other types of &lt;em&gt;skills&lt;/em&gt; you have noticed being related to a very good common sense?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zon3e4", "is_robot_indexable": true, "report_reasons": null, "author": "nevermindever42", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zon3e4/question_to_big_readers_fiction_romance_or_other/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zon3e4/question_to_big_readers_fiction_romance_or_other/", "subreddit_subscribers": 827935, "created_utc": 1671328928.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am at company A an Associate Director DS since 1.8 years, with no guarantee to move to director anytime soon. I have been offered a position of Principal DS at company B with 13% salary increase, sign-in bonus and payout of 70% of my unvested stocks value. Pay wise it is an upgrade.", "author_fullname": "t2_6kg9d73t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a downgrade going from Associate Director DS to Principal DS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoctli", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671302228.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am at company A an Associate Director DS since 1.8 years, with no guarantee to move to director anytime soon. I have been offered a position of Principal DS at company B with 13% salary increase, sign-in bonus and payout of 70% of my unvested stocks value. Pay wise it is an upgrade.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoctli", "is_robot_indexable": true, "report_reasons": null, "author": "IllustratorInfamous1", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoctli/is_it_a_downgrade_going_from_associate_director/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoctli/is_it_a_downgrade_going_from_associate_director/", "subreddit_subscribers": 827935, "created_utc": 1671302228.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}