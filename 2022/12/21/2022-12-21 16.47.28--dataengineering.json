{"kind": "Listing", "data": {"after": "t3_zr8dc7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hg3enfgk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL using pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zr2klf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 217, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 217, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/css_y0476Mrt9RkOS3q8shhEKoMFcKvUQ0LfPNgvw7U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_1": 1}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671577274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/nu453udgd67a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/nu453udgd67a1.jpg?auto=webp&amp;s=d6699a6c437b35ed864e3276247beade887f5094", "width": 218, "height": 250}, "resolutions": [{"url": "https://preview.redd.it/nu453udgd67a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e06fb5266a1bb770dc0a54ea18949d7f9837f5c", "width": 108, "height": 123}, {"url": "https://preview.redd.it/nu453udgd67a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0aa382863d5f31fce0074394fd2ab99b0a1fd44", "width": 216, "height": 247}], "variants": {}, "id": "ory8SATFBHDViWndbSP1RQbNt0nro3TGmkqRIrA99A8"}], "enabled": true}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 100, "id": "gid_1", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Shows the Silver Award... and that's it.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Silver", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "zr2klf", "is_robot_indexable": true, "report_reasons": null, "author": "Salmon-Advantage", "discussion_type": null, "num_comments": 137, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zr2klf/etl_using_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/nu453udgd67a1.jpg", "subreddit_subscribers": 83497, "created_utc": 1671577274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_88bdez9q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with large CSV files in Python from Scratch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 103, "top_awarded_type": null, "hide_score": false, "name": "t3_zr5b9b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": "transparent", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8_qChnSEWkB8dHdpadU_sqhbCwPBU1oZm00JQhRnqZI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671583489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "coraspe-ramses.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://coraspe-ramses.medium.com/working-with-large-csv-files-in-python-from-scratch-134587aed5f7", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HT3-q6jGQGGpf8o9Eu5aJv8U_2EMbKBu1tFoIRhViI8.jpg?auto=webp&amp;s=7a7d4d5cc7ab512371fd50302991bf9695a3b98d", "width": 1200, "height": 888}, "resolutions": [{"url": "https://external-preview.redd.it/HT3-q6jGQGGpf8o9Eu5aJv8U_2EMbKBu1tFoIRhViI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fac01ee6820d759066a6dbf0e168b39451921393", "width": 108, "height": 79}, {"url": "https://external-preview.redd.it/HT3-q6jGQGGpf8o9Eu5aJv8U_2EMbKBu1tFoIRhViI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a1dbab2212b5d5d2a5c7c93095d54f0b32d03029", "width": 216, "height": 159}, {"url": "https://external-preview.redd.it/HT3-q6jGQGGpf8o9Eu5aJv8U_2EMbKBu1tFoIRhViI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=46425ca0cfebec2e109461b49655d77121c88dff", "width": 320, "height": 236}, {"url": "https://external-preview.redd.it/HT3-q6jGQGGpf8o9Eu5aJv8U_2EMbKBu1tFoIRhViI8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fbce67bbcd5551f35413bee80e54161916d9cd8", "width": 640, "height": 473}, {"url": "https://external-preview.redd.it/HT3-q6jGQGGpf8o9Eu5aJv8U_2EMbKBu1tFoIRhViI8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6d652ce95c93d0964d5cc0bd6e3c9885eec054cd", "width": 960, "height": 710}, {"url": "https://external-preview.redd.it/HT3-q6jGQGGpf8o9Eu5aJv8U_2EMbKBu1tFoIRhViI8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e121f8aadc425a0bfe103304a9cb069e5f2d23d", "width": 1080, "height": 799}], "variants": {}, "id": "CErUCCyPZx58aJviNfuMZ102Mb5nbxW5eyQRLUXkA0A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zr5b9b", "is_robot_indexable": true, "report_reasons": null, "author": "ramses-coraspe", "discussion_type": null, "num_comments": 15, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/zr5b9b/working_with_large_csv_files_in_python_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://coraspe-ramses.medium.com/working-with-large-csv-files-in-python-from-scratch-134587aed5f7", "subreddit_subscribers": 83497, "created_utc": 1671583489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\nI\u2019ve been tasked with building master data based on data from SQL Server, text files, and possibly other other sources, like web services, in the future.\n\nIdeally, I would like a rules engine that is a single place that all the rules are stored (e.g. if this status is set, use this value; etc).\nThe intention is that everybody in the company would be able to view the rules if they wanted to, and eventually, I can get the SMEs to maintain them.\n\nThere seems to be a mass of options out there.\n\nDoes anyone have a recommendation on a rules engine where the rules read like a big spreadsheet?\n\nI can consider both cloud and on premises server options. It would be processing approximately 100m records daily with about 2500 rules.\n\nThank you!", "author_fullname": "t2_58kj666v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on a business rules engine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zrdq62", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671608233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI\u2019ve been tasked with building master data based on data from SQL Server, text files, and possibly other other sources, like web services, in the future.&lt;/p&gt;\n\n&lt;p&gt;Ideally, I would like a rules engine that is a single place that all the rules are stored (e.g. if this status is set, use this value; etc).\nThe intention is that everybody in the company would be able to view the rules if they wanted to, and eventually, I can get the SMEs to maintain them.&lt;/p&gt;\n\n&lt;p&gt;There seems to be a mass of options out there.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have a recommendation on a rules engine where the rules read like a big spreadsheet?&lt;/p&gt;\n\n&lt;p&gt;I can consider both cloud and on premises server options. It would be processing approximately 100m records daily with about 2500 rules.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zrdq62", "is_robot_indexable": true, "report_reasons": null, "author": "Andrew50000", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zrdq62/thoughts_on_a_business_rules_engine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zrdq62/thoughts_on_a_business_rules_engine/", "subreddit_subscribers": 83497, "created_utc": 1671608233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\n&amp;#x200B;\n\nI'm looking into applying to data engineering roles this upcoming January and want to best prepare myself for these interviews. I'm not looking into getting into any FAANG type companies. I'm more than happy to get a job at mid-sized companies such as Oracle, Walmart, AT&amp;T, Chevron, CVS Health, etc. type companies.\n\nJust trying to get my foot in the door at this point and get this experience. How would you best prepare for these types of companies. Is leetcode and advanced SQL necessary?\n\nSo far, I'm brushing up on data modeling, ETL, SQL, and Python. Looking for more insight if possible.\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_7gkhxmgl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are junior/entry-level data engineer interviews that are NOT FAANG", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zrgeyv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671618073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking into applying to data engineering roles this upcoming January and want to best prepare myself for these interviews. I&amp;#39;m not looking into getting into any FAANG type companies. I&amp;#39;m more than happy to get a job at mid-sized companies such as Oracle, Walmart, AT&amp;amp;T, Chevron, CVS Health, etc. type companies.&lt;/p&gt;\n\n&lt;p&gt;Just trying to get my foot in the door at this point and get this experience. How would you best prepare for these types of companies. Is leetcode and advanced SQL necessary?&lt;/p&gt;\n\n&lt;p&gt;So far, I&amp;#39;m brushing up on data modeling, ETL, SQL, and Python. Looking for more insight if possible.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "zrgeyv", "is_robot_indexable": true, "report_reasons": null, "author": "SnooWalruses7164", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zrgeyv/how_are_juniorentrylevel_data_engineer_interviews/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zrgeyv/how_are_juniorentrylevel_data_engineer_interviews/", "subreddit_subscribers": 83497, "created_utc": 1671618073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks. I am at a point of career when I have some knowledge &amp; experience (2.5 yoe in data), from basic to more advanced, in most important tools &amp; patterns in data engineering. SQL &amp; some NoSQL databases, Python, cloud, Spark, Airflow, Docker etc.\n\nAfter building some foundations, I begin to wonder if I should specialize in a particular tool. Right now, I am a kind of jack-of-all-trades. Aside of SQL &amp; Python, I haven't worked with any particular environment, like cloud provider or framework, for longer than a year. It gave me a chance to learn various tools in practice in a short period of time, but now I crave some stability.\n\nI wonder, should it stay that way and target to become an overall skilled engineer, with broad but narrow set of skills, or more of a 'T-shaped' person, building an in-depth knowledge of some particular f.e. data platform (like Snowflake/Databricks) or cloud provider (AWS/Azure/GCP), or maybe a particular branch of data engineering (like streaming) or domain knowledge (like finance)?\n\nWhat are your thoughts on this topic?", "author_fullname": "t2_9e7m1qmr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Specializing in one service as data engineer vs being a jack-of-all-trades", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zr0url", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671573143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks. I am at a point of career when I have some knowledge &amp;amp; experience (2.5 yoe in data), from basic to more advanced, in most important tools &amp;amp; patterns in data engineering. SQL &amp;amp; some NoSQL databases, Python, cloud, Spark, Airflow, Docker etc.&lt;/p&gt;\n\n&lt;p&gt;After building some foundations, I begin to wonder if I should specialize in a particular tool. Right now, I am a kind of jack-of-all-trades. Aside of SQL &amp;amp; Python, I haven&amp;#39;t worked with any particular environment, like cloud provider or framework, for longer than a year. It gave me a chance to learn various tools in practice in a short period of time, but now I crave some stability.&lt;/p&gt;\n\n&lt;p&gt;I wonder, should it stay that way and target to become an overall skilled engineer, with broad but narrow set of skills, or more of a &amp;#39;T-shaped&amp;#39; person, building an in-depth knowledge of some particular f.e. data platform (like Snowflake/Databricks) or cloud provider (AWS/Azure/GCP), or maybe a particular branch of data engineering (like streaming) or domain knowledge (like finance)?&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on this topic?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zr0url", "is_robot_indexable": true, "report_reasons": null, "author": "LewWariat", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zr0url/specializing_in_one_service_as_data_engineer_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zr0url/specializing_in_one_service_as_data_engineer_vs/", "subreddit_subscribers": 83497, "created_utc": 1671573143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there plug-ins / open source technologies that are production-ready to enable stream processing in Postgres? I'm looking to share data pipeline definitions and SQL queries across batch and realtime ie. streaming use cases, with Postgres as my database of choice.\n\nIn doing a little bit of research, I'm mainly seeing a plug-in called PipelineDB. Is this project alive / reliable? The github doesn't look like it's been updated in a few years and their website doesn't seem to exist anymore... If not them, any other best practices for stream processing in Postgres?", "author_fullname": "t2_qnjyn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming Data and Postgres", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zr5vml", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671584707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there plug-ins / open source technologies that are production-ready to enable stream processing in Postgres? I&amp;#39;m looking to share data pipeline definitions and SQL queries across batch and realtime ie. streaming use cases, with Postgres as my database of choice.&lt;/p&gt;\n\n&lt;p&gt;In doing a little bit of research, I&amp;#39;m mainly seeing a plug-in called PipelineDB. Is this project alive / reliable? The github doesn&amp;#39;t look like it&amp;#39;s been updated in a few years and their website doesn&amp;#39;t seem to exist anymore... If not them, any other best practices for stream processing in Postgres?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zr5vml", "is_robot_indexable": true, "report_reasons": null, "author": "vroomwaddle", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zr5vml/streaming_data_and_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zr5vml/streaming_data_and_postgres/", "subreddit_subscribers": 83497, "created_utc": 1671584707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for a 150-person company in the hospitality space.  Our data team currently consists of 2 people.  We have our transactional data stored in a Postgres database (about 50GB) hosted on Heroku.  We have lots of other data floating around in spreadsheets, etc. saved on network drives.  We have Google Analytics and Mixpanel set up, as well.\n\nThe overarching goal is for business users in Marketing, Finance, and Operations to be able to answer questions and gain insights using our data.  To enable this, we plan to build out a modern data stack, which will include:\n\n* ETL tools (considering Fivetran and Stitch for extract and load, and dbt for transformation)\n* cloud data warehouse or \"lakehouse\" (considering BigQuery, Snowflake, and Databricks)\n* visualization tools (considering Tableau, Looker, and Metabase)\n\nOur use cases will be really simple.  The business needs a web-based application to \"interact with\" our data (e.g. visualize trends, build reports, etc.).  Eventually, the data team will implement some run-of-the-mill machine learning models and analytics.\n\n**It's the cloud data warehouse (\"lakehouse\") piece that I'm really struggling with.**  Can I get the community's thoughts as to the pros and cons of the following providers?\n\n* Databricks\n* Snowflake (Snowpark)\n* Google BigQuery (BigLake)\n\nThanks in advance!", "author_fullname": "t2_l0vz55vz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with setting up a Modern Data Stack (MDS)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zr86gx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671591250.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671590925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a 150-person company in the hospitality space.  Our data team currently consists of 2 people.  We have our transactional data stored in a Postgres database (about 50GB) hosted on Heroku.  We have lots of other data floating around in spreadsheets, etc. saved on network drives.  We have Google Analytics and Mixpanel set up, as well.&lt;/p&gt;\n\n&lt;p&gt;The overarching goal is for business users in Marketing, Finance, and Operations to be able to answer questions and gain insights using our data.  To enable this, we plan to build out a modern data stack, which will include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ETL tools (considering Fivetran and Stitch for extract and load, and dbt for transformation)&lt;/li&gt;\n&lt;li&gt;cloud data warehouse or &amp;quot;lakehouse&amp;quot; (considering BigQuery, Snowflake, and Databricks)&lt;/li&gt;\n&lt;li&gt;visualization tools (considering Tableau, Looker, and Metabase)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Our use cases will be really simple.  The business needs a web-based application to &amp;quot;interact with&amp;quot; our data (e.g. visualize trends, build reports, etc.).  Eventually, the data team will implement some run-of-the-mill machine learning models and analytics.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;It&amp;#39;s the cloud data warehouse (&amp;quot;lakehouse&amp;quot;) piece that I&amp;#39;m really struggling with.&lt;/strong&gt;  Can I get the community&amp;#39;s thoughts as to the pros and cons of the following providers?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Databricks&lt;/li&gt;\n&lt;li&gt;Snowflake (Snowpark)&lt;/li&gt;\n&lt;li&gt;Google BigQuery (BigLake)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zr86gx", "is_robot_indexable": true, "report_reasons": null, "author": "long_delta", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zr86gx/help_with_setting_up_a_modern_data_stack_mds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zr86gx/help_with_setting_up_a_modern_data_stack_mds/", "subreddit_subscribers": 83497, "created_utc": 1671590925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have raw DICOM image metadata loaded into a staging table as a JSONB type. The structure of the JSON data is:\n\n    {\n        \"00080008\": {\n            \"vr\": \"CS\",\n            \"Value\": [\n                \"DERIVED\",\n                \"SECONDARY\",\n                \"OTHER\"\n            ]\n        },\n        \"00080012\": {\n            \"vr\": \"DA\",\n            \"Value\": [\n                \"20040826\"\n            ]\n        },\n        ...\n    }\n\nI need to extract *select* top-level keys and their associated value `\"Value\"` to create a new table wherein the key is the column name and `\"Value\"` is the value for the given record.\n\n&amp;#x200B;\n\nStaging table:\n\n&amp;#x200B;\n\n|id|data|\n|:-|:-|\n|1|{\"00080008\": {\"vr\": \"CS\", \"Value\": \\[\"DERIVED\", \"SECONDARY\", \"OTHER\"\\]}, ...}|\n\n&amp;#x200B;\n\nAnticipated table:\n\n&amp;#x200B;\n\n|id|ImageType|InstanceCreationDate|\n|:-|:-|:-|\n|1|{\"DERIVED\", \"SECONDARY\", \"OTHER\"}|{\"20040826\"}|\n\n&amp;#x200B;\n\nI have this current working query but it will not scale well as more tags are needed.\n\n    SELECT\n          meta.id\n        , \"00080008\".\"Value\" AS ImageType\n        , \"00080012\".\"Value\" AS InstanceCreationDate\n    FROM\n          file_metadata AS meta\n        , jsonb_to_record(data) AS tag(\n              \"00080008\" jsonb\n            , \"00080012\" jsonb\n            )\n        , jsonb_to_record(tag.\"00080008\") AS \"00080008\"(\n            \"Value\" text[]\n        )\n        , jsonb_to_record(tag.\"00080012\") AS \"00080012\"(\n            \"Value\" text[]\n        )\n    ;\n\nHow can I make this query more robust to handle many keys being extracted from the original JSON data?", "author_fullname": "t2_5dgh0avd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unnesting JSON in Postgres", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zr0663", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671571522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have raw DICOM image metadata loaded into a staging table as a JSONB type. The structure of the JSON data is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n    &amp;quot;00080008&amp;quot;: {\n        &amp;quot;vr&amp;quot;: &amp;quot;CS&amp;quot;,\n        &amp;quot;Value&amp;quot;: [\n            &amp;quot;DERIVED&amp;quot;,\n            &amp;quot;SECONDARY&amp;quot;,\n            &amp;quot;OTHER&amp;quot;\n        ]\n    },\n    &amp;quot;00080012&amp;quot;: {\n        &amp;quot;vr&amp;quot;: &amp;quot;DA&amp;quot;,\n        &amp;quot;Value&amp;quot;: [\n            &amp;quot;20040826&amp;quot;\n        ]\n    },\n    ...\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I need to extract &lt;em&gt;select&lt;/em&gt; top-level keys and their associated value &lt;code&gt;&amp;quot;Value&amp;quot;&lt;/code&gt; to create a new table wherein the key is the column name and &lt;code&gt;&amp;quot;Value&amp;quot;&lt;/code&gt; is the value for the given record.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Staging table:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;id&lt;/th&gt;\n&lt;th align=\"left\"&gt;data&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;{&amp;quot;00080008&amp;quot;: {&amp;quot;vr&amp;quot;: &amp;quot;CS&amp;quot;, &amp;quot;Value&amp;quot;: [&amp;quot;DERIVED&amp;quot;, &amp;quot;SECONDARY&amp;quot;, &amp;quot;OTHER&amp;quot;]}, ...}&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anticipated table:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;id&lt;/th&gt;\n&lt;th align=\"left\"&gt;ImageType&lt;/th&gt;\n&lt;th align=\"left\"&gt;InstanceCreationDate&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;{&amp;quot;DERIVED&amp;quot;, &amp;quot;SECONDARY&amp;quot;, &amp;quot;OTHER&amp;quot;}&lt;/td&gt;\n&lt;td align=\"left\"&gt;{&amp;quot;20040826&amp;quot;}&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have this current working query but it will not scale well as more tags are needed.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT\n      meta.id\n    , &amp;quot;00080008&amp;quot;.&amp;quot;Value&amp;quot; AS ImageType\n    , &amp;quot;00080012&amp;quot;.&amp;quot;Value&amp;quot; AS InstanceCreationDate\nFROM\n      file_metadata AS meta\n    , jsonb_to_record(data) AS tag(\n          &amp;quot;00080008&amp;quot; jsonb\n        , &amp;quot;00080012&amp;quot; jsonb\n        )\n    , jsonb_to_record(tag.&amp;quot;00080008&amp;quot;) AS &amp;quot;00080008&amp;quot;(\n        &amp;quot;Value&amp;quot; text[]\n    )\n    , jsonb_to_record(tag.&amp;quot;00080012&amp;quot;) AS &amp;quot;00080012&amp;quot;(\n        &amp;quot;Value&amp;quot; text[]\n    )\n;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;How can I make this query more robust to handle many keys being extracted from the original JSON data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zr0663", "is_robot_indexable": true, "report_reasons": null, "author": "dr_exercise", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zr0663/unnesting_json_in_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zr0663/unnesting_json_in_postgres/", "subreddit_subscribers": 83497, "created_utc": 1671571522.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Need some industry standard suggestions on building a ETL or ELT pipeline.\nWe are a small product based company focussed on billing software. We get to win new customers and that comes from a legacy database/application. We are looking to adopt either ETL or ELT model based on open source tools from market. Please suggest a method which could take in low code and time to develop a model.\n\nDiscussion made so far: \n\nDevelop a customer end windows batch executable file. This should enable the customer to run the executable and validate data based on the business rules to create CSV files. (They would run the batch file until the errors are phased out and a proper csv file is captured)\n\nCustomer should be able to upload final version of CSVs into Amazon S3 ( object versioning enabled).\n\nWe should be able to download those files and upload into Postgresql database ( minimum data validation here as data is already cured at the customer end)\n\nI am new to DE field and trying to share here for optimized way to develop the model in less time with low or no code.", "author_fullname": "t2_m6gnxiuj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CSV to Database based on data rules", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqva84", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671559849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need some industry standard suggestions on building a ETL or ELT pipeline.\nWe are a small product based company focussed on billing software. We get to win new customers and that comes from a legacy database/application. We are looking to adopt either ETL or ELT model based on open source tools from market. Please suggest a method which could take in low code and time to develop a model.&lt;/p&gt;\n\n&lt;p&gt;Discussion made so far: &lt;/p&gt;\n\n&lt;p&gt;Develop a customer end windows batch executable file. This should enable the customer to run the executable and validate data based on the business rules to create CSV files. (They would run the batch file until the errors are phased out and a proper csv file is captured)&lt;/p&gt;\n\n&lt;p&gt;Customer should be able to upload final version of CSVs into Amazon S3 ( object versioning enabled).&lt;/p&gt;\n\n&lt;p&gt;We should be able to download those files and upload into Postgresql database ( minimum data validation here as data is already cured at the customer end)&lt;/p&gt;\n\n&lt;p&gt;I am new to DE field and trying to share here for optimized way to develop the model in less time with low or no code.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "zqva84", "is_robot_indexable": true, "report_reasons": null, "author": "rajekum512", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqva84/csv_to_database_based_on_data_rules/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqva84/csv_to_database_based_on_data_rules/", "subreddit_subscribers": 83497, "created_utc": 1671559849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the best dashbording tool to use on a personal project?\n\nThanks!", "author_fullname": "t2_4d60lv03", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Dashboarding tools for projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zrn1v1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671630992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the best dashbording tool to use on a personal project?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zrn1v1", "is_robot_indexable": true, "report_reasons": null, "author": "Personpersonoerson", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zrn1v1/best_dashboarding_tools_for_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zrn1v1/best_dashboarding_tools_for_projects/", "subreddit_subscribers": 83497, "created_utc": 1671630992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background: I have been at a startup for 4 years as a DE (first job out of college). The startup has grown 10x since I joined so the role has shifted a lot as a result. The first 2ish years I was writing data pipeline scripts in Python, whereas the latter two years I\u2019ve been fully focused on data modeling (DBT / Snowflake). I ended up enjoying data modeling a lot more \u2014 I actually enjoy being a bit less \u201ctechy\u201d, and more that bridge between data and business (I majored in compsci and psych in undergrad so have always enjoyed a mix of hard / soft skills). Also, programming in SQL is really technically interesting for me. I didn\u2019t necessarily hate data pipeline work when I was doing it, but it was my first role out of college so I had no comparison. I enjoyed it quite a bit then but after doing data modeling work, the motivation is low to go back to do any rigorous data pipeline work (I still do code reviews for DEs who manage the ETL work).\n\nMy question is for my next role should I find specialized DE roles (aka Data Modeler roles) or should I go into something like BI / Analytics Engineering? Ideally I\u2019d be able to stay in Data Engineering, but just fully specialize in data modeling. \n\nAlso, not sure if super relevant, but I am finishing up a data science masters that I\u2019m doing part time that includes classes around data engineering, data viz, data ethics.\n\nThanks for any help!", "author_fullname": "t2_68eu0sys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling my favorite part of DE. Should I find specialized DE roles or move into BI / Analytics Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zriiq4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671623714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background: I have been at a startup for 4 years as a DE (first job out of college). The startup has grown 10x since I joined so the role has shifted a lot as a result. The first 2ish years I was writing data pipeline scripts in Python, whereas the latter two years I\u2019ve been fully focused on data modeling (DBT / Snowflake). I ended up enjoying data modeling a lot more \u2014 I actually enjoy being a bit less \u201ctechy\u201d, and more that bridge between data and business (I majored in compsci and psych in undergrad so have always enjoyed a mix of hard / soft skills). Also, programming in SQL is really technically interesting for me. I didn\u2019t necessarily hate data pipeline work when I was doing it, but it was my first role out of college so I had no comparison. I enjoyed it quite a bit then but after doing data modeling work, the motivation is low to go back to do any rigorous data pipeline work (I still do code reviews for DEs who manage the ETL work).&lt;/p&gt;\n\n&lt;p&gt;My question is for my next role should I find specialized DE roles (aka Data Modeler roles) or should I go into something like BI / Analytics Engineering? Ideally I\u2019d be able to stay in Data Engineering, but just fully specialize in data modeling. &lt;/p&gt;\n\n&lt;p&gt;Also, not sure if super relevant, but I am finishing up a data science masters that I\u2019m doing part time that includes classes around data engineering, data viz, data ethics.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zriiq4", "is_robot_indexable": true, "report_reasons": null, "author": "AdBeginning9561", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zriiq4/data_modeling_my_favorite_part_of_de_should_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zriiq4/data_modeling_my_favorite_part_of_de_should_i/", "subreddit_subscribers": 83497, "created_utc": 1671623714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can anyone here advice me on what the better and cost effective solution here is for ingesting streaming data? I'm inclined to say AWS Kinesis but I am not technically proficient with both to suggest this.", "author_fullname": "t2_arqcenpr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kafka Vs AWS Kinesis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zrhcsl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671621391.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anyone here advice me on what the better and cost effective solution here is for ingesting streaming data? I&amp;#39;m inclined to say AWS Kinesis but I am not technically proficient with both to suggest this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zrhcsl", "is_robot_indexable": true, "report_reasons": null, "author": "AMadRam", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zrhcsl/kafka_vs_aws_kinesis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zrhcsl/kafka_vs_aws_kinesis/", "subreddit_subscribers": 83497, "created_utc": 1671621391.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9wrfjdsj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rust visitor pattern and efficient DataFusion query federation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_zrgjw4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7p0DB0bKqvs_k1llYwDivUfSiiENWtIQAL_saFlY6vE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671618596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "splitgraph.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.splitgraph.com/blog/datafusion-filter-expr-visitor", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xSagZgWC-aoR6Ck2EFxow38MkSr8QrBShIx64eUAmbU.jpg?auto=webp&amp;s=9dad9245d0fbe33bc2ddfbd77a7c3555015a2144", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/xSagZgWC-aoR6Ck2EFxow38MkSr8QrBShIx64eUAmbU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6027862735db9c2640afae9f6fc6bffb4900fd7b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/xSagZgWC-aoR6Ck2EFxow38MkSr8QrBShIx64eUAmbU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=49befe3ac229c531b7fd372bd5b52756c4491208", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/xSagZgWC-aoR6Ck2EFxow38MkSr8QrBShIx64eUAmbU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f2a0c7529c196407c436a8f0e3cb0dd26a102b1", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/xSagZgWC-aoR6Ck2EFxow38MkSr8QrBShIx64eUAmbU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf20e9420162d29a704e2618c93f0fe0db2c0eb5", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/xSagZgWC-aoR6Ck2EFxow38MkSr8QrBShIx64eUAmbU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=03edf6ca10562d34fd7eb3935f478ec7ccf13bb2", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/xSagZgWC-aoR6Ck2EFxow38MkSr8QrBShIx64eUAmbU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3a2ee1998923f49c1f0c3d437ff68894369055e", "width": 1080, "height": 567}], "variants": {}, "id": "FOkKL6iab-aTE5bgB__YVyL30YwrptBbS18zNcxXDE8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zrgjw4", "is_robot_indexable": true, "report_reasons": null, "author": "gruuya", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zrgjw4/rust_visitor_pattern_and_efficient_datafusion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.splitgraph.com/blog/datafusion-filter-expr-visitor", "subreddit_subscribers": 83497, "created_utc": 1671618596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I'm new here, please let me know if this is the right place for questions like this.\n\nI'm working in a B2B SaaS company. Our product allows users to build content that can be shown for marketing/sales. I want to build out an analytics dashboard for my users so that they can see how well their content is doing and so on. This data is critical, I don't want it to drop off because of ad-blockers and the like.\n\n My questions are around how to collect this data within my own product. Here's a couple of options I've thought of:\n\n1. Native APIs - just build a \"track\" API of my own, and send data to my servers from the client. This feels like a straightforward solution, but feels a bit naive - I feel like there should be a better way to do this.\n2. Use a CDP, like segment.io - collect data and pipe it through segment on the client/server-side. Then send this across to my own server from segment. Feels like extra steps but could give me some flexibility in where I want to send this data later? I'm also concerned about segment being blocked on client web.\n\nIs there a better approach to doing this? How have you'll done it? Thanks!", "author_fullname": "t2_4yhzi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating/collecting analytics for my own product", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zraz97", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671599171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m new here, please let me know if this is the right place for questions like this.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working in a B2B SaaS company. Our product allows users to build content that can be shown for marketing/sales. I want to build out an analytics dashboard for my users so that they can see how well their content is doing and so on. This data is critical, I don&amp;#39;t want it to drop off because of ad-blockers and the like.&lt;/p&gt;\n\n&lt;p&gt;My questions are around how to collect this data within my own product. Here&amp;#39;s a couple of options I&amp;#39;ve thought of:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Native APIs - just build a &amp;quot;track&amp;quot; API of my own, and send data to my servers from the client. This feels like a straightforward solution, but feels a bit naive - I feel like there should be a better way to do this.&lt;/li&gt;\n&lt;li&gt;Use a CDP, like segment.io - collect data and pipe it through segment on the client/server-side. Then send this across to my own server from segment. Feels like extra steps but could give me some flexibility in where I want to send this data later? I&amp;#39;m also concerned about segment being blocked on client web.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Is there a better approach to doing this? How have you&amp;#39;ll done it? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zraz97", "is_robot_indexable": true, "report_reasons": null, "author": "SunshineParty", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zraz97/creatingcollecting_analytics_for_my_own_product/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zraz97/creatingcollecting_analytics_for_my_own_product/", "subreddit_subscribers": 83497, "created_utc": 1671599171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I would like to know if you have a pipeline for reviews in your company. Like having reviews data coming from App Store, Google Play Store and online reviews sites like Trustpilot, reviews\\[dot\\]io etc, then making a dashboard out of it.", "author_fullname": "t2_jvuhrmb0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does your company have a unified review dashboard?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqzxut", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671570979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I would like to know if you have a pipeline for reviews in your company. Like having reviews data coming from App Store, Google Play Store and online reviews sites like Trustpilot, reviews[dot]io etc, then making a dashboard out of it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zqzxut", "is_robot_indexable": true, "report_reasons": null, "author": "Necessary-Factor8861", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqzxut/does_your_company_have_a_unified_review_dashboard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqzxut/does_your_company_have_a_unified_review_dashboard/", "subreddit_subscribers": 83497, "created_utc": 1671570979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you create your Database migration services from AWS? Do you use the user interface (which means creating them manueally, task by task) or the API (so using your custom code/scripts)? I'm new to Data Engineering and in the process of getting on-boarded. Currently I need to replicate an existing task which means copy/pasting what is already there.\n\nAt the moment there have been 300+ tasks created and we are running in some performance issues with RDS. When trying to solve this issue one of the leading DEs had to stop and re-run each and every task. So, this is the point where I begin to question the total setup. To me, it seems someone threw the first red flag.\n\nSpeaking with my manager and the lead devs, nobody could give me an estimated figure how many more tasks need to be created to get the data migration done. Of course, there are phases, an sprints, and those meaningless daily standups which mean reporting to managers, etc...\n\nWe are migrating from various sources, oracle, Siemens MES, et... to Databricks as target, using Postgres in the middle.\n\nUnfortunately, I still know too little and must currently believe what is shown to me. I bring some experience in programming (webdev) and would have hoped that such DMS tasks are created accordingly via code.\n\nSo, please someone let me know. Is the above approach for 300+ normal? I can't help but have a gut feeling that the complete setup is fluffy. Is this the it is done usually, is it just my problem?\n\nBy end of December I will have passed 6 months. If coded, I personally would have used Scala (I'm currently teaching myself this language via Udemy, and some kind of fell in love with this language).\n\nShould I just give myself 6 more months, or start applying?\n\nI am somehow currently confused, perhaps disappointed, or disillusioned? Have I made a wrong decision?\n\nPersonally, it is important for me to learn and improve coding in the data engineering environment.\n\nEDIT:\n\nBackground 1: big pharma company outsourced the Data Engineering to well known consulting company. My employer now wants to bring back, insource the knowledge. So, this is meant by on-boarding. I get trained by the devs from the consulting company and some day should be able maintain the DE (with other colleagues). I just would like to know, are we getting sold a scalable setup or quick wins aka shit?\n\nBackground 2: All persons who signed/approved the current pipeline concept/architecture already left the company I was told.", "author_fullname": "t2_4fb1g9yu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you create your AWS DMS tasks using the UI or the API?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqy918", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671567825.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671566934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you create your Database migration services from AWS? Do you use the user interface (which means creating them manueally, task by task) or the API (so using your custom code/scripts)? I&amp;#39;m new to Data Engineering and in the process of getting on-boarded. Currently I need to replicate an existing task which means copy/pasting what is already there.&lt;/p&gt;\n\n&lt;p&gt;At the moment there have been 300+ tasks created and we are running in some performance issues with RDS. When trying to solve this issue one of the leading DEs had to stop and re-run each and every task. So, this is the point where I begin to question the total setup. To me, it seems someone threw the first red flag.&lt;/p&gt;\n\n&lt;p&gt;Speaking with my manager and the lead devs, nobody could give me an estimated figure how many more tasks need to be created to get the data migration done. Of course, there are phases, an sprints, and those meaningless daily standups which mean reporting to managers, etc...&lt;/p&gt;\n\n&lt;p&gt;We are migrating from various sources, oracle, Siemens MES, et... to Databricks as target, using Postgres in the middle.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, I still know too little and must currently believe what is shown to me. I bring some experience in programming (webdev) and would have hoped that such DMS tasks are created accordingly via code.&lt;/p&gt;\n\n&lt;p&gt;So, please someone let me know. Is the above approach for 300+ normal? I can&amp;#39;t help but have a gut feeling that the complete setup is fluffy. Is this the it is done usually, is it just my problem?&lt;/p&gt;\n\n&lt;p&gt;By end of December I will have passed 6 months. If coded, I personally would have used Scala (I&amp;#39;m currently teaching myself this language via Udemy, and some kind of fell in love with this language).&lt;/p&gt;\n\n&lt;p&gt;Should I just give myself 6 more months, or start applying?&lt;/p&gt;\n\n&lt;p&gt;I am somehow currently confused, perhaps disappointed, or disillusioned? Have I made a wrong decision?&lt;/p&gt;\n\n&lt;p&gt;Personally, it is important for me to learn and improve coding in the data engineering environment.&lt;/p&gt;\n\n&lt;p&gt;EDIT:&lt;/p&gt;\n\n&lt;p&gt;Background 1: big pharma company outsourced the Data Engineering to well known consulting company. My employer now wants to bring back, insource the knowledge. So, this is meant by on-boarding. I get trained by the devs from the consulting company and some day should be able maintain the DE (with other colleagues). I just would like to know, are we getting sold a scalable setup or quick wins aka shit?&lt;/p&gt;\n\n&lt;p&gt;Background 2: All persons who signed/approved the current pipeline concept/architecture already left the company I was told.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zqy918", "is_robot_indexable": true, "report_reasons": null, "author": "cptstoneee", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqy918/do_you_create_your_aws_dms_tasks_using_the_ui_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqy918/do_you_create_your_aws_dms_tasks_using_the_ui_or/", "subreddit_subscribers": 83497, "created_utc": 1671566934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,  I need some help in learning dbt for my new project and I don't know where to start. Earlier I've worked with Pyspark and SQL. So please can someone suggest from where I should start learning Dbt. If someone is already working on DBT projects, please dm me I want to know more about it.", "author_fullname": "t2_b4xkz1og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt for projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zrq46b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671635809.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,  I need some help in learning dbt for my new project and I don&amp;#39;t know where to start. Earlier I&amp;#39;ve worked with Pyspark and SQL. So please can someone suggest from where I should start learning Dbt. If someone is already working on DBT projects, please dm me I want to know more about it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zrq46b", "is_robot_indexable": true, "report_reasons": null, "author": "Ryukk11", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zrq46b/dbt_for_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zrq46b/dbt_for_projects/", "subreddit_subscribers": 83497, "created_utc": 1671635809.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been interviewing and got two offers - one of which is EXL Datasource. I\u2019m probably gonna take the other offer though.\n\nDoes anyone know EXL? I looked at Glassdoor and Indeed, and people say it\u2019s a good company but kind of disorganized and chaotic. \n\nI like the idea of working on different projects over time, so consulting fits me in that respect. But maybe this isn\u2019t the right company. Anyway would love to hear if you know anything.\n\nThanks!", "author_fullname": "t2_2o0q5m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anybody know data consulting firm EXL Datasource?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zrlldy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671628662.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been interviewing and got two offers - one of which is EXL Datasource. I\u2019m probably gonna take the other offer though.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know EXL? I looked at Glassdoor and Indeed, and people say it\u2019s a good company but kind of disorganized and chaotic. &lt;/p&gt;\n\n&lt;p&gt;I like the idea of working on different projects over time, so consulting fits me in that respect. But maybe this isn\u2019t the right company. Anyway would love to hear if you know anything.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zrlldy", "is_robot_indexable": true, "report_reasons": null, "author": "rotterdamn8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zrlldy/anybody_know_data_consulting_firm_exl_datasource/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zrlldy/anybody_know_data_consulting_firm_exl_datasource/", "subreddit_subscribers": 83497, "created_utc": 1671628662.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using Microsoft DQS to manage the quality of our data in our Data Warehouse in Microsoft SQL.\n\nDQS is clunky and buggy, and there is not enough support material out there. What alternatives do you recommend and where can i get the pricing and information for the company i work for?", "author_fullname": "t2_130figu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft Data Quality Services (DQS) Alternatives for DWH Quality management?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zrkjlf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671627003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using Microsoft DQS to manage the quality of our data in our Data Warehouse in Microsoft SQL.&lt;/p&gt;\n\n&lt;p&gt;DQS is clunky and buggy, and there is not enough support material out there. What alternatives do you recommend and where can i get the pricing and information for the company i work for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zrkjlf", "is_robot_indexable": true, "report_reasons": null, "author": "Ign_OranCe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zrkjlf/microsoft_data_quality_services_dqs_alternatives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zrkjlf/microsoft_data_quality_services_dqs_alternatives/", "subreddit_subscribers": 83497, "created_utc": 1671627003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working as a Data Engineer (mid level) on Data Science labs, mainly building pipelines/data assets to be used for modelling. I've been doing this for a year and was previously on a data graduate scheme for 18 months working mainly on data engineering/analysis projects. Currently using Airflow/Snowflake/Pyspark and we have starting moving onto AWS (Lambda, Step, SageMaker). I enjoy my work but thinking about changing and moving into the platform engineering space. I'm currently doing a Cloud DevOps course through work but thinking what would I need to move?", "author_fullname": "t2_4roqus0f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching to data platform engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zrip4e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671623979.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working as a Data Engineer (mid level) on Data Science labs, mainly building pipelines/data assets to be used for modelling. I&amp;#39;ve been doing this for a year and was previously on a data graduate scheme for 18 months working mainly on data engineering/analysis projects. Currently using Airflow/Snowflake/Pyspark and we have starting moving onto AWS (Lambda, Step, SageMaker). I enjoy my work but thinking about changing and moving into the platform engineering space. I&amp;#39;m currently doing a Cloud DevOps course through work but thinking what would I need to move?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zrip4e", "is_robot_indexable": true, "report_reasons": null, "author": "Prestigious_Math_658", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zrip4e/switching_to_data_platform_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zrip4e/switching_to_data_platform_engineering/", "subreddit_subscribers": 83497, "created_utc": 1671623979.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to know if you have a pipeline for reviews in your company. Like having reviews data coming from App Store, Google Play Store and online reviews sites like Trustpilot, reviews\\[dot\\]io etc, then making a dashboard out of it.", "author_fullname": "t2_jvuhrmb0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "As a DE, how do you deal with people/users reviews of your company product/services. Do you have a pipeline for that?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zrgqon", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671619272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to know if you have a pipeline for reviews in your company. Like having reviews data coming from App Store, Google Play Store and online reviews sites like Trustpilot, reviews[dot]io etc, then making a dashboard out of it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zrgqon", "is_robot_indexable": true, "report_reasons": null, "author": "Necessary-Factor8861", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zrgqon/as_a_de_how_do_you_deal_with_peopleusers_reviews/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zrgqon/as_a_de_how_do_you_deal_with_peopleusers_reviews/", "subreddit_subscribers": 83497, "created_utc": 1671619272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently I have been tasked with the painstaking process of parsing through ad tech unit ids and classifying them based on the strings they contain. \n\nI\u2019m trying to avoid a SQL nightmare in snowflake with engineering a solution for this. \n\nSo I\u2019m currently using python, pandas, re package for groupdict() objects, due to inconsistencies of the data. Then returning the data frame which I\u2019ll transfer back into our cloud warehouse due to\n\nI\u2019m fairly new to the best practices for this type of process as I\u2019m working to migrate from BI to DE, and am very much open to best processes or tools than can accomplish this task. \n\nFor example, if an ad unit ID is\n\nBobjoe/foo/bar/60s\n\nI\u2019ll use regex to parse:\n\n(?P&lt;group1&gt;patt)(?P&lt;group2&gt;patt)\n\nThen return the groupdict() and then can use json_normalize.", "author_fullname": "t2_tmd20iop", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimal methods to Create a scalable regex classification process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zr3u25", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671580378.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I have been tasked with the painstaking process of parsing through ad tech unit ids and classifying them based on the strings they contain. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to avoid a SQL nightmare in snowflake with engineering a solution for this. &lt;/p&gt;\n\n&lt;p&gt;So I\u2019m currently using python, pandas, re package for groupdict() objects, due to inconsistencies of the data. Then returning the data frame which I\u2019ll transfer back into our cloud warehouse due to&lt;/p&gt;\n\n&lt;p&gt;I\u2019m fairly new to the best practices for this type of process as I\u2019m working to migrate from BI to DE, and am very much open to best processes or tools than can accomplish this task. &lt;/p&gt;\n\n&lt;p&gt;For example, if an ad unit ID is&lt;/p&gt;\n\n&lt;p&gt;Bobjoe/foo/bar/60s&lt;/p&gt;\n\n&lt;p&gt;I\u2019ll use regex to parse:&lt;/p&gt;\n\n&lt;p&gt;(?P&amp;lt;group1&amp;gt;patt)(?P&amp;lt;group2&amp;gt;patt)&lt;/p&gt;\n\n&lt;p&gt;Then return the groupdict() and then can use json_normalize.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zr3u25", "is_robot_indexable": true, "report_reasons": null, "author": "robdiopoj", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zr3u25/optimal_methods_to_create_a_scalable_regex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zr3u25/optimal_methods_to_create_a_scalable_regex/", "subreddit_subscribers": 83497, "created_utc": 1671580378.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are pulling data from on-premises to AWS S3 bucket in parquet format using a Glue job. Planning to copy the data from S3 buck to Redshift tables in parallel. No transformation is needed. \n\nHave created an event notification for the S3 bucket (upon put and post of object) to invoke a lambda function that executes the Step function. In the Step function, have the state to start the Glue job that ingests data to S3 bucket and then do a parallel state of COPY lambda functions. \n\nIf I am testing manually by uploading files to S3 bucket, every time the Step function executes, it is loading the data multiple times into the tables due to parallel state. How can I make the Lambda functions in the Parallel state of Step functions execute only when that respective table's parquet file is uploaded into S3 bucket.\n\nIf Step functions is not the appropriate approach for this use case, what is the best solution to upload data to Redshift tables in parallel from S3 bucket with COPY command. This COPY process should be dependent on the Glue job completion.\n\nRead that Redshift auto copy but it is still in preview mode.\n\nAny help is appreciated.", "author_fullname": "t2_mspamalq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data flow to Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zquy11", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671559060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are pulling data from on-premises to AWS S3 bucket in parquet format using a Glue job. Planning to copy the data from S3 buck to Redshift tables in parallel. No transformation is needed. &lt;/p&gt;\n\n&lt;p&gt;Have created an event notification for the S3 bucket (upon put and post of object) to invoke a lambda function that executes the Step function. In the Step function, have the state to start the Glue job that ingests data to S3 bucket and then do a parallel state of COPY lambda functions. &lt;/p&gt;\n\n&lt;p&gt;If I am testing manually by uploading files to S3 bucket, every time the Step function executes, it is loading the data multiple times into the tables due to parallel state. How can I make the Lambda functions in the Parallel state of Step functions execute only when that respective table&amp;#39;s parquet file is uploaded into S3 bucket.&lt;/p&gt;\n\n&lt;p&gt;If Step functions is not the appropriate approach for this use case, what is the best solution to upload data to Redshift tables in parallel from S3 bucket with COPY command. This COPY process should be dependent on the Glue job completion.&lt;/p&gt;\n\n&lt;p&gt;Read that Redshift auto copy but it is still in preview mode.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zquy11", "is_robot_indexable": true, "report_reasons": null, "author": "Awsmason", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zquy11/data_flow_to_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zquy11/data_flow_to_cloud/", "subreddit_subscribers": 83497, "created_utc": 1671559060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "let's say I have source data like this:\n\ncustomers:\n\nid | name | updated_at\n---|---------|-----------------\n1 | bob | 2022-12-19\n2 | frank | 2022-12-18\n1 | bob2 | 2022-12-20\n\nand I want to create an SCD2 out of it\n\nwhat is the general process to do this in DBT?\n\nAs far as I understand, Snapshots work only with mutable data. Using a snapshot directly would have duplicates\n\nI can create an SCD1 based on source.customers latest data, and then snapshot that\n\nBut I'm not sure how I'd build the SCD2 with full history...\n\nTIA", "author_fullname": "t2_z3784il", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Slowly Changing Dimension Type 2 (SCD2) when source data is immutable / append-only?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zqu7ix", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671557268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;let&amp;#39;s say I have source data like this:&lt;/p&gt;\n\n&lt;p&gt;customers:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;id&lt;/th&gt;\n&lt;th&gt;name&lt;/th&gt;\n&lt;th&gt;updated_at&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;bob&lt;/td&gt;\n&lt;td&gt;2022-12-19&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;frank&lt;/td&gt;\n&lt;td&gt;2022-12-18&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;bob2&lt;/td&gt;\n&lt;td&gt;2022-12-20&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;and I want to create an SCD2 out of it&lt;/p&gt;\n\n&lt;p&gt;what is the general process to do this in DBT?&lt;/p&gt;\n\n&lt;p&gt;As far as I understand, Snapshots work only with mutable data. Using a snapshot directly would have duplicates&lt;/p&gt;\n\n&lt;p&gt;I can create an SCD1 based on source.customers latest data, and then snapshot that&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m not sure how I&amp;#39;d build the SCD2 with full history...&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zqu7ix", "is_robot_indexable": true, "report_reasons": null, "author": "boy_named_su", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zqu7ix/dbt_slowly_changing_dimension_type_2_scd2_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zqu7ix/dbt_slowly_changing_dimension_type_2_scd2_when/", "subreddit_subscribers": 83497, "created_utc": 1671557268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2wsvqwhg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bad Data Engineering Practices And How To Avoid Them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_zr8dc7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.14, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/UEgrAyFy8emTSb--GCAN6BZjmdj7MoBuO3wDKiDVwn0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671591476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "marktechpost.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.marktechpost.com/2022/12/20/bad-data-engineering-practices-and-how-to-avoid-them/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/to34Io9j3y0UDzSKvo-471H2SCQypFkkbm9AJkUdrcw.jpg?auto=webp&amp;s=6b6eaf12f7862897148af3e6514041d07c63bbf0", "width": 2560, "height": 1920}, "resolutions": [{"url": "https://external-preview.redd.it/to34Io9j3y0UDzSKvo-471H2SCQypFkkbm9AJkUdrcw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=805b0a5466903290e286e861d44d218c5cdc758d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/to34Io9j3y0UDzSKvo-471H2SCQypFkkbm9AJkUdrcw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2af4fbe6eb81c5bb5eb7eb1ea900ae0b826d3ba", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/to34Io9j3y0UDzSKvo-471H2SCQypFkkbm9AJkUdrcw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad7154ad64a4f1910f0cc517dc2093cacbba7bd5", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/to34Io9j3y0UDzSKvo-471H2SCQypFkkbm9AJkUdrcw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ff59fe8721dc1b7ee4fc0548c3a13e26073982e", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/to34Io9j3y0UDzSKvo-471H2SCQypFkkbm9AJkUdrcw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d31fd729a88b6ee0b38def48b50d8ee4d513314a", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/to34Io9j3y0UDzSKvo-471H2SCQypFkkbm9AJkUdrcw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ed60211ae8fac2acca9167d7213d3ecf6863eafa", "width": 1080, "height": 810}], "variants": {}, "id": "sXsKFUuaCBmtFu6sW9HGXbRPwkL0EYYrCxKIFJFq7vE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zr8dc7", "is_robot_indexable": true, "report_reasons": null, "author": "ai-lover", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zr8dc7/bad_data_engineering_practices_and_how_to_avoid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.marktechpost.com/2022/12/20/bad-data-engineering-practices-and-how-to-avoid-them/", "subreddit_subscribers": 83497, "created_utc": 1671591476.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}