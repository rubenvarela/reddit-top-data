{"kind": "Listing", "data": {"after": "t3_zezh0a", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_yeda6sl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheat sheets on data life cycle, PySpark, dbt, Kafka, BigQuery, Airflow, and Docker.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_zf01xv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/jnAuxFJ7FUK5Iahrv4PaSmv255B5F5x5gonxzbUuD10.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670416431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kdnuggets.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.kdnuggets.com/2022/12/7-essential-cheat-sheets-data-engineering.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qXKhO1YkZh9VnI_-EHqSiDah-R5MlCadytMq1X88YZI.jpg?auto=webp&amp;s=c9b7cd9c73faba76573308abafdf501ae9f4d32b", "width": 1000, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qXKhO1YkZh9VnI_-EHqSiDah-R5MlCadytMq1X88YZI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8494f67be4a98a5e99363aca96134d6c359b4046", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/qXKhO1YkZh9VnI_-EHqSiDah-R5MlCadytMq1X88YZI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bd8a77bba48bd792119d4f6ab606e9ae35316eee", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/qXKhO1YkZh9VnI_-EHqSiDah-R5MlCadytMq1X88YZI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fdf355ca2a0edbb10234946aa8a567b5676fa9c4", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/qXKhO1YkZh9VnI_-EHqSiDah-R5MlCadytMq1X88YZI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=10e3e2e2aa447ee1188f0c0af00a3c50bdcb14e1", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/qXKhO1YkZh9VnI_-EHqSiDah-R5MlCadytMq1X88YZI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f53a2e0e98e3be44fd7c218ae425fc3725a3e70", "width": 960, "height": 576}], "variants": {}, "id": "fyZAesjAe63njoQYSxc2IjZ_YgDi9aZJ-ZEcfay2d9k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zf01xv", "is_robot_indexable": true, "report_reasons": null, "author": "kingabzpro", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf01xv/cheat_sheets_on_data_life_cycle_pyspark_dbt_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.kdnuggets.com/2022/12/7-essential-cheat-sheets-data-engineering.html", "subreddit_subscribers": 82138, "created_utc": 1670416431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So basically the title. It seems like most companies went on rampage to hire data scientists, and now they're searching for 1-2 data engineers. There's no hierarchy that I know of for data engineers, and I have 3 offers from big companies to do their data engineering work in the cloud. In my present company we are a much bigger team, but we're paid shit.\n\nIn these big companies the pay is huge (3x to what I'm paid now) but the backdrop is I'll have to do most of the stuff along with another person to make a warehouse for their analysts/data scientists. Should I jump ship or stay here?", "author_fullname": "t2_l35gwhuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is It Bad To Be the Only Data Engineer Hired in a Company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zegx37", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670358043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So basically the title. It seems like most companies went on rampage to hire data scientists, and now they&amp;#39;re searching for 1-2 data engineers. There&amp;#39;s no hierarchy that I know of for data engineers, and I have 3 offers from big companies to do their data engineering work in the cloud. In my present company we are a much bigger team, but we&amp;#39;re paid shit.&lt;/p&gt;\n\n&lt;p&gt;In these big companies the pay is huge (3x to what I&amp;#39;m paid now) but the backdrop is I&amp;#39;ll have to do most of the stuff along with another person to make a warehouse for their analysts/data scientists. Should I jump ship or stay here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zegx37", "is_robot_indexable": true, "report_reasons": null, "author": "Senior_Anteater4688", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zegx37/is_it_bad_to_be_the_only_data_engineer_hired_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zegx37/is_it_bad_to_be_the_only_data_engineer_hired_in_a/", "subreddit_subscribers": 82138, "created_utc": 1670358043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Not sure if Analytics engineers is the correct term. I'm referring to the position that essentially transforms business oriented questions into queries and dashboards. ChatGPT seems to wipe the floor with such tasks and my manager is already hyping us to build something on top of it. \n\nI'm interested in hearing your opinions, as I'm rather green in the field.\n\nThanks!", "author_fullname": "t2_xptd9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analytics engineering with ChatGPT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zefba4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670354270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if Analytics engineers is the correct term. I&amp;#39;m referring to the position that essentially transforms business oriented questions into queries and dashboards. ChatGPT seems to wipe the floor with such tasks and my manager is already hyping us to build something on top of it. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in hearing your opinions, as I&amp;#39;m rather green in the field.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zefba4", "is_robot_indexable": true, "report_reasons": null, "author": "UltimateHorse", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zefba4/analytics_engineering_with_chatgpt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zefba4/analytics_engineering_with_chatgpt/", "subreddit_subscribers": 82138, "created_utc": 1670354270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any things you wished you had done during your first few weeks at your current job?\n\nI know I would have started earlier with a list of all the acronyms used in the data team. That would have saved me a headache later!", "author_fullname": "t2_3cfsa44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting a new Data Engineering job (mid level)- what would you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zewn9t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670404620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any things you wished you had done during your first few weeks at your current job?&lt;/p&gt;\n\n&lt;p&gt;I know I would have started earlier with a list of all the acronyms used in the data team. That would have saved me a headache later!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zewn9t", "is_robot_indexable": true, "report_reasons": null, "author": "squaricle", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zewn9t/starting_a_new_data_engineering_job_mid_level/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zewn9t/starting_a_new_data_engineering_job_mid_level/", "subreddit_subscribers": 82138, "created_utc": 1670404620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data engineer at a mid sized company.  Our current efforts are only in Azure using technologies like data factory and Synapse.\n\nI see a lot of talk on here about AWS, dbt, airflow, and many other tools that we don't use and I won't get to use given our workflows. With this trajectory, have I shot myself in the foot for future DE jobs if the direction of data engineering doesnt lie in the Azure space?", "author_fullname": "t2_1iuqwzgb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zejv8f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670364985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data engineer at a mid sized company.  Our current efforts are only in Azure using technologies like data factory and Synapse.&lt;/p&gt;\n\n&lt;p&gt;I see a lot of talk on here about AWS, dbt, airflow, and many other tools that we don&amp;#39;t use and I won&amp;#39;t get to use given our workflows. With this trajectory, have I shot myself in the foot for future DE jobs if the direction of data engineering doesnt lie in the Azure space?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zejv8f", "is_robot_indexable": true, "report_reasons": null, "author": "rennja", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zejv8f/azure_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zejv8f/azure_de/", "subreddit_subscribers": 82138, "created_utc": 1670364985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi redditors,\n\nThis is an interview question in Spark which has had me going into rabbithole of spark and yet I'm unable to find an answer. \n\nSo the question is, once you have submitted a spark job and see that it is running slow, how do you increase the cores/resources on the fly( without stopping and re running the job with increased resources). Is this even possible?", "author_fullname": "t2_98w2eb47", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview question in Spark yet to be solved", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zeg4nm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670356236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi redditors,&lt;/p&gt;\n\n&lt;p&gt;This is an interview question in Spark which has had me going into rabbithole of spark and yet I&amp;#39;m unable to find an answer. &lt;/p&gt;\n\n&lt;p&gt;So the question is, once you have submitted a spark job and see that it is running slow, how do you increase the cores/resources on the fly( without stopping and re running the job with increased resources). Is this even possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "zeg4nm", "is_robot_indexable": true, "report_reasons": null, "author": "cherryblossomslove", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zeg4nm/interview_question_in_spark_yet_to_be_solved/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zeg4nm/interview_question_in_spark_yet_to_be_solved/", "subreddit_subscribers": 82138, "created_utc": 1670356236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Basically I\u2019m wondering if there\u2019s a columnar or other NOSQL equivalent lightweight database that could be used for small pipelines in a pinch. Is that basically what parquet files are used for? Is there a way to bundle parquet files together into more of a database rather than standalone tables?", "author_fullname": "t2_1gyiwuuv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQLite NOSQL Alternatives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zevwlz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670401591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically I\u2019m wondering if there\u2019s a columnar or other NOSQL equivalent lightweight database that could be used for small pipelines in a pinch. Is that basically what parquet files are used for? Is there a way to bundle parquet files together into more of a database rather than standalone tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zevwlz", "is_robot_indexable": true, "report_reasons": null, "author": "trianglesteve", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zevwlz/sqlite_nosql_alternatives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zevwlz/sqlite_nosql_alternatives/", "subreddit_subscribers": 82138, "created_utc": 1670401591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It's honestly really impressive that ChatGPT can solve some Leetcode problems, but does anyone else think this will positively impact the tech interview process?  If interviews historically selected for candidates who pass these technical tests, doesn't it make sense to want to move to a system that an ML model cannot excel at?\n\nI can only see Leetcode still existing in the future to interview entry level engineers, because, what else are you going to ask someone who is just starting out in the field?\n\nBut for anyone with more than a year of experience or moving on to their second job, I'm thinking that interviewers will place much more weight into system design, past experience, and the ability to work well in a team.\n\nChatGPT does show that it's kind of funny to be asking engineers who have graduated several years ago questions from a class they took as first and second year students.", "author_fullname": "t2_3v6ob2bf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is ChatGPT going to affect the technical interview?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zf0t4r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670419117.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670418714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s honestly really impressive that ChatGPT can solve some Leetcode problems, but does anyone else think this will positively impact the tech interview process?  If interviews historically selected for candidates who pass these technical tests, doesn&amp;#39;t it make sense to want to move to a system that an ML model cannot excel at?&lt;/p&gt;\n\n&lt;p&gt;I can only see Leetcode still existing in the future to interview entry level engineers, because, what else are you going to ask someone who is just starting out in the field?&lt;/p&gt;\n\n&lt;p&gt;But for anyone with more than a year of experience or moving on to their second job, I&amp;#39;m thinking that interviewers will place much more weight into system design, past experience, and the ability to work well in a team.&lt;/p&gt;\n\n&lt;p&gt;ChatGPT does show that it&amp;#39;s kind of funny to be asking engineers who have graduated several years ago questions from a class they took as first and second year students.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zf0t4r", "is_robot_indexable": true, "report_reasons": null, "author": "data_preprocessing", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf0t4r/how_is_chatgpt_going_to_affect_the_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zf0t4r/how_is_chatgpt_going_to_affect_the_technical/", "subreddit_subscribers": 82138, "created_utc": 1670418714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,\n\nThis year, I'm doing the challenges in postgresql select statements. I'm quite enjoying this as what would be quite simple in other languages require me to reach for less commonly used functionality found in postgresql.\n\nFeedback always welcome! \n\n&amp;#x200B;\n\n\\[spoilers ahead\\]: \\_If you're also solving the puzzles, don't click on the link, as the repo contains solutions\\_\n\n[https://github.com/haleemur/advent-of-code-2022](https://github.com/haleemur/advent-of-code-2022)", "author_fullname": "t2_4ekasbko", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advent of Code 2022: postgresql", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zekcwl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "spoiler", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1670366161.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;This year, I&amp;#39;m doing the challenges in postgresql select statements. I&amp;#39;m quite enjoying this as what would be quite simple in other languages require me to reach for less commonly used functionality found in postgresql.&lt;/p&gt;\n\n&lt;p&gt;Feedback always welcome! &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;[spoilers ahead]: _If you&amp;#39;re also solving the puzzles, don&amp;#39;t click on the link, as the repo contains solutions_&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/haleemur/advent-of-code-2022\"&gt;https://github.com/haleemur/advent-of-code-2022&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?auto=webp&amp;s=caa2320c0b39e1e21541996c1c7a6cb2207ce60c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=50b6c4bed72729b860f4a284f2a87a2691a575b8", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=daf45427f8b9606c07ba74d8c423fb6dbe51ea81", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c30fe6eaa6d3d77802b40d43d46ec4fdab94793", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f7cd08c555c415c70e6a7c9f1601249e539d6c01", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=42b7b31d851ecbece66405a5895d435474ae4928", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d5832f396c79886ec271772b23ad1e887f4cc221", "width": 1080, "height": 540}], "variants": {"obfuscated": {"source": {"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?blur=40&amp;format=pjpg&amp;auto=webp&amp;s=0662d45cdb93a240e3a3557e6141b7000d958873", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;s=85eb167cf3ed33c7d46a0ccbcba3e564417499a0", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;s=c718e2a72120593085a510a241723d8c028cb2b5", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;s=883fe69e4a7781f742eea7e03af411aa568a9a25", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?width=640&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=089ec0434da7e7bb39d80169087eb34da780fbfe", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?width=960&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=8e5e52894225f018b2431af59ffbad8b5673d807", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/GsZpCtCz1KOWkDaqjdibAY6WYR32NDR-Qwdl61ga0-U.jpg?width=1080&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=f4a8b96602bc274a1516f427e90d194e22a05be8", "width": 1080, "height": 540}]}}, "id": "S8A3J0BS8lrnHzv3560K2xI4ngDSySrpLsLqHvwsZ_Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": true, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "zekcwl", "is_robot_indexable": true, "report_reasons": null, "author": "haaaaaal", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zekcwl/advent_of_code_2022_postgresql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zekcwl/advent_of_code_2022_postgresql/", "subreddit_subscribers": 82138, "created_utc": 1670366161.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For current DEs, what is one thing you were expecting going into the DE field vs what actually occurred?\n\nBoth the good and the bad", "author_fullname": "t2_3p620rl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expectations Vs Reality", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zf68rv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670431778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For current DEs, what is one thing you were expecting going into the DE field vs what actually occurred?&lt;/p&gt;\n\n&lt;p&gt;Both the good and the bad&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "DE @ Amazon/Lyft/Author of Ace DE Interview", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zf68rv", "is_robot_indexable": true, "report_reasons": null, "author": "chrisgarzon19", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/zf68rv/expectations_vs_reality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zf68rv/expectations_vs_reality/", "subreddit_subscribers": 82138, "created_utc": 1670431778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3yleu7rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How To Measure The ROI Of Your Data Spend?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zf9dob", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1670438048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "alvin.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.alvin.ai/posts/how-to-measure-the-roi-of-your-data-spend", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zf9dob", "is_robot_indexable": true, "report_reasons": null, "author": "gabsferreiradev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf9dob/how_to_measure_the_roi_of_your_data_spend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.alvin.ai/posts/how-to-measure-the-roi-of-your-data-spend", "subreddit_subscribers": 82138, "created_utc": 1670438048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for some advice on how to move my companies stack forward in an on prem environment, without any plans of moving to the cloud. \n\nWe are running SSIS for everything right now but ideally would like to move towards a more modern stack, but not sure what direction. Airflow, ADF, etc. \n\nI joined more recently and have a python/DS background and honestly prefer that world over SSIS, but the rest of the team is more classic ETL/SQL Server based, but willing to grow. the vibe is basically \u201cWhat best for future proofing, talent attraction, etc\u201d.\n\nNo real K8 infra but run VXrails, so potentially deployable. \n\nOnly hard stop is gotta keep the old on prem SQL Server. \ud83d\ude1e\n\nAppreciate the help!", "author_fullname": "t2_3sioksrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to move stack forward On- Prem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zer9b3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670385105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some advice on how to move my companies stack forward in an on prem environment, without any plans of moving to the cloud. &lt;/p&gt;\n\n&lt;p&gt;We are running SSIS for everything right now but ideally would like to move towards a more modern stack, but not sure what direction. Airflow, ADF, etc. &lt;/p&gt;\n\n&lt;p&gt;I joined more recently and have a python/DS background and honestly prefer that world over SSIS, but the rest of the team is more classic ETL/SQL Server based, but willing to grow. the vibe is basically \u201cWhat best for future proofing, talent attraction, etc\u201d.&lt;/p&gt;\n\n&lt;p&gt;No real K8 infra but run VXrails, so potentially deployable. &lt;/p&gt;\n\n&lt;p&gt;Only hard stop is gotta keep the old on prem SQL Server. \ud83d\ude1e&lt;/p&gt;\n\n&lt;p&gt;Appreciate the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zer9b3", "is_robot_indexable": true, "report_reasons": null, "author": "Remote-Stay", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zer9b3/how_to_move_stack_forward_on_prem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zer9b3/how_to_move_stack_forward_on_prem/", "subreddit_subscribers": 82138, "created_utc": 1670385105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Erlang is used to develop high-available, reliable, concurrent and scalable systems, it seems the language is used mostly in telecom systems. There is the amazing [WhatsApp case](https://blog.whatsapp.com/1-million-is-so-2011) using it to scale the number of simultaneous connections per server that show us the power of the language and the model it is build on. So why is it not widely used in Data Engineering at all?", "author_fullname": "t2_moqo59rs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is not functional programming languages as Erlang or Elixir extensively used in Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zek84k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670365845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Erlang is used to develop high-available, reliable, concurrent and scalable systems, it seems the language is used mostly in telecom systems. There is the amazing &lt;a href=\"https://blog.whatsapp.com/1-million-is-so-2011\"&gt;WhatsApp case&lt;/a&gt; using it to scale the number of simultaneous connections per server that show us the power of the language and the model it is build on. So why is it not widely used in Data Engineering at all?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Uq8iY6_RDS2pyrlqJzbV6rrFEi04sgPJYYilt0exvgY.jpg?auto=webp&amp;s=92aa12260415317a691bfb8d0bd74801ed6fd437", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Uq8iY6_RDS2pyrlqJzbV6rrFEi04sgPJYYilt0exvgY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=78e2e5b5a7261b381d21a7814a6ecf296e720309", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Uq8iY6_RDS2pyrlqJzbV6rrFEi04sgPJYYilt0exvgY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=19c607dac10bfe95d52be640ecd138856a888b1f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Uq8iY6_RDS2pyrlqJzbV6rrFEi04sgPJYYilt0exvgY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89c206e2ccc504eb45d10ae2661c0c10411411c1", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Uq8iY6_RDS2pyrlqJzbV6rrFEi04sgPJYYilt0exvgY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5f65a4f0462967cf9bc738aac66d1017c0040de", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Uq8iY6_RDS2pyrlqJzbV6rrFEi04sgPJYYilt0exvgY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=41feb45fd60d1ca671414401fe9743645e4f58be", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Uq8iY6_RDS2pyrlqJzbV6rrFEi04sgPJYYilt0exvgY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=117ae682267f03e21dd55f1a9687523c130cbe72", "width": 1080, "height": 567}], "variants": {}, "id": "ptnDQ1fv_7VR0hWhaFloBxw_rEWyBVOzcyTHVYFTOJU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zek84k", "is_robot_indexable": true, "report_reasons": null, "author": "gato_noir", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zek84k/why_is_not_functional_programming_languages_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zek84k/why_is_not_functional_programming_languages_as/", "subreddit_subscribers": 82138, "created_utc": 1670365845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm part of a research team at a smaller company which has worked in the field of datamodeling for 20+ yrs.  \n\n\nWe have repeatedly observed that for companies using DataWarehousing, documenting source systems provides a challenge.\n\nIn a small project team we would like to develop new and effective solutions for these problems.\n\nFor anyone with experience in this field, we would be highly grateful for your opinions on the following questions:\n\n1. What challenges do you know of when it comes to documenting a database/source system? \n\n2. What has proven to be especially challenging when documenting CRM systems such as Salesforce?\n\n3. Have you encountered any specific difficulties while documenting ERP systems like SAP?\n\nThank you very much in advance, we're looking forward to hearing from you :)", "author_fullname": "t2_uovnqo6g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inquiry about Database Documentation &amp; DataWarehousing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zf5auk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670429849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m part of a research team at a smaller company which has worked in the field of datamodeling for 20+ yrs.  &lt;/p&gt;\n\n&lt;p&gt;We have repeatedly observed that for companies using DataWarehousing, documenting source systems provides a challenge.&lt;/p&gt;\n\n&lt;p&gt;In a small project team we would like to develop new and effective solutions for these problems.&lt;/p&gt;\n\n&lt;p&gt;For anyone with experience in this field, we would be highly grateful for your opinions on the following questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;What challenges do you know of when it comes to documenting a database/source system? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What has proven to be especially challenging when documenting CRM systems such as Salesforce?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Have you encountered any specific difficulties while documenting ERP systems like SAP?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thank you very much in advance, we&amp;#39;re looking forward to hearing from you :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "zf5auk", "is_robot_indexable": true, "report_reasons": null, "author": "heureka_leo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf5auk/inquiry_about_database_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zf5auk/inquiry_about_database_documentation/", "subreddit_subscribers": 82138, "created_utc": 1670429849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4w5k4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The top 5 data trends to look out for in 2023 (by Velotix)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_zf10h0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/FCKC4DSuJ3rEs3oRJiqhNV762ulrj16qJzGaQzz-8H0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670419323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "velotix.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.velotix.ai/resources/blog/top-data-trends/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?auto=webp&amp;s=f6a6fcd59eff984940d3a85ed5a537b57fc18e1f", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95b746618bca90d0c6682c8a047a2ce0901f451f", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8a873fe73c58425a906cf03bf2c56520e12514fb", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b30bd008f3a0a0a7d6e8dd74dbcc4c68fa2cfcc", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3132c3e87483e8edae2a195232d85c7df62cf727", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=89c0dd19f8043f0560133a1ff2ded812ccff6073", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5fe8058972fb2f2ddb0bd7ff94b9502bbb0a5575", "width": 1080, "height": 565}], "variants": {}, "id": "pose6bqHwmZc9D-dEY3FCT4KDlRQPDN-dV8Q5QdEGZI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zf10h0", "is_robot_indexable": true, "report_reasons": null, "author": "cobano", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf10h0/the_top_5_data_trends_to_look_out_for_in_2023_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.velotix.ai/resources/blog/top-data-trends/", "subreddit_subscribers": 82138, "created_utc": 1670419323.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIn my company I have been assigned the task to create a data streaming pipeline to move data from mysql to a datalake.\n\nTo achieve this task, I have created a pipeline using Debezium, Kafka and Spark Structured Streaming.\n\nDebezium writes all the events to a single kafka topic.\n\nThe Spark job uses the \\`forEachBatch\\` function, it reads events, divides them by table, and makes sure to upsert events in each corresponding table using Delta.\n\nOne tough requirement to implement is that this data should be directly queried by data consumers, it is a tough requirement because due to grouping upserts by table I am changing the events order (performing upserts in an ordered way for each individual transaction would be extremely inefficient). This could lead to potential data inconsistencies because during the batch processing time some tables are more updated than others.\n\nOne way to mitigate this issue I came up with is to let users access these tables only through athena/snowflake, this way I can decouple what data I am showing from what data is actually inside the table by using the [symlink manifest files](https://docs.delta.io/latest/presto-integration.html#set-up-the-presto-trino-or-athena-to-delta-lake-integration-and-query-delta-tables). This is a workaround and not a definitive solution, because it reduces the time tables are inconsistent to the time it takes to refresh all the manifests.\n\nIs there any better way to achieve consistency between tables in a streaming system?\n\nI am also tempted to reduce inconsistency by minimizing latency , creating append-only tables, but I am afraid these tables will start growing exponentially.", "author_fullname": "t2_do2bj7xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming Jobs and tables consistency", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zewhcj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670406352.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670403957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;In my company I have been assigned the task to create a data streaming pipeline to move data from mysql to a datalake.&lt;/p&gt;\n\n&lt;p&gt;To achieve this task, I have created a pipeline using Debezium, Kafka and Spark Structured Streaming.&lt;/p&gt;\n\n&lt;p&gt;Debezium writes all the events to a single kafka topic.&lt;/p&gt;\n\n&lt;p&gt;The Spark job uses the `forEachBatch` function, it reads events, divides them by table, and makes sure to upsert events in each corresponding table using Delta.&lt;/p&gt;\n\n&lt;p&gt;One tough requirement to implement is that this data should be directly queried by data consumers, it is a tough requirement because due to grouping upserts by table I am changing the events order (performing upserts in an ordered way for each individual transaction would be extremely inefficient). This could lead to potential data inconsistencies because during the batch processing time some tables are more updated than others.&lt;/p&gt;\n\n&lt;p&gt;One way to mitigate this issue I came up with is to let users access these tables only through athena/snowflake, this way I can decouple what data I am showing from what data is actually inside the table by using the &lt;a href=\"https://docs.delta.io/latest/presto-integration.html#set-up-the-presto-trino-or-athena-to-delta-lake-integration-and-query-delta-tables\"&gt;symlink manifest files&lt;/a&gt;. This is a workaround and not a definitive solution, because it reduces the time tables are inconsistent to the time it takes to refresh all the manifests.&lt;/p&gt;\n\n&lt;p&gt;Is there any better way to achieve consistency between tables in a streaming system?&lt;/p&gt;\n\n&lt;p&gt;I am also tempted to reduce inconsistency by minimizing latency , creating append-only tables, but I am afraid these tables will start growing exponentially.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zewhcj", "is_robot_indexable": true, "report_reasons": null, "author": "somerandomdataeng", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/zewhcj/streaming_jobs_and_tables_consistency/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zewhcj/streaming_jobs_and_tables_consistency/", "subreddit_subscribers": 82138, "created_utc": 1670403957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my experience as a data engineer in a big tech, I wrote a mock data library that creates data based on the schema and Dtype you configure and have used it pretty extensively for testing pipelines. Never tested on production data, because anonymizing it and copying it out of prod workflow was not approved by data governance and legal teams :/    \n\n\nI'm curious how you folks in the industry do it!\n\n[View Poll](https://www.reddit.com/poll/zetwd0)", "author_fullname": "t2_msviuy1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you get the right data to test your ETL pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zetwd0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670393769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my experience as a data engineer in a big tech, I wrote a mock data library that creates data based on the schema and Dtype you configure and have used it pretty extensively for testing pipelines. Never tested on production data, because anonymizing it and copying it out of prod workflow was not approved by data governance and legal teams :/    &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious how you folks in the industry do it!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/zetwd0\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zetwd0", "is_robot_indexable": true, "report_reasons": null, "author": "vino_and_data", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1670998569803, "options": [{"text": "Mock data", "id": "20235477"}, {"text": "Copy production data to staging/test env", "id": "20235478"}, {"text": "Copy anonymized production data to staging", "id": "20235479"}, {"text": "Use data versioning tool to mirror prod env", "id": "20235480"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 171, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zetwd0/how_do_you_get_the_right_data_to_test_your_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/zetwd0/how_do_you_get_the_right_data_to_test_your_etl/", "subreddit_subscribers": 82138, "created_utc": 1670393769.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For those who do their transformations with SQL, how does your deployment pipeline work?\n\nDo you have powers to deploy the procedures, views, etc to production?  \nIf you need to handoff your code, how does that work?\nDo you check-in your code to a source control system?", "author_fullname": "t2_bdzq3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying SQL transformations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zeorv2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670377691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those who do their transformations with SQL, how does your deployment pipeline work?&lt;/p&gt;\n\n&lt;p&gt;Do you have powers to deploy the procedures, views, etc to production?&lt;br/&gt;\nIf you need to handoff your code, how does that work?\nDo you check-in your code to a source control system?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zeorv2", "is_robot_indexable": true, "report_reasons": null, "author": "Acewox", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zeorv2/deploying_sql_transformations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zeorv2/deploying_sql_transformations/", "subreddit_subscribers": 82138, "created_utc": 1670377691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi experts,\n\nWe have a test Airflow (actually Google Composer) cluster that consumes DAGs through a single repo and show the updates in Airflow web UI.\n\nBasically, whenever someone merges feature branches into `test` branch, a Jenkins job copies/pastes the whole repo into the appropriate Google bucket, which is used by the Composer cluster. This creates an obvious issue that one developer's merge will remove all changes of other developers, if they still need to test on `test` branch and have not merged into `master` yet.\n\nWhen the team is small, this is a non-issue. We rarely had conflicts. Now that the team is big enough, I'm thinking about making some changes. Here is the plan I'm considering:\n\nInstead of simply copying/pasting everything into the bucket, I'd write a GitHub Action that picks the files modified and only copy/paste those. Does it make sense? How would you approach the problem?", "author_fullname": "t2_ldvtxo0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage single repo Airflow test environment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zeonf6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670377327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi experts,&lt;/p&gt;\n\n&lt;p&gt;We have a test Airflow (actually Google Composer) cluster that consumes DAGs through a single repo and show the updates in Airflow web UI.&lt;/p&gt;\n\n&lt;p&gt;Basically, whenever someone merges feature branches into &lt;code&gt;test&lt;/code&gt; branch, a Jenkins job copies/pastes the whole repo into the appropriate Google bucket, which is used by the Composer cluster. This creates an obvious issue that one developer&amp;#39;s merge will remove all changes of other developers, if they still need to test on &lt;code&gt;test&lt;/code&gt; branch and have not merged into &lt;code&gt;master&lt;/code&gt; yet.&lt;/p&gt;\n\n&lt;p&gt;When the team is small, this is a non-issue. We rarely had conflicts. Now that the team is big enough, I&amp;#39;m thinking about making some changes. Here is the plan I&amp;#39;m considering:&lt;/p&gt;\n\n&lt;p&gt;Instead of simply copying/pasting everything into the bucket, I&amp;#39;d write a GitHub Action that picks the files modified and only copy/paste those. Does it make sense? How would you approach the problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zeonf6", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway20220231", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zeonf6/how_do_you_manage_single_repo_airflow_test/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zeonf6/how_do_you_manage_single_repo_airflow_test/", "subreddit_subscribers": 82138, "created_utc": 1670377327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, \n\nI'm pretty new to Data Engineering, and I got asked to capture user changes to the datawarehouse.\n\nI'm working on a project where the source table (Users) can change info (User changes city) or the data (row) disappears because User deletes the data.  \n\nI've looked around for articles, books (kimball) and this subreddit, and I have been reading that SCD Type 2 is the right one to implement in this case. \n\nProblem is, the source implements hard deletes on data, which means that I have to check all the rows and see if there's any deleted rows  and try to capture that deletion in the SCD 2 table.\n\nTLDR:\n\nSituation:\n\n\\- I have a source database that can have data changes or data deletes\n\nTask:\n\n\\- To track these changes or deletions in the DWH\n\nAction:\n\n\\- Implement Slowly Changing Dimensions Type 2?\n\n&amp;#x200B;\n\nPlease guide me :)", "author_fullname": "t2_5th3ljh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to implement SCD 2 with hard deleted rows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zeir6j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670362357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m pretty new to Data Engineering, and I got asked to capture user changes to the datawarehouse.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on a project where the source table (Users) can change info (User changes city) or the data (row) disappears because User deletes the data.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked around for articles, books (kimball) and this subreddit, and I have been reading that SCD Type 2 is the right one to implement in this case. &lt;/p&gt;\n\n&lt;p&gt;Problem is, the source implements hard deletes on data, which means that I have to check all the rows and see if there&amp;#39;s any deleted rows  and try to capture that deletion in the SCD 2 table.&lt;/p&gt;\n\n&lt;p&gt;TLDR:&lt;/p&gt;\n\n&lt;p&gt;Situation:&lt;/p&gt;\n\n&lt;p&gt;- I have a source database that can have data changes or data deletes&lt;/p&gt;\n\n&lt;p&gt;Task:&lt;/p&gt;\n\n&lt;p&gt;- To track these changes or deletions in the DWH&lt;/p&gt;\n\n&lt;p&gt;Action:&lt;/p&gt;\n\n&lt;p&gt;- Implement Slowly Changing Dimensions Type 2?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Please guide me :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zeir6j", "is_robot_indexable": true, "report_reasons": null, "author": "Bored_Gunner", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zeir6j/how_to_implement_scd_2_with_hard_deleted_rows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zeir6j/how_to_implement_scd_2_with_hard_deleted_rows/", "subreddit_subscribers": 82138, "created_utc": 1670362357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Any latest project in python would be great. I request you to support me by suggesting latest python projects. I did my search in the internet and most of the projects are pretty much out dated.\n\nBackground: I have completed basics of python and yet to start NumPy-pandas. Meanwhile I would like to start with a project for my current level in python and later extend it into a bigger project using NumPy-pandas and visualization", "author_fullname": "t2_q60xrlzp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "latest projects in Python relevant for advancement into Data science in the near future.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zfa4uz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670439607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any latest project in python would be great. I request you to support me by suggesting latest python projects. I did my search in the internet and most of the projects are pretty much out dated.&lt;/p&gt;\n\n&lt;p&gt;Background: I have completed basics of python and yet to start NumPy-pandas. Meanwhile I would like to start with a project for my current level in python and later extend it into a bigger project using NumPy-pandas and visualization&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zfa4uz", "is_robot_indexable": true, "report_reasons": null, "author": "hungry_man13", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zfa4uz/latest_projects_in_python_relevant_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zfa4uz/latest_projects_in_python_relevant_for/", "subreddit_subscribers": 82138, "created_utc": 1670439607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been trying to build a simple development environment for PySpark applications, object storage and metastore.\nEach service in a container and everything built in a docker compose yml file.\n\nI\u2019ve managed to build a PySpark image and make it communicate with Minio as an object storage so far.\n\nI\u2019m struggling with the \u201cmetastore\u201d part of my environment. I want to set up a metastore that allow me to spin up and down spark jobs and persist my tables. I\u2019ve looked into hive-standalone-metastore with an postgres instance but couldn\u2019t make it work.\n\nDoes somebody in this sub has any idea how to build something like that?", "author_fullname": "t2_5fmit0v9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark + Minio + Metastore Development Container", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zf5g7v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670430157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been trying to build a simple development environment for PySpark applications, object storage and metastore.\nEach service in a container and everything built in a docker compose yml file.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve managed to build a PySpark image and make it communicate with Minio as an object storage so far.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m struggling with the \u201cmetastore\u201d part of my environment. I want to set up a metastore that allow me to spin up and down spark jobs and persist my tables. I\u2019ve looked into hive-standalone-metastore with an postgres instance but couldn\u2019t make it work.&lt;/p&gt;\n\n&lt;p&gt;Does somebody in this sub has any idea how to build something like that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zf5g7v", "is_robot_indexable": true, "report_reasons": null, "author": "gabbom_XCII", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf5g7v/pyspark_minio_metastore_development_container/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zf5g7v/pyspark_minio_metastore_development_container/", "subreddit_subscribers": 82138, "created_utc": 1670430157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently, our team uses a failure callback (`on_failure_callback`) to trigger an OpsGenie alert that eventually sends a Slack message. However, sometimes Composer (we're on GCP) never initiates the task, which causes the task to be marked as failed but without triggering the callback (and therefore no Slack alert of the failure). In other cases, the k8s pod the task is running on gets evicted in the middle of the run so the Airflow logs just...stop. Again, no callback triggered and no alert.\n\nOne option we've explored is using external monitoring to ensure that one task completes within a certain time of the prior task, but that seems a bit clunky. Are there any configurations within Airflow itself that could be used to send an alert and avoid silent failures?", "author_fullname": "t2_4osay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you like to alert for Airflow errors?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zf40rw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670427220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently, our team uses a failure callback (&lt;code&gt;on_failure_callback&lt;/code&gt;) to trigger an OpsGenie alert that eventually sends a Slack message. However, sometimes Composer (we&amp;#39;re on GCP) never initiates the task, which causes the task to be marked as failed but without triggering the callback (and therefore no Slack alert of the failure). In other cases, the k8s pod the task is running on gets evicted in the middle of the run so the Airflow logs just...stop. Again, no callback triggered and no alert.&lt;/p&gt;\n\n&lt;p&gt;One option we&amp;#39;ve explored is using external monitoring to ensure that one task completes within a certain time of the prior task, but that seems a bit clunky. Are there any configurations within Airflow itself that could be used to send an alert and avoid silent failures?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zf40rw", "is_robot_indexable": true, "report_reasons": null, "author": "doom2", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf40rw/how_do_you_like_to_alert_for_airflow_errors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zf40rw/how_do_you_like_to_alert_for_airflow_errors/", "subreddit_subscribers": 82138, "created_utc": 1670427220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Would like to know any best practices available after a Web page has been scraped using Scrapy or Selenium. Thanks.", "author_fullname": "t2_eozceps7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Checking html file data integrity after scraping", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zf3oo9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670426385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would like to know any best practices available after a Web page has been scraped using Scrapy or Selenium. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zf3oo9", "is_robot_indexable": true, "report_reasons": null, "author": "Liily_07", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf3oo9/checking_html_file_data_integrity_after_scraping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zf3oo9/checking_html_file_data_integrity_after_scraping/", "subreddit_subscribers": 82138, "created_utc": 1670426385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,\n\nI'm interviewing for a data consultancy in a few days, and I want to make sure I prepare well. My previous roles in data were much less sophisticated than the role I am interviewing for, and I entered the roles via domain knowledge, not technical programming knowledge (although I have a good amount of that from uni). As a result I have a weirdly patchy understanding of data engineering and current best practices.\n\nI was hoping those of you who had worked through a technical interview of this format could give me some tips on how to approach and answer the problem. When I read it, I anticipate a system design(?) sort of problem.\n\n**Here's a paraphrase of the brief:**\n\n*\"Create a platform which integrates multiple data sources (GIS, AIS, Satellite, RF, VMS etc) to determine sections of a Branch of a subsea distributed network that are at the most risk, to inform maintenance schedules on the risky sections of the branch\"*\n\n**My answer is below.** If you were a DS interviewer, what would you think? Are there any systems I should name-drop - any due diligence I've missed? I feel confident that I have the high level concepts right, but some of the technical details might be missing.\n\nAnswer:\n\n1. **Prep work and problem definition**  \n\n   1. Who are end users; what systems do they use? Assume ERP tool. These are usually off the shelf, so we won't have to worry about frontend.\n   2. How frequently will it be updated? From weekly ELT job all the way to event-streaming. Assuming weekly frequency for this case.\n2. **Data Retrieval**  \n\n   1. Assuming we have immediate access to listed data sources, we'll need to ingest these into an intermediary data warehouse. Snowflake seems to be a popular choice?\n   2. Presumably, the information (i.e. GIS) on the Branch would be stored on an asset database somewhere. Load asset information on the network branch into the intermediary DW too.\n   3. This would all be done via an ELT arrangement (still getting used to this concept). A transform tool like dbt could process this data into a view (or views) useful for our algorithm.\n3. **Build a risk model/algorithm** that takes the data from the warehouse and processes it into a format that shows calculated risk level for each part of the branch.  \n\n   1. Connect to, and import the loaded and transformed data\n   2. Likely libraries - Geopandas, pandas\n   3. AIS, VMS, Satellite and RF data can be aggregated and checked against Branch GIS data to get risky parts of the branch - parts of the branch that are crossed the most by boats are the riskiest. I'd do a lot more here, but I think the exact algo is less important than the systems surrounding\n   4. Export model results to a db or file, on a weekly basis, to be consumed by ERP system. The ERP system would be programmed by maintenance planners to prioritise maintenance on branch sections with more crossings.", "author_fullname": "t2_8qzgm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Prep - Data Platform Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zezh0a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670414589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interviewing for a data consultancy in a few days, and I want to make sure I prepare well. My previous roles in data were much less sophisticated than the role I am interviewing for, and I entered the roles via domain knowledge, not technical programming knowledge (although I have a good amount of that from uni). As a result I have a weirdly patchy understanding of data engineering and current best practices.&lt;/p&gt;\n\n&lt;p&gt;I was hoping those of you who had worked through a technical interview of this format could give me some tips on how to approach and answer the problem. When I read it, I anticipate a system design(?) sort of problem.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here&amp;#39;s a paraphrase of the brief:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;&amp;quot;Create a platform which integrates multiple data sources (GIS, AIS, Satellite, RF, VMS etc) to determine sections of a Branch of a subsea distributed network that are at the most risk, to inform maintenance schedules on the risky sections of the branch&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My answer is below.&lt;/strong&gt; If you were a DS interviewer, what would you think? Are there any systems I should name-drop - any due diligence I&amp;#39;ve missed? I feel confident that I have the high level concepts right, but some of the technical details might be missing.&lt;/p&gt;\n\n&lt;p&gt;Answer:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Prep work and problem definition&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Who are end users; what systems do they use? Assume ERP tool. These are usually off the shelf, so we won&amp;#39;t have to worry about frontend.&lt;/li&gt;\n&lt;li&gt;How frequently will it be updated? From weekly ELT job all the way to event-streaming. Assuming weekly frequency for this case.&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data Retrieval&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Assuming we have immediate access to listed data sources, we&amp;#39;ll need to ingest these into an intermediary data warehouse. Snowflake seems to be a popular choice?&lt;/li&gt;\n&lt;li&gt;Presumably, the information (i.e. GIS) on the Branch would be stored on an asset database somewhere. Load asset information on the network branch into the intermediary DW too.&lt;/li&gt;\n&lt;li&gt;This would all be done via an ELT arrangement (still getting used to this concept). A transform tool like dbt could process this data into a view (or views) useful for our algorithm.&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Build a risk model/algorithm&lt;/strong&gt; that takes the data from the warehouse and processes it into a format that shows calculated risk level for each part of the branch.  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Connect to, and import the loaded and transformed data&lt;/li&gt;\n&lt;li&gt;Likely libraries - Geopandas, pandas&lt;/li&gt;\n&lt;li&gt;AIS, VMS, Satellite and RF data can be aggregated and checked against Branch GIS data to get risky parts of the branch - parts of the branch that are crossed the most by boats are the riskiest. I&amp;#39;d do a lot more here, but I think the exact algo is less important than the systems surrounding&lt;/li&gt;\n&lt;li&gt;Export model results to a db or file, on a weekly basis, to be consumed by ERP system. The ERP system would be programmed by maintenance planners to prioritise maintenance on branch sections with more crossings.&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "zezh0a", "is_robot_indexable": true, "report_reasons": null, "author": "tignition", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zezh0a/interview_prep_data_platform_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zezh0a/interview_prep_data_platform_engineer/", "subreddit_subscribers": 82138, "created_utc": 1670414589.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}