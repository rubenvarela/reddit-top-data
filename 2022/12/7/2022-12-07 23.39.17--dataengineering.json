{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_yeda6sl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheat sheets on data life cycle, PySpark, dbt, Kafka, BigQuery, Airflow, and Docker.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_zf01xv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/jnAuxFJ7FUK5Iahrv4PaSmv255B5F5x5gonxzbUuD10.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670416431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kdnuggets.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.kdnuggets.com/2022/12/7-essential-cheat-sheets-data-engineering.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qXKhO1YkZh9VnI_-EHqSiDah-R5MlCadytMq1X88YZI.jpg?auto=webp&amp;s=c9b7cd9c73faba76573308abafdf501ae9f4d32b", "width": 1000, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qXKhO1YkZh9VnI_-EHqSiDah-R5MlCadytMq1X88YZI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8494f67be4a98a5e99363aca96134d6c359b4046", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/qXKhO1YkZh9VnI_-EHqSiDah-R5MlCadytMq1X88YZI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bd8a77bba48bd792119d4f6ab606e9ae35316eee", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/qXKhO1YkZh9VnI_-EHqSiDah-R5MlCadytMq1X88YZI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fdf355ca2a0edbb10234946aa8a567b5676fa9c4", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/qXKhO1YkZh9VnI_-EHqSiDah-R5MlCadytMq1X88YZI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=10e3e2e2aa447ee1188f0c0af00a3c50bdcb14e1", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/qXKhO1YkZh9VnI_-EHqSiDah-R5MlCadytMq1X88YZI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f53a2e0e98e3be44fd7c218ae425fc3725a3e70", "width": 960, "height": 576}], "variants": {}, "id": "fyZAesjAe63njoQYSxc2IjZ_YgDi9aZJ-ZEcfay2d9k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zf01xv", "is_robot_indexable": true, "report_reasons": null, "author": "kingabzpro", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf01xv/cheat_sheets_on_data_life_cycle_pyspark_dbt_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.kdnuggets.com/2022/12/7-essential-cheat-sheets-data-engineering.html", "subreddit_subscribers": 82150, "created_utc": 1670416431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any things you wished you had done during your first few weeks at your current job?\n\nI know I would have started earlier with a list of all the acronyms used in the data team. That would have saved me a headache later!", "author_fullname": "t2_3cfsa44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting a new Data Engineering job (mid level)- what would you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zewn9t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670404620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any things you wished you had done during your first few weeks at your current job?&lt;/p&gt;\n\n&lt;p&gt;I know I would have started earlier with a list of all the acronyms used in the data team. That would have saved me a headache later!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zewn9t", "is_robot_indexable": true, "report_reasons": null, "author": "squaricle", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zewn9t/starting_a_new_data_engineering_job_mid_level/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zewn9t/starting_a_new_data_engineering_job_mid_level/", "subreddit_subscribers": 82150, "created_utc": 1670404620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For current DEs, what is one thing you were expecting going into the DE field vs what actually occurred?\n\nBoth the good and the bad", "author_fullname": "t2_3p620rl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expectations Vs Reality", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zf68rv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670431778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For current DEs, what is one thing you were expecting going into the DE field vs what actually occurred?&lt;/p&gt;\n\n&lt;p&gt;Both the good and the bad&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "DE @ Amazon/Lyft/Author of Ace DE Interview", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zf68rv", "is_robot_indexable": true, "report_reasons": null, "author": "chrisgarzon19", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/zf68rv/expectations_vs_reality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zf68rv/expectations_vs_reality/", "subreddit_subscribers": 82150, "created_utc": 1670431778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Basically I\u2019m wondering if there\u2019s a columnar or other NOSQL equivalent lightweight database that could be used for small pipelines in a pinch. Is that basically what parquet files are used for? Is there a way to bundle parquet files together into more of a database rather than standalone tables?", "author_fullname": "t2_1gyiwuuv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQLite NOSQL Alternatives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zevwlz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670401591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically I\u2019m wondering if there\u2019s a columnar or other NOSQL equivalent lightweight database that could be used for small pipelines in a pinch. Is that basically what parquet files are used for? Is there a way to bundle parquet files together into more of a database rather than standalone tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zevwlz", "is_robot_indexable": true, "report_reasons": null, "author": "trianglesteve", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zevwlz/sqlite_nosql_alternatives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zevwlz/sqlite_nosql_alternatives/", "subreddit_subscribers": 82150, "created_utc": 1670401591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3yleu7rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How To Measure The ROI Of Your Data Spend?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zf9dob", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1670438048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "alvin.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.alvin.ai/posts/how-to-measure-the-roi-of-your-data-spend", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zf9dob", "is_robot_indexable": true, "report_reasons": null, "author": "gabsferreiradev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf9dob/how_to_measure_the_roi_of_your_data_spend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.alvin.ai/posts/how-to-measure-the-roi-of-your-data-spend", "subreddit_subscribers": 82150, "created_utc": 1670438048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It's honestly really impressive that ChatGPT can solve some Leetcode problems, but does anyone else think this will positively impact the tech interview process?  If interviews historically selected for candidates who pass these technical tests, doesn't it make sense to want to move to a system that an ML model cannot excel at?\n\nI can only see Leetcode still existing in the future to interview entry level engineers, because, what else are you going to ask someone who is just starting out in the field?\n\nBut for anyone with more than a year of experience or moving on to their second job, I'm thinking that interviewers will place much more weight into system design, past experience, and the ability to work well in a team.\n\nChatGPT does show that it's kind of funny to be asking engineers who have graduated several years ago questions from a class they took as first and second year students.", "author_fullname": "t2_3v6ob2bf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is ChatGPT going to affect the technical interview?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zf0t4r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670419117.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670418714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s honestly really impressive that ChatGPT can solve some Leetcode problems, but does anyone else think this will positively impact the tech interview process?  If interviews historically selected for candidates who pass these technical tests, doesn&amp;#39;t it make sense to want to move to a system that an ML model cannot excel at?&lt;/p&gt;\n\n&lt;p&gt;I can only see Leetcode still existing in the future to interview entry level engineers, because, what else are you going to ask someone who is just starting out in the field?&lt;/p&gt;\n\n&lt;p&gt;But for anyone with more than a year of experience or moving on to their second job, I&amp;#39;m thinking that interviewers will place much more weight into system design, past experience, and the ability to work well in a team.&lt;/p&gt;\n\n&lt;p&gt;ChatGPT does show that it&amp;#39;s kind of funny to be asking engineers who have graduated several years ago questions from a class they took as first and second year students.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zf0t4r", "is_robot_indexable": true, "report_reasons": null, "author": "data_preprocessing", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf0t4r/how_is_chatgpt_going_to_affect_the_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zf0t4r/how_is_chatgpt_going_to_affect_the_technical/", "subreddit_subscribers": 82150, "created_utc": 1670418714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently, our team uses a failure callback (`on_failure_callback`) to trigger an OpsGenie alert that eventually sends a Slack message. However, sometimes Composer (we're on GCP) never initiates the task, which causes the task to be marked as failed but without triggering the callback (and therefore no Slack alert of the failure). In other cases, the k8s pod the task is running on gets evicted in the middle of the run so the Airflow logs just...stop. Again, no callback triggered and no alert.\n\nOne option we've explored is using external monitoring to ensure that one task completes within a certain time of the prior task, but that seems a bit clunky. Are there any configurations within Airflow itself that could be used to send an alert and avoid silent failures?", "author_fullname": "t2_4osay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you like to alert for Airflow errors?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zf40rw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670427220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently, our team uses a failure callback (&lt;code&gt;on_failure_callback&lt;/code&gt;) to trigger an OpsGenie alert that eventually sends a Slack message. However, sometimes Composer (we&amp;#39;re on GCP) never initiates the task, which causes the task to be marked as failed but without triggering the callback (and therefore no Slack alert of the failure). In other cases, the k8s pod the task is running on gets evicted in the middle of the run so the Airflow logs just...stop. Again, no callback triggered and no alert.&lt;/p&gt;\n\n&lt;p&gt;One option we&amp;#39;ve explored is using external monitoring to ensure that one task completes within a certain time of the prior task, but that seems a bit clunky. Are there any configurations within Airflow itself that could be used to send an alert and avoid silent failures?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zf40rw", "is_robot_indexable": true, "report_reasons": null, "author": "doom2", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf40rw/how_do_you_like_to_alert_for_airflow_errors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zf40rw/how_do_you_like_to_alert_for_airflow_errors/", "subreddit_subscribers": 82150, "created_utc": 1670427220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm part of a research team at a smaller company which has worked in the field of datamodeling for 20+ yrs.  \n\n\nWe have repeatedly observed that for companies using DataWarehousing, documenting source systems provides a challenge.\n\nIn a small project team we would like to develop new and effective solutions for these problems.\n\nFor anyone with experience in this field, we would be highly grateful for your opinions on the following questions:\n\n1. What challenges do you know of when it comes to documenting a database/source system? \n\n2. What has proven to be especially challenging when documenting CRM systems such as Salesforce?\n\n3. Have you encountered any specific difficulties while documenting ERP systems like SAP?\n\nThank you very much in advance, we're looking forward to hearing from you :)", "author_fullname": "t2_uovnqo6g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inquiry about Database Documentation &amp; DataWarehousing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zf5auk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670429849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m part of a research team at a smaller company which has worked in the field of datamodeling for 20+ yrs.  &lt;/p&gt;\n\n&lt;p&gt;We have repeatedly observed that for companies using DataWarehousing, documenting source systems provides a challenge.&lt;/p&gt;\n\n&lt;p&gt;In a small project team we would like to develop new and effective solutions for these problems.&lt;/p&gt;\n\n&lt;p&gt;For anyone with experience in this field, we would be highly grateful for your opinions on the following questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;What challenges do you know of when it comes to documenting a database/source system? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What has proven to be especially challenging when documenting CRM systems such as Salesforce?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Have you encountered any specific difficulties while documenting ERP systems like SAP?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thank you very much in advance, we&amp;#39;re looking forward to hearing from you :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "zf5auk", "is_robot_indexable": true, "report_reasons": null, "author": "heureka_leo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf5auk/inquiry_about_database_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zf5auk/inquiry_about_database_documentation/", "subreddit_subscribers": 82150, "created_utc": 1670429849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Any latest project in python would be great. I request you to support me by suggesting latest python projects. I did my search in the internet and most of the projects are pretty much out dated.\n\nBackground: I have completed basics of python and yet to start NumPy-pandas. Meanwhile I would like to start with a project for my current level in python and later extend it into a bigger project using NumPy-pandas and visualization", "author_fullname": "t2_q60xrlzp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "latest projects in Python relevant for advancement into Data science in the near future.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zfa4uz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670439607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any latest project in python would be great. I request you to support me by suggesting latest python projects. I did my search in the internet and most of the projects are pretty much out dated.&lt;/p&gt;\n\n&lt;p&gt;Background: I have completed basics of python and yet to start NumPy-pandas. Meanwhile I would like to start with a project for my current level in python and later extend it into a bigger project using NumPy-pandas and visualization&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zfa4uz", "is_robot_indexable": true, "report_reasons": null, "author": "hungry_man13", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zfa4uz/latest_projects_in_python_relevant_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zfa4uz/latest_projects_in_python_relevant_for/", "subreddit_subscribers": 82150, "created_utc": 1670439607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been trying to build a simple development environment for PySpark applications, object storage and metastore.\nEach service in a container and everything built in a docker compose yml file.\n\nI\u2019ve managed to build a PySpark image and make it communicate with Minio as an object storage so far.\n\nI\u2019m struggling with the \u201cmetastore\u201d part of my environment. I want to set up a metastore that allow me to spin up and down spark jobs and persist my tables. I\u2019ve looked into hive-standalone-metastore with an postgres instance but couldn\u2019t make it work.\n\nDoes somebody in this sub has any idea how to build something like that?", "author_fullname": "t2_5fmit0v9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark + Minio + Metastore Development Container", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zf5g7v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670430157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been trying to build a simple development environment for PySpark applications, object storage and metastore.\nEach service in a container and everything built in a docker compose yml file.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve managed to build a PySpark image and make it communicate with Minio as an object storage so far.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m struggling with the \u201cmetastore\u201d part of my environment. I want to set up a metastore that allow me to spin up and down spark jobs and persist my tables. I\u2019ve looked into hive-standalone-metastore with an postgres instance but couldn\u2019t make it work.&lt;/p&gt;\n\n&lt;p&gt;Does somebody in this sub has any idea how to build something like that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zf5g7v", "is_robot_indexable": true, "report_reasons": null, "author": "gabbom_XCII", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf5g7v/pyspark_minio_metastore_development_container/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zf5g7v/pyspark_minio_metastore_development_container/", "subreddit_subscribers": 82150, "created_utc": 1670430157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my experience as a data engineer in a big tech, I wrote a mock data library that creates data based on the schema and Dtype you configure and have used it pretty extensively for testing pipelines. Never tested on production data, because anonymizing it and copying it out of prod workflow was not approved by data governance and legal teams :/    \n\n\nI'm curious how you folks in the industry do it!\n\n[View Poll](https://www.reddit.com/poll/zetwd0)", "author_fullname": "t2_msviuy1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you get the right data to test your ETL pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zetwd0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670393769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my experience as a data engineer in a big tech, I wrote a mock data library that creates data based on the schema and Dtype you configure and have used it pretty extensively for testing pipelines. Never tested on production data, because anonymizing it and copying it out of prod workflow was not approved by data governance and legal teams :/    &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious how you folks in the industry do it!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/zetwd0\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zetwd0", "is_robot_indexable": true, "report_reasons": null, "author": "vino_and_data", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1670998569803, "options": [{"text": "Mock data", "id": "20235477"}, {"text": "Copy production data to staging/test env", "id": "20235478"}, {"text": "Copy anonymized production data to staging", "id": "20235479"}, {"text": "Use data versioning tool to mirror prod env", "id": "20235480"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 204, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zetwd0/how_do_you_get_the_right_data_to_test_your_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/zetwd0/how_do_you_get_the_right_data_to_test_your_etl/", "subreddit_subscribers": 82150, "created_utc": 1670393769.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for some advice on how to move my companies stack forward in an on prem environment, without any plans of moving to the cloud. \n\nWe are running SSIS for everything right now but ideally would like to move towards a more modern stack, but not sure what direction. Airflow, ADF, etc. \n\nI joined more recently and have a python/DS background and honestly prefer that world over SSIS, but the rest of the team is more classic ETL/SQL Server based, but willing to grow. the vibe is basically \u201cWhat best for future proofing, talent attraction, etc\u201d.\n\nNo real K8 infra but run VXrails, so potentially deployable. \n\nOnly hard stop is gotta keep the old on prem SQL Server. \ud83d\ude1e\n\nAppreciate the help!", "author_fullname": "t2_3sioksrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to move stack forward On- Prem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zer9b3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670385105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some advice on how to move my companies stack forward in an on prem environment, without any plans of moving to the cloud. &lt;/p&gt;\n\n&lt;p&gt;We are running SSIS for everything right now but ideally would like to move towards a more modern stack, but not sure what direction. Airflow, ADF, etc. &lt;/p&gt;\n\n&lt;p&gt;I joined more recently and have a python/DS background and honestly prefer that world over SSIS, but the rest of the team is more classic ETL/SQL Server based, but willing to grow. the vibe is basically \u201cWhat best for future proofing, talent attraction, etc\u201d.&lt;/p&gt;\n\n&lt;p&gt;No real K8 infra but run VXrails, so potentially deployable. &lt;/p&gt;\n\n&lt;p&gt;Only hard stop is gotta keep the old on prem SQL Server. \ud83d\ude1e&lt;/p&gt;\n\n&lt;p&gt;Appreciate the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zer9b3", "is_robot_indexable": true, "report_reasons": null, "author": "Remote-Stay", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zer9b3/how_to_move_stack_forward_on_prem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zer9b3/how_to_move_stack_forward_on_prem/", "subreddit_subscribers": 82150, "created_utc": 1670385105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4w5k4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The top 5 data trends to look out for in 2023 (by Velotix)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_zf10h0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/FCKC4DSuJ3rEs3oRJiqhNV762ulrj16qJzGaQzz-8H0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670419323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "velotix.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.velotix.ai/resources/blog/top-data-trends/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?auto=webp&amp;s=f6a6fcd59eff984940d3a85ed5a537b57fc18e1f", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95b746618bca90d0c6682c8a047a2ce0901f451f", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8a873fe73c58425a906cf03bf2c56520e12514fb", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b30bd008f3a0a0a7d6e8dd74dbcc4c68fa2cfcc", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3132c3e87483e8edae2a195232d85c7df62cf727", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=89c0dd19f8043f0560133a1ff2ded812ccff6073", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/RukJi1b-ipV6CDlreAxMEgfXAK8BkjcVXu1zOtMrbro.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5fe8058972fb2f2ddb0bd7ff94b9502bbb0a5575", "width": 1080, "height": 565}], "variants": {}, "id": "pose6bqHwmZc9D-dEY3FCT4KDlRQPDN-dV8Q5QdEGZI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zf10h0", "is_robot_indexable": true, "report_reasons": null, "author": "cobano", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf10h0/the_top_5_data_trends_to_look_out_for_in_2023_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.velotix.ai/resources/blog/top-data-trends/", "subreddit_subscribers": 82150, "created_utc": 1670419323.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my experience, I have done exception handling via populating NoSQL Database for exception and using that as source for notification trigger.\n\nAlternatively, I have also tried defining error codes for known exceptions, use that in notifications.\n\nCan you share the approaches you  follow in your Data Pipeline for Exception Handling and how do you use it for notification purpose.", "author_fullname": "t2_rxkr4y1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exception Handling in Data Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zexr7j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670408767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my experience, I have done exception handling via populating NoSQL Database for exception and using that as source for notification trigger.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, I have also tried defining error codes for known exceptions, use that in notifications.&lt;/p&gt;\n\n&lt;p&gt;Can you share the approaches you  follow in your Data Pipeline for Exception Handling and how do you use it for notification purpose.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zexr7j", "is_robot_indexable": true, "report_reasons": null, "author": "SoggyAbalone7392", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zexr7j/exception_handling_in_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zexr7j/exception_handling_in_data_pipeline/", "subreddit_subscribers": 82150, "created_utc": 1670408767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIn my company I have been assigned the task to create a data streaming pipeline to move data from mysql to a datalake.\n\nTo achieve this task, I have created a pipeline using Debezium, Kafka and Spark Structured Streaming.\n\nDebezium writes all the events to a single kafka topic.\n\nThe Spark job uses the \\`forEachBatch\\` function, it reads events, divides them by table, and makes sure to upsert events in each corresponding table using Delta.\n\nOne tough requirement to implement is that this data should be directly queried by data consumers, it is a tough requirement because due to grouping upserts by table I am changing the events order (performing upserts in an ordered way for each individual transaction would be extremely inefficient). This could lead to potential data inconsistencies because during the batch processing time some tables are more updated than others.\n\nOne way to mitigate this issue I came up with is to let users access these tables only through athena/snowflake, this way I can decouple what data I am showing from what data is actually inside the table by using the [symlink manifest files](https://docs.delta.io/latest/presto-integration.html#set-up-the-presto-trino-or-athena-to-delta-lake-integration-and-query-delta-tables). This is a workaround and not a definitive solution, because it reduces the time tables are inconsistent to the time it takes to refresh all the manifests.\n\nIs there any better way to achieve consistency between tables in a streaming system?\n\nI am also tempted to reduce inconsistency by minimizing latency , creating append-only tables, but I am afraid these tables will start growing exponentially.", "author_fullname": "t2_do2bj7xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming Jobs and tables consistency", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zewhcj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1670406352.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670403957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;In my company I have been assigned the task to create a data streaming pipeline to move data from mysql to a datalake.&lt;/p&gt;\n\n&lt;p&gt;To achieve this task, I have created a pipeline using Debezium, Kafka and Spark Structured Streaming.&lt;/p&gt;\n\n&lt;p&gt;Debezium writes all the events to a single kafka topic.&lt;/p&gt;\n\n&lt;p&gt;The Spark job uses the `forEachBatch` function, it reads events, divides them by table, and makes sure to upsert events in each corresponding table using Delta.&lt;/p&gt;\n\n&lt;p&gt;One tough requirement to implement is that this data should be directly queried by data consumers, it is a tough requirement because due to grouping upserts by table I am changing the events order (performing upserts in an ordered way for each individual transaction would be extremely inefficient). This could lead to potential data inconsistencies because during the batch processing time some tables are more updated than others.&lt;/p&gt;\n\n&lt;p&gt;One way to mitigate this issue I came up with is to let users access these tables only through athena/snowflake, this way I can decouple what data I am showing from what data is actually inside the table by using the &lt;a href=\"https://docs.delta.io/latest/presto-integration.html#set-up-the-presto-trino-or-athena-to-delta-lake-integration-and-query-delta-tables\"&gt;symlink manifest files&lt;/a&gt;. This is a workaround and not a definitive solution, because it reduces the time tables are inconsistent to the time it takes to refresh all the manifests.&lt;/p&gt;\n\n&lt;p&gt;Is there any better way to achieve consistency between tables in a streaming system?&lt;/p&gt;\n\n&lt;p&gt;I am also tempted to reduce inconsistency by minimizing latency , creating append-only tables, but I am afraid these tables will start growing exponentially.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zewhcj", "is_robot_indexable": true, "report_reasons": null, "author": "somerandomdataeng", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/zewhcj/streaming_jobs_and_tables_consistency/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zewhcj/streaming_jobs_and_tables_consistency/", "subreddit_subscribers": 82150, "created_utc": 1670403957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For those who do their transformations with SQL, how does your deployment pipeline work?\n\nDo you have powers to deploy the procedures, views, etc to production?  \nIf you need to handoff your code, how does that work?\nDo you check-in your code to a source control system?", "author_fullname": "t2_bdzq3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying SQL transformations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zeorv2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670377691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those who do their transformations with SQL, how does your deployment pipeline work?&lt;/p&gt;\n\n&lt;p&gt;Do you have powers to deploy the procedures, views, etc to production?&lt;br/&gt;\nIf you need to handoff your code, how does that work?\nDo you check-in your code to a source control system?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zeorv2", "is_robot_indexable": true, "report_reasons": null, "author": "Acewox", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zeorv2/deploying_sql_transformations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zeorv2/deploying_sql_transformations/", "subreddit_subscribers": 82150, "created_utc": 1670377691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi experts,\n\nWe have a test Airflow (actually Google Composer) cluster that consumes DAGs through a single repo and show the updates in Airflow web UI.\n\nBasically, whenever someone merges feature branches into `test` branch, a Jenkins job copies/pastes the whole repo into the appropriate Google bucket, which is used by the Composer cluster. This creates an obvious issue that one developer's merge will remove all changes of other developers, if they still need to test on `test` branch and have not merged into `master` yet.\n\nWhen the team is small, this is a non-issue. We rarely had conflicts. Now that the team is big enough, I'm thinking about making some changes. Here is the plan I'm considering:\n\nInstead of simply copying/pasting everything into the bucket, I'd write a GitHub Action that picks the files modified and only copy/paste those. Does it make sense? How would you approach the problem?", "author_fullname": "t2_ldvtxo0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage single repo Airflow test environment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zeonf6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670377327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi experts,&lt;/p&gt;\n\n&lt;p&gt;We have a test Airflow (actually Google Composer) cluster that consumes DAGs through a single repo and show the updates in Airflow web UI.&lt;/p&gt;\n\n&lt;p&gt;Basically, whenever someone merges feature branches into &lt;code&gt;test&lt;/code&gt; branch, a Jenkins job copies/pastes the whole repo into the appropriate Google bucket, which is used by the Composer cluster. This creates an obvious issue that one developer&amp;#39;s merge will remove all changes of other developers, if they still need to test on &lt;code&gt;test&lt;/code&gt; branch and have not merged into &lt;code&gt;master&lt;/code&gt; yet.&lt;/p&gt;\n\n&lt;p&gt;When the team is small, this is a non-issue. We rarely had conflicts. Now that the team is big enough, I&amp;#39;m thinking about making some changes. Here is the plan I&amp;#39;m considering:&lt;/p&gt;\n\n&lt;p&gt;Instead of simply copying/pasting everything into the bucket, I&amp;#39;d write a GitHub Action that picks the files modified and only copy/paste those. Does it make sense? How would you approach the problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zeonf6", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway20220231", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zeonf6/how_do_you_manage_single_repo_airflow_test/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zeonf6/how_do_you_manage_single_repo_airflow_test/", "subreddit_subscribers": 82150, "created_utc": 1670377327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://youtu.be/sPTERXmYnBs](https://youtu.be/sPTERXmYnBs)", "author_fullname": "t2_b7f9ay9o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to do Data wrangling in Azure Data Factory", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zfd4of", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1670445753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://youtu.be/sPTERXmYnBs\"&gt;https://youtu.be/sPTERXmYnBs&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6G0ZdkahjmkBRiO5UFYbd1b5nO3RJBhFmdRXiEkgR9A.jpg?auto=webp&amp;s=f26649082272ab50421638418f24f8cc086c180c", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/6G0ZdkahjmkBRiO5UFYbd1b5nO3RJBhFmdRXiEkgR9A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a0c989a442df24ea60a519a3b01e460956bf2be", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/6G0ZdkahjmkBRiO5UFYbd1b5nO3RJBhFmdRXiEkgR9A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d0178c615faf40c321c5ea2575a11e6c266c03d", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/6G0ZdkahjmkBRiO5UFYbd1b5nO3RJBhFmdRXiEkgR9A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a58e83f373da2d94a61cc425a9f2133941e318ea", "width": 320, "height": 240}], "variants": {}, "id": "fnGaLGugM-dKpwoivw1QbauQqVOD106EB8CftcXd8_U"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zfd4of", "is_robot_indexable": true, "report_reasons": null, "author": "balramprasad", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zfd4of/how_to_do_data_wrangling_in_azure_data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zfd4of/how_to_do_data_wrangling_in_azure_data_factory/", "subreddit_subscribers": 82150, "created_utc": 1670445753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Would like to know any best practices available after a Web page has been scraped using Scrapy or Selenium. Thanks.", "author_fullname": "t2_eozceps7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Checking html file data integrity after scraping", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zf3oo9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670426385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would like to know any best practices available after a Web page has been scraped using Scrapy or Selenium. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zf3oo9", "is_robot_indexable": true, "report_reasons": null, "author": "Liily_07", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf3oo9/checking_html_file_data_integrity_after_scraping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zf3oo9/checking_html_file_data_integrity_after_scraping/", "subreddit_subscribers": 82150, "created_utc": 1670426385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_h44fwr15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Streaming Explained", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_zf0tz8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ag9jlVxM_18?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Data Streaming, Explained\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Streaming, Explained", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ag9jlVxM_18?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Data Streaming, Explained\"&gt;&lt;/iframe&gt;", "author_name": "AltexSoft", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/ag9jlVxM_18/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@AltexSoft"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ag9jlVxM_18?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Data Streaming, Explained\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/zf0tz8", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/oOGEEuWnxyf8fkMRNvztAwyqFBC9SBs_eQ9pn0OeBoY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1670418781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/ag9jlVxM_18", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FifG7gY_or_z4R5xF-UKZzUPiSAd1Es5m6H31dMg2Tk.jpg?auto=webp&amp;s=ea398fb51ab5fd4301f5a1ced0baa28de126d06a", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/FifG7gY_or_z4R5xF-UKZzUPiSAd1Es5m6H31dMg2Tk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dffeb7fc1db245b3c51f1ce70aa5bd69471bb56", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/FifG7gY_or_z4R5xF-UKZzUPiSAd1Es5m6H31dMg2Tk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=94bdd7eddfa9d4d7b1862750c42a792d287e2374", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/FifG7gY_or_z4R5xF-UKZzUPiSAd1Es5m6H31dMg2Tk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c83d0f856e9c3d3a791253470b38ad8b04892063", "width": 320, "height": 240}], "variants": {}, "id": "aYyV6MrAht4kceUzbEOZ2eEXsr4R7VjwGQdFF4Gg-4I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zf0tz8", "is_robot_indexable": true, "report_reasons": null, "author": "TomSiderx", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zf0tz8/data_streaming_explained/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/ag9jlVxM_18", "subreddit_subscribers": 82150, "created_utc": 1670418781.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Streaming, Explained", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ag9jlVxM_18?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Data Streaming, Explained\"&gt;&lt;/iframe&gt;", "author_name": "AltexSoft", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/ag9jlVxM_18/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@AltexSoft"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,\n\nI'm interviewing for a data consultancy in a few days, and I want to make sure I prepare well. My previous roles in data were much less sophisticated than the role I am interviewing for, and I entered the roles via domain knowledge, not technical programming knowledge (although I have a good amount of that from uni). As a result I have a weirdly patchy understanding of data engineering and current best practices.\n\nI was hoping those of you who had worked through a technical interview of this format could give me some tips on how to approach and answer the problem. When I read it, I anticipate a system design(?) sort of problem.\n\n**Here's a paraphrase of the brief:**\n\n*\"Create a platform which integrates multiple data sources (GIS, AIS, Satellite, RF, VMS etc) to determine sections of a Branch of a subsea distributed network that are at the most risk, to inform maintenance schedules on the risky sections of the branch\"*\n\n**My answer is below.** If you were a DS interviewer, what would you think? Are there any systems I should name-drop - any due diligence I've missed? I feel confident that I have the high level concepts right, but some of the technical details might be missing.\n\nAnswer:\n\n1. **Prep work and problem definition**  \n\n   1. Who are end users; what systems do they use? Assume ERP tool. These are usually off the shelf, so we won't have to worry about frontend.\n   2. How frequently will it be updated? From weekly ELT job all the way to event-streaming. Assuming weekly frequency for this case.\n2. **Data Retrieval**  \n\n   1. Assuming we have immediate access to listed data sources, we'll need to ingest these into an intermediary data warehouse. Snowflake seems to be a popular choice?\n   2. Presumably, the information (i.e. GIS) on the Branch would be stored on an asset database somewhere. Load asset information on the network branch into the intermediary DW too.\n   3. This would all be done via an ELT arrangement (still getting used to this concept). A transform tool like dbt could process this data into a view (or views) useful for our algorithm.\n3. **Build a risk model/algorithm** that takes the data from the warehouse and processes it into a format that shows calculated risk level for each part of the branch.  \n\n   1. Connect to, and import the loaded and transformed data\n   2. Likely libraries - Geopandas, pandas\n   3. AIS, VMS, Satellite and RF data can be aggregated and checked against Branch GIS data to get risky parts of the branch - parts of the branch that are crossed the most by boats are the riskiest. I'd do a lot more here, but I think the exact algo is less important than the systems surrounding\n   4. Export model results to a db or file, on a weekly basis, to be consumed by ERP system. The ERP system would be programmed by maintenance planners to prioritise maintenance on branch sections with more crossings.", "author_fullname": "t2_8qzgm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Prep - Data Platform Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zezh0a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670414589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interviewing for a data consultancy in a few days, and I want to make sure I prepare well. My previous roles in data were much less sophisticated than the role I am interviewing for, and I entered the roles via domain knowledge, not technical programming knowledge (although I have a good amount of that from uni). As a result I have a weirdly patchy understanding of data engineering and current best practices.&lt;/p&gt;\n\n&lt;p&gt;I was hoping those of you who had worked through a technical interview of this format could give me some tips on how to approach and answer the problem. When I read it, I anticipate a system design(?) sort of problem.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here&amp;#39;s a paraphrase of the brief:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;&amp;quot;Create a platform which integrates multiple data sources (GIS, AIS, Satellite, RF, VMS etc) to determine sections of a Branch of a subsea distributed network that are at the most risk, to inform maintenance schedules on the risky sections of the branch&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My answer is below.&lt;/strong&gt; If you were a DS interviewer, what would you think? Are there any systems I should name-drop - any due diligence I&amp;#39;ve missed? I feel confident that I have the high level concepts right, but some of the technical details might be missing.&lt;/p&gt;\n\n&lt;p&gt;Answer:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Prep work and problem definition&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Who are end users; what systems do they use? Assume ERP tool. These are usually off the shelf, so we won&amp;#39;t have to worry about frontend.&lt;/li&gt;\n&lt;li&gt;How frequently will it be updated? From weekly ELT job all the way to event-streaming. Assuming weekly frequency for this case.&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data Retrieval&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Assuming we have immediate access to listed data sources, we&amp;#39;ll need to ingest these into an intermediary data warehouse. Snowflake seems to be a popular choice?&lt;/li&gt;\n&lt;li&gt;Presumably, the information (i.e. GIS) on the Branch would be stored on an asset database somewhere. Load asset information on the network branch into the intermediary DW too.&lt;/li&gt;\n&lt;li&gt;This would all be done via an ELT arrangement (still getting used to this concept). A transform tool like dbt could process this data into a view (or views) useful for our algorithm.&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Build a risk model/algorithm&lt;/strong&gt; that takes the data from the warehouse and processes it into a format that shows calculated risk level for each part of the branch.  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Connect to, and import the loaded and transformed data&lt;/li&gt;\n&lt;li&gt;Likely libraries - Geopandas, pandas&lt;/li&gt;\n&lt;li&gt;AIS, VMS, Satellite and RF data can be aggregated and checked against Branch GIS data to get risky parts of the branch - parts of the branch that are crossed the most by boats are the riskiest. I&amp;#39;d do a lot more here, but I think the exact algo is less important than the systems surrounding&lt;/li&gt;\n&lt;li&gt;Export model results to a db or file, on a weekly basis, to be consumed by ERP system. The ERP system would be programmed by maintenance planners to prioritise maintenance on branch sections with more crossings.&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "zezh0a", "is_robot_indexable": true, "report_reasons": null, "author": "tignition", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zezh0a/interview_prep_data_platform_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zezh0a/interview_prep_data_platform_engineer/", "subreddit_subscribers": 82150, "created_utc": 1670414589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey data folks, I have a question for all of you. I have received an offer as a senior data modeler for a mid size company ( reputed than my current company ) where in I will be working on Oracle and Netezza data modeling. Whereas currently I am working as senior data engineer working on Snowflake, ETL and modern data stack. I have 10+ years of experience (both on data engineering + modeling ). I appeared for data modeling interview as I thought it would be a senior role involving both DE + modelling and working in a more reputed company . But turns out , it's just data modeling. Should I join new company or continue in my current job ? Really confused, please tell your opinion.", "author_fullname": "t2_68lzz4hm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on Data Modeler Offer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zexp0r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670408559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey data folks, I have a question for all of you. I have received an offer as a senior data modeler for a mid size company ( reputed than my current company ) where in I will be working on Oracle and Netezza data modeling. Whereas currently I am working as senior data engineer working on Snowflake, ETL and modern data stack. I have 10+ years of experience (both on data engineering + modeling ). I appeared for data modeling interview as I thought it would be a senior role involving both DE + modelling and working in a more reputed company . But turns out , it&amp;#39;s just data modeling. Should I join new company or continue in my current job ? Really confused, please tell your opinion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zexp0r", "is_robot_indexable": true, "report_reasons": null, "author": "1aumron", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zexp0r/question_on_data_modeler_offer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zexp0r/question_on_data_modeler_offer/", "subreddit_subscribers": 82150, "created_utc": 1670408559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!\n\nWe have a second instrument to our lab which was installed about 6 months ago. The previous instrument has been here for 2 years. The old instrument has been feeding data to a server however the new instrument has been storing data on a local db.\n\nWe have collected quite a bit of data on the local database but are now transitioning the second machine to server the first machine is hooked up to. I have a back up of the local db but how do I merge it with the existing server db? There are about 50 tables with a mixture of integer ID\u2019s and GUIDs.\n\nI saw a YouTube video on transferring data via an autogenerated sql server script and was considering that as an option. I would not be able to manually add due to the size of the data.\n\nThanks!", "author_fullname": "t2_624odrt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Merging two data sources ideas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zetrlv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670393306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;We have a second instrument to our lab which was installed about 6 months ago. The previous instrument has been here for 2 years. The old instrument has been feeding data to a server however the new instrument has been storing data on a local db.&lt;/p&gt;\n\n&lt;p&gt;We have collected quite a bit of data on the local database but are now transitioning the second machine to server the first machine is hooked up to. I have a back up of the local db but how do I merge it with the existing server db? There are about 50 tables with a mixture of integer ID\u2019s and GUIDs.&lt;/p&gt;\n\n&lt;p&gt;I saw a YouTube video on transferring data via an autogenerated sql server script and was considering that as an option. I would not be able to manually add due to the size of the data.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zetrlv", "is_robot_indexable": true, "report_reasons": null, "author": "Helium0205", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zetrlv/merging_two_data_sources_ideas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zetrlv/merging_two_data_sources_ideas/", "subreddit_subscribers": 82150, "created_utc": 1670393306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What does DevOps mean to you? Is it useful to DE?\n\nI've been on many teams - in a previous career doing production support and then analytics the past few years. But now I ended up on a DevOps team. \n\nWhat this team does, for example:\n\n* Agile, including two week sprints and daily stand-up\n* CI/CD pipeline\n* Checking morning emails for job failures, then sending another email to an audience on the success/failure of all jobs. \n* Fielding and prioritizing user requests.\n* Formal release process, inform users what's going in Friday night. \n* Building ETLs with Datastage (federal agency with old tech)\n* Document, and document more. Government loves documents! \n\nI could go on, but I'm not here to complain. I just want to get an idea of what DevOps looks like for data engineers.", "author_fullname": "t2_2o0q5m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does DevOps look like on your team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zemo3o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1670371698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What does DevOps mean to you? Is it useful to DE?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been on many teams - in a previous career doing production support and then analytics the past few years. But now I ended up on a DevOps team. &lt;/p&gt;\n\n&lt;p&gt;What this team does, for example:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Agile, including two week sprints and daily stand-up&lt;/li&gt;\n&lt;li&gt;CI/CD pipeline&lt;/li&gt;\n&lt;li&gt;Checking morning emails for job failures, then sending another email to an audience on the success/failure of all jobs. &lt;/li&gt;\n&lt;li&gt;Fielding and prioritizing user requests.&lt;/li&gt;\n&lt;li&gt;Formal release process, inform users what&amp;#39;s going in Friday night. &lt;/li&gt;\n&lt;li&gt;Building ETLs with Datastage (federal agency with old tech)&lt;/li&gt;\n&lt;li&gt;Document, and document more. Government loves documents! &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I could go on, but I&amp;#39;m not here to complain. I just want to get an idea of what DevOps looks like for data engineers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zemo3o", "is_robot_indexable": true, "report_reasons": null, "author": "rotterdamn8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zemo3o/what_does_devops_look_like_on_your_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zemo3o/what_does_devops_look_like_on_your_team/", "subreddit_subscribers": 82150, "created_utc": 1670371698.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}