{"kind": "Listing", "data": {"after": "t3_zp66w9", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_y9d76v7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "wife bought me a 10tb drive for Christmas, it was mislabelled at the factory and it's actually a 12tb drive!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"e32q6ptsro6a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 164, "x": 108, "u": "https://preview.redd.it/e32q6ptsro6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc07ba06291c98f54871ecacc9ca96e7c1cbb659"}, {"y": 328, "x": 216, "u": "https://preview.redd.it/e32q6ptsro6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3268aad84863a4f181d28d0d674a313116a83d10"}, {"y": 485, "x": 320, "u": "https://preview.redd.it/e32q6ptsro6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a90a5640f2572aa0406253064e56e3c5f58fd0a2"}, {"y": 971, "x": 640, "u": "https://preview.redd.it/e32q6ptsro6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=05b48319920dc2655e59431c581bfb203fc3f9db"}, {"y": 1457, "x": 960, "u": "https://preview.redd.it/e32q6ptsro6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e1a98cd7e7da9aa324293c4a0e7a396ffac86fa"}, {"y": 1640, "x": 1080, "u": "https://preview.redd.it/e32q6ptsro6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dbbc61fdf98e909aebb071deafe391c2508f0593"}], "s": {"y": 1640, "x": 1080, "u": "https://preview.redd.it/e32q6ptsro6a1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=14955ec5e1bb389e430cb57dadac85810685004f"}, "id": "e32q6ptsro6a1"}, "pgm5o4wsro6a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 192, "x": 108, "u": "https://preview.redd.it/pgm5o4wsro6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3bb6888ff9d81533532991a5623e25f1ad2e36fb"}, {"y": 384, "x": 216, "u": "https://preview.redd.it/pgm5o4wsro6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=98e4a95d5dff5e514f95c882723eb326a0d4e8f2"}, {"y": 568, "x": 320, "u": "https://preview.redd.it/pgm5o4wsro6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cd401d395c3997294c0b4a49681e8facf7dc2a00"}, {"y": 1137, "x": 640, "u": "https://preview.redd.it/pgm5o4wsro6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b80ae9263bafd4ba4b1ac336273c971b906cb76"}, {"y": 1706, "x": 960, "u": "https://preview.redd.it/pgm5o4wsro6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=40e1eac36939f72bb733a82e1b799807e504b032"}, {"y": 1920, "x": 1080, "u": "https://preview.redd.it/pgm5o4wsro6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c6de686304bd34069493e73ec8ffd2c887379675"}], "s": {"y": 4032, "x": 2268, "u": "https://preview.redd.it/pgm5o4wsro6a1.jpg?width=2268&amp;format=pjpg&amp;auto=webp&amp;s=c16837b83bce23931c309dca7e92438ab95026e9"}, "id": "pgm5o4wsro6a1"}}, "name": "t3_zp2srs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 939, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "e32q6ptsro6a1", "id": 220470265}, {"media_id": "pgm5o4wsro6a1", "id": 220470266}]}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 939, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cnENuY7O9iqJflN1NbxecDGmwNN_bYcfWVhZfIQFFMY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671382175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zp2srs", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zp2srs", "is_robot_indexable": true, "report_reasons": null, "author": "joebaes1", "discussion_type": null, "num_comments": 137, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zp2srs/wife_bought_me_a_10tb_drive_for_christmas_it_was/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zp2srs", "subreddit_subscribers": 660025, "created_utc": 1671382175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_odki8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Tech Job! Found a box full of HDD. Boss said I can keep them. I\u2019m happy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zp569i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 254, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 254, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7rT8hAuqT3MRTR1MP1OPlAD6ARCTf2wUJ5qmFny8EAw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671389040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8uf4g6iqtq6a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8uf4g6iqtq6a1.jpg?auto=webp&amp;s=742f439986c868c421687ae06390ddd309ff173b", "width": 3024, "height": 4032}, "resolutions": [{"url": "https://preview.redd.it/8uf4g6iqtq6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a5ee1e7620de17342bd2f86995edd478f386251", "width": 108, "height": 144}, {"url": "https://preview.redd.it/8uf4g6iqtq6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b935252f7fd053e24e9929ee292df56abc502c91", "width": 216, "height": 288}, {"url": "https://preview.redd.it/8uf4g6iqtq6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33ecbeab74199128f5c16c32716524c6b07dc42c", "width": 320, "height": 426}, {"url": "https://preview.redd.it/8uf4g6iqtq6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa9e8dfa288c1a56da4a8534e5092b1f4c9d0b5d", "width": 640, "height": 853}, {"url": "https://preview.redd.it/8uf4g6iqtq6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a613b20f0f93fa5fdf8cc4445ad1311a90d628e", "width": 960, "height": 1280}, {"url": "https://preview.redd.it/8uf4g6iqtq6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a98f4c47cf953b6cefd2742c5b5a97755746e1f3", "width": 1080, "height": 1440}], "variants": {}, "id": "waPfL5UvWtGz8D9UFHcwEu0cbU_Zz873TExKpXg0as0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zp569i", "is_robot_indexable": true, "report_reasons": null, "author": "wicodly", "discussion_type": null, "num_comments": 101, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zp569i/new_tech_job_found_a_box_full_of_hdd_boss_said_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8uf4g6iqtq6a1.jpg", "subreddit_subscribers": 660025, "created_utc": 1671389040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "All AWS storage products charge outrageous egress fees, to the extend that backing up to AWS doesn't seem feasible for me. It's like Hotel California: upload is free, storage cost is okay, but you want your data back? Payback!\n\nFor example: s3 glacier us-east-1: $0.0036\u00a0per GB + $0.09 per GB egress (for first 10tb).\n\nhttps://aws.amazon.com/s3/pricing/\n\nSo if I backup 10 tb to aws and download it back at end of 1 year, it'll cost (0.0036 * 1024 * 10 * 12)+(0.09 * 1024 * 10)=442+921, i.e. 67% of my expense goes to egress. \n\nI'm like...no f way. Until I saw something else they offered: workdoc! Apparently there's no charge on egress. I reread it trice but yeah. $5 per user with 1tb included. You can allocate more storage per user but that'll cost more than creating a second user, so just create 10 users. That works out to 5 * 10 * 12=$600, less than half of glacier, and you don't have to wait 12 hours. Oh and there's a web gui.\n\nhttps://aws.amazon.com/workdocs/pricing/\n\nAre they doing this to compete with gdrive? I cannot imagine they omitted egress charges by mistake.", "author_fullname": "t2_ratqygnj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS storage without egress charges...an intended loophole?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpg3d8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671419780.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671418809.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;All AWS storage products charge outrageous egress fees, to the extend that backing up to AWS doesn&amp;#39;t seem feasible for me. It&amp;#39;s like Hotel California: upload is free, storage cost is okay, but you want your data back? Payback!&lt;/p&gt;\n\n&lt;p&gt;For example: s3 glacier us-east-1: $0.0036\u00a0per GB + $0.09 per GB egress (for first 10tb).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aws.amazon.com/s3/pricing/\"&gt;https://aws.amazon.com/s3/pricing/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So if I backup 10 tb to aws and download it back at end of 1 year, it&amp;#39;ll cost (0.0036 * 1024 * 10 * 12)+(0.09 * 1024 * 10)=442+921, i.e. 67% of my expense goes to egress. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m like...no f way. Until I saw something else they offered: workdoc! Apparently there&amp;#39;s no charge on egress. I reread it trice but yeah. $5 per user with 1tb included. You can allocate more storage per user but that&amp;#39;ll cost more than creating a second user, so just create 10 users. That works out to 5 * 10 * 12=$600, less than half of glacier, and you don&amp;#39;t have to wait 12 hours. Oh and there&amp;#39;s a web gui.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aws.amazon.com/workdocs/pricing/\"&gt;https://aws.amazon.com/workdocs/pricing/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Are they doing this to compete with gdrive? I cannot imagine they omitted egress charges by mistake.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?auto=webp&amp;s=8e3eb77ba905bb641af80fcf3efe1de0190ac8c2", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b015da4f990706696f7d06ac19bc75b807d90200", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44756ae9c6e1724356ccaef8214086d7d0cc95da", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a5696e6599c7d56b3770650b416341ba2102fd8", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4610f9bb7893259c61ba4fda892295f0da1a05ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9979842391099359ecaa7d0ce4c8c31f1e3bead7", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fe7c4eb58196f897578137b50f669a4707c9902", "width": 1080, "height": 567}], "variants": {}, "id": "RUqh18uQTwuGJocqdUcC-6UfvfWS63SRDdr8AQqU3uM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpg3d8", "is_robot_indexable": true, "report_reasons": null, "author": "lmux", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpg3d8/aws_storage_without_egress_chargesan_intended/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpg3d8/aws_storage_without_egress_chargesan_intended/", "subreddit_subscribers": 660025, "created_utc": 1671418809.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just wasted a day messing about with UNRAID to realize it's limited to 30 drives. :-/\n\nI have a sizable pile of 5TB, 4TB, 3TB and 2TB drives.  I live somewhere where power is cheap and cooling is fresh air for 9 out of 12 months.  I have JBODs and \"IT\" flashed controllers to run more than I have.  All told, it's around .5 PB... someday, I'd like to break the PB barrier, but today is not that day.\n\nCan someone recommend a single software platform to support various disk sizes, reasonable (N+2) resillency and easy growth/failure replacement?\n\nUNRAID, too few disks.  TrueNAS, would need a seperate pool for each disk size, replacement blows.  I know next to nothing about OpenMediaVault but am going to fire that up here soon to poke about.  I see people complaining about NFS speeds, but in general, I don't need this to be super fast.\n\nI run a plex server (I actually won't run that as a plugin, even if the platform supports it) that servers up and transcodes 4K HDR content, a few infrastructure VMs and some game servers, but outside of my plex server being a consumer of large disk, I don't have significant performance needs.  My VMs I'll either run local or if I feel the need to go back to multiple VM hosts, I'll come up with something seperate from my bulk storage.\n\nThanks for perusing my wall of text.  Curious what other folks are using for large drive count systems. \n\nI'll head off the \"12TB refurbs are cheap\" response with \"there's nothing cheaper than what you already have.\"", "author_fullname": "t2_er16k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Storage Software/Platform Recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpdnok", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671411753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wasted a day messing about with UNRAID to realize it&amp;#39;s limited to 30 drives. :-/&lt;/p&gt;\n\n&lt;p&gt;I have a sizable pile of 5TB, 4TB, 3TB and 2TB drives.  I live somewhere where power is cheap and cooling is fresh air for 9 out of 12 months.  I have JBODs and &amp;quot;IT&amp;quot; flashed controllers to run more than I have.  All told, it&amp;#39;s around .5 PB... someday, I&amp;#39;d like to break the PB barrier, but today is not that day.&lt;/p&gt;\n\n&lt;p&gt;Can someone recommend a single software platform to support various disk sizes, reasonable (N+2) resillency and easy growth/failure replacement?&lt;/p&gt;\n\n&lt;p&gt;UNRAID, too few disks.  TrueNAS, would need a seperate pool for each disk size, replacement blows.  I know next to nothing about OpenMediaVault but am going to fire that up here soon to poke about.  I see people complaining about NFS speeds, but in general, I don&amp;#39;t need this to be super fast.&lt;/p&gt;\n\n&lt;p&gt;I run a plex server (I actually won&amp;#39;t run that as a plugin, even if the platform supports it) that servers up and transcodes 4K HDR content, a few infrastructure VMs and some game servers, but outside of my plex server being a consumer of large disk, I don&amp;#39;t have significant performance needs.  My VMs I&amp;#39;ll either run local or if I feel the need to go back to multiple VM hosts, I&amp;#39;ll come up with something seperate from my bulk storage.&lt;/p&gt;\n\n&lt;p&gt;Thanks for perusing my wall of text.  Curious what other folks are using for large drive count systems. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll head off the &amp;quot;12TB refurbs are cheap&amp;quot; response with &amp;quot;there&amp;#39;s nothing cheaper than what you already have.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpdnok", "is_robot_indexable": true, "report_reasons": null, "author": "Thranx", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpdnok/looking_for_storage_softwareplatform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpdnok/looking_for_storage_softwareplatform/", "subreddit_subscribers": 660025, "created_utc": 1671411753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Hello internet,\n\nI  commonly use sites like keep2share to download files and have been  using mullvad to bypass data limitations. Some videos are broken into multiple parts and I often pull my phone out to download simultaneously.\n\nI  was wondering if there was a more efficient way to do this from one  device and if I could do something like linking different browser  windows to different IP addresses. I am constantly downloading files and  am perfectly fine with the time investment it may take to set something  like this up.  \nI thought of Jdownloader but I think that uses proxies versus a VPN correct?  \n\n\nSorry for the newbie questions", "author_fullname": "t2_uxz2rq9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using VPN to bypass file hoster limits", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpancs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671403485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello internet,&lt;/p&gt;\n\n&lt;p&gt;I  commonly use sites like keep2share to download files and have been  using mullvad to bypass data limitations. Some videos are broken into multiple parts and I often pull my phone out to download simultaneously.&lt;/p&gt;\n\n&lt;p&gt;I  was wondering if there was a more efficient way to do this from one  device and if I could do something like linking different browser  windows to different IP addresses. I am constantly downloading files and  am perfectly fine with the time investment it may take to set something  like this up.&lt;br/&gt;\nI thought of Jdownloader but I think that uses proxies versus a VPN correct?  &lt;/p&gt;\n\n&lt;p&gt;Sorry for the newbie questions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpancs", "is_robot_indexable": true, "report_reasons": null, "author": "ForeignEfficiency401", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpancs/using_vpn_to_bypass_file_hoster_limits/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpancs/using_vpn_to_bypass_file_hoster_limits/", "subreddit_subscribers": 660025, "created_utc": 1671403485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So to start, I want to say I have browsed the sub for hours for a solution, but there doesn't appear to be anything. So I thought maybe I'll just ask directly.\n\nI am stressed.\n\nI have a year until amazon drive shuts down, but only a month until Data uploads stop.\n\nI'm a photographer/videographer and a data hoarder at the same time. I have **2 NAS systems** that are linked to **2 different Amazon drive/photo accounts** and this set up has been amazing.\n\nSure I pay about 400$ a year, but in turn, I get Amazon's AI to sort and organize EVERYTHING that I put into the cloud drive. Whenever friends or family ask me for pics/vids, I can find and send it within minutes through Amazon. When I need to edit videos, I do just grab the files from folders that I sorted in the cloud drive (through the NAS clone). Any changes to the NAS are synced with Amazon. The fact that I don't have to pay for any picture files is also amazing considering I shoot in raw (very high data usage). It's also so convenient to just plug in an SD card into the computer and it automatically uploads to both the cloud and to my local NAS.\n\nI will assume that once cloud drive is removed, my entire set up will vanish.\n\nThe AI that sorts everything is unparrelled. I'm going to miss it more than anything. It's going to be extremely difficult for me to continue using Amazon photos after this and I'll probably just end up cancelling my membership.\n\nI need help to find a replacement workflow that I can sort of get to work. Cloud OR local\n\n1. It looks like my best option from this point is to self host everything. I hate doing this because I love hybrid setups. Mobile convenience is also pretty important. But if it must be done, oh well.\n2. I considered google drive. But google photos and drive are seperate, unlike amazon. No point. (Although I do love google drive's system too)\n3. I have already prepped for migration (in a way). While all the files are already backed to the NAS, I popped another copy onto harddrives.\n\nSo key requirements that I need.\n\n1. Some sort of auto sorting feature. Date and Faces are a requirement. Everything else is optional but wish it existed.\n2. Mobile app access and upload. All photos and vids from phone are auto uploaded.\n3. Easy sharing.\n\nOptional things I'd love.\n\n1. Amazon photos and google photos have amazing AI and features. Will miss it.\n2. The ability for friends to upload as well (only google photos has this. Not a part of my current set up, but would have been amazing)\n\nI have over 100k photos and 20k videos. Honestly, probably more. Some recent issues caused a desync and the cloud is missing about 1tb of data (yikes, right?)", "author_fullname": "t2_ov7nq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to do now after Amazon Drive is shutting down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zpm4ym", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671438635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So to start, I want to say I have browsed the sub for hours for a solution, but there doesn&amp;#39;t appear to be anything. So I thought maybe I&amp;#39;ll just ask directly.&lt;/p&gt;\n\n&lt;p&gt;I am stressed.&lt;/p&gt;\n\n&lt;p&gt;I have a year until amazon drive shuts down, but only a month until Data uploads stop.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a photographer/videographer and a data hoarder at the same time. I have &lt;strong&gt;2 NAS systems&lt;/strong&gt; that are linked to &lt;strong&gt;2 different Amazon drive/photo accounts&lt;/strong&gt; and this set up has been amazing.&lt;/p&gt;\n\n&lt;p&gt;Sure I pay about 400$ a year, but in turn, I get Amazon&amp;#39;s AI to sort and organize EVERYTHING that I put into the cloud drive. Whenever friends or family ask me for pics/vids, I can find and send it within minutes through Amazon. When I need to edit videos, I do just grab the files from folders that I sorted in the cloud drive (through the NAS clone). Any changes to the NAS are synced with Amazon. The fact that I don&amp;#39;t have to pay for any picture files is also amazing considering I shoot in raw (very high data usage). It&amp;#39;s also so convenient to just plug in an SD card into the computer and it automatically uploads to both the cloud and to my local NAS.&lt;/p&gt;\n\n&lt;p&gt;I will assume that once cloud drive is removed, my entire set up will vanish.&lt;/p&gt;\n\n&lt;p&gt;The AI that sorts everything is unparrelled. I&amp;#39;m going to miss it more than anything. It&amp;#39;s going to be extremely difficult for me to continue using Amazon photos after this and I&amp;#39;ll probably just end up cancelling my membership.&lt;/p&gt;\n\n&lt;p&gt;I need help to find a replacement workflow that I can sort of get to work. Cloud OR local&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;It looks like my best option from this point is to self host everything. I hate doing this because I love hybrid setups. Mobile convenience is also pretty important. But if it must be done, oh well.&lt;/li&gt;\n&lt;li&gt;I considered google drive. But google photos and drive are seperate, unlike amazon. No point. (Although I do love google drive&amp;#39;s system too)&lt;/li&gt;\n&lt;li&gt;I have already prepped for migration (in a way). While all the files are already backed to the NAS, I popped another copy onto harddrives.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So key requirements that I need.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Some sort of auto sorting feature. Date and Faces are a requirement. Everything else is optional but wish it existed.&lt;/li&gt;\n&lt;li&gt;Mobile app access and upload. All photos and vids from phone are auto uploaded.&lt;/li&gt;\n&lt;li&gt;Easy sharing.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Optional things I&amp;#39;d love.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Amazon photos and google photos have amazing AI and features. Will miss it.&lt;/li&gt;\n&lt;li&gt;The ability for friends to upload as well (only google photos has this. Not a part of my current set up, but would have been amazing)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I have over 100k photos and 20k videos. Honestly, probably more. Some recent issues caused a desync and the cloud is missing about 1tb of data (yikes, right?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpm4ym", "is_robot_indexable": true, "report_reasons": null, "author": "coolelel", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpm4ym/what_to_do_now_after_amazon_drive_is_shutting_down/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpm4ym/what_to_do_now_after_amazon_drive_is_shutting_down/", "subreddit_subscribers": 660025, "created_utc": 1671438635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I'm doing it in this way: I create a folder with the TV serie's name, then I create a folder foreach season, and there I put the files with the name of TV serie - episodie number - episodie title\n\nIf the TV series name is for example \"DataHoarder\":\n\n    DataHoarder/Season 1/DataHoarder - 1x01 - Episodie title.ext\n    DataHoarder/Season 2/DataHoarder - 2x01 - Episodie title.ext\n\nI'm curious, which is your naming convention?", "author_fullname": "t2_2sh9g5iz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you name your TV series folder and files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zp02be", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671374365.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m doing it in this way: I create a folder with the TV serie&amp;#39;s name, then I create a folder foreach season, and there I put the files with the name of TV serie - episodie number - episodie title&lt;/p&gt;\n\n&lt;p&gt;If the TV series name is for example &amp;quot;DataHoarder&amp;quot;:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;DataHoarder/Season 1/DataHoarder - 1x01 - Episodie title.ext\nDataHoarder/Season 2/DataHoarder - 2x01 - Episodie title.ext\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;m curious, which is your naming convention?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zp02be", "is_robot_indexable": true, "report_reasons": null, "author": "secon25", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zp02be/how_do_you_name_your_tv_series_folder_and_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zp02be/how_do_you_name_your_tv_series_folder_and_files/", "subreddit_subscribers": 660025, "created_utc": 1671374365.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Aloha! Among other things I'm a YouTube archivist on Linux system, with lots of videos to store, lots of HDDs of varying sizes, capacity (80-500GB) and age (several disks with BAD sectors that I wanna use as 3rd data duplicates). Then I have a single gaming PC with 2 HDD hotswap ports free. I'm looking for a hoarding setup solution, some guidance towards tools maybe? How could I improve my operation in relation to my low knowledge levels?\n\nHow I have it running currently, is I have a 1TB inside the PC that acts as \"landing\" for incoming data, which I manually process (rename, move around, compare files, remux), then I connect HDDs that I decide should pick them up. I keep track of all data in LibreOffice Spreadsheet: on HDD arrival I sometimes measure its SMART data and update it in the spreadsheet. Then on departure I open the disk with file manager in \"Flat View\", to then copy all file entries with their full path on that disk, paste into spreadsheet page dedicated for that disk (every disk has a dedicated page and I refer to them by unique numbers, magic marker'ed on the disks). All disks are encrypted and NTFS, but I'm looking to formatting them to something better (BTRFS? It says it verifies file integrity, so if the data decays over time it'll be automatically fixed?)\n\nI'm hoping for a solution that could automate some things, especially that spreadsheet, since it's hard to keep track off. I wish disks would be automatically indexed before ejection, and some database would keep track of:\n\n1. what videos are where,\n2. what videos don't have a duplicate on another disk (and which disk keeps that duplicate),\n3. disks are divided into groups A, A\\*, B, B\\*, where:\n\n* A's duplicate to other A's once;\n* \\*'s can't have its duplicates on another \\*;\n* B's duplicate from A's only (acting as third duplicate), amount of duplicates depends on amount of B's drives.\n\nAlso worth noting I'm quite a distro-hopper, so I appreciate solutions that I can migrate between OSes. Due to limited amount of drives I can inject simultaneously, I'm guessing data pooling isn't solution for me.", "author_fullname": "t2_7nxyjgm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cold-only storage solution with database indexing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpapw6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671404023.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671403677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Aloha! Among other things I&amp;#39;m a YouTube archivist on Linux system, with lots of videos to store, lots of HDDs of varying sizes, capacity (80-500GB) and age (several disks with BAD sectors that I wanna use as 3rd data duplicates). Then I have a single gaming PC with 2 HDD hotswap ports free. I&amp;#39;m looking for a hoarding setup solution, some guidance towards tools maybe? How could I improve my operation in relation to my low knowledge levels?&lt;/p&gt;\n\n&lt;p&gt;How I have it running currently, is I have a 1TB inside the PC that acts as &amp;quot;landing&amp;quot; for incoming data, which I manually process (rename, move around, compare files, remux), then I connect HDDs that I decide should pick them up. I keep track of all data in LibreOffice Spreadsheet: on HDD arrival I sometimes measure its SMART data and update it in the spreadsheet. Then on departure I open the disk with file manager in &amp;quot;Flat View&amp;quot;, to then copy all file entries with their full path on that disk, paste into spreadsheet page dedicated for that disk (every disk has a dedicated page and I refer to them by unique numbers, magic marker&amp;#39;ed on the disks). All disks are encrypted and NTFS, but I&amp;#39;m looking to formatting them to something better (BTRFS? It says it verifies file integrity, so if the data decays over time it&amp;#39;ll be automatically fixed?)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping for a solution that could automate some things, especially that spreadsheet, since it&amp;#39;s hard to keep track off. I wish disks would be automatically indexed before ejection, and some database would keep track of:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;what videos are where,&lt;/li&gt;\n&lt;li&gt;what videos don&amp;#39;t have a duplicate on another disk (and which disk keeps that duplicate),&lt;/li&gt;\n&lt;li&gt;disks are divided into groups A, A*, B, B*, where:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A&amp;#39;s duplicate to other A&amp;#39;s once;&lt;/li&gt;\n&lt;li&gt;*&amp;#39;s can&amp;#39;t have its duplicates on another *;&lt;/li&gt;\n&lt;li&gt;B&amp;#39;s duplicate from A&amp;#39;s only (acting as third duplicate), amount of duplicates depends on amount of B&amp;#39;s drives.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Also worth noting I&amp;#39;m quite a distro-hopper, so I appreciate solutions that I can migrate between OSes. Due to limited amount of drives I can inject simultaneously, I&amp;#39;m guessing data pooling isn&amp;#39;t solution for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpapw6", "is_robot_indexable": true, "report_reasons": null, "author": "Incredible_Violent", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpapw6/coldonly_storage_solution_with_database_indexing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpapw6/coldonly_storage_solution_with_database_indexing/", "subreddit_subscribers": 660025, "created_utc": 1671403677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, I'm pretty new to actually organizing my storage. I have loads of stuff that I've collected through the years in several HDDs and SSDs, maybe a dozen or more in total. I was considering getting a NAS so that I can lower the risk of losing anything and also make everything accessible to me easily. \n\nWhat I'm most concerned about is availability/reliability. AWS guarantees 99,999999999%. That's not something you can get at home. There are so many ways to just mess up. The cost to scale is also very linear for AWS, compared to a home system.\n\nI have unlimited Internet already.", "author_fullname": "t2_jqpu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using AWS S3 Glacier instead of building a NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zp6fxy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671392452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;m pretty new to actually organizing my storage. I have loads of stuff that I&amp;#39;ve collected through the years in several HDDs and SSDs, maybe a dozen or more in total. I was considering getting a NAS so that I can lower the risk of losing anything and also make everything accessible to me easily. &lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m most concerned about is availability/reliability. AWS guarantees 99,999999999%. That&amp;#39;s not something you can get at home. There are so many ways to just mess up. The cost to scale is also very linear for AWS, compared to a home system.&lt;/p&gt;\n\n&lt;p&gt;I have unlimited Internet already.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zp6fxy", "is_robot_indexable": true, "report_reasons": null, "author": "piponwa", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zp6fxy/using_aws_s3_glacier_instead_of_building_a_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zp6fxy/using_aws_s3_glacier_instead_of_building_a_nas/", "subreddit_subscribers": 660025, "created_utc": 1671392452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is my first time really looking into Smart data, but...this drive is on its way out right?  WD RED, 4TB\n\nSMART overall-health self-assessment test result: PASSED\n\nID# ATTRIBUTE\\_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN\\_FAILED RAW\\_VALUE\n\n1 Raw\\_Read\\_Error\\_Rate     0x002f   200   200   051    Pre-fail  Always       -       0\n\n3 Spin\\_Up\\_Time            0x0027   184   182   021    Pre-fail  Always       -       7775\n\n4 Start\\_Stop\\_Count        0x0032   063   063   000    Old\\_age   Always       -       37548\n\n5 Reallocated\\_Sector\\_Ct   0x0033   200   200   140    Pre-fail  Always       -       0\n\n7 Seek\\_Error\\_Rate         0x002e   200   200   000    Old\\_age   Always       -       0\n\n9 Power\\_On\\_Hours          0x0032   034   034   000    Old\\_age   Always       -       48846\n\n10 Spin\\_Retry\\_Count        0x0032   100   100   000    Old\\_age   Always       -       0\n\n11 Calibration\\_Retry\\_Count 0x0032   100   253   000    Old\\_age   Always       -       0\n\n12 Power\\_Cycle\\_Count       0x0032   100   100   000    Old\\_age   Always       -       55\n\n192 Power-Off\\_Retract\\_Count 0x0032   200   200   000    Old\\_age   Always       -       37\n\n193 Load\\_Cycle\\_Count        0x0032   188   188   000    Old\\_age   Always       -       38053\n\n194 Temperature\\_Celsius     0x0022   119   111   000    Old\\_age   Always       -       33\n\n196 Reallocated\\_Event\\_Count 0x0032   200   200   000    Old\\_age   Always       -       0\n\n197 Current\\_Pending\\_Sector  0x0032   200   200   000    Old\\_age   Always       -       0\n\n198 Offline\\_Uncorrectable   0x0030   100   253   000    Old\\_age   Offline      -       0\n\n199 UDMA\\_CRC\\_Error\\_Count    0x0032   200   200   000    Old\\_age   Always       -       0\n\n200 Multi\\_Zone\\_Error\\_Rate   0x0008   200   200   000    Old\\_age   Offline      -       9\n\n&amp;#x200B;\n\nSMART Error Log Version: 1\n\nNo Errors Logged\n\n&amp;#x200B;\n\nSMART Self-test log structure revision number 1\n\nNum  Test\\_Description    Status                  Remaining  LifeTime(hours)  LBA\\_of\\_first\\_error\n\n\\# 1  Short offline       Completed: read failure       60%     48846         546936\n\n\\# 2  Short offline       Completed: read failure       50%     48845         549840\n\n\\# 3  Extended offline    Completed: read failure       90%     48843         549840\n\n\\# 4  Short offline       Completed without error       00%     48843         -\n\n\\# 5  Short offline       Completed: read failure       60%     48842         546936\n\n\\# 6  Short offline       Completed: read failure       40%     48842         549840\n\n\\# 7  Short offline       Completed: read failure       40%     48842         549840\n\n\\# 8  Short offline       Completed without error       00%         0         -", "author_fullname": "t2_jt56euwm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Failing WD Red?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zphvtq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671424159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is my first time really looking into Smart data, but...this drive is on its way out right?  WD RED, 4TB&lt;/p&gt;\n\n&lt;p&gt;SMART overall-health self-assessment test result: PASSED&lt;/p&gt;\n\n&lt;p&gt;ID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE&lt;/p&gt;\n\n&lt;p&gt;1 Raw_Read_Error_Rate     0x002f   200   200   051    Pre-fail  Always       -       0&lt;/p&gt;\n\n&lt;p&gt;3 Spin_Up_Time            0x0027   184   182   021    Pre-fail  Always       -       7775&lt;/p&gt;\n\n&lt;p&gt;4 Start_Stop_Count        0x0032   063   063   000    Old_age   Always       -       37548&lt;/p&gt;\n\n&lt;p&gt;5 Reallocated_Sector_Ct   0x0033   200   200   140    Pre-fail  Always       -       0&lt;/p&gt;\n\n&lt;p&gt;7 Seek_Error_Rate         0x002e   200   200   000    Old_age   Always       -       0&lt;/p&gt;\n\n&lt;p&gt;9 Power_On_Hours          0x0032   034   034   000    Old_age   Always       -       48846&lt;/p&gt;\n\n&lt;p&gt;10 Spin_Retry_Count        0x0032   100   100   000    Old_age   Always       -       0&lt;/p&gt;\n\n&lt;p&gt;11 Calibration_Retry_Count 0x0032   100   253   000    Old_age   Always       -       0&lt;/p&gt;\n\n&lt;p&gt;12 Power_Cycle_Count       0x0032   100   100   000    Old_age   Always       -       55&lt;/p&gt;\n\n&lt;p&gt;192 Power-Off_Retract_Count 0x0032   200   200   000    Old_age   Always       -       37&lt;/p&gt;\n\n&lt;p&gt;193 Load_Cycle_Count        0x0032   188   188   000    Old_age   Always       -       38053&lt;/p&gt;\n\n&lt;p&gt;194 Temperature_Celsius     0x0022   119   111   000    Old_age   Always       -       33&lt;/p&gt;\n\n&lt;p&gt;196 Reallocated_Event_Count 0x0032   200   200   000    Old_age   Always       -       0&lt;/p&gt;\n\n&lt;p&gt;197 Current_Pending_Sector  0x0032   200   200   000    Old_age   Always       -       0&lt;/p&gt;\n\n&lt;p&gt;198 Offline_Uncorrectable   0x0030   100   253   000    Old_age   Offline      -       0&lt;/p&gt;\n\n&lt;p&gt;199 UDMA_CRC_Error_Count    0x0032   200   200   000    Old_age   Always       -       0&lt;/p&gt;\n\n&lt;p&gt;200 Multi_Zone_Error_Rate   0x0008   200   200   000    Old_age   Offline      -       9&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;SMART Error Log Version: 1&lt;/p&gt;\n\n&lt;p&gt;No Errors Logged&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;SMART Self-test log structure revision number 1&lt;/p&gt;\n\n&lt;p&gt;Num  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error&lt;/p&gt;\n\n&lt;p&gt;# 1  Short offline       Completed: read failure       60%     48846         546936&lt;/p&gt;\n\n&lt;p&gt;# 2  Short offline       Completed: read failure       50%     48845         549840&lt;/p&gt;\n\n&lt;p&gt;# 3  Extended offline    Completed: read failure       90%     48843         549840&lt;/p&gt;\n\n&lt;p&gt;# 4  Short offline       Completed without error       00%     48843         -&lt;/p&gt;\n\n&lt;p&gt;# 5  Short offline       Completed: read failure       60%     48842         546936&lt;/p&gt;\n\n&lt;p&gt;# 6  Short offline       Completed: read failure       40%     48842         549840&lt;/p&gt;\n\n&lt;p&gt;# 7  Short offline       Completed: read failure       40%     48842         549840&lt;/p&gt;\n\n&lt;p&gt;# 8  Short offline       Completed without error       00%         0         -&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zphvtq", "is_robot_indexable": true, "report_reasons": null, "author": "ExoticMushroom1016", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zphvtq/failing_wd_red/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zphvtq/failing_wd_red/", "subreddit_subscribers": 660025, "created_utc": 1671424159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Got a bunch of old VHS tapes from early to late nineties I'm trying to get converted over to digital. Obviously I'm aiming for as high of quality capture as I can get, but I've only got a few cheap options and my current setup is nowhere near ideal. \n\nWhat I have is a Genki Arcade capture card (it's and HDMI plug to the output of source and a USB-C that goes to the computer) and a cheap AV to HDMI converter. Able to get it hooked up to an old JVC VCR but the image quality is horrid. I've found a DVD/VHS player that does have an HDMI out port built into it. It's a Panasonic DMR-EZ48V. I could fit the capture card I've got straight to it and bypass the cheap converter. \n\nWould that be a better option to go, or should I invest more to a fully different setup?", "author_fullname": "t2_aaqxalqt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up to save some VHS tapes.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpfiz8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671417184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got a bunch of old VHS tapes from early to late nineties I&amp;#39;m trying to get converted over to digital. Obviously I&amp;#39;m aiming for as high of quality capture as I can get, but I&amp;#39;ve only got a few cheap options and my current setup is nowhere near ideal. &lt;/p&gt;\n\n&lt;p&gt;What I have is a Genki Arcade capture card (it&amp;#39;s and HDMI plug to the output of source and a USB-C that goes to the computer) and a cheap AV to HDMI converter. Able to get it hooked up to an old JVC VCR but the image quality is horrid. I&amp;#39;ve found a DVD/VHS player that does have an HDMI out port built into it. It&amp;#39;s a Panasonic DMR-EZ48V. I could fit the capture card I&amp;#39;ve got straight to it and bypass the cheap converter. &lt;/p&gt;\n\n&lt;p&gt;Would that be a better option to go, or should I invest more to a fully different setup?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpfiz8", "is_robot_indexable": true, "report_reasons": null, "author": "TheFluBug", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpfiz8/setting_up_to_save_some_vhs_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpfiz8/setting_up_to_save_some_vhs_tapes/", "subreddit_subscribers": 660025, "created_utc": 1671417184.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just moving my Unraid server across to new hardware based around an ASRock Z790 Pro RS MoBo (have flashed the latest stable BIOS). \n\nI originally set up Unraid last month on an old/ repurposed Xeon based Thinkserver TS140.\n\nI moved the drives across (6TB Seagate, 8TB Seagate, 10TB WD, 18TB WD)\n\nPlus I'd been running the 2 x 20TB WDs unshucked as the parity drives via USB, so I shucked these and put them in the new case as I now have the PSU and drive bays to handle them.\n\nWhen I first turned the new PC on, only the Seagates were detected, both in BIOS &amp; Unraid.\n\nSo I tried taping pin 3 on the WD drives. The 10TB &amp; 18TB are now being detected in BIOS &amp; Unraid, but the 20TBs still aren't being detected.\n\nThe Thinkserver was running in Legacy BIOS mode. The ASRock doesn't have a legacy option (that I can find), so is running as UEFI. Would that affect this?\n\nAny suggestions of something else I can try to get the 20TBs working?\n\nAre others using these drives with a recent UEFI only motherboard?", "author_fullname": "t2_h5frl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pair of shucked 20TB WD200EDGZ WD Elements drives not detected with new motherboard, even with pin 3 masked", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpd7sa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671410482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just moving my Unraid server across to new hardware based around an ASRock Z790 Pro RS MoBo (have flashed the latest stable BIOS). &lt;/p&gt;\n\n&lt;p&gt;I originally set up Unraid last month on an old/ repurposed Xeon based Thinkserver TS140.&lt;/p&gt;\n\n&lt;p&gt;I moved the drives across (6TB Seagate, 8TB Seagate, 10TB WD, 18TB WD)&lt;/p&gt;\n\n&lt;p&gt;Plus I&amp;#39;d been running the 2 x 20TB WDs unshucked as the parity drives via USB, so I shucked these and put them in the new case as I now have the PSU and drive bays to handle them.&lt;/p&gt;\n\n&lt;p&gt;When I first turned the new PC on, only the Seagates were detected, both in BIOS &amp;amp; Unraid.&lt;/p&gt;\n\n&lt;p&gt;So I tried taping pin 3 on the WD drives. The 10TB &amp;amp; 18TB are now being detected in BIOS &amp;amp; Unraid, but the 20TBs still aren&amp;#39;t being detected.&lt;/p&gt;\n\n&lt;p&gt;The Thinkserver was running in Legacy BIOS mode. The ASRock doesn&amp;#39;t have a legacy option (that I can find), so is running as UEFI. Would that affect this?&lt;/p&gt;\n\n&lt;p&gt;Any suggestions of something else I can try to get the 20TBs working?&lt;/p&gt;\n\n&lt;p&gt;Are others using these drives with a recent UEFI only motherboard?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpd7sa", "is_robot_indexable": true, "report_reasons": null, "author": "ceestars", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpd7sa/pair_of_shucked_20tb_wd200edgz_wd_elements_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpd7sa/pair_of_shucked_20tb_wd200edgz_wd_elements_drives/", "subreddit_subscribers": 660025, "created_utc": 1671410482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The sequence of events:\n\n1. I have this  hdd (TOSHIBA HDTB420EK3AA) and i want to use it to backup some internal drives on a desktop. I write a bash script to do this, i mess up,  i corrupt the data, and i have to format it. \n2. Trying to format this thing makes my pc freeze so badly i have to hard reset. I try the same with other machines, and different operating systmes, each one freezing aswell. The winner is an old laptop on lubuntu that manages to successfully format the drive in just over an hour. \n3. I go back to the first desktop and modify the script, i run it and it kinda works.  By looking at rsync output i notice that the average speed is 500 kb/s, reaching 40kb/s at some points. At some point it starts copying an .IPCH file that makes it reach 0 byte/s and then nothing. \n4. I decide to exclude this file and other folders containing very large amount  of small files, since it looks that the speed slows down especially in these cases. For what i know HDDs are ass in this type of operations but this is just ridiculous. \n5. So i stop the rsync, and everything freezes again. I can't even open the file explorer. I boot windows and use diskpart this time, then i install DiskInfo.  \n\nhttps://preview.redd.it/20pxjo5urq6a1.png?width=670&amp;format=png&amp;auto=webp&amp;s=5f70d06a653ae8e4fda4bbd6e43376a6e578c157\n\nTo me it looks very ok, but in the 20 minutes it took me to write this, diskpart managed to go from 0% to 2% in the process of formatting the disk. \n\nThe disk is almost factory new, but i can't send it back anymore. Any ideas?", "author_fullname": "t2_5wyz3lvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should i keep using this external hdd?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"20pxjo5urq6a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 114, "x": 108, "u": "https://preview.redd.it/20pxjo5urq6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b2ccb00d921eabdf655f7a7ba7b36bd5f043def"}, {"y": 229, "x": 216, "u": "https://preview.redd.it/20pxjo5urq6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ab30365ea0e1f70df33d22d957b285d4a1271b7"}, {"y": 339, "x": 320, "u": "https://preview.redd.it/20pxjo5urq6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=46da3a48fa0ed6e77e698e3b96b503aaddc3e20d"}, {"y": 679, "x": 640, "u": "https://preview.redd.it/20pxjo5urq6a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4a70b82e6eca0c0e05c799b4cfa5197aebd75017"}], "s": {"y": 711, "x": 670, "u": "https://preview.redd.it/20pxjo5urq6a1.png?width=670&amp;format=png&amp;auto=webp&amp;s=5f70d06a653ae8e4fda4bbd6e43376a6e578c157"}, "id": "20pxjo5urq6a1"}}, "name": "t3_zpbwmp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YIJfwFDyi8NAIsF3466KrDqfC5hDcq2t2L6SDHI14Hs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671406857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The sequence of events:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I have this  hdd (TOSHIBA HDTB420EK3AA) and i want to use it to backup some internal drives on a desktop. I write a bash script to do this, i mess up,  i corrupt the data, and i have to format it. &lt;/li&gt;\n&lt;li&gt;Trying to format this thing makes my pc freeze so badly i have to hard reset. I try the same with other machines, and different operating systmes, each one freezing aswell. The winner is an old laptop on lubuntu that manages to successfully format the drive in just over an hour. &lt;/li&gt;\n&lt;li&gt;I go back to the first desktop and modify the script, i run it and it kinda works.  By looking at rsync output i notice that the average speed is 500 kb/s, reaching 40kb/s at some points. At some point it starts copying an .IPCH file that makes it reach 0 byte/s and then nothing. &lt;/li&gt;\n&lt;li&gt;I decide to exclude this file and other folders containing very large amount  of small files, since it looks that the speed slows down especially in these cases. For what i know HDDs are ass in this type of operations but this is just ridiculous. &lt;/li&gt;\n&lt;li&gt;So i stop the rsync, and everything freezes again. I can&amp;#39;t even open the file explorer. I boot windows and use diskpart this time, then i install DiskInfo.&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/20pxjo5urq6a1.png?width=670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f70d06a653ae8e4fda4bbd6e43376a6e578c157\"&gt;https://preview.redd.it/20pxjo5urq6a1.png?width=670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f70d06a653ae8e4fda4bbd6e43376a6e578c157&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To me it looks very ok, but in the 20 minutes it took me to write this, diskpart managed to go from 0% to 2% in the process of formatting the disk. &lt;/p&gt;\n\n&lt;p&gt;The disk is almost factory new, but i can&amp;#39;t send it back anymore. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpbwmp", "is_robot_indexable": true, "report_reasons": null, "author": "Cute_Rub_9074", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpbwmp/should_i_keep_using_this_external_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpbwmp/should_i_keep_using_this_external_hdd/", "subreddit_subscribers": 660025, "created_utc": 1671406857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I feel like I'll know what the unfortunate answer is, but the million dollar question is has anyone found a fast way to calculate the hamming distance between a very very very large set of hashes?\n\nI have a dataset (that will continue to grow) that I'm planning on granting limited access to the public via way of allowing the user upload an item, then the server checks against the database of hashes, and shows information from similar hashes. The issue is that the database of hashes will be over 500 hundred million.\n\nFrom my understanding of finding similar/identical content, hamming distance is the fastest approach in calculating the difference between items? The issue I am seeing is that this must be calculated for each query? So the server is checking the 500 hundred million images each and every time someone wants to check?\n\nIs there any way to speed this process that I'm not seeing in my research? How do things like deduplication software, or reverse/similiar image searches work so fast? What's their secret? Just more compute and they're harnessing a shit ton of compute for each query? [TinEye](https://services.tineye.com/TinEyeAPI) claims to be able to \"search a 57.6 billion web image index in real-time.\" but how?? This must be the KFC's secret herbs and spices or the Coke recipe and I'm just shit out of luck?", "author_fullname": "t2_4z6dy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "fastest way to go about calculating hamming distance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zp7foz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671395223.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671395036.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like I&amp;#39;ll know what the unfortunate answer is, but the million dollar question is has anyone found a fast way to calculate the hamming distance between a very very very large set of hashes?&lt;/p&gt;\n\n&lt;p&gt;I have a dataset (that will continue to grow) that I&amp;#39;m planning on granting limited access to the public via way of allowing the user upload an item, then the server checks against the database of hashes, and shows information from similar hashes. The issue is that the database of hashes will be over 500 hundred million.&lt;/p&gt;\n\n&lt;p&gt;From my understanding of finding similar/identical content, hamming distance is the fastest approach in calculating the difference between items? The issue I am seeing is that this must be calculated for each query? So the server is checking the 500 hundred million images each and every time someone wants to check?&lt;/p&gt;\n\n&lt;p&gt;Is there any way to speed this process that I&amp;#39;m not seeing in my research? How do things like deduplication software, or reverse/similiar image searches work so fast? What&amp;#39;s their secret? Just more compute and they&amp;#39;re harnessing a shit ton of compute for each query? &lt;a href=\"https://services.tineye.com/TinEyeAPI\"&gt;TinEye&lt;/a&gt; claims to be able to &amp;quot;search a 57.6 billion web image index in real-time.&amp;quot; but how?? This must be the KFC&amp;#39;s secret herbs and spices or the Coke recipe and I&amp;#39;m just shit out of luck?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "100+TB offline | 1.45PB @ Google Drive", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zp7foz", "is_robot_indexable": true, "report_reasons": null, "author": "AdamLynch", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zp7foz/fastest_way_to_go_about_calculating_hamming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zp7foz/fastest_way_to_go_about_calculating_hamming/", "subreddit_subscribers": 660025, "created_utc": 1671395036.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i put a folder on media fire 613 files a while ago and removed it from my computer for some more space i cant redownload it because of the bulk download feture i will go insane please help", "author_fullname": "t2_dspn9abd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "mediafire bulk downloader site?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zp6fr4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671392438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i put a folder on media fire 613 files a while ago and removed it from my computer for some more space i cant redownload it because of the bulk download feture i will go insane please help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zp6fr4", "is_robot_indexable": true, "report_reasons": null, "author": "TheEurekaEffect_64", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zp6fr4/mediafire_bulk_downloader_site/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zp6fr4/mediafire_bulk_downloader_site/", "subreddit_subscribers": 660025, "created_utc": 1671392438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a NAS for backup, media server, VMs, etc. It's only serving myself, so I don't like the idea of spinning 6+ spinning up drives up/down when I occasionally need to access a few media files daily when when I reboot for maintenance, testing, or just don't use it for maybe weeks at a time max.\n\nAs I understand, SSDs rarely fail aside from reaching write limit, which is something measurable (and even then, I've heard of people having Samsung SSDs that are nearly twice the TBW that's in the spec). I don't plan on running RAID, since downtime and access to massive amount of data isn't important to me--instead, I prefer frequent incremental backups, occasional full backups, as well as longer life of drives and lower power consumption.\n\nI will only ever deal with 8-16TB worth of data I want online at the moment, rest is cold storage (combined total of ~100TB hard limit for the foreseeable, I only have 60TB data total). I prefer high TBW because performance is not important for me and backing up constant flow of new large media files is.\n\nAssuming high TBW SSDs are suitable for such a use case, what high TBW SSDs do you recommend? I was initially looking at Samsung SSDs because they are the gold standard in general, but I've come across Intel DC S3610 as a recommendation that is spec'd ~5x TBW more (10.7PB). I'm not normally one to look for used storage but people seem to have success with used ones from Ebay that are probably pulled from servers after their intended service and they usually a reasonable amount of remaining life at a great price. Unless these somehow fail in other ways, I don't see how they aren't very popular. This model is 7 years old though, so I'm thinking there might be better options (not necessarily price/TB).\n\nAny thoughts and suggestions are much appreciated.", "author_fullname": "t2_wwrd4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "High TBW SSDs for low power NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zp4h3n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671387130.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a NAS for backup, media server, VMs, etc. It&amp;#39;s only serving myself, so I don&amp;#39;t like the idea of spinning 6+ spinning up drives up/down when I occasionally need to access a few media files daily when when I reboot for maintenance, testing, or just don&amp;#39;t use it for maybe weeks at a time max.&lt;/p&gt;\n\n&lt;p&gt;As I understand, SSDs rarely fail aside from reaching write limit, which is something measurable (and even then, I&amp;#39;ve heard of people having Samsung SSDs that are nearly twice the TBW that&amp;#39;s in the spec). I don&amp;#39;t plan on running RAID, since downtime and access to massive amount of data isn&amp;#39;t important to me--instead, I prefer frequent incremental backups, occasional full backups, as well as longer life of drives and lower power consumption.&lt;/p&gt;\n\n&lt;p&gt;I will only ever deal with 8-16TB worth of data I want online at the moment, rest is cold storage (combined total of ~100TB hard limit for the foreseeable, I only have 60TB data total). I prefer high TBW because performance is not important for me and backing up constant flow of new large media files is.&lt;/p&gt;\n\n&lt;p&gt;Assuming high TBW SSDs are suitable for such a use case, what high TBW SSDs do you recommend? I was initially looking at Samsung SSDs because they are the gold standard in general, but I&amp;#39;ve come across Intel DC S3610 as a recommendation that is spec&amp;#39;d ~5x TBW more (10.7PB). I&amp;#39;m not normally one to look for used storage but people seem to have success with used ones from Ebay that are probably pulled from servers after their intended service and they usually a reasonable amount of remaining life at a great price. Unless these somehow fail in other ways, I don&amp;#39;t see how they aren&amp;#39;t very popular. This model is 7 years old though, so I&amp;#39;m thinking there might be better options (not necessarily price/TB).&lt;/p&gt;\n\n&lt;p&gt;Any thoughts and suggestions are much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zp4h3n", "is_robot_indexable": true, "report_reasons": null, "author": "rofic", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zp4h3n/high_tbw_ssds_for_low_power_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zp4h3n/high_tbw_ssds_for_low_power_nas/", "subreddit_subscribers": 660025, "created_utc": 1671387130.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "hi! so i have what i think is a pretty obscure question/problem to (hopefully) solve lol. there are some songs i downloaded on apple music earlier this year by a certain artist, and recently (i'm not sure exactly when, but within the last few weeks) that artist removed some of those songs (their earliest releases) from all streaming platforms, rendering me unable to play them on my computer or phone.\n\nordinarily, if this happened (and i've certainly had it happen before) i would just find those songs on the internet and download the files to reupload to my apple music library on my computer so i could continue to be able to listen to them normally, but in this case, the artist isn't well known enough for their songs to be on any mp3 sites (trust me, i've checked) or soulseek, and if they were ever posted by the artist on youtube or anywhere else, they've since been taken down there as well.\n\nthe one positive in this scenario is that i keep very regular time machine backups of my macbook pro, so i was able to find a backup of my apple music library file from about a month ago (before the songs were removed from streaming), and also pull the .m4p files of all the songs that were removed and return them to the itunes media folder where they had been before the songs were removed- meaning if i turn off wi-fi on my computer and then use the option key when opening the apple music application so i can open the old version of my music library (instead of the current version where the songs are greyed out and unplayable), i'm still able to play the songs with no issue (since there's no internet connection for the apple music application to realize the songs have been removed since the backup i'm accessing was made).\n\ni was initially relieved when this worked, as up until that point, i was worried that the songs were completely and totally lost to the void and i'd never be able to hear them again, but the problem is that it's obviously quite a hassle to listen to them that way, and doesn't enable me to listen to them on my phone (plus needing to have my wi-fi off is a huge pain), so it's really not a viable solution beyond the super occasional listen. does anyone have any ideas for how i can use what i have at my disposal (apple music drm protected .m4p files and a time machine backup of my apple music library) to achieve my desired end result of acquiring regular (non protected) files of those songs to then reupload to my apple music library for full playability?\n\nall of the supposed apple music drm removal applications i've seen/checked out/tried really just record the audio playback, and many of them won't even work given that the songs aren't currently available in apple music (which they would have to be to be accessible to those programs). i also tried downloading some other screen audio recording software, but the quality was really really subpar. i don't need a 100% lossless quality solution (although that would be nice), but if the difference in quality is going to be obvious even just playing the song off of my computer's built in speakers, that doesn't really cut it.\n\nthe ideal solution for me would be an actual way to convert the apple music drm protected m4p files to mp3/m4a/aac/etc, but from the internet scouring i've done so far, that doesn't seem currently possible (although i would LOVE to find out i'm wrong about that lol). i would also be perfectly satisfied with finding the audio files online somewhere, but from all the searching i've done at this point, i really do think that unfortunately the artist is too obscure for anyone to have uploaded them. my only other thought on that idea was if there was some site i haven't heard of that automatically has any song uploaded to streaming (even if/after they're removed) but the only site i'm currently aware of that's close to that is one that only has songs that are still actively available on streaming- so it does have this artist's other music, but not the removed songs i'm looking for.\n\nanyways, if you've read this whole massive post, thank you lol- and any ideas or suggestions for a solution to this would be immensely appreciated!!!!!", "author_fullname": "t2_v2626iwu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "trying to obtain/convert files of obscure songs removed from streaming (potentially using apple music drm protected m4p files from time machine backup)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zp3oml", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671384736.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi! so i have what i think is a pretty obscure question/problem to (hopefully) solve lol. there are some songs i downloaded on apple music earlier this year by a certain artist, and recently (i&amp;#39;m not sure exactly when, but within the last few weeks) that artist removed some of those songs (their earliest releases) from all streaming platforms, rendering me unable to play them on my computer or phone.&lt;/p&gt;\n\n&lt;p&gt;ordinarily, if this happened (and i&amp;#39;ve certainly had it happen before) i would just find those songs on the internet and download the files to reupload to my apple music library on my computer so i could continue to be able to listen to them normally, but in this case, the artist isn&amp;#39;t well known enough for their songs to be on any mp3 sites (trust me, i&amp;#39;ve checked) or soulseek, and if they were ever posted by the artist on youtube or anywhere else, they&amp;#39;ve since been taken down there as well.&lt;/p&gt;\n\n&lt;p&gt;the one positive in this scenario is that i keep very regular time machine backups of my macbook pro, so i was able to find a backup of my apple music library file from about a month ago (before the songs were removed from streaming), and also pull the .m4p files of all the songs that were removed and return them to the itunes media folder where they had been before the songs were removed- meaning if i turn off wi-fi on my computer and then use the option key when opening the apple music application so i can open the old version of my music library (instead of the current version where the songs are greyed out and unplayable), i&amp;#39;m still able to play the songs with no issue (since there&amp;#39;s no internet connection for the apple music application to realize the songs have been removed since the backup i&amp;#39;m accessing was made).&lt;/p&gt;\n\n&lt;p&gt;i was initially relieved when this worked, as up until that point, i was worried that the songs were completely and totally lost to the void and i&amp;#39;d never be able to hear them again, but the problem is that it&amp;#39;s obviously quite a hassle to listen to them that way, and doesn&amp;#39;t enable me to listen to them on my phone (plus needing to have my wi-fi off is a huge pain), so it&amp;#39;s really not a viable solution beyond the super occasional listen. does anyone have any ideas for how i can use what i have at my disposal (apple music drm protected .m4p files and a time machine backup of my apple music library) to achieve my desired end result of acquiring regular (non protected) files of those songs to then reupload to my apple music library for full playability?&lt;/p&gt;\n\n&lt;p&gt;all of the supposed apple music drm removal applications i&amp;#39;ve seen/checked out/tried really just record the audio playback, and many of them won&amp;#39;t even work given that the songs aren&amp;#39;t currently available in apple music (which they would have to be to be accessible to those programs). i also tried downloading some other screen audio recording software, but the quality was really really subpar. i don&amp;#39;t need a 100% lossless quality solution (although that would be nice), but if the difference in quality is going to be obvious even just playing the song off of my computer&amp;#39;s built in speakers, that doesn&amp;#39;t really cut it.&lt;/p&gt;\n\n&lt;p&gt;the ideal solution for me would be an actual way to convert the apple music drm protected m4p files to mp3/m4a/aac/etc, but from the internet scouring i&amp;#39;ve done so far, that doesn&amp;#39;t seem currently possible (although i would LOVE to find out i&amp;#39;m wrong about that lol). i would also be perfectly satisfied with finding the audio files online somewhere, but from all the searching i&amp;#39;ve done at this point, i really do think that unfortunately the artist is too obscure for anyone to have uploaded them. my only other thought on that idea was if there was some site i haven&amp;#39;t heard of that automatically has any song uploaded to streaming (even if/after they&amp;#39;re removed) but the only site i&amp;#39;m currently aware of that&amp;#39;s close to that is one that only has songs that are still actively available on streaming- so it does have this artist&amp;#39;s other music, but not the removed songs i&amp;#39;m looking for.&lt;/p&gt;\n\n&lt;p&gt;anyways, if you&amp;#39;ve read this whole massive post, thank you lol- and any ideas or suggestions for a solution to this would be immensely appreciated!!!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zp3oml", "is_robot_indexable": true, "report_reasons": null, "author": "k66613", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zp3oml/trying_to_obtainconvert_files_of_obscure_songs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zp3oml/trying_to_obtainconvert_files_of_obscure_songs/", "subreddit_subscribers": 660025, "created_utc": 1671384736.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Here is the laptop](https://www.amazon.com/gp/product/B07B7VFTN9/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1) if you're interested but the relevant part is the ports:\n\n* SATA 3 internal, looks to be the laptop 7mm form factor\n* 2 USB 3.1 gen1 ports, 5Gbits/s or \\~500mb/s theorical speed; 1 type A and 1 type C\n\nThe ports could also be usb 3.2 gen1, there is conflicting info on even the manufacturer site but it's \\~500mb/s. I am assuming SATA 3 due to the year, could not find specs on this.\n\nI'm flexible, I want at least 4tb storage but will get as much as 8tb if I see a deal. My current plan is to use an external HDD with one of the usb ports as my backup drive, and use the sata as the primary.\n\n**Options**\n\n1. Get [this HDD](https://www.amazon.com/Seagate-BarraCuda-Internal-2-5-Inch-ST5000LM000/dp/B07MWDMD4J/ref=sr_1_4?crid=2DKFXY9EAE6T0&amp;keywords=2.5in+sata3+HDD+5tb&amp;qid=1671416580&amp;s=electronics&amp;sprefix=2.5in+sata3+hdd+5tb%2Celectronics%2C61&amp;sr=1-4), the case has enough wiggle room that I could probably fit it, or leave the case off, or get a sata extension wire.\n2. Get a sata extension wire and feed it to a 3.5 external HDD. My main concern is if it will be compatible.\n3. Use both usb ports for two external HDDs, the SATA can't take more than 2tb\n\nI'm looking to keep thing relatively cheap and don't want to get a rack or PSU right now; this is my first data-hoarding project. Will either of my options be compatible without a PSU? Is this stupid and I should sell the laptop and use my RPi with a rack instead? Also open to suggestions for which sata HDD and external hdd I should get.", "author_fullname": "t2_hu3ysrzv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to use an old laptop for a nextcloud server and am wondering if I should use internal SATA for the storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpfmvv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671417499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.amazon.com/gp/product/B07B7VFTN9/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;amp;psc=1\"&gt;Here is the laptop&lt;/a&gt; if you&amp;#39;re interested but the relevant part is the ports:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SATA 3 internal, looks to be the laptop 7mm form factor&lt;/li&gt;\n&lt;li&gt;2 USB 3.1 gen1 ports, 5Gbits/s or ~500mb/s theorical speed; 1 type A and 1 type C&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The ports could also be usb 3.2 gen1, there is conflicting info on even the manufacturer site but it&amp;#39;s ~500mb/s. I am assuming SATA 3 due to the year, could not find specs on this.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m flexible, I want at least 4tb storage but will get as much as 8tb if I see a deal. My current plan is to use an external HDD with one of the usb ports as my backup drive, and use the sata as the primary.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Options&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Get &lt;a href=\"https://www.amazon.com/Seagate-BarraCuda-Internal-2-5-Inch-ST5000LM000/dp/B07MWDMD4J/ref=sr_1_4?crid=2DKFXY9EAE6T0&amp;amp;keywords=2.5in+sata3+HDD+5tb&amp;amp;qid=1671416580&amp;amp;s=electronics&amp;amp;sprefix=2.5in+sata3+hdd+5tb%2Celectronics%2C61&amp;amp;sr=1-4\"&gt;this HDD&lt;/a&gt;, the case has enough wiggle room that I could probably fit it, or leave the case off, or get a sata extension wire.&lt;/li&gt;\n&lt;li&gt;Get a sata extension wire and feed it to a 3.5 external HDD. My main concern is if it will be compatible.&lt;/li&gt;\n&lt;li&gt;Use both usb ports for two external HDDs, the SATA can&amp;#39;t take more than 2tb&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m looking to keep thing relatively cheap and don&amp;#39;t want to get a rack or PSU right now; this is my first data-hoarding project. Will either of my options be compatible without a PSU? Is this stupid and I should sell the laptop and use my RPi with a rack instead? Also open to suggestions for which sata HDD and external hdd I should get.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpfmvv", "is_robot_indexable": true, "report_reasons": null, "author": "PornAlt12323456", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpfmvv/looking_to_use_an_old_laptop_for_a_nextcloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpfmvv/looking_to_use_an_old_laptop_for_a_nextcloud/", "subreddit_subscribers": 660025, "created_utc": 1671417499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\n**Background:** I have 2 external HDDs (WD My Book) bought from BestBuy. They have both been formatted as MacOS Extended or HFS+. I own both a Windows laptop and a Macbook Pro; I use my Macbook Pro more, which is why I chose an Apple filesystem. I have historically been able to write to both of these disks using both laptops (I use Paragon HFS+ to write to the disk using my Windows laptop; I've done this for 2 years now with various other hard drives). Both hard drives have only been in use for a few months; one is constantly used for file scraping, the other is purely for backup use... the file scraping one only started having problems 2 days ago. The backup hard drive started having problems 2 months ago. (I thought it was only a one-off thing, but seems to be a recurring issue now). I have other hard drives formatted the same way and used the same way, with no issues.\n\n**Issue:** At some point, after accumulating around \\~2TB (or slightly less) on each of them, I get a notification saying the disk is full even if there's around 6TB of free space left. Turns out, the drives aren't really full... I'm just unable to write to the disk (copy files ***to*** the drive, rewrite/edit, save files etc). However, I can continue to do any read functions (copy files ***from*** the disk to my local hard drive, open files/watch videos). This issue occurs on both the Windows and Mac laptops, so I don't think it's because I'm using an HFS+ file system.\n\n**Other details:**\n\n1. On Windows, if I try to write to the disk, File Explorer just says the disk is full. EDIT: It also ejects easily on Windows (\"safely remove hardware\"), unlike DiskUtility or Finder on Mac (see point 5 below).\n2. On my Mac, if I try to write to the disk (e.g. copy or move a file to the disk), Finder will just say \"Preparing to copy filenamehere.ext\"... but it  just stays like that indefinitely. If I try to cancel the copy, it causes Finder to freeze up/become unnavigable and eventually (after 15-30 mins) the entire computer freezes up too. If I try to stop the process early by restarting the computer/shutting down the computer, it just gets stuck. There is no way around this, other than to do a hard reboot (pushing down on the power button until the computer restarts).\n3. There is usually a \"small window\" when I am able to write to the disk. Most of the time, when my Macbook has been newly booted/restarted, I have a 10-15 minute window where I am able to write to the disk. Sometimes, it doesn't work at all even if the computer has been newly restarted. Usually this occurs after a hard reboot (see point 2 above).\n4. I tried using CrystalDiskInfo (I've seen some people use it on this sub)... it gives a blue rating on both disks and says they are \"good\" (doesn't give a % health rating though).\n5. I tried DiskUtility to see if there are problems. Sometimes, it says there's there are orphan blocks (usually after a hard reboot in point 2), and it gets repaired. See screenshot below. However, the problem still occurs EVEN IF DiskUtility says the disk is completely ok. There are also times where it won't eject or unmount from a Macbook, usually after the 15 minute window mentioned in point 3 (so DiskUtility can't even work on it). When this occurs, the only way to eject the disk is to restart (and if that fails, do a hard reboot).\n\nhttps://preview.redd.it/s3zilr4gmp6a1.png?width=935&amp;format=png&amp;auto=webp&amp;s=180cad374e27b81a220f195b0cc856af4711cb72\n\nSo what is going on here? Ultimately, I'm asking for help with the following:\n\n* Are these disks dying? Should I just discard them?\n* If not, how do I find a way to consistently write to these disks?\n* How do I prevent this in the future?\n\nI should also add that I'm kind of a tech noob, so please ELI5 where you can. Thank you in advance.", "author_fullname": "t2_pvdlv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "8TB Disks Unable to Write But Both Can Read, CrystalDiskInfo says Health Status is \"Good\" (no %)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 97, "top_awarded_type": null, "hide_score": false, "media_metadata": {"s3zilr4gmp6a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 75, "x": 108, "u": "https://preview.redd.it/s3zilr4gmp6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45327f7c8075058a679fa090b155ae7831047d74"}, {"y": 151, "x": 216, "u": "https://preview.redd.it/s3zilr4gmp6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bcf7175f4331aaee423d6ce95e9f578aff4a3090"}, {"y": 223, "x": 320, "u": "https://preview.redd.it/s3zilr4gmp6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=72641060a69e1e3eb5e530ff1714aeb4b4dad482"}, {"y": 447, "x": 640, "u": "https://preview.redd.it/s3zilr4gmp6a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f68930c53699ef4ce03cb28bbddd98aed05ee6f"}], "s": {"y": 654, "x": 935, "u": "https://preview.redd.it/s3zilr4gmp6a1.png?width=935&amp;format=png&amp;auto=webp&amp;s=180cad374e27b81a220f195b0cc856af4711cb72"}, "id": "s3zilr4gmp6a1"}}, "name": "t3_zp6mi6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/mqhjzHLzgASIw6AI-lO9yRXS2WBSlU0H9fufLA05Ar8.jpg", "edited": 1671394617.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671392937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; I have 2 external HDDs (WD My Book) bought from BestBuy. They have both been formatted as MacOS Extended or HFS+. I own both a Windows laptop and a Macbook Pro; I use my Macbook Pro more, which is why I chose an Apple filesystem. I have historically been able to write to both of these disks using both laptops (I use Paragon HFS+ to write to the disk using my Windows laptop; I&amp;#39;ve done this for 2 years now with various other hard drives). Both hard drives have only been in use for a few months; one is constantly used for file scraping, the other is purely for backup use... the file scraping one only started having problems 2 days ago. The backup hard drive started having problems 2 months ago. (I thought it was only a one-off thing, but seems to be a recurring issue now). I have other hard drives formatted the same way and used the same way, with no issues.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Issue:&lt;/strong&gt; At some point, after accumulating around ~2TB (or slightly less) on each of them, I get a notification saying the disk is full even if there&amp;#39;s around 6TB of free space left. Turns out, the drives aren&amp;#39;t really full... I&amp;#39;m just unable to write to the disk (copy files &lt;strong&gt;&lt;em&gt;to&lt;/em&gt;&lt;/strong&gt; the drive, rewrite/edit, save files etc). However, I can continue to do any read functions (copy files &lt;strong&gt;&lt;em&gt;from&lt;/em&gt;&lt;/strong&gt; the disk to my local hard drive, open files/watch videos). This issue occurs on both the Windows and Mac laptops, so I don&amp;#39;t think it&amp;#39;s because I&amp;#39;m using an HFS+ file system.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Other details:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;On Windows, if I try to write to the disk, File Explorer just says the disk is full. EDIT: It also ejects easily on Windows (&amp;quot;safely remove hardware&amp;quot;), unlike DiskUtility or Finder on Mac (see point 5 below).&lt;/li&gt;\n&lt;li&gt;On my Mac, if I try to write to the disk (e.g. copy or move a file to the disk), Finder will just say &amp;quot;Preparing to copy filenamehere.ext&amp;quot;... but it  just stays like that indefinitely. If I try to cancel the copy, it causes Finder to freeze up/become unnavigable and eventually (after 15-30 mins) the entire computer freezes up too. If I try to stop the process early by restarting the computer/shutting down the computer, it just gets stuck. There is no way around this, other than to do a hard reboot (pushing down on the power button until the computer restarts).&lt;/li&gt;\n&lt;li&gt;There is usually a &amp;quot;small window&amp;quot; when I am able to write to the disk. Most of the time, when my Macbook has been newly booted/restarted, I have a 10-15 minute window where I am able to write to the disk. Sometimes, it doesn&amp;#39;t work at all even if the computer has been newly restarted. Usually this occurs after a hard reboot (see point 2 above).&lt;/li&gt;\n&lt;li&gt;I tried using CrystalDiskInfo (I&amp;#39;ve seen some people use it on this sub)... it gives a blue rating on both disks and says they are &amp;quot;good&amp;quot; (doesn&amp;#39;t give a % health rating though).&lt;/li&gt;\n&lt;li&gt;I tried DiskUtility to see if there are problems. Sometimes, it says there&amp;#39;s there are orphan blocks (usually after a hard reboot in point 2), and it gets repaired. See screenshot below. However, the problem still occurs EVEN IF DiskUtility says the disk is completely ok. There are also times where it won&amp;#39;t eject or unmount from a Macbook, usually after the 15 minute window mentioned in point 3 (so DiskUtility can&amp;#39;t even work on it). When this occurs, the only way to eject the disk is to restart (and if that fails, do a hard reboot).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s3zilr4gmp6a1.png?width=935&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=180cad374e27b81a220f195b0cc856af4711cb72\"&gt;https://preview.redd.it/s3zilr4gmp6a1.png?width=935&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=180cad374e27b81a220f195b0cc856af4711cb72&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So what is going on here? Ultimately, I&amp;#39;m asking for help with the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Are these disks dying? Should I just discard them?&lt;/li&gt;\n&lt;li&gt;If not, how do I find a way to consistently write to these disks?&lt;/li&gt;\n&lt;li&gt;How do I prevent this in the future?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I should also add that I&amp;#39;m kind of a tech noob, so please ELI5 where you can. Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zp6mi6", "is_robot_indexable": true, "report_reasons": null, "author": "throwawayawerty", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zp6mi6/8tb_disks_unable_to_write_but_both_can_read/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zp6mi6/8tb_disks_unable_to_write_but_both_can_read/", "subreddit_subscribers": 660025, "created_utc": 1671392937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title.", "author_fullname": "t2_6lxl9l3q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to transfer files from an external SSD to Google Drive without first having to download them on your PC/smartphone?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zp4gac", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671387064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zp4gac", "is_robot_indexable": true, "report_reasons": null, "author": "-Sh33ph3rd3r-", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zp4gac/is_it_possible_to_transfer_files_from_an_external/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zp4gac/is_it_possible_to_transfer_files_from_an_external/", "subreddit_subscribers": 660025, "created_utc": 1671387064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Edit - more info]\n\nMorning fellas, newbie here\n\nI would like to create a local home cloud with at least 4 TB for storing downloaded TV series, game installers and personal and family photos. Everything should be accessible through home WiFi.\n\nRight now I have an external portable 1TB HDD connected to my home router serving as SMB storage which I can access as a shared driver on my local network \ud83d\ude01 It ain't much but it's honest work...\n\nFor the long run I know it isn't great and it's prone to failure, losing all old photos.\n\n# NAS\n\nFrom a local store I can get a \n\n* \u20ac100 Seagate IronWolf Pro 4TB 7200RPM 128MB SATA III (on sale) - Single one for now\n* \u20ac170 NAS Synology DiskStation DS220j **2 bays**\n\n**Is this a great solution for my needs, or is it too overkill? Thanks!**", "author_fullname": "t2_ocu1qmth", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice. Local home cloud (for Reading mostly)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zoxj0o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671367262.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671366472.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[Edit - more info]&lt;/p&gt;\n\n&lt;p&gt;Morning fellas, newbie here&lt;/p&gt;\n\n&lt;p&gt;I would like to create a local home cloud with at least 4 TB for storing downloaded TV series, game installers and personal and family photos. Everything should be accessible through home WiFi.&lt;/p&gt;\n\n&lt;p&gt;Right now I have an external portable 1TB HDD connected to my home router serving as SMB storage which I can access as a shared driver on my local network \ud83d\ude01 It ain&amp;#39;t much but it&amp;#39;s honest work...&lt;/p&gt;\n\n&lt;p&gt;For the long run I know it isn&amp;#39;t great and it&amp;#39;s prone to failure, losing all old photos.&lt;/p&gt;\n\n&lt;h1&gt;NAS&lt;/h1&gt;\n\n&lt;p&gt;From a local store I can get a &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;\u20ac100 Seagate IronWolf Pro 4TB 7200RPM 128MB SATA III (on sale) - Single one for now&lt;/li&gt;\n&lt;li&gt;\u20ac170 NAS Synology DiskStation DS220j &lt;strong&gt;2 bays&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Is this a great solution for my needs, or is it too overkill? Thanks!&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zoxj0o", "is_robot_indexable": true, "report_reasons": null, "author": "pshawSounds", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zoxj0o/need_advice_local_home_cloud_for_reading_mostly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zoxj0o/need_advice_local_home_cloud_for_reading_mostly/", "subreddit_subscribers": 660025, "created_utc": 1671366472.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don\u2019t know if this is the right place to ask, but I have lurked for a while and have become confident that this community is knowledgeable enough to call out a potentially unoptimized setup given the details.\n\nSo I will do just that, describe my setup and then attempt to explain and fact-check my assumption of the issue/request advice.\n\nHopefully in thoroughly detailing everything, any questions someone might have should already be answered and suggestions will be well founded, but I am always open to answer questions that will clarify.\n\nJust to get this out of the way, everything is connected to Ethernet and I have nearly a gigabit send/receive which is more than enough for the amount of users I have. My point being that I am not here to waste anyones time to complain about a problem as apparent as buffering due to throttling my networks upload bandwidth.\n\nI have a Synology FS1018 that I had originally intended to run Plex on. It would have probably worked fine, but not all the content in my library can direct play, and I didn\u2019t feel like going out of my way to collect specific versions of content just for Plex.\n\nSo after about a year of getting frustrated with the Synology\u2019s processing power and inability to handle multiple streams that required re-encodes, I decided to build a PC capable of transcoding. \n\nIt\u2019s running windows 10 and has a ryzen 3900x and nvidia quadro p2000. I can admit that it is probably overkill and plenty powerful to handle the amount of users I will ever have watching at one time.\n\nIf I am correct about the problem which I am still getting to, the following will be a major indication that I messed up to anyone with more experience. \n\nI created a mount of the Synology Plex share on this transcoding machine in file explorer and specified the directories accordingly in Plex as the libraries. So now the only difference is Plex is running off of this windows machine instead and is accessing the content through a network share.\n\nI never moved the data off of the Synology for multiple reasons.\n\nFirstly, in my limited and potentially naive understanding, I wasn\u2019t aware of any reason I would need to. Especially when I considered that both systems are connected through gigabit Ethernet directly into the router.\n\nThe more legitimate reason being that I couldn\u2019t even if I wanted; the Synology is running a RAID 5 array, so it wouldn\u2019t be as easy as pulling the drives out to move them, and I didn\u2019t intend on buying more storage specifically for the machine I wanted to designate to transcoding. \n\nSo to finally demonstrate the problem, imagine there is a user streaming a 4K movie as well as another user that is watching a 1080p movie, both requiring a transcode. The Quadro is capable of handling both of these just fine, and according to others who have tested the p2000, it is capable of handling multiple 4k transcodes. So in this hypothetical, I check the task manager to see why the server is having issues with playback, and the bandwidth column is maxed out fluctuating between high 90s and 100%. The server still seems to be able to handle a handful of lower resolution streams just fine.\n\nSo with all that explained, the problem I am now facing doesn\u2019t seem to be related to the performance capabilities of the transcoding machine. It seems to be a problem regarding access-rate throttling between the windows machine and Synology, especially since it is transcoding the files to the windows machine and then sending them. I would just like to be sure this is the issue and if it is, what is my course of action from here.\n\nI am currently writing this in a hotel room and am going to be on vacation until Christmas, otherwise I would provide a screenshot of the task manager when it\u2019s being \u201cthrottled\u201d or whatever is happening. \n\nIf you have read everything this far, thank you. I don\u2019t even know if I could expect myself to read all this, but I really want to know what the best move is in my situation.", "author_fullname": "t2_gbr854b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I think I\u2019m getting bottlenecked by access-rate within my network.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zovr52", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671359791.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don\u2019t know if this is the right place to ask, but I have lurked for a while and have become confident that this community is knowledgeable enough to call out a potentially unoptimized setup given the details.&lt;/p&gt;\n\n&lt;p&gt;So I will do just that, describe my setup and then attempt to explain and fact-check my assumption of the issue/request advice.&lt;/p&gt;\n\n&lt;p&gt;Hopefully in thoroughly detailing everything, any questions someone might have should already be answered and suggestions will be well founded, but I am always open to answer questions that will clarify.&lt;/p&gt;\n\n&lt;p&gt;Just to get this out of the way, everything is connected to Ethernet and I have nearly a gigabit send/receive which is more than enough for the amount of users I have. My point being that I am not here to waste anyones time to complain about a problem as apparent as buffering due to throttling my networks upload bandwidth.&lt;/p&gt;\n\n&lt;p&gt;I have a Synology FS1018 that I had originally intended to run Plex on. It would have probably worked fine, but not all the content in my library can direct play, and I didn\u2019t feel like going out of my way to collect specific versions of content just for Plex.&lt;/p&gt;\n\n&lt;p&gt;So after about a year of getting frustrated with the Synology\u2019s processing power and inability to handle multiple streams that required re-encodes, I decided to build a PC capable of transcoding. &lt;/p&gt;\n\n&lt;p&gt;It\u2019s running windows 10 and has a ryzen 3900x and nvidia quadro p2000. I can admit that it is probably overkill and plenty powerful to handle the amount of users I will ever have watching at one time.&lt;/p&gt;\n\n&lt;p&gt;If I am correct about the problem which I am still getting to, the following will be a major indication that I messed up to anyone with more experience. &lt;/p&gt;\n\n&lt;p&gt;I created a mount of the Synology Plex share on this transcoding machine in file explorer and specified the directories accordingly in Plex as the libraries. So now the only difference is Plex is running off of this windows machine instead and is accessing the content through a network share.&lt;/p&gt;\n\n&lt;p&gt;I never moved the data off of the Synology for multiple reasons.&lt;/p&gt;\n\n&lt;p&gt;Firstly, in my limited and potentially naive understanding, I wasn\u2019t aware of any reason I would need to. Especially when I considered that both systems are connected through gigabit Ethernet directly into the router.&lt;/p&gt;\n\n&lt;p&gt;The more legitimate reason being that I couldn\u2019t even if I wanted; the Synology is running a RAID 5 array, so it wouldn\u2019t be as easy as pulling the drives out to move them, and I didn\u2019t intend on buying more storage specifically for the machine I wanted to designate to transcoding. &lt;/p&gt;\n\n&lt;p&gt;So to finally demonstrate the problem, imagine there is a user streaming a 4K movie as well as another user that is watching a 1080p movie, both requiring a transcode. The Quadro is capable of handling both of these just fine, and according to others who have tested the p2000, it is capable of handling multiple 4k transcodes. So in this hypothetical, I check the task manager to see why the server is having issues with playback, and the bandwidth column is maxed out fluctuating between high 90s and 100%. The server still seems to be able to handle a handful of lower resolution streams just fine.&lt;/p&gt;\n\n&lt;p&gt;So with all that explained, the problem I am now facing doesn\u2019t seem to be related to the performance capabilities of the transcoding machine. It seems to be a problem regarding access-rate throttling between the windows machine and Synology, especially since it is transcoding the files to the windows machine and then sending them. I would just like to be sure this is the issue and if it is, what is my course of action from here.&lt;/p&gt;\n\n&lt;p&gt;I am currently writing this in a hotel room and am going to be on vacation until Christmas, otherwise I would provide a screenshot of the task manager when it\u2019s being \u201cthrottled\u201d or whatever is happening. &lt;/p&gt;\n\n&lt;p&gt;If you have read everything this far, thank you. I don\u2019t even know if I could expect myself to read all this, but I really want to know what the best move is in my situation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zovr52", "is_robot_indexable": true, "report_reasons": null, "author": "N72826", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zovr52/i_think_im_getting_bottlenecked_by_accessrate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zovr52/i_think_im_getting_bottlenecked_by_accessrate/", "subreddit_subscribers": 660025, "created_utc": 1671359791.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey nerds, figured this would be the best place to ask, I'm wanting to get into digital photography and am going to be getting 2 drives (one 3.5 TB WD Black for general stuff and a 1TB Black for a backup of important pictures). I know you aren't supposed to move drives while they're on for multiple reasons but can I set my drive tall ways then plug it in to dump all my photos? I dont see why I couldn't but I've also never really seen it anywhere. Thanks!", "author_fullname": "t2_7ctrlfu3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vertical Running Drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zphsp5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671423882.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey nerds, figured this would be the best place to ask, I&amp;#39;m wanting to get into digital photography and am going to be getting 2 drives (one 3.5 TB WD Black for general stuff and a 1TB Black for a backup of important pictures). I know you aren&amp;#39;t supposed to move drives while they&amp;#39;re on for multiple reasons but can I set my drive tall ways then plug it in to dump all my photos? I dont see why I couldn&amp;#39;t but I&amp;#39;ve also never really seen it anywhere. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zphsp5", "is_robot_indexable": true, "report_reasons": null, "author": "Ill_Requirement_6839", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zphsp5/vertical_running_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zphsp5/vertical_running_drives/", "subreddit_subscribers": 660025, "created_utc": 1671423882.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Will be building my first server rack using spare parts and new parts with about 10TB new storage in addition to my pc\u2019s original 5TB.\nWill be using a spare BM450 motherboard, any recommendations for the CPU?", "author_fullname": "t2_4dvwe7y0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "This Christmas I join you 15TB strong!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpafii", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671402906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Will be building my first server rack using spare parts and new parts with about 10TB new storage in addition to my pc\u2019s original 5TB.\nWill be using a spare BM450 motherboard, any recommendations for the CPU?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpafii", "is_robot_indexable": true, "report_reasons": null, "author": "Op2-0", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpafii/this_christmas_i_join_you_15tb_strong/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpafii/this_christmas_i_join_you_15tb_strong/", "subreddit_subscribers": 660025, "created_utc": 1671402906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm working on trying to solidify my backup strategy. I do think it's fairly solid, but there are a couple things I know that I can do better. \n\nCurrent strategy:\nCentralized storage via Synology NAS (1621+ w/ 32GB ECC RAM) with btrfs.\nNAS snapshots hourly and maintains snapshot through typical pruning measures. \nNAS backs up to connected Seagate Backup 8TB drive and Backblaze B2 via Hyper Backup.\nNUC11 running Windows with read-only user connects to shares and backs up to connected external hard drive via Arq 7.  Have rotational cold-storage via Arq as well.  \n\nA couple things I'd like to fix: \n\n* NUC11 running Windows (will likely convert to Hypervisor of some sort, even if just HyperV), was part of a project.\n* Potentially move the external drive hanging off of the NUC to an internal 2.5 HDD.\n* Add a backup tool that is open source, even if it's just a cold backup. \n* Potentially remove NUC11 in general, as I'm running less on it and have a beefy desktop, and far fewer full-time running services than I did.  Would move the service or two to run off of the NAS. Though, this would mean trying to incorporate my desktop into initiating a backup process of some sort, potentially manually, which makes it less desirable.\n\n\nThe amount of data that I would consider critical is very low, somewhere in between 200-300GB\n\nAny suggestions on how I could make this more solid?", "author_fullname": "t2_4ff7pn53", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rate/Help My Backup Strategy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zp66w9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671392880.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671391783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on trying to solidify my backup strategy. I do think it&amp;#39;s fairly solid, but there are a couple things I know that I can do better. &lt;/p&gt;\n\n&lt;p&gt;Current strategy:\nCentralized storage via Synology NAS (1621+ w/ 32GB ECC RAM) with btrfs.\nNAS snapshots hourly and maintains snapshot through typical pruning measures. \nNAS backs up to connected Seagate Backup 8TB drive and Backblaze B2 via Hyper Backup.\nNUC11 running Windows with read-only user connects to shares and backs up to connected external hard drive via Arq 7.  Have rotational cold-storage via Arq as well.  &lt;/p&gt;\n\n&lt;p&gt;A couple things I&amp;#39;d like to fix: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;NUC11 running Windows (will likely convert to Hypervisor of some sort, even if just HyperV), was part of a project.&lt;/li&gt;\n&lt;li&gt;Potentially move the external drive hanging off of the NUC to an internal 2.5 HDD.&lt;/li&gt;\n&lt;li&gt;Add a backup tool that is open source, even if it&amp;#39;s just a cold backup. &lt;/li&gt;\n&lt;li&gt;Potentially remove NUC11 in general, as I&amp;#39;m running less on it and have a beefy desktop, and far fewer full-time running services than I did.  Would move the service or two to run off of the NAS. Though, this would mean trying to incorporate my desktop into initiating a backup process of some sort, potentially manually, which makes it less desirable.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The amount of data that I would consider critical is very low, somewhere in between 200-300GB&lt;/p&gt;\n\n&lt;p&gt;Any suggestions on how I could make this more solid?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zp66w9", "is_robot_indexable": true, "report_reasons": null, "author": "StrongCommission", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zp66w9/ratehelp_my_backup_strategy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zp66w9/ratehelp_my_backup_strategy/", "subreddit_subscribers": 660025, "created_utc": 1671391783.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}