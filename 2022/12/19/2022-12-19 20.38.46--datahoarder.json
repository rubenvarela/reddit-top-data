{"kind": "Listing", "data": {"after": "t3_zpm4ym", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I make this post to get an update of current state of the storage technology and also seek to find answer for wheather i should make backups to HDDs vs SSD.\n\nCurrent Situation:-\nI have around 500 gb of Family photos from 2001 on a Seagate external HDD, it lasted for 7 years and data is well and good right now.\n\nI already have backups on 2 different machines and the external HDD. It's now time again to migrate my external HDD to new Hardware and I am conflicted on what should I choose moving further.\n\nUntil now my photos have been jumping CDs to HDD and I am at a crossroads again weather to switch from HDD to SSD or HDD are still better for cold storage long term.\n\nI did fair bit of research and I am aware Optical Media would be my best bet, namely M Disk or BD disks. Unfortunately where I live I cannot source them reliably and affordably enough.\n\nI browsed reddit threads from past few years.\nLike [this](https://www.reddit.com/r/DataHoarder/comments/jiwqqy/are_ssds_more_reliable_than_hard_drives) from 2 years ago which says SSDs are better.\n\nI have consistently found a narrative that newer SSDs are better alternative than HDDs.\n\nMy primary concern is not number of read writes in SSDs. Often they are in 100s of TBW which I presume I won't hit because of the nature of my storage needs.\n\nI fear data corruption and chip failure rather than running out of read writes.\n\nThe disk I chose weather SSD or an HDD will probably be left on shelf with about twice a year plugging into PC to add new photos.\n\n\nWhat do you guys think would be a good choice ?\n\nShould I keep moving forward with a new HDD or are SSD a smarter choice?\n\nWhatever I choose I would probably rely on for at least next 4-5 years, with backups of course.", "author_fullname": "t2_6meaucy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Long term storage: SSDs vs HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpt6es", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671460405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I make this post to get an update of current state of the storage technology and also seek to find answer for wheather i should make backups to HDDs vs SSD.&lt;/p&gt;\n\n&lt;p&gt;Current Situation:-\nI have around 500 gb of Family photos from 2001 on a Seagate external HDD, it lasted for 7 years and data is well and good right now.&lt;/p&gt;\n\n&lt;p&gt;I already have backups on 2 different machines and the external HDD. It&amp;#39;s now time again to migrate my external HDD to new Hardware and I am conflicted on what should I choose moving further.&lt;/p&gt;\n\n&lt;p&gt;Until now my photos have been jumping CDs to HDD and I am at a crossroads again weather to switch from HDD to SSD or HDD are still better for cold storage long term.&lt;/p&gt;\n\n&lt;p&gt;I did fair bit of research and I am aware Optical Media would be my best bet, namely M Disk or BD disks. Unfortunately where I live I cannot source them reliably and affordably enough.&lt;/p&gt;\n\n&lt;p&gt;I browsed reddit threads from past few years.\nLike &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/jiwqqy/are_ssds_more_reliable_than_hard_drives\"&gt;this&lt;/a&gt; from 2 years ago which says SSDs are better.&lt;/p&gt;\n\n&lt;p&gt;I have consistently found a narrative that newer SSDs are better alternative than HDDs.&lt;/p&gt;\n\n&lt;p&gt;My primary concern is not number of read writes in SSDs. Often they are in 100s of TBW which I presume I won&amp;#39;t hit because of the nature of my storage needs.&lt;/p&gt;\n\n&lt;p&gt;I fear data corruption and chip failure rather than running out of read writes.&lt;/p&gt;\n\n&lt;p&gt;The disk I chose weather SSD or an HDD will probably be left on shelf with about twice a year plugging into PC to add new photos.&lt;/p&gt;\n\n&lt;p&gt;What do you guys think would be a good choice ?&lt;/p&gt;\n\n&lt;p&gt;Should I keep moving forward with a new HDD or are SSD a smarter choice?&lt;/p&gt;\n\n&lt;p&gt;Whatever I choose I would probably rely on for at least next 4-5 years, with backups of course.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zpt6es", "is_robot_indexable": true, "report_reasons": null, "author": "alsu2launda", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpt6es/long_term_storage_ssds_vs_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpt6es/long_term_storage_ssds_vs_hdd/", "subreddit_subscribers": 660059, "created_utc": 1671460405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "All AWS storage products charge outrageous egress fees, to the extend that backing up to AWS doesn't seem feasible for me. It's like Hotel California: upload is free, storage cost is okay, but you want your data back? Payback!\n\nFor example: s3 glacier us-east-1: $0.0036\u00a0per GB + $0.09 per GB egress (for first 10tb).\n\nhttps://aws.amazon.com/s3/pricing/\n\nSo if I backup 10 tb to aws and download it back at end of 1 year, it'll cost (0.0036 * 1024 * 10 * 12)+(0.09 * 1024 * 10)=442+921, i.e. 67% of my expense goes to egress. \n\nI'm like...no f way. Until I saw something else they offered: workdoc! Apparently there's no charge on egress. I reread it trice but yeah. $5 per user with 1tb included. You can allocate more storage per user but that'll cost more than creating a second user, so just create 10 users. That works out to 5 * 10 * 12=$600, less than half of glacier, and you don't have to wait 12 hours. Oh and there's a web gui.\n\nhttps://aws.amazon.com/workdocs/pricing/\n\nAre they doing this to compete with gdrive? I cannot imagine they omitted egress charges by mistake.", "author_fullname": "t2_ratqygnj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS storage without egress charges...an intended loophole?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpg3d8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671419780.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671418809.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;All AWS storage products charge outrageous egress fees, to the extend that backing up to AWS doesn&amp;#39;t seem feasible for me. It&amp;#39;s like Hotel California: upload is free, storage cost is okay, but you want your data back? Payback!&lt;/p&gt;\n\n&lt;p&gt;For example: s3 glacier us-east-1: $0.0036\u00a0per GB + $0.09 per GB egress (for first 10tb).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aws.amazon.com/s3/pricing/\"&gt;https://aws.amazon.com/s3/pricing/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So if I backup 10 tb to aws and download it back at end of 1 year, it&amp;#39;ll cost (0.0036 * 1024 * 10 * 12)+(0.09 * 1024 * 10)=442+921, i.e. 67% of my expense goes to egress. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m like...no f way. Until I saw something else they offered: workdoc! Apparently there&amp;#39;s no charge on egress. I reread it trice but yeah. $5 per user with 1tb included. You can allocate more storage per user but that&amp;#39;ll cost more than creating a second user, so just create 10 users. That works out to 5 * 10 * 12=$600, less than half of glacier, and you don&amp;#39;t have to wait 12 hours. Oh and there&amp;#39;s a web gui.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aws.amazon.com/workdocs/pricing/\"&gt;https://aws.amazon.com/workdocs/pricing/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Are they doing this to compete with gdrive? I cannot imagine they omitted egress charges by mistake.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?auto=webp&amp;s=8e3eb77ba905bb641af80fcf3efe1de0190ac8c2", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b015da4f990706696f7d06ac19bc75b807d90200", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44756ae9c6e1724356ccaef8214086d7d0cc95da", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a5696e6599c7d56b3770650b416341ba2102fd8", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4610f9bb7893259c61ba4fda892295f0da1a05ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9979842391099359ecaa7d0ce4c8c31f1e3bead7", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/hP3JsQBbdiWXwCXIY7kxszHzO6LbWjz8ZxP8CTk9bJs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fe7c4eb58196f897578137b50f669a4707c9902", "width": 1080, "height": 567}], "variants": {}, "id": "RUqh18uQTwuGJocqdUcC-6UfvfWS63SRDdr8AQqU3uM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpg3d8", "is_robot_indexable": true, "report_reasons": null, "author": "lmux", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpg3d8/aws_storage_without_egress_chargesan_intended/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpg3d8/aws_storage_without_egress_chargesan_intended/", "subreddit_subscribers": 660059, "created_utc": 1671418809.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i am quite out of the loop with computer technology, with reliable brands and etc.\nmy computer is from 2011, and will only be ugraded in 2025 when windows 10 dies.\nbut i do need hard drives as i am running out of space.\n\ni tried searching but could not find a simple and realistic comparrison showing that NAS disk are really better for desktop storage.\n======================================\nit's mostly for flac and wave music, png, svg and jpg images  and mkv, avi and mp4 movies\ngoing to migrate to (03) 4tb hard disks  , so actually i need to buy 6 for backup reasons,  \n\ni keep hearing i should buy the seagate ironwolf instead of barracuda, and begs the quesiton if its not overkill?  with 6 drives i would spend  u$ 120 more with ironwolf.\nfrom what i read NAS drives are made to run hot and not get affected from vibrations. isn't that because nas drives are all cramped together in a tiny box?\n\nso in a big server case both vibrations and temperature will not be a concern right?\n\nmy computer runs rather cold, hard disks usually around 30~35 celsius, (a big old server case from the 2000's) \n\n\nthanks!\n\n\nps: not raid, just manual backup using software that does incremental copy", "author_fullname": "t2_hs76g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "are nas drives not overkill for desktop?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpykxu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671472739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i am quite out of the loop with computer technology, with reliable brands and etc.\nmy computer is from 2011, and will only be ugraded in 2025 when windows 10 dies.\nbut i do need hard drives as i am running out of space.&lt;/p&gt;\n\n&lt;h1&gt;i tried searching but could not find a simple and realistic comparrison showing that NAS disk are really better for desktop storage.&lt;/h1&gt;\n\n&lt;p&gt;it&amp;#39;s mostly for flac and wave music, png, svg and jpg images  and mkv, avi and mp4 movies\ngoing to migrate to (03) 4tb hard disks  , so actually i need to buy 6 for backup reasons,  &lt;/p&gt;\n\n&lt;p&gt;i keep hearing i should buy the seagate ironwolf instead of barracuda, and begs the quesiton if its not overkill?  with 6 drives i would spend  u$ 120 more with ironwolf.\nfrom what i read NAS drives are made to run hot and not get affected from vibrations. isn&amp;#39;t that because nas drives are all cramped together in a tiny box?&lt;/p&gt;\n\n&lt;p&gt;so in a big server case both vibrations and temperature will not be a concern right?&lt;/p&gt;\n\n&lt;p&gt;my computer runs rather cold, hard disks usually around 30~35 celsius, (a big old server case from the 2000&amp;#39;s) &lt;/p&gt;\n\n&lt;p&gt;thanks!&lt;/p&gt;\n\n&lt;p&gt;ps: not raid, just manual backup using software that does incremental copy&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpykxu", "is_robot_indexable": true, "report_reasons": null, "author": "vanderzee", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpykxu/are_nas_drives_not_overkill_for_desktop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpykxu/are_nas_drives_not_overkill_for_desktop/", "subreddit_subscribers": 660059, "created_utc": 1671472739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using gallery-dl to download media and metadata for a list of people I follow. However, despite using an archive file, it still iterates through their entire tweet history. It does skip downloading old tweets, but I'd rather it just get everything new and then move onto the next URL once it encounters a tweet it already has. That would save me a TON of time (and rate limits).\n\nAll the documentation and suggestions I've found thus far involve skipping downloads, which it's already doing.\n\nAlso, since moving to a new computer and setting things up again, files it already downloaded are displaying as `./gallery-dl/twitter/[user]/?` in the terminal output instead of showing the filenames. Not sure why that is happening.\n\nConfig is below. Any suggestions?\n\n\t{\n\t\t\"extractor\": {\n\t\t\t\"twitter\": {\n\t\t\t\t\"cookies\": {\n\t\t\t\t\t\"auth_token\": \"[redacted]\"\n\t\t\t\t},\n\t\t\t\t\"archive\": \"~/twitter/archive-twitter.sqlite3\",\n\t\t\t\t\"postprocessors\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"name\": \"metadata\",\n\t\t\t\t\t\t\"event\": \"post\",\n\t\t\t\t\t\t\"filename\": \"{tweet_id}.json\"\n\t\t\t\t\t}\n\t\t\t\t],\n\t\t\t\t\"expand\": false,\n\t\t\t\t\"cards\": false,\n\t\t\t\t\"quoted\": false,\n\t\t\t\t\"retweets\": false,\n\t\t\t\t\"text-tweets\": false,\n\t\t\t\t\"unique\": true,\n\t\t\t\t\"videos\": true,\n\t\t\t\t\"timeline\": {\n\t\t\t\t\t\"strategy\": \"media\"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\nI run with the following command: `gallery-dl -c ./gallery-dl.no-text.conf -i urls.txt`", "author_fullname": "t2_4m88o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[gallery-dl / Twitter] Is there a way to skip all remaining tweets for a user once everything new has been collected?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpwmt5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671468428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using gallery-dl to download media and metadata for a list of people I follow. However, despite using an archive file, it still iterates through their entire tweet history. It does skip downloading old tweets, but I&amp;#39;d rather it just get everything new and then move onto the next URL once it encounters a tweet it already has. That would save me a TON of time (and rate limits).&lt;/p&gt;\n\n&lt;p&gt;All the documentation and suggestions I&amp;#39;ve found thus far involve skipping downloads, which it&amp;#39;s already doing.&lt;/p&gt;\n\n&lt;p&gt;Also, since moving to a new computer and setting things up again, files it already downloaded are displaying as &lt;code&gt;./gallery-dl/twitter/[user]/?&lt;/code&gt; in the terminal output instead of showing the filenames. Not sure why that is happening.&lt;/p&gt;\n\n&lt;p&gt;Config is below. Any suggestions?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n    &amp;quot;extractor&amp;quot;: {\n        &amp;quot;twitter&amp;quot;: {\n            &amp;quot;cookies&amp;quot;: {\n                &amp;quot;auth_token&amp;quot;: &amp;quot;[redacted]&amp;quot;\n            },\n            &amp;quot;archive&amp;quot;: &amp;quot;~/twitter/archive-twitter.sqlite3&amp;quot;,\n            &amp;quot;postprocessors&amp;quot;: [\n                {\n                    &amp;quot;name&amp;quot;: &amp;quot;metadata&amp;quot;,\n                    &amp;quot;event&amp;quot;: &amp;quot;post&amp;quot;,\n                    &amp;quot;filename&amp;quot;: &amp;quot;{tweet_id}.json&amp;quot;\n                }\n            ],\n            &amp;quot;expand&amp;quot;: false,\n            &amp;quot;cards&amp;quot;: false,\n            &amp;quot;quoted&amp;quot;: false,\n            &amp;quot;retweets&amp;quot;: false,\n            &amp;quot;text-tweets&amp;quot;: false,\n            &amp;quot;unique&amp;quot;: true,\n            &amp;quot;videos&amp;quot;: true,\n            &amp;quot;timeline&amp;quot;: {\n                &amp;quot;strategy&amp;quot;: &amp;quot;media&amp;quot;\n            }\n        }\n    }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I run with the following command: &lt;code&gt;gallery-dl -c ./gallery-dl.no-text.conf -i urls.txt&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpwmt5", "is_robot_indexable": true, "report_reasons": null, "author": "turaiel", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpwmt5/gallerydl_twitter_is_there_a_way_to_skip_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpwmt5/gallerydl_twitter_is_there_a_way_to_skip_all/", "subreddit_subscribers": 660059, "created_utc": 1671468428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Hello internet,\n\nI  commonly use sites like keep2share to download files and have been  using mullvad to bypass data limitations. Some videos are broken into multiple parts and I often pull my phone out to download simultaneously.\n\nI  was wondering if there was a more efficient way to do this from one  device and if I could do something like linking different browser  windows to different IP addresses. I am constantly downloading files and  am perfectly fine with the time investment it may take to set something  like this up.  \nI thought of Jdownloader but I think that uses proxies versus a VPN correct?  \n\n\nSorry for the newbie questions", "author_fullname": "t2_uxz2rq9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using VPN to bypass file hoster limits", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpancs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671403485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello internet,&lt;/p&gt;\n\n&lt;p&gt;I  commonly use sites like keep2share to download files and have been  using mullvad to bypass data limitations. Some videos are broken into multiple parts and I often pull my phone out to download simultaneously.&lt;/p&gt;\n\n&lt;p&gt;I  was wondering if there was a more efficient way to do this from one  device and if I could do something like linking different browser  windows to different IP addresses. I am constantly downloading files and  am perfectly fine with the time investment it may take to set something  like this up.&lt;br/&gt;\nI thought of Jdownloader but I think that uses proxies versus a VPN correct?  &lt;/p&gt;\n\n&lt;p&gt;Sorry for the newbie questions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpancs", "is_robot_indexable": true, "report_reasons": null, "author": "ForeignEfficiency401", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpancs/using_vpn_to_bypass_file_hoster_limits/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpancs/using_vpn_to_bypass_file_hoster_limits/", "subreddit_subscribers": 660059, "created_utc": 1671403485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just wasted a day messing about with UNRAID to realize it's limited to 30 drives. :-/\n\nI have a sizable pile of 5TB, 4TB, 3TB and 2TB drives.  I live somewhere where power is cheap and cooling is fresh air for 9 out of 12 months.  I have JBODs and \"IT\" flashed controllers to run more than I have.  All told, it's around .5 PB... someday, I'd like to break the PB barrier, but today is not that day.\n\nCan someone recommend a single software platform to support various disk sizes, reasonable (N+2) resillency and easy growth/failure replacement?\n\nUNRAID, too few disks.  TrueNAS, would need a seperate pool for each disk size, replacement blows.  I know next to nothing about OpenMediaVault but am going to fire that up here soon to poke about.  I see people complaining about NFS speeds, but in general, I don't need this to be super fast.\n\nI run a plex server (I actually won't run that as a plugin, even if the platform supports it) that servers up and transcodes 4K HDR content, a few infrastructure VMs and some game servers, but outside of my plex server being a consumer of large disk, I don't have significant performance needs.  My VMs I'll either run local or if I feel the need to go back to multiple VM hosts, I'll come up with something seperate from my bulk storage.\n\nThanks for perusing my wall of text.  Curious what other folks are using for large drive count systems. \n\nI'll head off the \"12TB refurbs are cheap\" response with \"there's nothing cheaper than what you already have.\"", "author_fullname": "t2_er16k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Storage Software/Platform Recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpdnok", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671411753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wasted a day messing about with UNRAID to realize it&amp;#39;s limited to 30 drives. :-/&lt;/p&gt;\n\n&lt;p&gt;I have a sizable pile of 5TB, 4TB, 3TB and 2TB drives.  I live somewhere where power is cheap and cooling is fresh air for 9 out of 12 months.  I have JBODs and &amp;quot;IT&amp;quot; flashed controllers to run more than I have.  All told, it&amp;#39;s around .5 PB... someday, I&amp;#39;d like to break the PB barrier, but today is not that day.&lt;/p&gt;\n\n&lt;p&gt;Can someone recommend a single software platform to support various disk sizes, reasonable (N+2) resillency and easy growth/failure replacement?&lt;/p&gt;\n\n&lt;p&gt;UNRAID, too few disks.  TrueNAS, would need a seperate pool for each disk size, replacement blows.  I know next to nothing about OpenMediaVault but am going to fire that up here soon to poke about.  I see people complaining about NFS speeds, but in general, I don&amp;#39;t need this to be super fast.&lt;/p&gt;\n\n&lt;p&gt;I run a plex server (I actually won&amp;#39;t run that as a plugin, even if the platform supports it) that servers up and transcodes 4K HDR content, a few infrastructure VMs and some game servers, but outside of my plex server being a consumer of large disk, I don&amp;#39;t have significant performance needs.  My VMs I&amp;#39;ll either run local or if I feel the need to go back to multiple VM hosts, I&amp;#39;ll come up with something seperate from my bulk storage.&lt;/p&gt;\n\n&lt;p&gt;Thanks for perusing my wall of text.  Curious what other folks are using for large drive count systems. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll head off the &amp;quot;12TB refurbs are cheap&amp;quot; response with &amp;quot;there&amp;#39;s nothing cheaper than what you already have.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": ".5PB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpdnok", "is_robot_indexable": true, "report_reasons": null, "author": "Thranx", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zpdnok/looking_for_storage_softwareplatform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpdnok/looking_for_storage_softwareplatform/", "subreddit_subscribers": 660059, "created_utc": 1671411753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "One is from a dead laptop and another is from a dead Xbox, how would I go about formatting these for use in my PC?", "author_fullname": "t2_ermf43eo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would I go about formatting these?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_zq087c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.52, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b0EpOH2CPSprNp8jM7z3cSvlG9-Izz1cUSm161ZLRGE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671476334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One is from a dead laptop and another is from a dead Xbox, how would I go about formatting these for use in my PC?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ocv9y5ra1y6a1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ocv9y5ra1y6a1.jpg?auto=webp&amp;s=1b7394afb5c271aee7674d9df0da69c325d04a46", "width": 3015, "height": 3438}, "resolutions": [{"url": "https://preview.redd.it/ocv9y5ra1y6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=71c594bdee7d2d01f5a6bd89d2083df5d3767408", "width": 108, "height": 123}, {"url": "https://preview.redd.it/ocv9y5ra1y6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=18fd0017c279f43b0b03c5cee5a0b290a90dd519", "width": 216, "height": 246}, {"url": "https://preview.redd.it/ocv9y5ra1y6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=04da4d7c0108f943c377252817a2e9a20fbaca51", "width": 320, "height": 364}, {"url": "https://preview.redd.it/ocv9y5ra1y6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e4ff9bd6abe9f157c448628870bc2d2f0c4fa3aa", "width": 640, "height": 729}, {"url": "https://preview.redd.it/ocv9y5ra1y6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=80c73e35791d30b0d4cf87e787033f0440d8254e", "width": 960, "height": 1094}, {"url": "https://preview.redd.it/ocv9y5ra1y6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=06249e89adc548e76aabfaf773eb58b5fff92aac", "width": 1080, "height": 1231}], "variants": {}, "id": "t-auPoMG1lw49pI0lM_OdoIj8_253IUPcx27fNqNVV8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zq087c", "is_robot_indexable": true, "report_reasons": null, "author": "NaroDoesStuff", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zq087c/how_would_i_go_about_formatting_these/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ocv9y5ra1y6a1.jpg", "subreddit_subscribers": 660059, "created_utc": 1671476334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_9qnme", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "These be dead, right? Don't use them any more?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": true, "name": "t3_zq1vy9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/OGYPngUSzUrdoQArVSKVEvhpHp0Mw-CpLmQK0fwbqWM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671480070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/f7mmndvtuw6a1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/f7mmndvtuw6a1.png?auto=webp&amp;s=d161aa9dc58f3b8a269ebf5aacaf460184b3d81c", "width": 1348, "height": 816}, "resolutions": [{"url": "https://preview.redd.it/f7mmndvtuw6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=32949479eb9e110c6f73540f182dad687f9a702b", "width": 108, "height": 65}, {"url": "https://preview.redd.it/f7mmndvtuw6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=01bd95cd4d0fa03cf9602966bd54699a7c1426a6", "width": 216, "height": 130}, {"url": "https://preview.redd.it/f7mmndvtuw6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=753b605921ac9385b08e100bce4037b340eb9497", "width": 320, "height": 193}, {"url": "https://preview.redd.it/f7mmndvtuw6a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3aadbad17db0c3363e5ab77c235d09b9bc6a370", "width": 640, "height": 387}, {"url": "https://preview.redd.it/f7mmndvtuw6a1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5056adebbe6bf1f40ddb75d5b1b5debb8c2c414", "width": 960, "height": 581}, {"url": "https://preview.redd.it/f7mmndvtuw6a1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb996fb0eb0d181a474c543e88fe2abef06d8741", "width": 1080, "height": 653}], "variants": {}, "id": "Uin8-z9wY7-QItvNWLAY0Ksq8DRMoXXv8ubv1tuUvvU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zq1vy9", "is_robot_indexable": true, "report_reasons": null, "author": "skeptibat", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zq1vy9/these_be_dead_right_dont_use_them_any_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/f7mmndvtuw6a1.png", "subreddit_subscribers": 660059, "created_utc": 1671480070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Surprisingly few sources on this - officially WD say it's 16 TB, which after some time magically became 20 TB on a 2-bay NAS.\n\nI don't get what's imposing the limitation though - it's the same controller, same power - what is there to stop me from putting 2x 16 TB drives in there? Or is it just a hard-limit set by WD for whatever reasons and the NAS simply will not boot with such drives?\n\nOne theory is that the \"advertised\" capacity is what it is, going by the maximum available HDD size **at that time.** I can't confirm that though.\n\nI'd like to find out if maybe someone's already tried it before I buy the drives. Thoughts ?", "author_fullname": "t2_2zwlr2wk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD MyCloud EX2 Ultra - max capacity ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zptm0k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671461469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Surprisingly few sources on this - officially WD say it&amp;#39;s 16 TB, which after some time magically became 20 TB on a 2-bay NAS.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t get what&amp;#39;s imposing the limitation though - it&amp;#39;s the same controller, same power - what is there to stop me from putting 2x 16 TB drives in there? Or is it just a hard-limit set by WD for whatever reasons and the NAS simply will not boot with such drives?&lt;/p&gt;\n\n&lt;p&gt;One theory is that the &amp;quot;advertised&amp;quot; capacity is what it is, going by the maximum available HDD size &lt;strong&gt;at that time.&lt;/strong&gt; I can&amp;#39;t confirm that though.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to find out if maybe someone&amp;#39;s already tried it before I buy the drives. Thoughts ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zptm0k", "is_robot_indexable": true, "report_reasons": null, "author": "Teacan83", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zptm0k/wd_mycloud_ex2_ultra_max_capacity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zptm0k/wd_mycloud_ex2_ultra_max_capacity/", "subreddit_subscribers": 660059, "created_utc": 1671461469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://github.com/mxmlnkn/ratarmount](https://github.com/mxmlnkn/ratarmount)\n\nYou might already know ratarmount thanks to it being mentioned [here](https://www.reddit.com/r/DataHoarder/comments/z29axm/help_seed_zlibrary_on_ipfs/).\n\nI've been writing ratarmount as an alternative to archivemount because the latter was too slow for large archives. Contrary to the name, it also supports ZIP and RAR archives to some extent. I've been using it to bundle collections of small files, e.g., pidgin chat logs, into TAR archives because HDDs don't like reading and writing and copying those. Furthermore, as a data hoarder, those files are only used read-only anyway, so storing them in an archive is not an issue. Because I now know that TAR archives are only a simple concatenation of the actual files preceded by 512 B of metadata, I'm not even worried about data format rot anymore. It would be simple enough to write a crude TAR extractor from scratch.\n\nAt this point, ratarmount has too many features to name them all but one of my favorites is the recursive bind mounting of folders. You could for example do `ratarmount --recursion-depth 1 /media/my-large-drive /media/mld-mounted` and then you would be able to view my-large-drive via mld-mounted as if all archives in the former were extracted.\n\nCurrently, I'm dedicating my time to one of ratarmount's backends: pragzip, a multithreaded parallel gzip decoder with random access support. This should speed things up further for gzip-compressed TAR archives.\n\nI'd be happy to hear about suggestions or even bug reports if bugs were encountered.", "author_fullname": "t2_upy204yv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Access your archives without extracting them with ratarmount", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpmi44", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671440078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/mxmlnkn/ratarmount\"&gt;https://github.com/mxmlnkn/ratarmount&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You might already know ratarmount thanks to it being mentioned &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/z29axm/help_seed_zlibrary_on_ipfs/\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been writing ratarmount as an alternative to archivemount because the latter was too slow for large archives. Contrary to the name, it also supports ZIP and RAR archives to some extent. I&amp;#39;ve been using it to bundle collections of small files, e.g., pidgin chat logs, into TAR archives because HDDs don&amp;#39;t like reading and writing and copying those. Furthermore, as a data hoarder, those files are only used read-only anyway, so storing them in an archive is not an issue. Because I now know that TAR archives are only a simple concatenation of the actual files preceded by 512 B of metadata, I&amp;#39;m not even worried about data format rot anymore. It would be simple enough to write a crude TAR extractor from scratch.&lt;/p&gt;\n\n&lt;p&gt;At this point, ratarmount has too many features to name them all but one of my favorites is the recursive bind mounting of folders. You could for example do &lt;code&gt;ratarmount --recursion-depth 1 /media/my-large-drive /media/mld-mounted&lt;/code&gt; and then you would be able to view my-large-drive via mld-mounted as if all archives in the former were extracted.&lt;/p&gt;\n\n&lt;p&gt;Currently, I&amp;#39;m dedicating my time to one of ratarmount&amp;#39;s backends: pragzip, a multithreaded parallel gzip decoder with random access support. This should speed things up further for gzip-compressed TAR archives.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d be happy to hear about suggestions or even bug reports if bugs were encountered.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?auto=webp&amp;s=18c94eb1a9806e8d732e747144a42e65922bfd1f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4c4a0b2b3230c3246202b8eda828b8c676c34aa5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1780b9b08d081324f2e248c3c13f68f352ec65b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c75f5ed7ba04f01298b7e65fe1eda0d81272db1b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=92897dd24d79979b5d06d85fb06073465ab862e1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=80dd2205c70656937d97500612a1851f03747aa5", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/MbdQXSFd4Dj8NogJFtZq1CuFlQ33qPJVxJnw48syG5M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=81544b3361a3105d62ebfd99f93571482ea2a1e1", "width": 1080, "height": 540}], "variants": {}, "id": "E_-IiemZPNkstx_dji0Br7cF26Y0H42JpsdEwRT38EA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpmi44", "is_robot_indexable": true, "report_reasons": null, "author": "mxmlnkn", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpmi44/access_your_archives_without_extracting_them_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpmi44/access_your_archives_without_extracting_them_with/", "subreddit_subscribers": 660059, "created_utc": 1671440078.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "How would I go about cleaning this out properly? I figured all this dust cant be good for the tape or the drive", "author_fullname": "t2_17jter", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Got this LTO-5 Drive off eBay, inside of it is pretty dusty", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"zojwxjhjtw6a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b2f7a7fd7278a62b8b3cd070e4ffcb698d0b1d6"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2015d73bded00c0b1d993fb6864949f132109fed"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=750f323558600d5ff8e2481377a67826bd2ddd07"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2badf36f170a71e3ee70afccbb6d88479f82ff9"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1f3b51a52597c238b4b8f275238de5dc798f91f"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=66cc46cd6403ed34b2418b0d53bad7a16f504485"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/zojwxjhjtw6a1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=5d9f60490687be8d09b9218f55fbf182ecf41640"}, "id": "zojwxjhjtw6a1"}, "7pz5ejhjtw6a1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2400d448c834d4c9af2458c038edf93b99216ded"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=66572fa6e68152688db64535d1509f35bbb61003"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=91d2fcb3f3ada387ff5162b07dfc3a0039c6c8a4"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eea21df7a3e58b31572a32fc4776af144a733b79"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a35a493780e562d36c5b59c80fcaba21bf70071"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=58eb95df1f6da057d0d9d476a5a7e4515aae68c2"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/7pz5ejhjtw6a1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=93717ea4626d106605ac098b78a320883ee57d23"}, "id": "7pz5ejhjtw6a1"}}, "name": "t3_zq1oer", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "ups": 3, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "zojwxjhjtw6a1", "id": 220856999}, {"media_id": "7pz5ejhjtw6a1", "id": 220857000}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/I8nI1PFChUJ3MG4k_g-rHxh3015T9xFHw5nvKkZRRis.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671479616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How would I go about cleaning this out properly? I figured all this dust cant be good for the tape or the drive&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/zq1oer", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zq1oer", "is_robot_indexable": true, "report_reasons": null, "author": "JustKeKe23", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zq1oer/got_this_lto5_drive_off_ebay_inside_of_it_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/zq1oer", "subreddit_subscribers": 660059, "created_utc": 1671479616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone!\n\nRecently, I've learned about the Kiwix project that lets you download valuable sites completely in one \".zim\" file that can be browsed offline. I found several interesting zims in the Kiwix library, such as ArchWiki, StackExchange, AllTheTropes. I wonder, is there a way to have a local TV Tropes copy for offline browsing?", "author_fullname": "t2_26qth9a3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to download TV Tropes for offline browsing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zptksv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671461395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;Recently, I&amp;#39;ve learned about the Kiwix project that lets you download valuable sites completely in one &amp;quot;.zim&amp;quot; file that can be browsed offline. I found several interesting zims in the Kiwix library, such as ArchWiki, StackExchange, AllTheTropes. I wonder, is there a way to have a local TV Tropes copy for offline browsing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zptksv", "is_robot_indexable": true, "report_reasons": null, "author": "ChrysoliteAzalea", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zptksv/is_there_a_way_to_download_tv_tropes_for_offline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zptksv/is_there_a_way_to_download_tv_tropes_for_offline/", "subreddit_subscribers": 660059, "created_utc": 1671461395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Aloha! Among other things I'm a YouTube archivist on Linux system, with lots of videos to store, lots of HDDs of varying sizes, capacity (80-500GB) and age (several disks with BAD sectors that I wanna use as 3rd data duplicates). Then I have a single gaming PC with 2 HDD hotswap ports free. I'm looking for a hoarding setup solution, some guidance towards tools maybe? How could I improve my operation in relation to my low knowledge levels?\n\nHow I have it running currently, is I have a 1TB inside the PC that acts as \"landing\" for incoming data, which I manually process (rename, move around, compare files, remux), then I connect HDDs that I decide should pick them up. I keep track of all data in LibreOffice Spreadsheet: on HDD arrival I sometimes measure its SMART data and update it in the spreadsheet. Then on departure I open the disk with file manager in \"Flat View\", to then copy all file entries with their full path on that disk, paste into spreadsheet page dedicated for that disk (every disk has a dedicated page and I refer to them by unique numbers, magic marker'ed on the disks). All disks are encrypted and NTFS, but I'm looking to formatting them to something better (BTRFS? It says it verifies file integrity, so if the data decays over time it'll be automatically fixed?)\n\nI'm hoping for a solution that could automate some things, especially that spreadsheet, since it's hard to keep track off. I wish disks would be automatically indexed before ejection, and some database would keep track of:\n\n1. what videos are where,\n2. what videos don't have a duplicate on another disk (and which disk keeps that duplicate),\n3. disks are divided into groups A, A\\*, B, B\\*, where:\n\n* A's duplicate to other A's once;\n* \\*'s can't have its duplicates on another \\*;\n* B's duplicate from A's only (acting as third duplicate), amount of duplicates depends on amount of B's drives.\n\nAlso worth noting I'm quite a distro-hopper, so I appreciate solutions that I can migrate between OSes. Due to limited amount of drives I can inject simultaneously, I'm guessing data pooling isn't solution for me.", "author_fullname": "t2_7nxyjgm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cold-only storage solution with database indexing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpapw6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671404023.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671403677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Aloha! Among other things I&amp;#39;m a YouTube archivist on Linux system, with lots of videos to store, lots of HDDs of varying sizes, capacity (80-500GB) and age (several disks with BAD sectors that I wanna use as 3rd data duplicates). Then I have a single gaming PC with 2 HDD hotswap ports free. I&amp;#39;m looking for a hoarding setup solution, some guidance towards tools maybe? How could I improve my operation in relation to my low knowledge levels?&lt;/p&gt;\n\n&lt;p&gt;How I have it running currently, is I have a 1TB inside the PC that acts as &amp;quot;landing&amp;quot; for incoming data, which I manually process (rename, move around, compare files, remux), then I connect HDDs that I decide should pick them up. I keep track of all data in LibreOffice Spreadsheet: on HDD arrival I sometimes measure its SMART data and update it in the spreadsheet. Then on departure I open the disk with file manager in &amp;quot;Flat View&amp;quot;, to then copy all file entries with their full path on that disk, paste into spreadsheet page dedicated for that disk (every disk has a dedicated page and I refer to them by unique numbers, magic marker&amp;#39;ed on the disks). All disks are encrypted and NTFS, but I&amp;#39;m looking to formatting them to something better (BTRFS? It says it verifies file integrity, so if the data decays over time it&amp;#39;ll be automatically fixed?)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping for a solution that could automate some things, especially that spreadsheet, since it&amp;#39;s hard to keep track off. I wish disks would be automatically indexed before ejection, and some database would keep track of:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;what videos are where,&lt;/li&gt;\n&lt;li&gt;what videos don&amp;#39;t have a duplicate on another disk (and which disk keeps that duplicate),&lt;/li&gt;\n&lt;li&gt;disks are divided into groups A, A*, B, B*, where:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A&amp;#39;s duplicate to other A&amp;#39;s once;&lt;/li&gt;\n&lt;li&gt;*&amp;#39;s can&amp;#39;t have its duplicates on another *;&lt;/li&gt;\n&lt;li&gt;B&amp;#39;s duplicate from A&amp;#39;s only (acting as third duplicate), amount of duplicates depends on amount of B&amp;#39;s drives.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Also worth noting I&amp;#39;m quite a distro-hopper, so I appreciate solutions that I can migrate between OSes. Due to limited amount of drives I can inject simultaneously, I&amp;#39;m guessing data pooling isn&amp;#39;t solution for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpapw6", "is_robot_indexable": true, "report_reasons": null, "author": "Incredible_Violent", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpapw6/coldonly_storage_solution_with_database_indexing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpapw6/coldonly_storage_solution_with_database_indexing/", "subreddit_subscribers": 660059, "created_utc": 1671403677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I feel like I'll know what the unfortunate answer is, but the million dollar question is has anyone found a fast way to calculate the hamming distance between a very very very large set of hashes?\n\nI have a dataset (that will continue to grow) that I'm planning on granting limited access to the public via way of allowing the user upload an item, then the server checks against the database of hashes, and shows information from similar hashes. The issue is that the database of hashes will be over 500 hundred million.\n\nFrom my understanding of finding similar/identical content, hamming distance is the fastest approach in calculating the difference between items? The issue I am seeing is that this must be calculated for each query? So the server is checking the 500 hundred million images each and every time someone wants to check?\n\nIs there any way to speed this process that I'm not seeing in my research? How do things like deduplication software, or reverse/similiar image searches work so fast? What's their secret? Just more compute and they're harnessing a shit ton of compute for each query? [TinEye](https://services.tineye.com/TinEyeAPI) claims to be able to \"search a 57.6 billion web image index in real-time.\" but how?? This must be the KFC's secret herbs and spices or the Coke recipe and I'm just shit out of luck?", "author_fullname": "t2_4z6dy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "fastest way to go about calculating hamming distance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zp7foz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671395223.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671395036.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like I&amp;#39;ll know what the unfortunate answer is, but the million dollar question is has anyone found a fast way to calculate the hamming distance between a very very very large set of hashes?&lt;/p&gt;\n\n&lt;p&gt;I have a dataset (that will continue to grow) that I&amp;#39;m planning on granting limited access to the public via way of allowing the user upload an item, then the server checks against the database of hashes, and shows information from similar hashes. The issue is that the database of hashes will be over 500 hundred million.&lt;/p&gt;\n\n&lt;p&gt;From my understanding of finding similar/identical content, hamming distance is the fastest approach in calculating the difference between items? The issue I am seeing is that this must be calculated for each query? So the server is checking the 500 hundred million images each and every time someone wants to check?&lt;/p&gt;\n\n&lt;p&gt;Is there any way to speed this process that I&amp;#39;m not seeing in my research? How do things like deduplication software, or reverse/similiar image searches work so fast? What&amp;#39;s their secret? Just more compute and they&amp;#39;re harnessing a shit ton of compute for each query? &lt;a href=\"https://services.tineye.com/TinEyeAPI\"&gt;TinEye&lt;/a&gt; claims to be able to &amp;quot;search a 57.6 billion web image index in real-time.&amp;quot; but how?? This must be the KFC&amp;#39;s secret herbs and spices or the Coke recipe and I&amp;#39;m just shit out of luck?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "100+TB offline | 1.45PB @ Google Drive", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zp7foz", "is_robot_indexable": true, "report_reasons": null, "author": "AdamLynch", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/zp7foz/fastest_way_to_go_about_calculating_hamming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zp7foz/fastest_way_to_go_about_calculating_hamming/", "subreddit_subscribers": 660059, "created_utc": 1671395036.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Howdy, fellow hoarders!\n\nI'm looking for a solution to use to store about 2 TB of data (family photos and videos backup) online that does not require me to install an application to do so.\n\nI've looked at some of the other posts here and have an idea of the major suppliers already, but not which ones offer an option to just map a drive (open to any protocols sorted by linux).\n\nDoes anyone else have this use case and doing this with a provider, please?\n\nIdeally I'll go with the cheapest cost/year. I don't want nor need anything fancy, just storage space I can map to have another backip of family photos and videos. \n\nThank you in advance.\n\nPs- I asked iDrive and tgey responded that I must have their app installed.", "author_fullname": "t2_emara", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Storage w/o Install", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zptkha", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671461745.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671461373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy, fellow hoarders!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a solution to use to store about 2 TB of data (family photos and videos backup) online that does not require me to install an application to do so.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked at some of the other posts here and have an idea of the major suppliers already, but not which ones offer an option to just map a drive (open to any protocols sorted by linux).&lt;/p&gt;\n\n&lt;p&gt;Does anyone else have this use case and doing this with a provider, please?&lt;/p&gt;\n\n&lt;p&gt;Ideally I&amp;#39;ll go with the cheapest cost/year. I don&amp;#39;t want nor need anything fancy, just storage space I can map to have another backip of family photos and videos. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n\n&lt;p&gt;Ps- I asked iDrive and tgey responded that I must have their app installed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zptkha", "is_robot_indexable": true, "report_reasons": null, "author": "trancekat", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zptkha/cloud_storage_wo_install/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zptkha/cloud_storage_wo_install/", "subreddit_subscribers": 660059, "created_utc": 1671461373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The sequence of events:\n\n1. I have this  hdd (TOSHIBA HDTB420EK3AA) and i want to use it to backup some internal drives on a desktop. I write a bash script to do this, i mess up,  i corrupt the data, and i have to format it. \n2. Trying to format this thing makes my pc freeze so badly i have to hard reset. I try the same with other machines, and different operating systmes, each one freezing aswell. The winner is an old laptop on lubuntu that manages to successfully format the drive in just over an hour. \n3. I go back to the first desktop and modify the script, i run it and it kinda works.  By looking at rsync output i notice that the average speed is 500 kb/s, reaching 40kb/s at some points. At some point it starts copying an .IPCH file that makes it reach 0 byte/s and then nothing. \n4. I decide to exclude this file and other folders containing very large amount  of small files, since it looks that the speed slows down especially in these cases. For what i know HDDs are ass in this type of operations but this is just ridiculous. \n5. So i stop the rsync, and everything freezes again. I can't even open the file explorer. I boot windows and use diskpart this time, then i install DiskInfo.  \n\nhttps://preview.redd.it/20pxjo5urq6a1.png?width=670&amp;format=png&amp;auto=webp&amp;s=5f70d06a653ae8e4fda4bbd6e43376a6e578c157\n\nTo me it looks very ok, but in the 20 minutes it took me to write this, diskpart managed to go from 0% to 2% in the process of formatting the disk. \n\nThe disk is almost factory new, but i can't send it back anymore. Any ideas?", "author_fullname": "t2_5wyz3lvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should i keep using this external hdd?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"20pxjo5urq6a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 114, "x": 108, "u": "https://preview.redd.it/20pxjo5urq6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b2ccb00d921eabdf655f7a7ba7b36bd5f043def"}, {"y": 229, "x": 216, "u": "https://preview.redd.it/20pxjo5urq6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ab30365ea0e1f70df33d22d957b285d4a1271b7"}, {"y": 339, "x": 320, "u": "https://preview.redd.it/20pxjo5urq6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=46da3a48fa0ed6e77e698e3b96b503aaddc3e20d"}, {"y": 679, "x": 640, "u": "https://preview.redd.it/20pxjo5urq6a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4a70b82e6eca0c0e05c799b4cfa5197aebd75017"}], "s": {"y": 711, "x": 670, "u": "https://preview.redd.it/20pxjo5urq6a1.png?width=670&amp;format=png&amp;auto=webp&amp;s=5f70d06a653ae8e4fda4bbd6e43376a6e578c157"}, "id": "20pxjo5urq6a1"}}, "name": "t3_zpbwmp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/YIJfwFDyi8NAIsF3466KrDqfC5hDcq2t2L6SDHI14Hs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671406857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The sequence of events:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I have this  hdd (TOSHIBA HDTB420EK3AA) and i want to use it to backup some internal drives on a desktop. I write a bash script to do this, i mess up,  i corrupt the data, and i have to format it. &lt;/li&gt;\n&lt;li&gt;Trying to format this thing makes my pc freeze so badly i have to hard reset. I try the same with other machines, and different operating systmes, each one freezing aswell. The winner is an old laptop on lubuntu that manages to successfully format the drive in just over an hour. &lt;/li&gt;\n&lt;li&gt;I go back to the first desktop and modify the script, i run it and it kinda works.  By looking at rsync output i notice that the average speed is 500 kb/s, reaching 40kb/s at some points. At some point it starts copying an .IPCH file that makes it reach 0 byte/s and then nothing. &lt;/li&gt;\n&lt;li&gt;I decide to exclude this file and other folders containing very large amount  of small files, since it looks that the speed slows down especially in these cases. For what i know HDDs are ass in this type of operations but this is just ridiculous. &lt;/li&gt;\n&lt;li&gt;So i stop the rsync, and everything freezes again. I can&amp;#39;t even open the file explorer. I boot windows and use diskpart this time, then i install DiskInfo.&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/20pxjo5urq6a1.png?width=670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f70d06a653ae8e4fda4bbd6e43376a6e578c157\"&gt;https://preview.redd.it/20pxjo5urq6a1.png?width=670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f70d06a653ae8e4fda4bbd6e43376a6e578c157&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To me it looks very ok, but in the 20 minutes it took me to write this, diskpart managed to go from 0% to 2% in the process of formatting the disk. &lt;/p&gt;\n\n&lt;p&gt;The disk is almost factory new, but i can&amp;#39;t send it back anymore. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpbwmp", "is_robot_indexable": true, "report_reasons": null, "author": "Cute_Rub_9074", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpbwmp/should_i_keep_using_this_external_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpbwmp/should_i_keep_using_this_external_hdd/", "subreddit_subscribers": 660059, "created_utc": 1671406857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Can y'all recommend external hard drives that have a USB-C port instead of those old micro-B 3.0 or so connectors? I'm tired of carrying a dozen cables with me all the time &amp; if a little research before buying my next HDD means 1 less cable, it'll be wonderful!\n\nExcluding the premium out-of-my-budget drives like Lacie... so far I've only found the WD My Passport Ultra to have a type C port. Strange how type C still isn't \"normal\" in the end of 2022.", "author_fullname": "t2_735tdxy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External Hard Drives with USB-C", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpyy1u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671473490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can y&amp;#39;all recommend external hard drives that have a USB-C port instead of those old micro-B 3.0 or so connectors? I&amp;#39;m tired of carrying a dozen cables with me all the time &amp;amp; if a little research before buying my next HDD means 1 less cable, it&amp;#39;ll be wonderful!&lt;/p&gt;\n\n&lt;p&gt;Excluding the premium out-of-my-budget drives like Lacie... so far I&amp;#39;ve only found the WD My Passport Ultra to have a type C port. Strange how type C still isn&amp;#39;t &amp;quot;normal&amp;quot; in the end of 2022.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "zpyy1u", "is_robot_indexable": true, "report_reasons": null, "author": "samforelli", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpyy1u/external_hard_drives_with_usbc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpyy1u/external_hard_drives_with_usbc/", "subreddit_subscribers": 660059, "created_utc": 1671473490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI didn't really know where to ask this so here we go: Could someone download me a beta build of a [game](https://www.betaarchive.com/database/view_release.php?uuid=3ab7e5af-33f8-4238-8425-d64bcd9350bf) from beta archive? If this isn't allowed here, does anyone know a place where i can get help? Unfortunetly i don't really have anything special or rare for that FTP access...", "author_fullname": "t2_99z205qy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beta Archive mirroring help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpx52d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671469526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I didn&amp;#39;t really know where to ask this so here we go: Could someone download me a beta build of a &lt;a href=\"https://www.betaarchive.com/database/view_release.php?uuid=3ab7e5af-33f8-4238-8425-d64bcd9350bf\"&gt;game&lt;/a&gt; from beta archive? If this isn&amp;#39;t allowed here, does anyone know a place where i can get help? Unfortunetly i don&amp;#39;t really have anything special or rare for that FTP access...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpx52d", "is_robot_indexable": true, "report_reasons": null, "author": "vbl37", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpx52d/beta_archive_mirroring_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpx52d/beta_archive_mirroring_help/", "subreddit_subscribers": 660059, "created_utc": 1671469526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was wondering has anybody else used Hetzner Storage Box? How was your experiences? Would prefer Hetzner or some  cloud provider like Google Drive,IceDrive, Dropbox, etc) ?", "author_fullname": "t2_499ta741", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hetzner Storage Box or Cloud providers (Google Drive,DropBox, etc)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpue0f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671463315.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering has anybody else used Hetzner Storage Box? How was your experiences? Would prefer Hetzner or some  cloud provider like Google Drive,IceDrive, Dropbox, etc) ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpue0f", "is_robot_indexable": true, "report_reasons": null, "author": "Thrillsteam", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpue0f/hetzner_storage_box_or_cloud_providers_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpue0f/hetzner_storage_box_or_cloud_providers_google/", "subreddit_subscribers": 660059, "created_utc": 1671463315.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys, I'm currently running a Synology DS218 with single 8T HDD from WD, a WD RED \"WD80EFZX\" (5400 RPM, sata600, 128m cache) configured with SHR. \n\nDo you think it's OK to mix with \"WD8003FFBX\" (WD RED PRO 7200 RPM, sata600, 256 chache) ?\n\nI know RAID configurations should be built with identical drives and I'm worried about the different cache size and speed (even if here [https://aphnetworks.com/reviews/western-digital-red-wd80efzx-8tb/2](https://aphnetworks.com/reviews/western-digital-red-wd80efzx-8tb/2) it is mentioned that even the old WD80EFZX runs at 7200RPM despite being classified as a \"5400 RPM drive class\" by WD).\n\nI have the opportunity to get a new WD RED PRO (WD8003FFBX) for less then the cost of the \"equivalent\"  WD RED PLUS (WD80EFZZ) which replaced my actual drive WD80EFZX in WD lineup. Probably chosing the WD RED PLUS is a safer choice... but IF in a SHR 2 drive scenario there are no drowbacks other than limiting the faster drive in mixing \"different\" HHD (from cache and RPM prospective) I would opt for the WD RED PRO.... \n\nThank you very much in advance for your support !", "author_fullname": "t2_4d2nvalj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mixing WD RED / WD RED PRO (WD80EFZX+WD8003FFBX)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zppage", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.36, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671450002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I&amp;#39;m currently running a Synology DS218 with single 8T HDD from WD, a WD RED &amp;quot;WD80EFZX&amp;quot; (5400 RPM, sata600, 128m cache) configured with SHR. &lt;/p&gt;\n\n&lt;p&gt;Do you think it&amp;#39;s OK to mix with &amp;quot;WD8003FFBX&amp;quot; (WD RED PRO 7200 RPM, sata600, 256 chache) ?&lt;/p&gt;\n\n&lt;p&gt;I know RAID configurations should be built with identical drives and I&amp;#39;m worried about the different cache size and speed (even if here &lt;a href=\"https://aphnetworks.com/reviews/western-digital-red-wd80efzx-8tb/2\"&gt;https://aphnetworks.com/reviews/western-digital-red-wd80efzx-8tb/2&lt;/a&gt; it is mentioned that even the old WD80EFZX runs at 7200RPM despite being classified as a &amp;quot;5400 RPM drive class&amp;quot; by WD).&lt;/p&gt;\n\n&lt;p&gt;I have the opportunity to get a new WD RED PRO (WD8003FFBX) for less then the cost of the &amp;quot;equivalent&amp;quot;  WD RED PLUS (WD80EFZZ) which replaced my actual drive WD80EFZX in WD lineup. Probably chosing the WD RED PLUS is a safer choice... but IF in a SHR 2 drive scenario there are no drowbacks other than limiting the faster drive in mixing &amp;quot;different&amp;quot; HHD (from cache and RPM prospective) I would opt for the WD RED PRO.... &lt;/p&gt;\n\n&lt;p&gt;Thank you very much in advance for your support !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zppage", "is_robot_indexable": true, "report_reasons": null, "author": "IPD-ita", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zppage/mixing_wd_red_wd_red_pro_wd80efzxwd8003ffbx/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zppage/mixing_wd_red_wd_red_pro_wd80efzxwd8003ffbx/", "subreddit_subscribers": 660059, "created_utc": 1671450002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Here is the laptop](https://www.amazon.com/gp/product/B07B7VFTN9/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1) if you're interested but the relevant part is the ports:\n\n* SATA 3 internal, looks to be the laptop 7mm form factor\n* 2 USB 3.1 gen1 ports, 5Gbits/s or \\~500mb/s theorical speed; 1 type A and 1 type C\n\nThe ports could also be usb 3.2 gen1, there is conflicting info on even the manufacturer site but it's \\~500mb/s. I am assuming SATA 3 due to the year, could not find specs on this.\n\nI'm flexible, I want at least 4tb storage but will get as much as 8tb if I see a deal. My current plan is to use an external HDD with one of the usb ports as my backup drive, and use the sata as the primary.\n\n**Options**\n\n1. Get [this HDD](https://www.amazon.com/Seagate-BarraCuda-Internal-2-5-Inch-ST5000LM000/dp/B07MWDMD4J/ref=sr_1_4?crid=2DKFXY9EAE6T0&amp;keywords=2.5in+sata3+HDD+5tb&amp;qid=1671416580&amp;s=electronics&amp;sprefix=2.5in+sata3+hdd+5tb%2Celectronics%2C61&amp;sr=1-4), the case has enough wiggle room that I could probably fit it, or leave the case off, or get a sata extension wire.\n2. Get a sata extension wire and feed it to a 3.5 external HDD. My main concern is if it will be compatible.\n3. Use both usb ports for two external HDDs, the SATA can't take more than 2tb\n\nI'm looking to keep thing relatively cheap and don't want to get a rack or PSU right now; this is my first data-hoarding project. Will either of my options be compatible without a PSU? Is this stupid and I should sell the laptop and use my RPi with a rack instead? Also open to suggestions for which sata HDD and external hdd I should get.", "author_fullname": "t2_hu3ysrzv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to use an old laptop for a nextcloud server and am wondering if I should use internal SATA for the storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpfmvv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671417499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.amazon.com/gp/product/B07B7VFTN9/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;amp;psc=1\"&gt;Here is the laptop&lt;/a&gt; if you&amp;#39;re interested but the relevant part is the ports:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SATA 3 internal, looks to be the laptop 7mm form factor&lt;/li&gt;\n&lt;li&gt;2 USB 3.1 gen1 ports, 5Gbits/s or ~500mb/s theorical speed; 1 type A and 1 type C&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The ports could also be usb 3.2 gen1, there is conflicting info on even the manufacturer site but it&amp;#39;s ~500mb/s. I am assuming SATA 3 due to the year, could not find specs on this.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m flexible, I want at least 4tb storage but will get as much as 8tb if I see a deal. My current plan is to use an external HDD with one of the usb ports as my backup drive, and use the sata as the primary.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Options&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Get &lt;a href=\"https://www.amazon.com/Seagate-BarraCuda-Internal-2-5-Inch-ST5000LM000/dp/B07MWDMD4J/ref=sr_1_4?crid=2DKFXY9EAE6T0&amp;amp;keywords=2.5in+sata3+HDD+5tb&amp;amp;qid=1671416580&amp;amp;s=electronics&amp;amp;sprefix=2.5in+sata3+hdd+5tb%2Celectronics%2C61&amp;amp;sr=1-4\"&gt;this HDD&lt;/a&gt;, the case has enough wiggle room that I could probably fit it, or leave the case off, or get a sata extension wire.&lt;/li&gt;\n&lt;li&gt;Get a sata extension wire and feed it to a 3.5 external HDD. My main concern is if it will be compatible.&lt;/li&gt;\n&lt;li&gt;Use both usb ports for two external HDDs, the SATA can&amp;#39;t take more than 2tb&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m looking to keep thing relatively cheap and don&amp;#39;t want to get a rack or PSU right now; this is my first data-hoarding project. Will either of my options be compatible without a PSU? Is this stupid and I should sell the laptop and use my RPi with a rack instead? Also open to suggestions for which sata HDD and external hdd I should get.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpfmvv", "is_robot_indexable": true, "report_reasons": null, "author": "PornAlt12323456", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpfmvv/looking_to_use_an_old_laptop_for_a_nextcloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpfmvv/looking_to_use_an_old_laptop_for_a_nextcloud/", "subreddit_subscribers": 660059, "created_utc": 1671417499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Got a bunch of old VHS tapes from early to late nineties I'm trying to get converted over to digital. Obviously I'm aiming for as high of quality capture as I can get, but I've only got a few cheap options and my current setup is nowhere near ideal. \n\nWhat I have is a Genki Arcade capture card (it's and HDMI plug to the output of source and a USB-C that goes to the computer) and a cheap AV to HDMI converter. Able to get it hooked up to an old JVC VCR but the image quality is horrid. I've found a DVD/VHS player that does have an HDMI out port built into it. It's a Panasonic DMR-EZ48V. I could fit the capture card I've got straight to it and bypass the cheap converter. \n\nWould that be a better option to go, or should I invest more to a fully different setup?", "author_fullname": "t2_aaqxalqt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up to save some VHS tapes.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpfiz8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671417184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got a bunch of old VHS tapes from early to late nineties I&amp;#39;m trying to get converted over to digital. Obviously I&amp;#39;m aiming for as high of quality capture as I can get, but I&amp;#39;ve only got a few cheap options and my current setup is nowhere near ideal. &lt;/p&gt;\n\n&lt;p&gt;What I have is a Genki Arcade capture card (it&amp;#39;s and HDMI plug to the output of source and a USB-C that goes to the computer) and a cheap AV to HDMI converter. Able to get it hooked up to an old JVC VCR but the image quality is horrid. I&amp;#39;ve found a DVD/VHS player that does have an HDMI out port built into it. It&amp;#39;s a Panasonic DMR-EZ48V. I could fit the capture card I&amp;#39;ve got straight to it and bypass the cheap converter. &lt;/p&gt;\n\n&lt;p&gt;Would that be a better option to go, or should I invest more to a fully different setup?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpfiz8", "is_robot_indexable": true, "report_reasons": null, "author": "TheFluBug", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpfiz8/setting_up_to_save_some_vhs_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpfiz8/setting_up_to_save_some_vhs_tapes/", "subreddit_subscribers": 660059, "created_utc": 1671417184.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just moving my Unraid server across to new hardware based around an ASRock Z790 Pro RS MoBo (have flashed the latest stable BIOS). \n\nI originally set up Unraid last month on an old/ repurposed Xeon based Thinkserver TS140.\n\nI moved the drives across (6TB Seagate, 8TB Seagate, 10TB WD, 18TB WD)\n\nPlus I'd been running the 2 x 20TB WDs unshucked as the parity drives via USB, so I shucked these and put them in the new case as I now have the PSU and drive bays to handle them.\n\nWhen I first turned the new PC on, only the Seagates were detected, both in BIOS &amp; Unraid.\n\nSo I tried taping pin 3 on the WD drives. The 10TB &amp; 18TB are now being detected in BIOS &amp; Unraid, but the 20TBs still aren't being detected.\n\nThe Thinkserver was running in Legacy BIOS mode. The ASRock doesn't have a legacy option (that I can find), so is running as UEFI. Would that affect this?\n\nAny suggestions of something else I can try to get the 20TBs working?\n\nAre others using these drives with a recent UEFI only motherboard?", "author_fullname": "t2_h5frl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pair of shucked 20TB WD200EDGZ WD Elements drives not detected with new motherboard, even with pin 3 masked", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpd7sa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.36, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671410482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just moving my Unraid server across to new hardware based around an ASRock Z790 Pro RS MoBo (have flashed the latest stable BIOS). &lt;/p&gt;\n\n&lt;p&gt;I originally set up Unraid last month on an old/ repurposed Xeon based Thinkserver TS140.&lt;/p&gt;\n\n&lt;p&gt;I moved the drives across (6TB Seagate, 8TB Seagate, 10TB WD, 18TB WD)&lt;/p&gt;\n\n&lt;p&gt;Plus I&amp;#39;d been running the 2 x 20TB WDs unshucked as the parity drives via USB, so I shucked these and put them in the new case as I now have the PSU and drive bays to handle them.&lt;/p&gt;\n\n&lt;p&gt;When I first turned the new PC on, only the Seagates were detected, both in BIOS &amp;amp; Unraid.&lt;/p&gt;\n\n&lt;p&gt;So I tried taping pin 3 on the WD drives. The 10TB &amp;amp; 18TB are now being detected in BIOS &amp;amp; Unraid, but the 20TBs still aren&amp;#39;t being detected.&lt;/p&gt;\n\n&lt;p&gt;The Thinkserver was running in Legacy BIOS mode. The ASRock doesn&amp;#39;t have a legacy option (that I can find), so is running as UEFI. Would that affect this?&lt;/p&gt;\n\n&lt;p&gt;Any suggestions of something else I can try to get the 20TBs working?&lt;/p&gt;\n\n&lt;p&gt;Are others using these drives with a recent UEFI only motherboard?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpd7sa", "is_robot_indexable": true, "report_reasons": null, "author": "ceestars", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpd7sa/pair_of_shucked_20tb_wd200edgz_wd_elements_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpd7sa/pair_of_shucked_20tb_wd200edgz_wd_elements_drives/", "subreddit_subscribers": 660059, "created_utc": 1671410482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Preferably with an option to do it by \"top\" so as to get the high quality posts first. Mostly this is for nsfw subreddits I'm worried may be banned or something, so pictures and videos are the priority over text posts.\n\nI found [this](https://github.com/aliparlakci/bulk-downloader-for-reddit), but I'm not 100% understanding the instructions. This is how I'm understanding it, if you know better I would appreciate the help.\n\n1) Download python 3.9 or above. I assume I get this from python.org. Once I've gotten this how do I actually open it though?\n\n2) I need to paste \"python3 -m pip install bdfr --upgrade\" into python which will install the downloader?\n\n3) If I wanted to, for example, download the top voted pictures from the past month of the subreddit /r/aww into a file called \"aww\" with each picture being named the subreddit then title then post IF, I would type the following: \"bdfr download E:\\subredditarchives\\aww --subreddit aww --sort top --time month --no-dupes --file-scheme {SUBREDDIT}_{TITLE}_{POSTID}\"?", "author_fullname": "t2_fxa6t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a simple way to download the posts of a subreddit in bulk?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zprbhf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671458958.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671455781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Preferably with an option to do it by &amp;quot;top&amp;quot; so as to get the high quality posts first. Mostly this is for nsfw subreddits I&amp;#39;m worried may be banned or something, so pictures and videos are the priority over text posts.&lt;/p&gt;\n\n&lt;p&gt;I found &lt;a href=\"https://github.com/aliparlakci/bulk-downloader-for-reddit\"&gt;this&lt;/a&gt;, but I&amp;#39;m not 100% understanding the instructions. This is how I&amp;#39;m understanding it, if you know better I would appreciate the help.&lt;/p&gt;\n\n&lt;p&gt;1) Download python 3.9 or above. I assume I get this from python.org. Once I&amp;#39;ve gotten this how do I actually open it though?&lt;/p&gt;\n\n&lt;p&gt;2) I need to paste &amp;quot;python3 -m pip install bdfr --upgrade&amp;quot; into python which will install the downloader?&lt;/p&gt;\n\n&lt;p&gt;3) If I wanted to, for example, download the top voted pictures from the past month of the subreddit &lt;a href=\"/r/aww\"&gt;/r/aww&lt;/a&gt; into a file called &amp;quot;aww&amp;quot; with each picture being named the subreddit then title then post IF, I would type the following: &amp;quot;bdfr download E:\\subredditarchives\\aww --subreddit aww --sort top --time month --no-dupes --file-scheme {SUBREDDIT}&lt;em&gt;{TITLE}&lt;/em&gt;{POSTID}&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?auto=webp&amp;s=79964651227925e81dab2225e179f766b7ce9cd9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d9db54a5f92c02b4ac3561a278cacd6a798ce38", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c15e8eb33f2dec8663f718ef091fdd1814ad7e9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b278fef3bbf93b8f9e90248f62d0a61dbd0c97f3", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a3e26ce2879ce29570b5fcd6fa3ee1b896ee8fbc", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b53ccbe0a13a14e258e90118afa420f37b8de454", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/z70QhQ3QuUyobBlYwf0wMUwuk58nkOaeTfAXz-LJ0CI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e70d92f1bcd7d55ee949fcff06d63bf88dd2a0c6", "width": 1080, "height": 540}], "variants": {}, "id": "t6fY0ytyGKU6aQ5wNobIAf9tnsQeHpyD7J-f0ZOYDsk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zprbhf", "is_robot_indexable": true, "report_reasons": null, "author": "onlytoask", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zprbhf/is_there_a_simple_way_to_download_the_posts_of_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zprbhf/is_there_a_simple_way_to_download_the_posts_of_a/", "subreddit_subscribers": 660059, "created_utc": 1671455781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So to start, I want to say I have browsed the sub for hours for a solution, but there doesn't appear to be anything. So I thought maybe I'll just ask directly.\n\nI am stressed.\n\nI have a year until amazon drive shuts down, but only a month until Data uploads stop.\n\nI'm a photographer/videographer and a data hoarder at the same time. I have **2 NAS systems** that are linked to **2 different Amazon drive/photo accounts** and this set up has been amazing.\n\nSure I pay about 400$ a year, but in turn, I get Amazon's AI to sort and organize EVERYTHING that I put into the cloud drive. Whenever friends or family ask me for pics/vids, I can find and send it within minutes through Amazon. When I need to edit videos, I do just grab the files from folders that I sorted in the cloud drive (through the NAS clone). Any changes to the NAS are synced with Amazon. The fact that I don't have to pay for any picture files is also amazing considering I shoot in raw (very high data usage). It's also so convenient to just plug in an SD card into the computer and it automatically uploads to both the cloud and to my local NAS.\n\nI will assume that once cloud drive is removed, my entire set up will vanish.\n\nThe AI that sorts everything is unparrelled. I'm going to miss it more than anything. It's going to be extremely difficult for me to continue using Amazon photos after this and I'll probably just end up cancelling my membership.\n\nI need help to find a replacement workflow that I can sort of get to work. Cloud OR local\n\n1. It looks like my best option from this point is to self host everything. I hate doing this because I love hybrid setups. Mobile convenience is also pretty important. But if it must be done, oh well.\n2. I considered google drive. But google photos and drive are seperate, unlike amazon. No point. (Although I do love google drive's system too)\n3. I have already prepped for migration (in a way). While all the files are already backed to the NAS, I popped another copy onto harddrives.\n\nSo key requirements that I need.\n\n1. Some sort of auto sorting feature. Date and Faces are a requirement. Everything else is optional but wish it existed.\n2. Mobile app access and upload. All photos and vids from phone are auto uploaded.\n3. Easy sharing.\n\nOptional things I'd love.\n\n1. Amazon photos and google photos have amazing AI and features. Will miss it.\n2. The ability for friends to upload as well (only google photos has this. Not a part of my current set up, but would have been amazing)\n\nI have over 100k photos and 20k videos (per server). Honestly, probably more. Some recent issues caused a desync and the cloud is missing about 1tb of data (yikes, right?)", "author_fullname": "t2_ov7nq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to do now after Amazon Drive is shutting down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zpm4ym", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.48, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671444110.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671438635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So to start, I want to say I have browsed the sub for hours for a solution, but there doesn&amp;#39;t appear to be anything. So I thought maybe I&amp;#39;ll just ask directly.&lt;/p&gt;\n\n&lt;p&gt;I am stressed.&lt;/p&gt;\n\n&lt;p&gt;I have a year until amazon drive shuts down, but only a month until Data uploads stop.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a photographer/videographer and a data hoarder at the same time. I have &lt;strong&gt;2 NAS systems&lt;/strong&gt; that are linked to &lt;strong&gt;2 different Amazon drive/photo accounts&lt;/strong&gt; and this set up has been amazing.&lt;/p&gt;\n\n&lt;p&gt;Sure I pay about 400$ a year, but in turn, I get Amazon&amp;#39;s AI to sort and organize EVERYTHING that I put into the cloud drive. Whenever friends or family ask me for pics/vids, I can find and send it within minutes through Amazon. When I need to edit videos, I do just grab the files from folders that I sorted in the cloud drive (through the NAS clone). Any changes to the NAS are synced with Amazon. The fact that I don&amp;#39;t have to pay for any picture files is also amazing considering I shoot in raw (very high data usage). It&amp;#39;s also so convenient to just plug in an SD card into the computer and it automatically uploads to both the cloud and to my local NAS.&lt;/p&gt;\n\n&lt;p&gt;I will assume that once cloud drive is removed, my entire set up will vanish.&lt;/p&gt;\n\n&lt;p&gt;The AI that sorts everything is unparrelled. I&amp;#39;m going to miss it more than anything. It&amp;#39;s going to be extremely difficult for me to continue using Amazon photos after this and I&amp;#39;ll probably just end up cancelling my membership.&lt;/p&gt;\n\n&lt;p&gt;I need help to find a replacement workflow that I can sort of get to work. Cloud OR local&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;It looks like my best option from this point is to self host everything. I hate doing this because I love hybrid setups. Mobile convenience is also pretty important. But if it must be done, oh well.&lt;/li&gt;\n&lt;li&gt;I considered google drive. But google photos and drive are seperate, unlike amazon. No point. (Although I do love google drive&amp;#39;s system too)&lt;/li&gt;\n&lt;li&gt;I have already prepped for migration (in a way). While all the files are already backed to the NAS, I popped another copy onto harddrives.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So key requirements that I need.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Some sort of auto sorting feature. Date and Faces are a requirement. Everything else is optional but wish it existed.&lt;/li&gt;\n&lt;li&gt;Mobile app access and upload. All photos and vids from phone are auto uploaded.&lt;/li&gt;\n&lt;li&gt;Easy sharing.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Optional things I&amp;#39;d love.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Amazon photos and google photos have amazing AI and features. Will miss it.&lt;/li&gt;\n&lt;li&gt;The ability for friends to upload as well (only google photos has this. Not a part of my current set up, but would have been amazing)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I have over 100k photos and 20k videos (per server). Honestly, probably more. Some recent issues caused a desync and the cloud is missing about 1tb of data (yikes, right?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "zpm4ym", "is_robot_indexable": true, "report_reasons": null, "author": "coolelel", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/zpm4ym/what_to_do_now_after_amazon_drive_is_shutting_down/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/zpm4ym/what_to_do_now_after_amazon_drive_is_shutting_down/", "subreddit_subscribers": 660059, "created_utc": 1671438635.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}