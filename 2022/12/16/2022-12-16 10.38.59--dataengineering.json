{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3kxbd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visual Studio Marketplace: This extension enables you to connect to Snowflake, write and execute sql queries, and view results without leaving VS Code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_zmt4ho", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": "#46d160", "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 70, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/SbuJpySGenVX_rmkxapJCaOMnIQ4gkmzR3XX2-cEOHY.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1671131053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "marketplace.visualstudio.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://marketplace.visualstudio.com/items?itemName=snowflake.snowflake-vsc", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EWxbX-1mN7UU8lUJ-dX9K8FJ5oNH6f1ZlgzHvRKmTao.jpg?auto=webp&amp;s=f8c9739073e49b958cac1de1c39d6ee35101a5a4", "width": 96, "height": 96}, "resolutions": [], "variants": {}, "id": "DrOLKRokq5fbqBBSJgJl9KkvfL_reTY01TFJh5dKVNM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "honorary mod | Snowflake", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zmt4ho", "is_robot_indexable": true, "report_reasons": null, "author": "fhoffa", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/zmt4ho/visual_studio_marketplace_this_extension_enables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://marketplace.visualstudio.com/items?itemName=snowflake.snowflake-vsc", "subreddit_subscribers": 83048, "created_utc": 1671131053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My manager has tasked me with setting up Airflow on AWS. Depending on the cost I guess my options range from ECS, MWAA or simply deploying it on EC2. In any case my question relates to writing DAGs: Airflow is merely a scheduler and is not supposed to perform the actual ETL work. I want to deploy Airflow and write DAGs in a way that will be scalable and isn't going to incur unnecessary costs, so with that in mind, are there any tips/best practices to keep in mind when writing DAGs? Any operators I should avoid in keeping with the principle that Airflow shouldn't be performing the business logic?\n\nAs an aside, where do you typically store connection credentials in prod? Is the Airflow UI sufficient or is it better to use environment variables?\n\nAppreciate the help.", "author_fullname": "t2_sv9s22ti", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you use Airflow 'properly'?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zn35zt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671156133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My manager has tasked me with setting up Airflow on AWS. Depending on the cost I guess my options range from ECS, MWAA or simply deploying it on EC2. In any case my question relates to writing DAGs: Airflow is merely a scheduler and is not supposed to perform the actual ETL work. I want to deploy Airflow and write DAGs in a way that will be scalable and isn&amp;#39;t going to incur unnecessary costs, so with that in mind, are there any tips/best practices to keep in mind when writing DAGs? Any operators I should avoid in keeping with the principle that Airflow shouldn&amp;#39;t be performing the business logic?&lt;/p&gt;\n\n&lt;p&gt;As an aside, where do you typically store connection credentials in prod? Is the Airflow UI sufficient or is it better to use environment variables?&lt;/p&gt;\n\n&lt;p&gt;Appreciate the help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zn35zt", "is_robot_indexable": true, "report_reasons": null, "author": "Timely-Section-9951", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zn35zt/how_do_you_use_airflow_properly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zn35zt/how_do_you_use_airflow_properly/", "subreddit_subscribers": 83048, "created_utc": 1671156133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is this statement right? If it's wrong, what's wrong with it?", "author_fullname": "t2_18xb5x05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"data engineers do the EL, analytics engineers or BI developers do the T\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zmo8tt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671119107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is this statement right? If it&amp;#39;s wrong, what&amp;#39;s wrong with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zmo8tt", "is_robot_indexable": true, "report_reasons": null, "author": "the-data-scientist", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zmo8tt/data_engineers_do_the_el_analytics_engineers_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zmo8tt/data_engineers_do_the_el_analytics_engineers_or/", "subreddit_subscribers": 83048, "created_utc": 1671119107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nLong time reader but first time posting. I work as a DE at a company on the east coast. I recently heard about the Data Council 2023 on Linkedin. I am wondering if anyone else is planning on going and thinks its worth it? I am planning on asking my Manager if they would let me go and pay for the ticket =).\n\n[https://www.datacouncil.ai/austin](https://www.datacouncil.ai/austin)\n\nThanks!", "author_fullname": "t2_w10li", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Council Austin 2023 Worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zmotqi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671120510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;Long time reader but first time posting. I work as a DE at a company on the east coast. I recently heard about the Data Council 2023 on Linkedin. I am wondering if anyone else is planning on going and thinks its worth it? I am planning on asking my Manager if they would let me go and pay for the ticket =).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.datacouncil.ai/austin\"&gt;https://www.datacouncil.ai/austin&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vtOgy8N6J3Qo10XkiST3peVNOgy1RNy7-OIR31x9aAE.jpg?auto=webp&amp;s=e1c3c27b104284d40885e6c1ed9af06713c03964", "width": 300, "height": 175}, "resolutions": [{"url": "https://external-preview.redd.it/vtOgy8N6J3Qo10XkiST3peVNOgy1RNy7-OIR31x9aAE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fdfef73cc0b899de8052ddbbcc343b8036a837c3", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/vtOgy8N6J3Qo10XkiST3peVNOgy1RNy7-OIR31x9aAE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b36c49fa2a82cbdead33d708db4bff3a1d46b721", "width": 216, "height": 126}], "variants": {}, "id": "x-oVdR_oAS9C4_m2hD1pEbbloi749mpo_9RxDiwDkQE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zmotqi", "is_robot_indexable": true, "report_reasons": null, "author": "TheMortyKwest", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zmotqi/data_council_austin_2023_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zmotqi/data_council_austin_2023_worth_it/", "subreddit_subscribers": 83048, "created_utc": 1671120510.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "DBT is increasing their cloud offers price.", "author_fullname": "t2_26bfpckc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Updating dbt Cloud pricing to support long-term community growth 50$-&gt;100$", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_zmtwls", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/TkhDzFrm6bVfwWkeo2jQRtbxzUeEE09FWSLdu8inF00.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671132987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "getdbt.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;DBT is increasing their cloud offers price.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.getdbt.com/blog/dbt-cloud-package-update/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?auto=webp&amp;s=edabdd18634aef29128ecc0d5693053ba1a95f6e", "width": 2560, "height": 1440}, "resolutions": [{"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5df6a50eec64ce3d126f3a47e1746feaf267cebb", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d02b260d5dddd4e2e7c80420970b653c3cdddab", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aa646e05e744be5f524016d4c775b326ef90c2d1", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e29469bfc58fee1f76e41ce74322e006445b9bb6", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7b72e416c6ef27bd12b9f3a5a19ef15ad9e8249", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=285b785bfdba2ba6f0da014cf261fbe2d5f57f3d", "width": 1080, "height": 607}], "variants": {}, "id": "1atsLEhqeX9kFaissrSt8acZk9ienTNBpjUXMcidAMU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zmtwls", "is_robot_indexable": true, "report_reasons": null, "author": "holarou", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zmtwls/updating_dbt_cloud_pricing_to_support_longterm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.getdbt.com/blog/dbt-cloud-package-update/", "subreddit_subscribers": 83048, "created_utc": 1671132987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are on MSSQL 2016 right now and want to transition to the cloud. \n\nWe do a lot of elt/etl with SSIS right now and I would like to transition to DBT for the Transformation step.  We use Tableau for reporting and transform a lot of data which we than export to other systems or send reports per email. \n\nIn the future we want to do more with Notebooks and Python jobs which we can't do right now.\n\nThe consultant team my company has hired wanted us to transition to SQL Database, which we tried and was a managing disaster for us. We have more than 4tb of Data and we do 95% OLAP and 5% OLTP. Not having Cross DB Queries was a nogo for us, since SQL Database only supports up to 1TB of data and is very cost intensive for that. I don't understand why the consultancy chose this for us, since they knew how much data we use. (Do I have a misconception here?)  \nNow they want us to transition to Azure Synapse. I tried it for a few weeks and I really did not liked it. I had the feeling that Synapse is a managing nightmare.  \nOther Datawarehouses like Google BigQuery and Snowflake seem a lot more mature to me, but I am not able to try them in full extend in my company right now (It just would be very time consuming and we have a consultant for a reason)  \nThe Consultant told us, that he wouldn't use Bigquery because of Data Privacy aspects (its google) and Snowflake because Snowflake is 10x the cost of Synapse and they don't offer support.  \nI think BigQuery or Snowflake would be a better fit for us, because we could use DBT and still Load Data with Azure DataFactory and use Databricks or maybe some other tool for Python code in Notebooks. Since we are in the Cloud anyways, we are not very limited on the tooling we can use.  \n\n\nI am able to drive the decision in which warehouse and tooling we use and refractor our Code (It has to be done, no one who wrote the SQL code is working in the company anymore and no one understands the logic behind the scripts. Every small change takes ages to apply.)\n\nWhat Platform would you choose?", "author_fullname": "t2_1jlb0188", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transition to cloud. What Warehouse to choose?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zmp76z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671121428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are on MSSQL 2016 right now and want to transition to the cloud. &lt;/p&gt;\n\n&lt;p&gt;We do a lot of elt/etl with SSIS right now and I would like to transition to DBT for the Transformation step.  We use Tableau for reporting and transform a lot of data which we than export to other systems or send reports per email. &lt;/p&gt;\n\n&lt;p&gt;In the future we want to do more with Notebooks and Python jobs which we can&amp;#39;t do right now.&lt;/p&gt;\n\n&lt;p&gt;The consultant team my company has hired wanted us to transition to SQL Database, which we tried and was a managing disaster for us. We have more than 4tb of Data and we do 95% OLAP and 5% OLTP. Not having Cross DB Queries was a nogo for us, since SQL Database only supports up to 1TB of data and is very cost intensive for that. I don&amp;#39;t understand why the consultancy chose this for us, since they knew how much data we use. (Do I have a misconception here?)&lt;br/&gt;\nNow they want us to transition to Azure Synapse. I tried it for a few weeks and I really did not liked it. I had the feeling that Synapse is a managing nightmare.&lt;br/&gt;\nOther Datawarehouses like Google BigQuery and Snowflake seem a lot more mature to me, but I am not able to try them in full extend in my company right now (It just would be very time consuming and we have a consultant for a reason)&lt;br/&gt;\nThe Consultant told us, that he wouldn&amp;#39;t use Bigquery because of Data Privacy aspects (its google) and Snowflake because Snowflake is 10x the cost of Synapse and they don&amp;#39;t offer support.&lt;br/&gt;\nI think BigQuery or Snowflake would be a better fit for us, because we could use DBT and still Load Data with Azure DataFactory and use Databricks or maybe some other tool for Python code in Notebooks. Since we are in the Cloud anyways, we are not very limited on the tooling we can use.  &lt;/p&gt;\n\n&lt;p&gt;I am able to drive the decision in which warehouse and tooling we use and refractor our Code (It has to be done, no one who wrote the SQL code is working in the company anymore and no one understands the logic behind the scripts. Every small change takes ages to apply.)&lt;/p&gt;\n\n&lt;p&gt;What Platform would you choose?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zmp76z", "is_robot_indexable": true, "report_reasons": null, "author": "KindaRoot", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zmp76z/transition_to_cloud_what_warehouse_to_choose/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zmp76z/transition_to_cloud_what_warehouse_to_choose/", "subreddit_subscribers": 83048, "created_utc": 1671121428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ons03791", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why we use Google Cloud Composer? Benefits and costs of using Airflow in a cloud-native environment.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_zmnyfa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/lK9CUIAMkEoPAb7AcZnOSG8AxTJCetQWUG9rj1KUrhs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671118423.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/tinyclues-vision/why-we-use-cloud-composer-b107624f858e", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DocKeNQuyH0pMw7V6gPF-V3wRjJC1jya07sLQW6Zu5E.jpg?auto=webp&amp;s=8c975bdf5af84588462185b3321afa967a5e6eb5", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/DocKeNQuyH0pMw7V6gPF-V3wRjJC1jya07sLQW6Zu5E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=45da0fc641725bf193538bab7639012c6650e960", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/DocKeNQuyH0pMw7V6gPF-V3wRjJC1jya07sLQW6Zu5E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e2f0662cff59439fd4261169030b941ad1e2d580", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/DocKeNQuyH0pMw7V6gPF-V3wRjJC1jya07sLQW6Zu5E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=652966af0a0595946881c5b63b54088f5051cbd3", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/DocKeNQuyH0pMw7V6gPF-V3wRjJC1jya07sLQW6Zu5E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=48982cebad0b7e54a48d31437c38a35f76e79c11", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/DocKeNQuyH0pMw7V6gPF-V3wRjJC1jya07sLQW6Zu5E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=677c900c1ee78636c84ad54a4bf9fffbd32d9cb7", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/DocKeNQuyH0pMw7V6gPF-V3wRjJC1jya07sLQW6Zu5E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6f55cab76efbc9b1983a324227311fd2d2905d9f", "width": 1080, "height": 720}], "variants": {}, "id": "RSSQlEOqBq9B6X0Cq9nE2kEmCeXs1KE4UkUuwJteuuQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zmnyfa", "is_robot_indexable": true, "report_reasons": null, "author": "pacolocopepito", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zmnyfa/why_we_use_google_cloud_composer_benefits_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/tinyclues-vision/why-we-use-cloud-composer-b107624f858e", "subreddit_subscribers": 83048, "created_utc": 1671118423.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://adventofcode.com/](https://adventofcode.com/)\n\nAnyone else did/doing this?\n\nDid you find it difficult?\n\nIt's basically a programming puzzle for every day on the \"advent calendar\"", "author_fullname": "t2_8chdw7c4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advent of Code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zn11ds", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671149904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://adventofcode.com/\"&gt;https://adventofcode.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Anyone else did/doing this?&lt;/p&gt;\n\n&lt;p&gt;Did you find it difficult?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s basically a programming puzzle for every day on the &amp;quot;advent calendar&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zn11ds", "is_robot_indexable": true, "report_reasons": null, "author": "pdxtechnologist", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zn11ds/advent_of_code/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zn11ds/advent_of_code/", "subreddit_subscribers": 83048, "created_utc": 1671149904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team owns a single repo for all of our Spark jobs, and each pipeline is fairly large. Some are 1000+ lines of code and most of the input datasets are 20-50 columns wide. This makes select statements really ugly and bloats the code even more.\n\n\nSome of the input datasets are shared between multiple pipelines, and some of the pipelines consume the output of other pipelines (in our repo). This means we would ideally like to share some code between them (e.g. reading a common dataset &amp; the schema).\n\n\nHow would you structure this? Are there any example projects that do something similar? It seems like all the example projects I've found are usually just a single pipleline, ~100 lines of code total, and with smaller input datasets. What about when it gets bigger and you want to break some pipelines into multiple files and reuse schemas &amp; reader classes between pipelines?", "author_fullname": "t2_229opcl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you structure a repo with 10+ ETL pipelines and shared code?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zn8m07", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671174222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team owns a single repo for all of our Spark jobs, and each pipeline is fairly large. Some are 1000+ lines of code and most of the input datasets are 20-50 columns wide. This makes select statements really ugly and bloats the code even more.&lt;/p&gt;\n\n&lt;p&gt;Some of the input datasets are shared between multiple pipelines, and some of the pipelines consume the output of other pipelines (in our repo). This means we would ideally like to share some code between them (e.g. reading a common dataset &amp;amp; the schema).&lt;/p&gt;\n\n&lt;p&gt;How would you structure this? Are there any example projects that do something similar? It seems like all the example projects I&amp;#39;ve found are usually just a single pipleline, ~100 lines of code total, and with smaller input datasets. What about when it gets bigger and you want to break some pipelines into multiple files and reuse schemas &amp;amp; reader classes between pipelines?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zn8m07", "is_robot_indexable": true, "report_reasons": null, "author": "y8MAC", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zn8m07/how_would_you_structure_a_repo_with_10_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zn8m07/how_would_you_structure_a_repo_with_10_etl/", "subreddit_subscribers": 83048, "created_utc": 1671174222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a set of Tableau dashboards linked to datamarts on Bigquery.\nI work with Hyper extracts to get my data from Bigquery to Tableau.\nMy workbooks contain filters for business logic and date range filters to compare data between two date ranges.\n\nI also have a user logic. Which means that my Bigquery tables have a row for each user and in Tableau views I apply user filter to have the data that is only linked to that specific user. \n\nThe problem is that my dashboards are loading very slowly. Sometimes it could take to 10mins for loading a workbook. Which is not good for user experience.\n\n\nThe size of my Bigquery Tables : 2 to 4 Tb\nNumber of tableau users : ~ 400 users\n\nHave you faced such a situation when working with Tableau ? If so what are your advices to reducing the loading time of my dashboards ? Or could you tell how do you use Tableau in a large scale ?\n\nThank you", "author_fullname": "t2_98269xyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My Tableau Dashboards are slow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zn0c5t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671148065.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a set of Tableau dashboards linked to datamarts on Bigquery.\nI work with Hyper extracts to get my data from Bigquery to Tableau.\nMy workbooks contain filters for business logic and date range filters to compare data between two date ranges.&lt;/p&gt;\n\n&lt;p&gt;I also have a user logic. Which means that my Bigquery tables have a row for each user and in Tableau views I apply user filter to have the data that is only linked to that specific user. &lt;/p&gt;\n\n&lt;p&gt;The problem is that my dashboards are loading very slowly. Sometimes it could take to 10mins for loading a workbook. Which is not good for user experience.&lt;/p&gt;\n\n&lt;p&gt;The size of my Bigquery Tables : 2 to 4 Tb\nNumber of tableau users : ~ 400 users&lt;/p&gt;\n\n&lt;p&gt;Have you faced such a situation when working with Tableau ? If so what are your advices to reducing the loading time of my dashboards ? Or could you tell how do you use Tableau in a large scale ?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zn0c5t", "is_robot_indexable": true, "report_reasons": null, "author": "Electronic-Mountain9", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zn0c5t/my_tableau_dashboards_are_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zn0c5t/my_tableau_dashboards_are_slow/", "subreddit_subscribers": 83048, "created_utc": 1671148065.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hopefully a quick one.\n\nHow far down /opt/airflow/dags folder structure does Airflow go looking for dag.py files? Do they need to be right there, or could they be scattered in subdirectories below this?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow path question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zmzw2t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671146991.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hopefully a quick one.&lt;/p&gt;\n\n&lt;p&gt;How far down /opt/airflow/dags folder structure does Airflow go looking for dag.py files? Do they need to be right there, or could they be scattered in subdirectories below this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zmzw2t", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zmzw2t/airflow_path_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zmzw2t/airflow_path_question/", "subreddit_subscribers": 83048, "created_utc": 1671146991.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, \n\nRecently laid off and I'm on an E-3 visa (specific to Australia and US) and I got 60 days to find a new role so I can stay in the US! I've really enjoyed the DE portions of my work experience so I'm trying to stay in it!\n\nI've had half and half responses to my resume (a lot of rejections) so hoping to get some feedback, I'm thinking it might be too wordy but not sure how to reduce the content and show experience?\n\nhttps://preview.redd.it/sftb7h2gh46a1.png?width=759&amp;format=png&amp;auto=webp&amp;s=b50f79ec4ef70552576e2cdaef2e2badd2e619b7", "author_fullname": "t2_iymv93m4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resume Review/Career advice for a US Alien", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"sftb7h2gh46a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 141, "x": 108, "u": "https://preview.redd.it/sftb7h2gh46a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e69f0195e0d456bf60c92a90f2d2bf66ceef92f1"}, {"y": 282, "x": 216, "u": "https://preview.redd.it/sftb7h2gh46a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb3559bcb1fde5614faaa2fe478871bf65cf2ad7"}, {"y": 418, "x": 320, "u": "https://preview.redd.it/sftb7h2gh46a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c6d2b5ef4e2b873dc493e0b658f59db666349eb"}, {"y": 836, "x": 640, "u": "https://preview.redd.it/sftb7h2gh46a1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ed3b76b6ff3481bacf77d5b3ee734b65930dc7e"}], "s": {"y": 992, "x": 759, "u": "https://preview.redd.it/sftb7h2gh46a1.png?width=759&amp;format=png&amp;auto=webp&amp;s=b50f79ec4ef70552576e2cdaef2e2badd2e619b7"}, "id": "sftb7h2gh46a1"}}, "name": "t3_zmvo2q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fC3vSKmutfafhkMdZqJ8wkgaSYLrBn9PeW4woaiMKno.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671137346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;Recently laid off and I&amp;#39;m on an E-3 visa (specific to Australia and US) and I got 60 days to find a new role so I can stay in the US! I&amp;#39;ve really enjoyed the DE portions of my work experience so I&amp;#39;m trying to stay in it!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had half and half responses to my resume (a lot of rejections) so hoping to get some feedback, I&amp;#39;m thinking it might be too wordy but not sure how to reduce the content and show experience?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/sftb7h2gh46a1.png?width=759&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b50f79ec4ef70552576e2cdaef2e2badd2e619b7\"&gt;https://preview.redd.it/sftb7h2gh46a1.png?width=759&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b50f79ec4ef70552576e2cdaef2e2badd2e619b7&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "zmvo2q", "is_robot_indexable": true, "report_reasons": null, "author": "jonnrd", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zmvo2q/resume_reviewcareer_advice_for_a_us_alien/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zmvo2q/resume_reviewcareer_advice_for_a_us_alien/", "subreddit_subscribers": 83048, "created_utc": 1671137346.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_le8pcuu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SCC. Fault Tolerance: Just Make It Two", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_zmpggb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zYiIiHzHm8csebUdLbzA8qHwhnpdLcbGhNnjUJV04OM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671122063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "superprotocol.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://superprotocol.medium.com/scc-fault-tolerance-just-make-it-two-3bd3926ec798", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5yb_029B57AZpl2RIA15ox0ALPqOTWPhpjDEp--cNY8.jpg?auto=webp&amp;s=a4d6ddf90679e855dad0a46dc575891febea49dd", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/5yb_029B57AZpl2RIA15ox0ALPqOTWPhpjDEp--cNY8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=935ae4d632321dbc3353d9baaa606597c5848497", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/5yb_029B57AZpl2RIA15ox0ALPqOTWPhpjDEp--cNY8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c177a65f2987a86f4e17a704465e72172cc4854", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/5yb_029B57AZpl2RIA15ox0ALPqOTWPhpjDEp--cNY8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b11cb84740e6b2a00fed034d4f30001f30e276e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/5yb_029B57AZpl2RIA15ox0ALPqOTWPhpjDEp--cNY8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f2c92496b089697d3cb336ff29b51f71dab8c24", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/5yb_029B57AZpl2RIA15ox0ALPqOTWPhpjDEp--cNY8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a3e841116e252a733984ac534dfbb235245aad0", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/5yb_029B57AZpl2RIA15ox0ALPqOTWPhpjDEp--cNY8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=94c87a2679bfe636b9e55ff0767a0ab27ac39eb7", "width": 1080, "height": 607}], "variants": {}, "id": "s_YyvZ1gZ5Ao-vSu__F0CArJbLHneHJuvg3qnl1iIdM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "zmpggb", "is_robot_indexable": true, "report_reasons": null, "author": "Emotional-Instance82", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zmpggb/scc_fault_tolerance_just_make_it_two/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://superprotocol.medium.com/scc-fault-tolerance-just-make-it-two-3bd3926ec798", "subreddit_subscribers": 83048, "created_utc": 1671122063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to find a best practice for handling some pipelines with DBT + Delta in Dagster.\n\nFor example:\n\n- Pipeline for automatically loading Delta with DBT and reverting back to last version (time travel)\n\n\n\nWe're currently using S3 + Delta, so we're manually writing loading for Delta into DBT. (e.g. select * from delta.filename)\n\nPerhaps external tables would be better for this use case?", "author_fullname": "t2_5yeb5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Request] Any good examples of using DBT + Dagster + Delta?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_znal53", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671182414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to find a best practice for handling some pipelines with DBT + Delta in Dagster.&lt;/p&gt;\n\n&lt;p&gt;For example:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Pipeline for automatically loading Delta with DBT and reverting back to last version (time travel)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We&amp;#39;re currently using S3 + Delta, so we&amp;#39;re manually writing loading for Delta into DBT. (e.g. select * from delta.filename)&lt;/p&gt;\n\n&lt;p&gt;Perhaps external tables would be better for this use case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "znal53", "is_robot_indexable": true, "report_reasons": null, "author": "entinthemountains", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/znal53/request_any_good_examples_of_using_dbt_dagster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/znal53/request_any_good_examples_of_using_dbt_dagster/", "subreddit_subscribers": 83048, "created_utc": 1671182414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\n&amp;#x200B;\n\nI currently work in a Microsoft SQL Server environment. I'm working on my resume right now so I can start applying for entry-level data engineer positions.\n\nCurrently, I extract data from the SQL database, implement them into Power BI, clean up the data(null values, bad data), add new columns if necessary, export to CSV and insert them back into the database.\n\nI'm not sure if this method would be considered ETL which is why I'm asking you guys. If it's considered ETL, then I have some experience where it might put me over the edge and give me more interviews. \n\nLooking for advice and I don't mind any blunt answers.\n\n&amp;#x200B;\n\nThank you!", "author_fullname": "t2_7gkhxmgl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would this method be considered ETL and is it worth putting on my resume?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zn8vv9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671175306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I currently work in a Microsoft SQL Server environment. I&amp;#39;m working on my resume right now so I can start applying for entry-level data engineer positions.&lt;/p&gt;\n\n&lt;p&gt;Currently, I extract data from the SQL database, implement them into Power BI, clean up the data(null values, bad data), add new columns if necessary, export to CSV and insert them back into the database.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if this method would be considered ETL which is why I&amp;#39;m asking you guys. If it&amp;#39;s considered ETL, then I have some experience where it might put me over the edge and give me more interviews. &lt;/p&gt;\n\n&lt;p&gt;Looking for advice and I don&amp;#39;t mind any blunt answers.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zn8vv9", "is_robot_indexable": true, "report_reasons": null, "author": "SnooWalruses7164", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zn8vv9/would_this_method_be_considered_etl_and_is_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zn8vv9/would_this_method_be_considered_etl_and_is_it/", "subreddit_subscribers": 83048, "created_utc": 1671175306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I know I am one of those people that had to create another DE bootcamp thread but I would love to get some advice on which DE bootcamp that would suit me the best for my next steps in my career.\n\nI am currently working full time as a Business Analysis Manager at a Fortune 500 tech company in the Bay Area for the last five years. In this role, I have been acting as a Swiss Army Knife of sorts being a half-PM and a half-DA specializing in developing data quality, integration, and automation solutions that I then pass onto a BI or a DS team to actually develop in production for us. Most of the data is related to sales and is often anchored by our CRM (SFDC) and with no-code intermediate solutions (such as LeanData or SFDC Flows). From time to time, I do prototype out custom scripts that could be used to fulfill some functionality (like connecting to Snowflake and SAP), but ultimately the ETL solutions I draft are pushed to actual DE's and Dev's to fully build out. I also maintain reports and dashboards on our data health and do the typical update meetings to our stakeholders.\n\nHowever in the past half a year, I have been wanting to go more towards the path of being a pure BI Analyst, AE, or DE. I have SQL and Tableau knowledge (as I still have to do analysis and prototyping) and some Python knowledge, but not enough to be a reliable scripter. Furthermore, I have no hands-on knowledge of deeper things like Data Lakehouses or Data Orchestration as my job does not require me to use those directly (again we got DE's and Dev's for that). Therefore, I am considering a bootcamp to help fill those gaps and hopefully get me caught up on the technical stack of my skills and sharp enough to pass technical interviews for future BI roles.\n\nCurrently, I am looking at three in particular, but each of them has a different approach to how they teach:\n\n1. Jigsaw Labs\n2. Data Engineering Camp\n3. WeCloudData\n\nJigsaw Labs: \n\nA six month course that teaches 'Data Engineering' from a SWE perspective of knowing how to write efficient and well designed Python and SQL code to properly do large-scale ETL processes and only in the second half of the course to focus on Cloud Computing (AWS) and Data Pipelines using Airflow, Fivetran and DBT. According to some of my friends in the DE and Data Infra space that this course is the most useful as the key to being a solid BI/AE/DE is knowing how to write effective Python code and SQL queries that follows a good data design and everything from there should come naturally.\n\nThis course is also the most expensive of the three and also includes an externship that you can work with an actual company or non-profit to apply the taught skills to build a real-world applied project and gives an additional nine months of post-course support, especially in sharpening your Python/SQL LC practice for those interviews.\n\nHowever, the course does not cover anything with Data Lakehouses or Data Streaming, I feel like the most you will be out of the course is a solid BI Analyst, but you will still need to cover the gaps of the rest on your own, which makes me question how much worth I will be for my career switch in the future to like an AE.\n\nData Engineering Camp:\n\nThey have been touted pretty well in this subreddit and the course founder is pretty active on this subreddit as well. His course is the shortest of the three and he seems to cover all of the key bits in the DE toolbox and expectations including Lakehouses (Spark, Databricks) and Streaming (Kafka). However, I am not sure all of this coverage would make me come out of it effective enough to be taken seriously for a career change without further supplementation. It sounds it is great if I wanted to be like a TPM that would work more with an AE/DE team though.\n\nAlso from a instruction POV, I will have to take classes between 2AM to 5AM since I am in the West Coast US and they are being taught from Australia.\n\nWeCloudData:\n\nThis one seems to be in the middle of Jigsaw Labs and DEC. It is six months and seems to cover all of the basis that Jigsaw and DEC is doing, but not in the detail on the Python/SQL front like Jigsaw. Therefore, I feel like this one will get me the good grasp of what I need to know as an AE/DE, but I am leery if it still lacks the best approach to give me the core foundations needed for my Python/SQL chops. They also provide six months of mentorship and career prep after the course but not externship if you do the part-time program.\n\nTherefore, I am torn if I trade off the philosophy of Jigsaw Labs to teach you the true fundamentals but at the expense of learning the higher-level tools/processes that is also needed, or do a more comprehensive curriculum like DEC or WeCloudData coming from my own experience and aspirations.\n\nAny suggestions is appreciated as I want to do the best bootcamp that would yield me the best foot forward for my next career move. I know a good number of you would tell me to don't bother with any of them and just self-learn (cheaper), but trust me, I learn the best by being instructed, especially if I want to be taught the foundations correctly.\n\nThx!", "author_fullname": "t2_b7b2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best DE Bootcamp based on my experience and objectives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zn1vhc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671152263.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I know I am one of those people that had to create another DE bootcamp thread but I would love to get some advice on which DE bootcamp that would suit me the best for my next steps in my career.&lt;/p&gt;\n\n&lt;p&gt;I am currently working full time as a Business Analysis Manager at a Fortune 500 tech company in the Bay Area for the last five years. In this role, I have been acting as a Swiss Army Knife of sorts being a half-PM and a half-DA specializing in developing data quality, integration, and automation solutions that I then pass onto a BI or a DS team to actually develop in production for us. Most of the data is related to sales and is often anchored by our CRM (SFDC) and with no-code intermediate solutions (such as LeanData or SFDC Flows). From time to time, I do prototype out custom scripts that could be used to fulfill some functionality (like connecting to Snowflake and SAP), but ultimately the ETL solutions I draft are pushed to actual DE&amp;#39;s and Dev&amp;#39;s to fully build out. I also maintain reports and dashboards on our data health and do the typical update meetings to our stakeholders.&lt;/p&gt;\n\n&lt;p&gt;However in the past half a year, I have been wanting to go more towards the path of being a pure BI Analyst, AE, or DE. I have SQL and Tableau knowledge (as I still have to do analysis and prototyping) and some Python knowledge, but not enough to be a reliable scripter. Furthermore, I have no hands-on knowledge of deeper things like Data Lakehouses or Data Orchestration as my job does not require me to use those directly (again we got DE&amp;#39;s and Dev&amp;#39;s for that). Therefore, I am considering a bootcamp to help fill those gaps and hopefully get me caught up on the technical stack of my skills and sharp enough to pass technical interviews for future BI roles.&lt;/p&gt;\n\n&lt;p&gt;Currently, I am looking at three in particular, but each of them has a different approach to how they teach:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Jigsaw Labs&lt;/li&gt;\n&lt;li&gt;Data Engineering Camp&lt;/li&gt;\n&lt;li&gt;WeCloudData&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Jigsaw Labs: &lt;/p&gt;\n\n&lt;p&gt;A six month course that teaches &amp;#39;Data Engineering&amp;#39; from a SWE perspective of knowing how to write efficient and well designed Python and SQL code to properly do large-scale ETL processes and only in the second half of the course to focus on Cloud Computing (AWS) and Data Pipelines using Airflow, Fivetran and DBT. According to some of my friends in the DE and Data Infra space that this course is the most useful as the key to being a solid BI/AE/DE is knowing how to write effective Python code and SQL queries that follows a good data design and everything from there should come naturally.&lt;/p&gt;\n\n&lt;p&gt;This course is also the most expensive of the three and also includes an externship that you can work with an actual company or non-profit to apply the taught skills to build a real-world applied project and gives an additional nine months of post-course support, especially in sharpening your Python/SQL LC practice for those interviews.&lt;/p&gt;\n\n&lt;p&gt;However, the course does not cover anything with Data Lakehouses or Data Streaming, I feel like the most you will be out of the course is a solid BI Analyst, but you will still need to cover the gaps of the rest on your own, which makes me question how much worth I will be for my career switch in the future to like an AE.&lt;/p&gt;\n\n&lt;p&gt;Data Engineering Camp:&lt;/p&gt;\n\n&lt;p&gt;They have been touted pretty well in this subreddit and the course founder is pretty active on this subreddit as well. His course is the shortest of the three and he seems to cover all of the key bits in the DE toolbox and expectations including Lakehouses (Spark, Databricks) and Streaming (Kafka). However, I am not sure all of this coverage would make me come out of it effective enough to be taken seriously for a career change without further supplementation. It sounds it is great if I wanted to be like a TPM that would work more with an AE/DE team though.&lt;/p&gt;\n\n&lt;p&gt;Also from a instruction POV, I will have to take classes between 2AM to 5AM since I am in the West Coast US and they are being taught from Australia.&lt;/p&gt;\n\n&lt;p&gt;WeCloudData:&lt;/p&gt;\n\n&lt;p&gt;This one seems to be in the middle of Jigsaw Labs and DEC. It is six months and seems to cover all of the basis that Jigsaw and DEC is doing, but not in the detail on the Python/SQL front like Jigsaw. Therefore, I feel like this one will get me the good grasp of what I need to know as an AE/DE, but I am leery if it still lacks the best approach to give me the core foundations needed for my Python/SQL chops. They also provide six months of mentorship and career prep after the course but not externship if you do the part-time program.&lt;/p&gt;\n\n&lt;p&gt;Therefore, I am torn if I trade off the philosophy of Jigsaw Labs to teach you the true fundamentals but at the expense of learning the higher-level tools/processes that is also needed, or do a more comprehensive curriculum like DEC or WeCloudData coming from my own experience and aspirations.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions is appreciated as I want to do the best bootcamp that would yield me the best foot forward for my next career move. I know a good number of you would tell me to don&amp;#39;t bother with any of them and just self-learn (cheaper), but trust me, I learn the best by being instructed, especially if I want to be taught the foundations correctly.&lt;/p&gt;\n\n&lt;p&gt;Thx!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zn1vhc", "is_robot_indexable": true, "report_reasons": null, "author": "StrikeSaber47", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zn1vhc/best_de_bootcamp_based_on_my_experience_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zn1vhc/best_de_bootcamp_based_on_my_experience_and/", "subreddit_subscribers": 83048, "created_utc": 1671152263.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, new here. Hope I can get some advices here so that I can get myself ready.\n\n**Background:** my friend works for a big engineering consulting company, its *Transport* section (data analytics ML team) will need to hire a *graduate/junior* data engineer early 2023. My friend gave me a direction to prepare (PS: not all of them), he also suggested to follow some Udemy courses and get familiar those tools with hands-on activities.\n\n1. SQL,\n2. Hadoop,\n3. Data Lake,\n4. Data Factory,\n5. different types of data storage\n6. data management\n7. knowledge about Data visualisation and cloud services also a bonus. (The company is using Azure)\n\nAt the end, he mentioned \"Of course not all of them\".\n\n\\----------------------------------------------------------------------------------------------------------------------------------------------\n\n**About myself:** 15 years in retail sales, have no IT work experience.\n\n1. Currently studying Postgraduate Diploma in Computer Science, enables me with some basic understanding of Python Numpy, Pandas, tensorflow, skit-learn, some modeling( SVM, RandomForest and others\u2026)\n2. I also self learnt Tableau Prep, Tableau desktop (Analyst Licence)\n3. plus, I am learning SQL from W3C and practicing SQL on Hacker Rank.\n4. I was certified with Azure fundamental AZ900.\n\nThis is me, a *newbie* trying to move into IT industry.\n\n\\----------------------------------------------------------------------------------------------------------------------------------------------\n\nCan some one kindly give me some detailed direction? I wanted to get myself prepared and so I can try my best to have this opportunity. My questions as follow:\n\n* Hadoop is part of Apache, dealing with big data. But, I have no idea what is it and how to get into it (need to have some Javascript coding skill?), *any suggestions?*\n* About Data lake/factory, as that company is using Azure but I am not sure how heavy the company is relying on cloud/Azure, *should I get myself certified with MS Azure Data Engineering - Azure Synapse?* (it is a more difficult certificate and I am not quite confident I could pass it in a short time-frame). - passed AZ900 within 2 weeks at 730/1000.\n* About SQL, as I am still at the beginner level, and I have been practicing everyday on Hacker Rank, I never touched any real-world project. Any suggestion for this?\n\nMany Thanks!", "author_fullname": "t2_fa8si", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advices please, preparing for Data Engineer role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zmydot", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671143631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, new here. Hope I can get some advices here so that I can get myself ready.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; my friend works for a big engineering consulting company, its &lt;em&gt;Transport&lt;/em&gt; section (data analytics ML team) will need to hire a &lt;em&gt;graduate/junior&lt;/em&gt; data engineer early 2023. My friend gave me a direction to prepare (PS: not all of them), he also suggested to follow some Udemy courses and get familiar those tools with hands-on activities.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;SQL,&lt;/li&gt;\n&lt;li&gt;Hadoop,&lt;/li&gt;\n&lt;li&gt;Data Lake,&lt;/li&gt;\n&lt;li&gt;Data Factory,&lt;/li&gt;\n&lt;li&gt;different types of data storage&lt;/li&gt;\n&lt;li&gt;data management&lt;/li&gt;\n&lt;li&gt;knowledge about Data visualisation and cloud services also a bonus. (The company is using Azure)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;At the end, he mentioned &amp;quot;Of course not all of them&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;----------------------------------------------------------------------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;About myself:&lt;/strong&gt; 15 years in retail sales, have no IT work experience.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Currently studying Postgraduate Diploma in Computer Science, enables me with some basic understanding of Python Numpy, Pandas, tensorflow, skit-learn, some modeling( SVM, RandomForest and others\u2026)&lt;/li&gt;\n&lt;li&gt;I also self learnt Tableau Prep, Tableau desktop (Analyst Licence)&lt;/li&gt;\n&lt;li&gt;plus, I am learning SQL from W3C and practicing SQL on Hacker Rank.&lt;/li&gt;\n&lt;li&gt;I was certified with Azure fundamental AZ900.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This is me, a &lt;em&gt;newbie&lt;/em&gt; trying to move into IT industry.&lt;/p&gt;\n\n&lt;p&gt;----------------------------------------------------------------------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;Can some one kindly give me some detailed direction? I wanted to get myself prepared and so I can try my best to have this opportunity. My questions as follow:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Hadoop is part of Apache, dealing with big data. But, I have no idea what is it and how to get into it (need to have some Javascript coding skill?), &lt;em&gt;any suggestions?&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;About Data lake/factory, as that company is using Azure but I am not sure how heavy the company is relying on cloud/Azure, &lt;em&gt;should I get myself certified with MS Azure Data Engineering - Azure Synapse?&lt;/em&gt; (it is a more difficult certificate and I am not quite confident I could pass it in a short time-frame). - passed AZ900 within 2 weeks at 730/1000.&lt;/li&gt;\n&lt;li&gt;About SQL, as I am still at the beginner level, and I have been practicing everyday on Hacker Rank, I never touched any real-world project. Any suggestion for this?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Many Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zmydot", "is_robot_indexable": true, "report_reasons": null, "author": "raulfanc", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zmydot/advices_please_preparing_for_data_engineer_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zmydot/advices_please_preparing_for_data_engineer_role/", "subreddit_subscribers": 83048, "created_utc": 1671143631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey,\nCan anyone please help me with  the compensation(base, bonus, stocks) details for Walmart?\n\nAlso, how is the work life balance for Walmart New Ventures team for USA and Canada locations?\n\nAny insight is appreciated.", "author_fullname": "t2_eizebsp9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Walmart compensation and wlb?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_znae3d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671181622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,\nCan anyone please help me with  the compensation(base, bonus, stocks) details for Walmart?&lt;/p&gt;\n\n&lt;p&gt;Also, how is the work life balance for Walmart New Ventures team for USA and Canada locations?&lt;/p&gt;\n\n&lt;p&gt;Any insight is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "znae3d", "is_robot_indexable": true, "report_reasons": null, "author": "_DarthVader__", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/znae3d/walmart_compensation_and_wlb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/znae3d/walmart_compensation_and_wlb/", "subreddit_subscribers": 83048, "created_utc": 1671181622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BLOG: Connecting Tableau to Apache Iceberg Tables with Dremio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_zmwcdk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/n2zACMZcLFc7Yyr5acAv9WvSMILDCJpYgQxkkRYzRxw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671138978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dremio.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dremio.com/blog/connecting-tableau-to-apache-iceberg-tables-with-dremio/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p2f2MgfF8XkhjGFzcRhWjbUx6pPKXWggmmF1-cxoII0.jpg?auto=webp&amp;s=07395959a9e40cef289185fc8fc8f2de3c3c7384", "width": 3335, "height": 1746}, "resolutions": [{"url": "https://external-preview.redd.it/p2f2MgfF8XkhjGFzcRhWjbUx6pPKXWggmmF1-cxoII0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23fc86b64cc793fea85bf35d768224974d94d5a8", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/p2f2MgfF8XkhjGFzcRhWjbUx6pPKXWggmmF1-cxoII0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=12267436b3fd6b9e559eb9a59b8fd0fee8d28250", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/p2f2MgfF8XkhjGFzcRhWjbUx6pPKXWggmmF1-cxoII0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26376c12ec271f8fb4a22789460159b2965d2a17", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/p2f2MgfF8XkhjGFzcRhWjbUx6pPKXWggmmF1-cxoII0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6285894d5eb79b1e4e9d09d88ef8b0bdbad884d2", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/p2f2MgfF8XkhjGFzcRhWjbUx6pPKXWggmmF1-cxoII0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6998a0c75a0dddfae84227363057efad156f57a0", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/p2f2MgfF8XkhjGFzcRhWjbUx6pPKXWggmmF1-cxoII0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af42416ec5807f27152476e65b9737426b4d3b30", "width": 1080, "height": 565}], "variants": {}, "id": "pD6NcajLmujbwwIiT-EEafDBEgp11MJoeW3AhDUx5g0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zmwcdk", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zmwcdk/blog_connecting_tableau_to_apache_iceberg_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dremio.com/blog/connecting-tableau-to-apache-iceberg-tables-with-dremio/", "subreddit_subscribers": 83048, "created_utc": 1671138978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For some context, I work on a small-medium sized company as a junior BI developer, on a product team. I have little to none opening with the IT team (which does everything including \"Data Engineering\") because the developers can't be messaged or can't talk with anyone else on the company besides themselves (weird and red flag, but ok).\n\nI use Apache Superset to query and build charts/dashboards. I'm not 100% sure about the data structure we have so I will be giving some estimates here. The database which I use is a copy of the original transactional database, and it even has some reports which use/consume this copy on production environment for god's sake (and that I wasn't told before an incident - I ran a query which was too heavy for the DB and it dropped out, and once just by LOADING a dashboard it also dropped out). I'm not sure what our ETL or ELT is, they won't tell me. I was hoping to do something about this (have been studying Airbyte, dbt and some Data Warehouses) but I'm still too new on this field.\n\nMy questions are:\n\n1- Why a copy of the transactional DB shoud not be used, in terms of performance and on database theory, what is the difference between running a COUNT, a SUM, on transactional and analytical databases? Does it has to do with relational X columnar databases?\n\n2- A data engineering on tech team should be able to solve this issue? What would he implement, a well structured Data Warehouse? A better ETL/ELT process?\n\n3- Is the database dropping my fault?", "author_fullname": "t2_eoiu7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why copy of transactional databases shouldn't be used (conceptually)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zmu60m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671133626.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For some context, I work on a small-medium sized company as a junior BI developer, on a product team. I have little to none opening with the IT team (which does everything including &amp;quot;Data Engineering&amp;quot;) because the developers can&amp;#39;t be messaged or can&amp;#39;t talk with anyone else on the company besides themselves (weird and red flag, but ok).&lt;/p&gt;\n\n&lt;p&gt;I use Apache Superset to query and build charts/dashboards. I&amp;#39;m not 100% sure about the data structure we have so I will be giving some estimates here. The database which I use is a copy of the original transactional database, and it even has some reports which use/consume this copy on production environment for god&amp;#39;s sake (and that I wasn&amp;#39;t told before an incident - I ran a query which was too heavy for the DB and it dropped out, and once just by LOADING a dashboard it also dropped out). I&amp;#39;m not sure what our ETL or ELT is, they won&amp;#39;t tell me. I was hoping to do something about this (have been studying Airbyte, dbt and some Data Warehouses) but I&amp;#39;m still too new on this field.&lt;/p&gt;\n\n&lt;p&gt;My questions are:&lt;/p&gt;\n\n&lt;p&gt;1- Why a copy of the transactional DB shoud not be used, in terms of performance and on database theory, what is the difference between running a COUNT, a SUM, on transactional and analytical databases? Does it has to do with relational X columnar databases?&lt;/p&gt;\n\n&lt;p&gt;2- A data engineering on tech team should be able to solve this issue? What would he implement, a well structured Data Warehouse? A better ETL/ELT process?&lt;/p&gt;\n\n&lt;p&gt;3- Is the database dropping my fault?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zmu60m", "is_robot_indexable": true, "report_reasons": null, "author": "ModaFaca", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zmu60m/why_copy_of_transactional_databases_shouldnt_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zmu60m/why_copy_of_transactional_databases_shouldnt_be/", "subreddit_subscribers": 83048, "created_utc": 1671133626.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_sa3mbz4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Re-evaluating Kafka: issues and alternatives for real-time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "name": "t3_zmu0bk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.54, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ucy8AO7ZYzrs-fHBBGKwpWguPP4MGYLpp_11Y3q6o3k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671133234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "towardsdatascience.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://towardsdatascience.com/re-evaluating-kafka-issues-and-alternatives-for-real-time-395573418f27?utm_source=Reddit&amp;utm_medium=Reddit&amp;utm_campaign=reddit&amp;utm_id=Kafka-r", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6h1xwSphjfB8GYCJ9dxklG90G4Fwzd5oKbuFDZZPBQo.jpg?auto=webp&amp;s=6138c3be5fa586a983ff85042ef3b1fb0829f419", "width": 1200, "height": 742}, "resolutions": [{"url": "https://external-preview.redd.it/6h1xwSphjfB8GYCJ9dxklG90G4Fwzd5oKbuFDZZPBQo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e121b3cfe85826b1b852b0b687dfd6f65800d1d7", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/6h1xwSphjfB8GYCJ9dxklG90G4Fwzd5oKbuFDZZPBQo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c49b7d41bb4daea420154e0785502f2a0d917db0", "width": 216, "height": 133}, {"url": "https://external-preview.redd.it/6h1xwSphjfB8GYCJ9dxklG90G4Fwzd5oKbuFDZZPBQo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b25d461aaf3bba913b1a3b7b1c91d4f0a68a77b", "width": 320, "height": 197}, {"url": "https://external-preview.redd.it/6h1xwSphjfB8GYCJ9dxklG90G4Fwzd5oKbuFDZZPBQo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=46562d3e012c38e2ba644fe4b457b265cdae2590", "width": 640, "height": 395}, {"url": "https://external-preview.redd.it/6h1xwSphjfB8GYCJ9dxklG90G4Fwzd5oKbuFDZZPBQo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=877d8352076ee84a4902f851e03052d9607c4b49", "width": 960, "height": 593}, {"url": "https://external-preview.redd.it/6h1xwSphjfB8GYCJ9dxklG90G4Fwzd5oKbuFDZZPBQo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0706cfc7d313f37e5fc182bd267ecdaf10ba712f", "width": 1080, "height": 667}], "variants": {}, "id": "9hYkfLEnanmo1iVgYZcYMncPYrmbD_Mo1WxIq2SckDA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zmu0bk", "is_robot_indexable": true, "report_reasons": null, "author": "MooJerseyCreamery", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zmu0bk/reevaluating_kafka_issues_and_alternatives_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://towardsdatascience.com/re-evaluating-kafka-issues-and-alternatives-for-real-time-395573418f27?utm_source=Reddit&amp;utm_medium=Reddit&amp;utm_campaign=reddit&amp;utm_id=Kafka-r", "subreddit_subscribers": 83048, "created_utc": 1671133234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "You can detect fraud or other irregularities in your data warehousing by looking for patterns in the data that don't seem to make sense, such as if a company is seeing a lot of orders from a certain country, but the orders do not seem to be legitimate. Another way to detect fraud is to look for sudden changes in the data: if a company's sales suddenly increase by a large amount, this could be a sign that someone is trying to fraudulently inflate the company's sales figures. Finally, you can use data warehousing to detect fraud by comparing the data from different sources: if your accounting system reports that your sales figures are higher than they actually are, but your data warehouse shows them lower than they actually are\u2014or vice versa\u2014this could be a sign of fraud.", "author_fullname": "t2_gk917caz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Detecting fraud or other irregularities with Data Warehousing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zn0mgd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671148815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You can detect fraud or other irregularities in your data warehousing by looking for patterns in the data that don&amp;#39;t seem to make sense, such as if a company is seeing a lot of orders from a certain country, but the orders do not seem to be legitimate. Another way to detect fraud is to look for sudden changes in the data: if a company&amp;#39;s sales suddenly increase by a large amount, this could be a sign that someone is trying to fraudulently inflate the company&amp;#39;s sales figures. Finally, you can use data warehousing to detect fraud by comparing the data from different sources: if your accounting system reports that your sales figures are higher than they actually are, but your data warehouse shows them lower than they actually are\u2014or vice versa\u2014this could be a sign of fraud.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zn0mgd", "is_robot_indexable": true, "report_reasons": null, "author": "joyful_reader", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zn0mgd/detecting_fraud_or_other_irregularities_with_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zn0mgd/detecting_fraud_or_other_irregularities_with_data/", "subreddit_subscribers": 83048, "created_utc": 1671148815.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}