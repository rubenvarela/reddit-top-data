{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So my organization has had O365 with the power platform for a few years with a handful of individuals getting really good at treating SharePoint as a database for there power apps(\"front-end\") and power bi reports with power automate work around. The company doesn't want to invest in a proper database like SQL server or full Datavers.\n\nI think this is a recipe for disaster and would like to get options for or against continue using SharePoint as a \"database\".", "author_fullname": "t2_8yface1u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organization wants to use SharePoint as a \"database\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsprzt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671725543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So my organization has had O365 with the power platform for a few years with a handful of individuals getting really good at treating SharePoint as a database for there power apps(&amp;quot;front-end&amp;quot;) and power bi reports with power automate work around. The company doesn&amp;#39;t want to invest in a proper database like SQL server or full Datavers.&lt;/p&gt;\n\n&lt;p&gt;I think this is a recipe for disaster and would like to get options for or against continue using SharePoint as a &amp;quot;database&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zsprzt", "is_robot_indexable": true, "report_reasons": null, "author": "Benmagz", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsprzt/organization_wants_to_use_sharepoint_as_a_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsprzt/organization_wants_to_use_sharepoint_as_a_database/", "subreddit_subscribers": 83869, "created_utc": 1671725543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I have been reading Martin Kleppmann's Designing Data-Intensive Applications (DDIA), watching his lecture on YouTube, and reading a few other courses' slides and articles on distributed systems.\n\nI have a bunch of questions specifically regarding storage engines, replication, and consensus, but I'm not sure I can state them as clearly as I'd like. I have tried googling them but am not satisfied.\n\nI was wondering if I could throw my thoughts and questions to someone who has read and is confident about the fundamentals of databases and distributed systems as presented in DDIA.", "author_fullname": "t2_1nsaaq48", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone I can DM about Designing Data Intensive Applications and/or distributed systems?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsm4q2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671728177.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671715824.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I have been reading Martin Kleppmann&amp;#39;s Designing Data-Intensive Applications (DDIA), watching his lecture on YouTube, and reading a few other courses&amp;#39; slides and articles on distributed systems.&lt;/p&gt;\n\n&lt;p&gt;I have a bunch of questions specifically regarding storage engines, replication, and consensus, but I&amp;#39;m not sure I can state them as clearly as I&amp;#39;d like. I have tried googling them but am not satisfied.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if I could throw my thoughts and questions to someone who has read and is confident about the fundamentals of databases and distributed systems as presented in DDIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zsm4q2", "is_robot_indexable": true, "report_reasons": null, "author": "FantasticGrape", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsm4q2/anyone_i_can_dm_about_designing_data_intensive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsm4q2/anyone_i_can_dm_about_designing_data_intensive/", "subreddit_subscribers": 83869, "created_utc": 1671715824.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys so I just graduated and I wanna jumpstart my career in data! I\u2019m a bit anxious about my resume specially because of all the time frames. 2020 was quite rough and I had to delay my graduation :( On the upside, I got an early entrance into my Masters program this year. I don\u2019t know how hiring managers will react to this as well as the rest of my CV. Any suggestions at all are greatly appreciated (: thanks", "author_fullname": "t2_jqznxjjy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with resume! Just graduated (:", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zsfsqf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": "transparent", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NPCQq7GqBJT-egQ5IANNtWSTfq2h70H_QHeACo_CduA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671694184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys so I just graduated and I wanna jumpstart my career in data! I\u2019m a bit anxious about my resume specially because of all the time frames. 2020 was quite rough and I had to delay my graduation :( On the upside, I got an early entrance into my Masters program this year. I don\u2019t know how hiring managers will react to this as well as the rest of my CV. Any suggestions at all are greatly appreciated (: thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/lcid78131g7a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/lcid78131g7a1.jpg?auto=webp&amp;s=27285ff9a5c374e09a742353327396e6d38de89c", "width": 1125, "height": 1473}, "resolutions": [{"url": "https://preview.redd.it/lcid78131g7a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eb8032ff6aa43102c741db23d859a84adf7d6a3c", "width": 108, "height": 141}, {"url": "https://preview.redd.it/lcid78131g7a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=632336463c3f31c24488bf7e39732d72f8d47798", "width": 216, "height": 282}, {"url": "https://preview.redd.it/lcid78131g7a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ed5438cf3c4a4d86a2e7b8f2bc1abd8ef795abf", "width": 320, "height": 418}, {"url": "https://preview.redd.it/lcid78131g7a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=62f3deae97604ea098f2a28019f31674695cf85c", "width": 640, "height": 837}, {"url": "https://preview.redd.it/lcid78131g7a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e863378aedee8785c4840504763ccf16a573d91", "width": 960, "height": 1256}, {"url": "https://preview.redd.it/lcid78131g7a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=403872f811113edaa469194fd2a78dc5908d5b82", "width": 1080, "height": 1414}], "variants": {}, "id": "CVpR-GgVXeO2ojIzyTRtCXsdtucIWKrWXtIPVWUEDWs"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "zsfsqf", "is_robot_indexable": true, "report_reasons": null, "author": "D1N4D4N1", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/zsfsqf/help_with_resume_just_graduated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/lcid78131g7a1.jpg", "subreddit_subscribers": 83869, "created_utc": 1671694184.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all.\n\n&amp;#x200B;\n\nI am fairly new to the concept of testing my code. I just did some learning on how to use Pytest, but problem was that all the examples in the learning material was relatively simplistic and self-contained. When I try to apply it to my own code, I find myself wondering if the test **even make sense.**\n\n&amp;#x200B;\n\nI want to test the following piece of code that utilized AWS services (simplifed for ease of reading - full code can be seen at bottem):\n\n&amp;#x200B;\n\n    def download_file-bucket(bucket, obj, key):\n    connect_to_s3\n        download_csvfile_to_tmp_directory\n    \n    def parse_data_and_send:\n        parse_rows_form_csv\n        for row in csv:\n            payload = row.payload\n            send_to_sqs_queue(payload)\n    \n    def send_to_sqs_queue(payload):\n        connect_to_sqs\n        sqs.sendMessage(payload, connection)\n\nThat is the gist of it.\n\n&amp;#x200B;\n\nThe entire project is set up in Terraform using CodeBuild as the CI/CD tool, and I would like to run these unit test every time the code is deploying.\n\n&amp;#x200B;\n\n**My problem is this:**\n\nDoes it make sense to make unit tests here? I can test each function sure, but all of the functions rely on some AWS ressource that **may or may not exist**, depending on where in the development cycle I am in.\n\n&amp;#x200B;\n\nEven if it does exist, testing it on the real ressources would create a set of new problems, as f.eks. adding a file to the s3 bucket triggers code which will genererate new files etc.\n\n&amp;#x200B;\n\n**Two possible ways out I have considered:**\n\n1. Add a \"tests\" module in the Terraform deployment which is identical to the original ressources, but the ressources will only exist for the duration of the tests, and which will the tear-down when tests are concluded. However, this seems excessive\n2. Mock the outside dependencies like S3 bucket or sqs-queue. However, I feel this invalidates tests, as pretty much all the tests do are test if these outside dependencies work.\n\n&amp;#x200B;\n\nHow would you guys handle this situation?\n\n&amp;#x200B;\n\n*Real code:*\n\n    import os\n    import boto3\n    import logging\n    import pandas as pd\n    from distutils.log import error\n    import datetime\n    \n    # Set up logging\n    logging.basicConfig()\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    \n    \n    def s3_download_file(bucket, obj, dest):\n        s3 = session.client(\"s3\")\n    \n        try:\n            s3.download_file(\n                Bucket=bucket,\n                Key=obj,\n                Filename=dest\n            )\n        except:\n            logger.info(f\"Error downloading {obj} to {dest}...\")\n            logger.info(error)\n        else:\n            logger.info(f\"Successfully downloaded {obj} to {dest}...\")\n    \n    \n    def send_to_sqs(payload):\n        sqs = session.client(\"sqs\")\n        ts = datetime.datetime.utcnow().isoformat()\n        Queue_Url = os.environ[\"queue_url\"]\n        response = sqs.send_message(\n            QueueUrl=Queue_Url,\n            MessageBody=payload,\n            MessageAttributes={\n                \"_MessageSent\": {\n                    \"DataType\": \"String\",\n                    \"StringValue\": ts\n                }\n            }\n        )\n        id = response[\"MessageId\"]\n        httpStatus = response[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n        logger.info(f\"Message {id} sent with status: {httpStatus}...\")\n    \n    \n    def handler(event, context):\n        # Fish out bucket and object name from event payload\n        bucket = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]\n        obj = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]\n        logger.info(f\"Get {obj} from {bucket}...\")\n        dest = os.path.abspath(os.path.join(cwd, tmpfiles, obj))\n    \n        # Log event received\n        logger.info(\"Received event...\")\n        logger.info(event)\n    \n        # Download file\n        s3_download_file(bucket, obj, dest)\n    \n        # Eventify data\n        df = pd.read_csv(dest)\n        rows = len(df.index)\n    \n        for i in range(0, rows):\n            tmp_df = df\n            row = tmp_df.iloc[[i], :]\n    \n            # Convert to JSON\n            payload = row.to_json(orient=\"records\")\n            send_to_sqs(payload)\n    \n    \n    ### START OF CODE ###\n    cwd = os.getcwd()\n    parent = os.path.join(cwd, os.pardir)\n    grandparent = os.path.join(parent, os.pardir)\n    tmpfiles = os.path.join(grandparent, \"tmp\")\n    \n    session = boto3.Session(region_name=\"eu-central-1\")", "author_fullname": "t2_onmeo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do unit tests make sense here?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsl60x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671712712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am fairly new to the concept of testing my code. I just did some learning on how to use Pytest, but problem was that all the examples in the learning material was relatively simplistic and self-contained. When I try to apply it to my own code, I find myself wondering if the test &lt;strong&gt;even make sense.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I want to test the following piece of code that utilized AWS services (simplifed for ease of reading - full code can be seen at bottem):&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def download_file-bucket(bucket, obj, key):\nconnect_to_s3\n    download_csvfile_to_tmp_directory\n\ndef parse_data_and_send:\n    parse_rows_form_csv\n    for row in csv:\n        payload = row.payload\n        send_to_sqs_queue(payload)\n\ndef send_to_sqs_queue(payload):\n    connect_to_sqs\n    sqs.sendMessage(payload, connection)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;That is the gist of it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The entire project is set up in Terraform using CodeBuild as the CI/CD tool, and I would like to run these unit test every time the code is deploying.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My problem is this:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Does it make sense to make unit tests here? I can test each function sure, but all of the functions rely on some AWS ressource that &lt;strong&gt;may or may not exist&lt;/strong&gt;, depending on where in the development cycle I am in.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Even if it does exist, testing it on the real ressources would create a set of new problems, as f.eks. adding a file to the s3 bucket triggers code which will genererate new files etc.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Two possible ways out I have considered:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Add a &amp;quot;tests&amp;quot; module in the Terraform deployment which is identical to the original ressources, but the ressources will only exist for the duration of the tests, and which will the tear-down when tests are concluded. However, this seems excessive&lt;/li&gt;\n&lt;li&gt;Mock the outside dependencies like S3 bucket or sqs-queue. However, I feel this invalidates tests, as pretty much all the tests do are test if these outside dependencies work.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How would you guys handle this situation?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Real code:&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import os\nimport boto3\nimport logging\nimport pandas as pd\nfrom distutils.log import error\nimport datetime\n\n# Set up logging\nlogging.basicConfig()\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n\ndef s3_download_file(bucket, obj, dest):\n    s3 = session.client(&amp;quot;s3&amp;quot;)\n\n    try:\n        s3.download_file(\n            Bucket=bucket,\n            Key=obj,\n            Filename=dest\n        )\n    except:\n        logger.info(f&amp;quot;Error downloading {obj} to {dest}...&amp;quot;)\n        logger.info(error)\n    else:\n        logger.info(f&amp;quot;Successfully downloaded {obj} to {dest}...&amp;quot;)\n\n\ndef send_to_sqs(payload):\n    sqs = session.client(&amp;quot;sqs&amp;quot;)\n    ts = datetime.datetime.utcnow().isoformat()\n    Queue_Url = os.environ[&amp;quot;queue_url&amp;quot;]\n    response = sqs.send_message(\n        QueueUrl=Queue_Url,\n        MessageBody=payload,\n        MessageAttributes={\n            &amp;quot;_MessageSent&amp;quot;: {\n                &amp;quot;DataType&amp;quot;: &amp;quot;String&amp;quot;,\n                &amp;quot;StringValue&amp;quot;: ts\n            }\n        }\n    )\n    id = response[&amp;quot;MessageId&amp;quot;]\n    httpStatus = response[&amp;quot;ResponseMetadata&amp;quot;][&amp;quot;HTTPStatusCode&amp;quot;]\n    logger.info(f&amp;quot;Message {id} sent with status: {httpStatus}...&amp;quot;)\n\n\ndef handler(event, context):\n    # Fish out bucket and object name from event payload\n    bucket = event[&amp;quot;Records&amp;quot;][0][&amp;quot;s3&amp;quot;][&amp;quot;bucket&amp;quot;][&amp;quot;name&amp;quot;]\n    obj = event[&amp;quot;Records&amp;quot;][0][&amp;quot;s3&amp;quot;][&amp;quot;object&amp;quot;][&amp;quot;key&amp;quot;]\n    logger.info(f&amp;quot;Get {obj} from {bucket}...&amp;quot;)\n    dest = os.path.abspath(os.path.join(cwd, tmpfiles, obj))\n\n    # Log event received\n    logger.info(&amp;quot;Received event...&amp;quot;)\n    logger.info(event)\n\n    # Download file\n    s3_download_file(bucket, obj, dest)\n\n    # Eventify data\n    df = pd.read_csv(dest)\n    rows = len(df.index)\n\n    for i in range(0, rows):\n        tmp_df = df\n        row = tmp_df.iloc[[i], :]\n\n        # Convert to JSON\n        payload = row.to_json(orient=&amp;quot;records&amp;quot;)\n        send_to_sqs(payload)\n\n\n### START OF CODE ###\ncwd = os.getcwd()\nparent = os.path.join(cwd, os.pardir)\ngrandparent = os.path.join(parent, os.pardir)\ntmpfiles = os.path.join(grandparent, &amp;quot;tmp&amp;quot;)\n\nsession = boto3.Session(region_name=&amp;quot;eu-central-1&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zsl60x", "is_robot_indexable": true, "report_reasons": null, "author": "Hinkakan", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsl60x/do_unit_tests_make_sense_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsl60x/do_unit_tests_make_sense_here/", "subreddit_subscribers": 83869, "created_utc": 1671712712.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "NeetCode made a roadmap for his LeetCode videos - the video highlights the Algorithm Roadmap. \n\nhttps://youtu.be/jgQjes7MgTM", "author_fullname": "t2_lsot44zd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free NeetCode roadmap - Algorithms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsukex", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671737303.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;NeetCode made a roadmap for his LeetCode videos - the video highlights the Algorithm Roadmap. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/jgQjes7MgTM\"&gt;https://youtu.be/jgQjes7MgTM&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wh4UR_E4G9SSIIfDGkJMM8UPDIuSTXHQr4fWty7NTEQ.jpg?auto=webp&amp;s=7af2d1c84a68ef4d260a9fc94fd7e4a22aed9ab9", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/wh4UR_E4G9SSIIfDGkJMM8UPDIuSTXHQr4fWty7NTEQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3251676dc9d60254361f4f27df1a920b321bce85", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/wh4UR_E4G9SSIIfDGkJMM8UPDIuSTXHQr4fWty7NTEQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ee974a3cb7eaf9c08dd898294cf224c71c95422c", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/wh4UR_E4G9SSIIfDGkJMM8UPDIuSTXHQr4fWty7NTEQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72583e12dc86b7cf1bef58c95e8954388f1e4a25", "width": 320, "height": 240}], "variants": {}, "id": "dHs2aasnZvVnQ0e37PkJe5xmYOS4rdJ-gSCFupc8ttI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "zsukex", "is_robot_indexable": true, "report_reasons": null, "author": "FunkMasterDraven", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsukex/free_neetcode_roadmap_algorithms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsukex/free_neetcode_roadmap_algorithms/", "subreddit_subscribers": 83869, "created_utc": 1671737303.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm sure there are quite a lot of data engineers here that work with IoT data of different kinds (connected factories, remote sensing etc.). I'm curious what your tech stack looks like and what you think of it, could it be architected better if you hade the time and money?", "author_fullname": "t2_8ay2350q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IoT data engineers, what does your tech stack look like?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsuefu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671736875.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sure there are quite a lot of data engineers here that work with IoT data of different kinds (connected factories, remote sensing etc.). I&amp;#39;m curious what your tech stack looks like and what you think of it, could it be architected better if you hade the time and money?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zsuefu", "is_robot_indexable": true, "report_reasons": null, "author": "rosenloev", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsuefu/iot_data_engineers_what_does_your_tech_stack_look/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsuefu/iot_data_engineers_what_does_your_tech_stack_look/", "subreddit_subscribers": 83869, "created_utc": 1671736875.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "DDIA is the data system design bible but I'm curious who has read it. Is it for junior DE folks starting up, or data architects who are usually involved in big data system design, and so on.  \n\n\nIf you have read it, what do you work as? When did you read it?\n\n[View Poll](https://www.reddit.com/poll/zss3om)", "author_fullname": "t2_msviuy1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you read DDIA (Designing Data Intensive Applications) by Kleppmann?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zss3om", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671731157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;DDIA is the data system design bible but I&amp;#39;m curious who has read it. Is it for junior DE folks starting up, or data architects who are usually involved in big data system design, and so on.  &lt;/p&gt;\n\n&lt;p&gt;If you have read it, what do you work as? When did you read it?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/zss3om\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zss3om", "is_robot_indexable": true, "report_reasons": null, "author": "vino_and_data", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1671990357117, "options": [{"text": "I'm a Junior DE, and have read it", "id": "20554181"}, {"text": "I'm a Mid/Senior DE and have read it", "id": "20554182"}, {"text": "I'm at Data Architect level or above, and have read it", "id": "20554183"}, {"text": "I work as DE (Junior/Mid/Senior) but never read it or planning to read", "id": "20554184"}, {"text": "Just here to see the results", "id": "20554185"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 209, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zss3om/have_you_read_ddia_designing_data_intensive/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/zss3om/have_you_read_ddia_designing_data_intensive/", "subreddit_subscribers": 83869, "created_utc": 1671731157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some questions about databases and distributed systems as presented in Martin Kleppmann's Designing Data-Intensive Applications. My questions relate to storage engines, indices, replication, consistency, and consensus. I will try to frame them so that people who haven't read the book can still attempt to answer.\n\nFor some context, in Chapter 3 (Storage and Retrieval), Kleppmann talks about the \"Data Structures That Power Your Database.\" These include hash indexes; SSTables and LSM-Trees; and B-Trees. In Chapter 9 (Consistency and Consensus), Kleppmann talks about consistency models like linearizability, consensus, and more.\n\n&amp;#x200B;\n\n(1) Are SSTables the only way to store the actual database records/rows? Kleppmann's writing suggests it is. I think Postgres stores row/record data in what is called a heap file. The heap file does not have any sort of order, so it obviously isn't comparable to SSTables.\n\nI am just wondering why Kleppmann makes it seem as though SSTables are the only way rows/records are being stored.\n\n&amp;#x200B;\n\n(2) Just to confirm, do LSM-Trees and B-Trees store the index data or the row/record data? I believe they store the index data because Kleppmann uses terms like \"log-structured *index*\" (and not log-structured data storage). And, the SSTable stores the database records/rows, right?\n\nAny additional clarification between where/how the index data is stored versus the actual database's rows/records would be nice.\n\n&amp;#x200B;\n\n(3) In Chapter 9, Kleppmann says that multi-leader and leaderless replication schemes (in databases) are not linearizable and probably not linearizable, respectively. However, the example he gives to motivate linearizability seems to require a multi-leader or leaderless scheme.\n\nSpecifically, he talks about how to ensure linearizability when two people are trying to create an account on some application with the same username at the same time. We want to ensure that this process is linearizable so that we don't end up creating two accounts with the same username.\n\nBut, this necessitates that writes are happening on two nodes! So, why is Kleppmann giving an example that requires a multi-leader or leaderless scheme when he's told us previously that they're (probably) not linearizable?\n\n&amp;#x200B;\n\n(4a) What are the different ways that one can achieve linearizability? Here is what I think, after reading Chapter 9:\n\nFirst, with a single-leader, asynchronous replication scheme, you must send all reads and writes to the leader to ensure linearizability. This means the followers are basically only there for fault tolerance/availability (i.e. to replace the leader in case it goes down). This is a big drawback because we initially talked about how most applications are read-heavy, so the ability to read from any follower in a single leader *was* valuable. But, you can't do this if you want linearizability.\n\nSecond, with a leaderless replication scheme, you can achieve linearizability using total order broadcast. Specifically, you can send a request to any node and that node will do a total-order broadcast to every other node declaring what it wants to do in a global log. You can read or write from any node.\n\nThird, with a leaderless replication scheme, I think you can also use atomic commits where a single node reaches out to a coordinator; the coordinator uses a 2-Phase-Commit to ensure all nodes execute that node's request. This is similar to the previous method in that you have to contact every other node. You can read or write from any node. Kleppmann doesn't say you can use this to achieve linearizability, but I think this works?\n\nFourth, you can use a consensus algorithm like Raft or Paxos and a majority quorum to agree on the next request. You can read or write from any node.\n\nFifth, you can use a consensus algorithm like Raft or Paxos and, instead of using a majority quorum, sequence your operations through a single, elected leader (i.e. every node sends its read or writes through a chosen leader). I guess you can read or write from any node, but technically it all goes to a single leader.\n\n(4b) Relating to the first method above, Kleppmann says \"As discussed, single-leader replication determines a total order of operations by choosing one node as the leader and sequencing all operations on a single CPU core on the leader. The challenge then is **how to scale the system if the throughput is greater than a single leader can handle**, and also how to handle failover if the leader fails...\" I don't think he addresses the bolded part. How can a method that relies on a single leader scale?\n\nI have glossed over some important details, e.g. relating to fault tolerance, above. For instance, I don't think the second method is fault-tolerant since you need to ensure every node receives the message, so you're screwed if a single node goes down.\n\nThese questions are pretty specific, and they might not be the clearest, so please share any questions about my questions or the information I presented. Everything is useful to me.", "author_fullname": "t2_1nsaaq48", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about Designing Data-Intensive Applications, databases, and distributed systems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsqsid", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1671728585.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671728054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some questions about databases and distributed systems as presented in Martin Kleppmann&amp;#39;s Designing Data-Intensive Applications. My questions relate to storage engines, indices, replication, consistency, and consensus. I will try to frame them so that people who haven&amp;#39;t read the book can still attempt to answer.&lt;/p&gt;\n\n&lt;p&gt;For some context, in Chapter 3 (Storage and Retrieval), Kleppmann talks about the &amp;quot;Data Structures That Power Your Database.&amp;quot; These include hash indexes; SSTables and LSM-Trees; and B-Trees. In Chapter 9 (Consistency and Consensus), Kleppmann talks about consistency models like linearizability, consensus, and more.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(1) Are SSTables the only way to store the actual database records/rows? Kleppmann&amp;#39;s writing suggests it is. I think Postgres stores row/record data in what is called a heap file. The heap file does not have any sort of order, so it obviously isn&amp;#39;t comparable to SSTables.&lt;/p&gt;\n\n&lt;p&gt;I am just wondering why Kleppmann makes it seem as though SSTables are the only way rows/records are being stored.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(2) Just to confirm, do LSM-Trees and B-Trees store the index data or the row/record data? I believe they store the index data because Kleppmann uses terms like &amp;quot;log-structured &lt;em&gt;index&lt;/em&gt;&amp;quot; (and not log-structured data storage). And, the SSTable stores the database records/rows, right?&lt;/p&gt;\n\n&lt;p&gt;Any additional clarification between where/how the index data is stored versus the actual database&amp;#39;s rows/records would be nice.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(3) In Chapter 9, Kleppmann says that multi-leader and leaderless replication schemes (in databases) are not linearizable and probably not linearizable, respectively. However, the example he gives to motivate linearizability seems to require a multi-leader or leaderless scheme.&lt;/p&gt;\n\n&lt;p&gt;Specifically, he talks about how to ensure linearizability when two people are trying to create an account on some application with the same username at the same time. We want to ensure that this process is linearizable so that we don&amp;#39;t end up creating two accounts with the same username.&lt;/p&gt;\n\n&lt;p&gt;But, this necessitates that writes are happening on two nodes! So, why is Kleppmann giving an example that requires a multi-leader or leaderless scheme when he&amp;#39;s told us previously that they&amp;#39;re (probably) not linearizable?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(4a) What are the different ways that one can achieve linearizability? Here is what I think, after reading Chapter 9:&lt;/p&gt;\n\n&lt;p&gt;First, with a single-leader, asynchronous replication scheme, you must send all reads and writes to the leader to ensure linearizability. This means the followers are basically only there for fault tolerance/availability (i.e. to replace the leader in case it goes down). This is a big drawback because we initially talked about how most applications are read-heavy, so the ability to read from any follower in a single leader &lt;em&gt;was&lt;/em&gt; valuable. But, you can&amp;#39;t do this if you want linearizability.&lt;/p&gt;\n\n&lt;p&gt;Second, with a leaderless replication scheme, you can achieve linearizability using total order broadcast. Specifically, you can send a request to any node and that node will do a total-order broadcast to every other node declaring what it wants to do in a global log. You can read or write from any node.&lt;/p&gt;\n\n&lt;p&gt;Third, with a leaderless replication scheme, I think you can also use atomic commits where a single node reaches out to a coordinator; the coordinator uses a 2-Phase-Commit to ensure all nodes execute that node&amp;#39;s request. This is similar to the previous method in that you have to contact every other node. You can read or write from any node. Kleppmann doesn&amp;#39;t say you can use this to achieve linearizability, but I think this works?&lt;/p&gt;\n\n&lt;p&gt;Fourth, you can use a consensus algorithm like Raft or Paxos and a majority quorum to agree on the next request. You can read or write from any node.&lt;/p&gt;\n\n&lt;p&gt;Fifth, you can use a consensus algorithm like Raft or Paxos and, instead of using a majority quorum, sequence your operations through a single, elected leader (i.e. every node sends its read or writes through a chosen leader). I guess you can read or write from any node, but technically it all goes to a single leader.&lt;/p&gt;\n\n&lt;p&gt;(4b) Relating to the first method above, Kleppmann says &amp;quot;As discussed, single-leader replication determines a total order of operations by choosing one node as the leader and sequencing all operations on a single CPU core on the leader. The challenge then is &lt;strong&gt;how to scale the system if the throughput is greater than a single leader can handle&lt;/strong&gt;, and also how to handle failover if the leader fails...&amp;quot; I don&amp;#39;t think he addresses the bolded part. How can a method that relies on a single leader scale?&lt;/p&gt;\n\n&lt;p&gt;I have glossed over some important details, e.g. relating to fault tolerance, above. For instance, I don&amp;#39;t think the second method is fault-tolerant since you need to ensure every node receives the message, so you&amp;#39;re screwed if a single node goes down.&lt;/p&gt;\n\n&lt;p&gt;These questions are pretty specific, and they might not be the clearest, so please share any questions about my questions or the information I presented. Everything is useful to me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zsqsid", "is_robot_indexable": true, "report_reasons": null, "author": "FantasticGrape", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsqsid/questions_about_designing_dataintensive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsqsid/questions_about_designing_dataintensive/", "subreddit_subscribers": 83869, "created_utc": 1671728054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nHi everyone, hope u guys are doing well.\n\nSo i've been asked at work as a beginner literally i just started, to optimize the loading processe of data to SQL server using apach spark, i looked for some tools and i've landed on Spooq which is an ETL tool based on the Apache Spark framework. Tho i didn't  quite understand how does the class  HiveLoader  establish connection to the data base in Sql server without a user or a password or driver.\n\nthe link to the project: [https://github.com/Breaka84/Spooq/blob/master/spooq/loader/hive\\_loader.py](https://github.com/Breaka84/Spooq/blob/master/spooq/loader/hive_loader.py)\n\nCan anyone please guide me through this process i'm kind of lost , i frankly don't know where should i start!", "author_fullname": "t2_bey8nkel", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Spooq to load a large scale of data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsusr6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671737894.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Hi everyone, hope u guys are doing well.&lt;/p&gt;\n\n&lt;p&gt;So i&amp;#39;ve been asked at work as a beginner literally i just started, to optimize the loading processe of data to SQL server using apach spark, i looked for some tools and i&amp;#39;ve landed on Spooq which is an ETL tool based on the Apache Spark framework. Tho i didn&amp;#39;t  quite understand how does the class  HiveLoader  establish connection to the data base in Sql server without a user or a password or driver.&lt;/p&gt;\n\n&lt;p&gt;the link to the project: &lt;a href=\"https://github.com/Breaka84/Spooq/blob/master/spooq/loader/hive_loader.py\"&gt;https://github.com/Breaka84/Spooq/blob/master/spooq/loader/hive_loader.py&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Can anyone please guide me through this process i&amp;#39;m kind of lost , i frankly don&amp;#39;t know where should i start!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wR2QxcrofxIJIaUQWjmtt_GPZAEmUFSt_g9lHELBxMo.jpg?auto=webp&amp;s=862f986994bb3583a74ca28b25e6c01824dc43f8", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/wR2QxcrofxIJIaUQWjmtt_GPZAEmUFSt_g9lHELBxMo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=66a0d56948c9d9e1aadd33af30d171647b51d4e7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/wR2QxcrofxIJIaUQWjmtt_GPZAEmUFSt_g9lHELBxMo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc888cb5b33104111fbebac312d2803781d926f2", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/wR2QxcrofxIJIaUQWjmtt_GPZAEmUFSt_g9lHELBxMo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=764c836bb3d33f7973767be1b83128b4f6a86852", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/wR2QxcrofxIJIaUQWjmtt_GPZAEmUFSt_g9lHELBxMo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d26e451054ce2d3f32d2cebe20aca5cf9e1fdba1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/wR2QxcrofxIJIaUQWjmtt_GPZAEmUFSt_g9lHELBxMo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b99ccd36d991c6f82fd8eb4692bbd85a9181558", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/wR2QxcrofxIJIaUQWjmtt_GPZAEmUFSt_g9lHELBxMo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=537477d7b7566bd70f5d9e4f4c1c1073dde38819", "width": 1080, "height": 540}], "variants": {}, "id": "nqFPrlHLiNC0fzuXDSo5wy7Ely2VtkRHWHdgqY947Yo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zsusr6", "is_robot_indexable": true, "report_reasons": null, "author": "No-Requirement3570", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsusr6/using_spooq_to_load_a_large_scale_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsusr6/using_spooq_to_load_a_large_scale_of_data/", "subreddit_subscribers": 83869, "created_utc": 1671737894.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I currently deal with 50+ TB data processing and I consider using Databricks DLT as solution to my problem with performance. Currently I deal with pretty complicated silver layer logic (joining 30+ tables, KPI calculations) and refreshing rate is 24h now but we want to go to even 5 minutes rate. \n\nI would appreciate the feedback of users that dealt with it and of course - please point the red flags, painpoints and if possible - more reliable/cheaper solutions than DLT.", "author_fullname": "t2_7mnlik68", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks DLT - limitations, pricing and alternatives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zskpj6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671711148.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I currently deal with 50+ TB data processing and I consider using Databricks DLT as solution to my problem with performance. Currently I deal with pretty complicated silver layer logic (joining 30+ tables, KPI calculations) and refreshing rate is 24h now but we want to go to even 5 minutes rate. &lt;/p&gt;\n\n&lt;p&gt;I would appreciate the feedback of users that dealt with it and of course - please point the red flags, painpoints and if possible - more reliable/cheaper solutions than DLT.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zskpj6", "is_robot_indexable": true, "report_reasons": null, "author": "Astherol", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zskpj6/databricks_dlt_limitations_pricing_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zskpj6/databricks_dlt_limitations_pricing_and/", "subreddit_subscribers": 83869, "created_utc": 1671711148.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to use DBR 7.3 as I need ubuntu 18.04 for my specific work. but the problem is that I am not able to see my github code in the cluster with 7.3 DBR.\n\nI did some research and i found that the support of having github code in /Workspace/Repos is only provided in 11+ DBR.\n\nI am new to databricks, and i want to ask the experienced data engineers here who would have worked with the previous version of DBR, how to enable /mount the /Workspace directory ?", "author_fullname": "t2_869m5ow7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[DataBricks] How to mount /Workspace/Repos folder in DBR below 11.3 ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsge27", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671696081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to use DBR 7.3 as I need ubuntu 18.04 for my specific work. but the problem is that I am not able to see my github code in the cluster with 7.3 DBR.&lt;/p&gt;\n\n&lt;p&gt;I did some research and i found that the support of having github code in /Workspace/Repos is only provided in 11+ DBR.&lt;/p&gt;\n\n&lt;p&gt;I am new to databricks, and i want to ask the experienced data engineers here who would have worked with the previous version of DBR, how to enable /mount the /Workspace directory ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zsge27", "is_robot_indexable": true, "report_reasons": null, "author": "Jaded_Click9591", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsge27/databricks_how_to_mount_workspacerepos_folder_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsge27/databricks_how_to_mount_workspacerepos_folder_in/", "subreddit_subscribers": 83869, "created_utc": 1671696081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are using EMRStudio and it appears to be so flaky that it's pretty much unusable.  \n\n\n- The notebook server will just die sometimes and throw you into the EMR Notebook list.\n- When you launch a workspace, it takes several minutes... and when it's finally ready it spams a new tab that steals focus while you're trying to do something else.\n- Attaching/detaching to a EMR cluster makes the whole UI reload. Sorry, you didn't want that code you were just writing!\n- Sometimes you need to attach to a cluster more than once to successfully be able to start a kernel.\n- Collaboration mode lets you share a notebook/workspace with another user. But it's limited to 5 collaborators per workspace and there are no access controls. Basically collaborate means they have access to everything and you can't just share a single notebook as you'd typically like to do when sharing some tables or charts with a colleague.\n\nI get this \"helpful\" error the first time I attach to an EMR cluster too:\n```\nThe code failed because of a fatal error:\n\tSession 0 did not start up in 60 seconds..\n\nSome things to try:\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\nc) Restart the kernel.\n```\n\nHas anyone used it successfully without having an aneurysm?", "author_fullname": "t2_unlas68x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is EMRStudio the worst AWS product they've released?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zs9tls", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671676982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are using EMRStudio and it appears to be so flaky that it&amp;#39;s pretty much unusable.  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The notebook server will just die sometimes and throw you into the EMR Notebook list.&lt;/li&gt;\n&lt;li&gt;When you launch a workspace, it takes several minutes... and when it&amp;#39;s finally ready it spams a new tab that steals focus while you&amp;#39;re trying to do something else.&lt;/li&gt;\n&lt;li&gt;Attaching/detaching to a EMR cluster makes the whole UI reload. Sorry, you didn&amp;#39;t want that code you were just writing!&lt;/li&gt;\n&lt;li&gt;Sometimes you need to attach to a cluster more than once to successfully be able to start a kernel.&lt;/li&gt;\n&lt;li&gt;Collaboration mode lets you share a notebook/workspace with another user. But it&amp;#39;s limited to 5 collaborators per workspace and there are no access controls. Basically collaborate means they have access to everything and you can&amp;#39;t just share a single notebook as you&amp;#39;d typically like to do when sharing some tables or charts with a colleague.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I get this &amp;quot;helpful&amp;quot; error the first time I attach to an EMR cluster too:\n```\nThe code failed because of a fatal error:\n    Session 0 did not start up in 60 seconds..&lt;/p&gt;\n\n&lt;p&gt;Some things to try:\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\nc) Restart the kernel.\n```&lt;/p&gt;\n\n&lt;p&gt;Has anyone used it successfully without having an aneurysm?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zs9tls", "is_robot_indexable": true, "report_reasons": null, "author": "elephant_gate_332", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zs9tls/is_emrstudio_the_worst_aws_product_theyve_released/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zs9tls/is_emrstudio_the_worst_aws_product_theyve_released/", "subreddit_subscribers": 83869, "created_utc": 1671676982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, just wanted some outside opinions on Boolean flags in a dimensional model. If I have Boolean flags that represent the outcome of a business process, not the input attributes, would you put that in the dimension table or the fact table?\n\nI\u2019ve been leaning toward fact because it is a outcome, but I also know traditional dimensional modeling is metrics only in fact table.", "author_fullname": "t2_gge8z8qb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling - Boolean Flags", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsq4ro", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671726405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, just wanted some outside opinions on Boolean flags in a dimensional model. If I have Boolean flags that represent the outcome of a business process, not the input attributes, would you put that in the dimension table or the fact table?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been leaning toward fact because it is a outcome, but I also know traditional dimensional modeling is metrics only in fact table.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zsq4ro", "is_robot_indexable": true, "report_reasons": null, "author": "No_Professional_9685", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsq4ro/data_modeling_boolean_flags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsq4ro/data_modeling_boolean_flags/", "subreddit_subscribers": 83869, "created_utc": 1671726405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to this area and it is quite confusing to understand the differences between these vendors:\n\nInformatica\n\nCloudera\n\nOracle\n\nSnowflake\n\nVertica\n\nAlteryx\n\nDataiku\n\nMy understandign that Informatica and Cloudera is about data integration, Oracle is about data management, Snowflake and Vertica are more analytical??, alteryx is no-code data integration + analytics and Dataiku is more about collaboration and orchestration of data science workflows. \n\nNow, firstly, I am not confident that my understanding is correct and if you could even call all of them data management systems. Secondly, it seems like there is at least a lot of overlap between them? Would you just pick one or use some sort of combination?", "author_fullname": "t2_5fbb0xj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me understand the difference between database management systems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zse9fz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671689435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to this area and it is quite confusing to understand the differences between these vendors:&lt;/p&gt;\n\n&lt;p&gt;Informatica&lt;/p&gt;\n\n&lt;p&gt;Cloudera&lt;/p&gt;\n\n&lt;p&gt;Oracle&lt;/p&gt;\n\n&lt;p&gt;Snowflake&lt;/p&gt;\n\n&lt;p&gt;Vertica&lt;/p&gt;\n\n&lt;p&gt;Alteryx&lt;/p&gt;\n\n&lt;p&gt;Dataiku&lt;/p&gt;\n\n&lt;p&gt;My understandign that Informatica and Cloudera is about data integration, Oracle is about data management, Snowflake and Vertica are more analytical??, alteryx is no-code data integration + analytics and Dataiku is more about collaboration and orchestration of data science workflows. &lt;/p&gt;\n\n&lt;p&gt;Now, firstly, I am not confident that my understanding is correct and if you could even call all of them data management systems. Secondly, it seems like there is at least a lot of overlap between them? Would you just pick one or use some sort of combination?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zse9fz", "is_robot_indexable": true, "report_reasons": null, "author": "kultuhtu", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zse9fz/help_me_understand_the_difference_between/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zse9fz/help_me_understand_the_difference_between/", "subreddit_subscribers": 83869, "created_utc": 1671689435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Converting an ETL script to Software-Defined Assets | Dagster Blog", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_zst80d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/m8bYDsj1QdDDnhGehNiGjZz1dQdNsKI3nMo67UAzEUA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671733889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/dagster-script-to-assets", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/d7Eoe-cRzlBsFJCyYhS_a0-F59YHACCH-1Ked-Uu3BQ.jpg?auto=webp&amp;s=207093ea4eb9417f27beb114c48e955066dc789c", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/d7Eoe-cRzlBsFJCyYhS_a0-F59YHACCH-1Ked-Uu3BQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0db6b12e6264fdabd21e34820c30dd6710267a71", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/d7Eoe-cRzlBsFJCyYhS_a0-F59YHACCH-1Ked-Uu3BQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b98cc03e897ff0181df58fed35cfff1f18b69c6c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/d7Eoe-cRzlBsFJCyYhS_a0-F59YHACCH-1Ked-Uu3BQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=21f538f322f9f7f9ed14dc67ef777c296f895067", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/d7Eoe-cRzlBsFJCyYhS_a0-F59YHACCH-1Ked-Uu3BQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1bfd8c0deb5c4396eac51ca159c89236f22c946f", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/d7Eoe-cRzlBsFJCyYhS_a0-F59YHACCH-1Ked-Uu3BQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=72f2ed5dd59466653f614dacb64422eb54046170", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/d7Eoe-cRzlBsFJCyYhS_a0-F59YHACCH-1Ked-Uu3BQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5890eaf71abfe62f468ce82cd7fbdccc574c98f2", "width": 1080, "height": 567}], "variants": {}, "id": "Gc8vBf05T5Q2q1pr-v0LEtTsCcHKQIIrzdn6YpAEhQg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zst80d", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zst80d/converting_an_etl_script_to_softwaredefined/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/dagster-script-to-assets", "subreddit_subscribers": 83869, "created_utc": 1671733889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a Data Science background but have found myself being the default Data Engineer as well for our company. We have a lot of requests for historic information. \n\nex) \nHow many sellers did we have on this day in the past?\nWho was the manager on this account on this day in the past?\n\nI have always tried to re-build the transformed data from source each run. However, I am hitting limitations with some of the historical numbers like this. Particularly prepping the data for easy ingestion into BI tools. \n\nIt seems like we have either have some more complicated code in the ETL or BI layer to figure out historical figures. Or store every entity's status for every day in the past. \n\nWhat have you found to be best? I get worried about appending new rows each day to a historical table in case we have to change a definition or find a bug and can't rerun the whole table historically. \n\nAny guidance or experiences here?", "author_fullname": "t2_62ipd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you best handle historic metrics? Re-create history each run or append new version each day?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsscjr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671731755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Data Science background but have found myself being the default Data Engineer as well for our company. We have a lot of requests for historic information. &lt;/p&gt;\n\n&lt;p&gt;ex) \nHow many sellers did we have on this day in the past?\nWho was the manager on this account on this day in the past?&lt;/p&gt;\n\n&lt;p&gt;I have always tried to re-build the transformed data from source each run. However, I am hitting limitations with some of the historical numbers like this. Particularly prepping the data for easy ingestion into BI tools. &lt;/p&gt;\n\n&lt;p&gt;It seems like we have either have some more complicated code in the ETL or BI layer to figure out historical figures. Or store every entity&amp;#39;s status for every day in the past. &lt;/p&gt;\n\n&lt;p&gt;What have you found to be best? I get worried about appending new rows each day to a historical table in case we have to change a definition or find a bug and can&amp;#39;t rerun the whole table historically. &lt;/p&gt;\n\n&lt;p&gt;Any guidance or experiences here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "zsscjr", "is_robot_indexable": true, "report_reasons": null, "author": "Deerz", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsscjr/how_do_you_best_handle_historic_metrics_recreate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsscjr/how_do_you_best_handle_historic_metrics_recreate/", "subreddit_subscribers": 83869, "created_utc": 1671731755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone here has any experience/heard about these 2 certification programs?  I'm an IT BA with SQL/Python experience who is looking to make a transition to BI or data analytics field as a stepping stone to become a data engineer (a stepping stone because I don't think I have sufficient technical knowledge to become DE now).  I heard some good things about UW's BUSINESS INTELLIGENCE &amp; DATABASE DEVELOPMENT certification program, but I wonder if UCI's Database Management or Data Science certification is any good?\n\n[https://www.pce.uw.edu/certificates/business-intelligence-and-database-development](https://www.pce.uw.edu/certificates/business-intelligence-and-database-development)\n\n[https://ce.uci.edu/areas/it/database\\_mgmt/](https://ce.uci.edu/areas/it/database_mgmt/)\n\n[https://ce.uci.edu/areas/it/data\\_science/](https://ce.uci.edu/areas/it/data_science/)", "author_fullname": "t2_jq4nx76n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UW's and UCI's certification program", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsgnw3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671696962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone here has any experience/heard about these 2 certification programs?  I&amp;#39;m an IT BA with SQL/Python experience who is looking to make a transition to BI or data analytics field as a stepping stone to become a data engineer (a stepping stone because I don&amp;#39;t think I have sufficient technical knowledge to become DE now).  I heard some good things about UW&amp;#39;s BUSINESS INTELLIGENCE &amp;amp; DATABASE DEVELOPMENT certification program, but I wonder if UCI&amp;#39;s Database Management or Data Science certification is any good?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.pce.uw.edu/certificates/business-intelligence-and-database-development\"&gt;https://www.pce.uw.edu/certificates/business-intelligence-and-database-development&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ce.uci.edu/areas/it/database_mgmt/\"&gt;https://ce.uci.edu/areas/it/database_mgmt/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ce.uci.edu/areas/it/data_science/\"&gt;https://ce.uci.edu/areas/it/data_science/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xWBF4FgezyUcnUHN1egT8_PughjPVucKnd134vvSsPg.jpg?auto=webp&amp;s=f1d2d07e4dabde76a3725b349267696f621dda39", "width": 2000, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/xWBF4FgezyUcnUHN1egT8_PughjPVucKnd134vvSsPg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecaea0c0ebbb4b00b2fc979d6b288d3a1fb6c17d", "width": 108, "height": 32}, {"url": "https://external-preview.redd.it/xWBF4FgezyUcnUHN1egT8_PughjPVucKnd134vvSsPg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e26df7396c45d84bc46d3d6d67a2dd305ef6bbbd", "width": 216, "height": 64}, {"url": "https://external-preview.redd.it/xWBF4FgezyUcnUHN1egT8_PughjPVucKnd134vvSsPg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=647d5a090fb506d9beba8c8e560526187ff6ed05", "width": 320, "height": 96}, {"url": "https://external-preview.redd.it/xWBF4FgezyUcnUHN1egT8_PughjPVucKnd134vvSsPg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e04608d6e5b2d76cc884569044c9c5ab29f914a0", "width": 640, "height": 192}, {"url": "https://external-preview.redd.it/xWBF4FgezyUcnUHN1egT8_PughjPVucKnd134vvSsPg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=86849049f74239a5153ec1a24413da77389548ff", "width": 960, "height": 288}, {"url": "https://external-preview.redd.it/xWBF4FgezyUcnUHN1egT8_PughjPVucKnd134vvSsPg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7a5899feb42b3bf93c766d8391e1fa8751e2dfb2", "width": 1080, "height": 324}], "variants": {}, "id": "KyQT6qC1geg5x74wBAu1AitKvbeimK94y50w5XQLs-I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zsgnw3", "is_robot_indexable": true, "report_reasons": null, "author": "tulipz123", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsgnw3/uws_and_ucis_certification_program/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsgnw3/uws_and_ucis_certification_program/", "subreddit_subscribers": 83869, "created_utc": 1671696962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Delta Lake quick start for Spark, Rust, Python, and more.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "name": "t3_zt1oo7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/K33ysDez8p4bKKUNXbQbThO00LJdQGR9yavfg-cQQOE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671755222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/delta-io/delta-docs/tree/main/static/quickstart_docker", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/c3HlKq2XD2kRA8M2Gs3txy4YUNKAXYxi_zrONGnD6xA.jpg?auto=webp&amp;s=c9cce79d37bc54993deae4d2116f4dc1b4e293d5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/c3HlKq2XD2kRA8M2Gs3txy4YUNKAXYxi_zrONGnD6xA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9406cb9902ce2056a5ded25dc82d9c2b750d7cc4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/c3HlKq2XD2kRA8M2Gs3txy4YUNKAXYxi_zrONGnD6xA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b81b34dbad340592773af9691a862b9a11d58ad1", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/c3HlKq2XD2kRA8M2Gs3txy4YUNKAXYxi_zrONGnD6xA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1c4eace392b665e725ce3531fb6ef729989da6a1", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/c3HlKq2XD2kRA8M2Gs3txy4YUNKAXYxi_zrONGnD6xA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=21656922eb45a2cbd42ef917f1fd3f1f74efed3c", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/c3HlKq2XD2kRA8M2Gs3txy4YUNKAXYxi_zrONGnD6xA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=22b9c050da3152da8a76f5eadd658c4a37502d30", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/c3HlKq2XD2kRA8M2Gs3txy4YUNKAXYxi_zrONGnD6xA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=acdd5960945b893c4607e958aa09bcd549815fdd", "width": 1080, "height": 540}], "variants": {}, "id": "9S7Py6rx2_jKBT44E-vXCYohH4HWZVDtMqg-tCS_c_Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "zt1oo7", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zt1oo7/a_delta_lake_quick_start_for_spark_rust_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/delta-io/delta-docs/tree/main/static/quickstart_docker", "subreddit_subscribers": 83869, "created_utc": 1671755222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "First of all, i want to thank all of you guys for the posts here in this sub, it really helped me land my first DE internship while in college, thanks to the great amount of resources and guidance given here. I start in january and in this meantime im reviewing some concepts of DE. Im good at python, not great, but can make my way through Googling and average in SQL, i know window functions and the basics.\n\nThe stack is python, sql and some aws tools. I already did some digging and  followed and end to end project on youtube to get a grasp at aws tools and what they are capable of, but im feeling a little overwhelmed by the amount of resources available. I did a little project using the twitter api, airflow and EC2 out of curiosity, nothing big or fancy, just the barebones to make it work and try to understand more about the tools. Im reading 'Data Pipelines Pocket Reference' again and started 'Fundamentals of Data Engineering'. Am i good to go on my internship or should i improve more my python and sql base?", "author_fullname": "t2_5r22gi6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much should i expect to learn on the job as an intern?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsqeg4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671727061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all, i want to thank all of you guys for the posts here in this sub, it really helped me land my first DE internship while in college, thanks to the great amount of resources and guidance given here. I start in january and in this meantime im reviewing some concepts of DE. Im good at python, not great, but can make my way through Googling and average in SQL, i know window functions and the basics.&lt;/p&gt;\n\n&lt;p&gt;The stack is python, sql and some aws tools. I already did some digging and  followed and end to end project on youtube to get a grasp at aws tools and what they are capable of, but im feeling a little overwhelmed by the amount of resources available. I did a little project using the twitter api, airflow and EC2 out of curiosity, nothing big or fancy, just the barebones to make it work and try to understand more about the tools. Im reading &amp;#39;Data Pipelines Pocket Reference&amp;#39; again and started &amp;#39;Fundamentals of Data Engineering&amp;#39;. Am i good to go on my internship or should i improve more my python and sql base?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zsqeg4", "is_robot_indexable": true, "report_reasons": null, "author": "Upstairs-Ad-1466", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsqeg4/how_much_should_i_expect_to_learn_on_the_job_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsqeg4/how_much_should_i_expect_to_learn_on_the_job_as/", "subreddit_subscribers": 83869, "created_utc": 1671727061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It has been challenging for the tech industry lately, and the data engineering positions were relatively low, to begin with. I've had tough luck recently getting interviews for data engineering intern positions, where I only landed a single technical interview.\n\nI rarely post on Reddit, but I've started to think I'm doing something wrong, especially with the lack of feedback. I would appreciate guidance regarding my resume, my projects, projects to add, etc ...\n\n&amp;#x200B;\n\n[https://docs.google.com/document/d/1E5cYR4PSi86IFgZNi8V4WtuEbtpxzc5J/edit?usp=sharing&amp;ouid=114192761163778317150&amp;rtpof=true&amp;sd=true](https://docs.google.com/document/d/1E5cYR4PSi86IFgZNi8V4WtuEbtpxzc5J/edit?usp=sharing&amp;ouid=114192761163778317150&amp;rtpof=true&amp;sd=true)", "author_fullname": "t2_881mpsrv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to land my first data engineering position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zszuxf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671750775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It has been challenging for the tech industry lately, and the data engineering positions were relatively low, to begin with. I&amp;#39;ve had tough luck recently getting interviews for data engineering intern positions, where I only landed a single technical interview.&lt;/p&gt;\n\n&lt;p&gt;I rarely post on Reddit, but I&amp;#39;ve started to think I&amp;#39;m doing something wrong, especially with the lack of feedback. I would appreciate guidance regarding my resume, my projects, projects to add, etc ...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.google.com/document/d/1E5cYR4PSi86IFgZNi8V4WtuEbtpxzc5J/edit?usp=sharing&amp;amp;ouid=114192761163778317150&amp;amp;rtpof=true&amp;amp;sd=true\"&gt;https://docs.google.com/document/d/1E5cYR4PSi86IFgZNi8V4WtuEbtpxzc5J/edit?usp=sharing&amp;amp;ouid=114192761163778317150&amp;amp;rtpof=true&amp;amp;sd=true&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rfe8PoD7BaBkaa_nO5LnQqttAebARxYcQ1TOE5L5H50.jpg?auto=webp&amp;s=6cf244dc5077bfb246e641ba7ad00544fe4b26a0", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/rfe8PoD7BaBkaa_nO5LnQqttAebARxYcQ1TOE5L5H50.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d3c825e1c52b7e40d8f08660247e908bb4b3c7a", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/rfe8PoD7BaBkaa_nO5LnQqttAebARxYcQ1TOE5L5H50.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=156b023114b4fabee06250e53e333b10e47f5c28", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/rfe8PoD7BaBkaa_nO5LnQqttAebARxYcQ1TOE5L5H50.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7f1c24704abc97ca22107ac8ae4a94ded9ca884a", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/rfe8PoD7BaBkaa_nO5LnQqttAebARxYcQ1TOE5L5H50.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=784e0dfc051d4b6969b066b6d088f8c72baef39f", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/rfe8PoD7BaBkaa_nO5LnQqttAebARxYcQ1TOE5L5H50.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a09d625e1815f1e5b0e8a886b747b7c78dfd1d31", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/rfe8PoD7BaBkaa_nO5LnQqttAebARxYcQ1TOE5L5H50.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=29b5bdb5eb8489d978ded063b682a48c10dcb26c", "width": 1080, "height": 567}], "variants": {}, "id": "FZaMGs_jjAu_KG2EfScnoa7K9Y0K7ER-nga_1UXXVaw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "zszuxf", "is_robot_indexable": true, "report_reasons": null, "author": "263Iz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zszuxf/trying_to_land_my_first_data_engineering_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zszuxf/trying_to_land_my_first_data_engineering_position/", "subreddit_subscribers": 83869, "created_utc": 1671750775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# My Request in a Nutshell\n\nPlease help me figure out an ETL and storage approach for 100s of GBs of website log data for user queries and click behaviors. It will be subject mostly to text-based analyses. \n\nOR \n\nHelp me figure out how to roughly specify the kind of system we need, so that we can hire contractors who can figure out the details and build it. \n\nFundamentally my questions are:\n\n* Do we want to put this data in a database? \n* If so, what kind? \n* If not, what alternative mode of file storage and access would you recommend, and why? \n\n# Problem Background\n\nMy division will soon start receiving samples of the web user logs for the product that we work on, a major database of scientific abstracts. I and one other data analyst in my division are the best qualified people available to figure out what kind of system to set up for storing, processing, and analyzing this incoming data.  But neither of us has ever worked with data at this scale. \n\nThere is money to hire some contractors for the project who could bring in skills we don't have, but we don\u2019t know what we want them to build. Our usual datasets are under 1G, so our good-enough \u201ctech stack\u201d is typically CSVs and Pandas. We know we need a more sophisticated approach than that, but what? \n\nI\u2019ve identified numerous candidates for transitioning from pandas to other tools for greater efficiency and speed. Polars and vaex seem like our best bets. I feel less out of my depth looking at this stuff, but if you have comments on those options, please go ahead.\n\nThe part I\u2019m still totally lost on is replacing the \u201cCSV on my hard drive\u201d part: what is the best way to go from the raw files we receive to something we can efficiently clean, store, and extract subsets for further analyses? **I\u2019m quite comfortable with the principles of RDBMs and using SQL, but** **my head is spinning from trying to understand all the types of NoSQL DBs, database-like libraries like duckDB, and alternatives like querying JSON files directly with something like OctoSQL.** I can\u2019t find good detailed explanations about why we would choose one approach or tool over another, or even how to think about comparing them.\n\nA super basic example of why I can\u2019t figure this out: I know that operating over joins is computationally costly and slow. But if the alternative is operating on lists nested inside complex objects, isn\u2019t that also computationally costly and slow? Which one is better? Short of actually building and testing two systems, how can I compare this kind of thing? \ud83e\udd2f \n\n# Details\n\n**The Data**\n\n* Selected web log data including:\n   * user queries of the database\n   * user clicks on citations in their query results. \n   * user clicks for certain other actions that indicate relevant results, like saving or sharing a result\n* Samples will include about 21 million queries, plus their click data. Probably 2-4 samples per year. \n* We will combine this with data from other sources about the citations and scientific vocabulary (subject headings)\n* Model:\n   * As objects, we would probably use 4 classes: Queries, Clicks, Citations, and Subject Headings. \n   * In a mostly normalized relational database, we will need a minimum of 7 tables, due to n-to-n relationships. (Queries return multiple citations/citations can be returned by multiple queries, etc.)\n* Log data will arrive as flat text files representing JSON objects, as far as I know. Minor details of this might be negotiable. They may be willing to give us different file formats, for example. \n\n**To Support**\n\n* Mostly text operations, especially:\n* Typical text prep for cleaning and analyzing various aspects of queries or the citations of interest, like case changes, tokenizing, stripping whitespace, etc.\n* Text matching, especially for filtering/subsetting, and often across multiple objects/tables (like matching part of the query text to part of the citation text).  \n* We will need to subset the original repeatedly to get the right data for different projects. (So the entire set must be stored for reuse, we can\u2019t filter it first and just store the part we need.)\n\n**Restrictions and Resources**\n\n* We *might* be able to get space and compute time on our institutional AWS. \n* Otherwise, due to data sensitivity, everything has to stay in house. Our main resources are our laptops.\n* We can manage our own programming libraries through conda. Any other new software we want to install has to go through security approval. \n\nGood lord, I hope you redditors have advice on how to tackle this. I'm just a librarian-turned-data-analyst doing my best out here. Thanks for your help!", "author_fullname": "t2_atd9kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me figure out ETL and storage for user search and click logs. So lost in all the DB alternatives.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsuk2o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671737279.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;My Request in a Nutshell&lt;/h1&gt;\n\n&lt;p&gt;Please help me figure out an ETL and storage approach for 100s of GBs of website log data for user queries and click behaviors. It will be subject mostly to text-based analyses. &lt;/p&gt;\n\n&lt;p&gt;OR &lt;/p&gt;\n\n&lt;p&gt;Help me figure out how to roughly specify the kind of system we need, so that we can hire contractors who can figure out the details and build it. &lt;/p&gt;\n\n&lt;p&gt;Fundamentally my questions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Do we want to put this data in a database? &lt;/li&gt;\n&lt;li&gt;If so, what kind? &lt;/li&gt;\n&lt;li&gt;If not, what alternative mode of file storage and access would you recommend, and why? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Problem Background&lt;/h1&gt;\n\n&lt;p&gt;My division will soon start receiving samples of the web user logs for the product that we work on, a major database of scientific abstracts. I and one other data analyst in my division are the best qualified people available to figure out what kind of system to set up for storing, processing, and analyzing this incoming data.  But neither of us has ever worked with data at this scale. &lt;/p&gt;\n\n&lt;p&gt;There is money to hire some contractors for the project who could bring in skills we don&amp;#39;t have, but we don\u2019t know what we want them to build. Our usual datasets are under 1G, so our good-enough \u201ctech stack\u201d is typically CSVs and Pandas. We know we need a more sophisticated approach than that, but what? &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve identified numerous candidates for transitioning from pandas to other tools for greater efficiency and speed. Polars and vaex seem like our best bets. I feel less out of my depth looking at this stuff, but if you have comments on those options, please go ahead.&lt;/p&gt;\n\n&lt;p&gt;The part I\u2019m still totally lost on is replacing the \u201cCSV on my hard drive\u201d part: what is the best way to go from the raw files we receive to something we can efficiently clean, store, and extract subsets for further analyses? &lt;strong&gt;I\u2019m quite comfortable with the principles of RDBMs and using SQL, but&lt;/strong&gt; &lt;strong&gt;my head is spinning from trying to understand all the types of NoSQL DBs, database-like libraries like duckDB, and alternatives like querying JSON files directly with something like OctoSQL.&lt;/strong&gt; I can\u2019t find good detailed explanations about why we would choose one approach or tool over another, or even how to think about comparing them.&lt;/p&gt;\n\n&lt;p&gt;A super basic example of why I can\u2019t figure this out: I know that operating over joins is computationally costly and slow. But if the alternative is operating on lists nested inside complex objects, isn\u2019t that also computationally costly and slow? Which one is better? Short of actually building and testing two systems, how can I compare this kind of thing? \ud83e\udd2f &lt;/p&gt;\n\n&lt;h1&gt;Details&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;The Data&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Selected web log data including:\n\n&lt;ul&gt;\n&lt;li&gt;user queries of the database&lt;/li&gt;\n&lt;li&gt;user clicks on citations in their query results. &lt;/li&gt;\n&lt;li&gt;user clicks for certain other actions that indicate relevant results, like saving or sharing a result&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Samples will include about 21 million queries, plus their click data. Probably 2-4 samples per year. &lt;/li&gt;\n&lt;li&gt;We will combine this with data from other sources about the citations and scientific vocabulary (subject headings)&lt;/li&gt;\n&lt;li&gt;Model:\n\n&lt;ul&gt;\n&lt;li&gt;As objects, we would probably use 4 classes: Queries, Clicks, Citations, and Subject Headings. &lt;/li&gt;\n&lt;li&gt;In a mostly normalized relational database, we will need a minimum of 7 tables, due to n-to-n relationships. (Queries return multiple citations/citations can be returned by multiple queries, etc.)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Log data will arrive as flat text files representing JSON objects, as far as I know. Minor details of this might be negotiable. They may be willing to give us different file formats, for example. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;To Support&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Mostly text operations, especially:&lt;/li&gt;\n&lt;li&gt;Typical text prep for cleaning and analyzing various aspects of queries or the citations of interest, like case changes, tokenizing, stripping whitespace, etc.&lt;/li&gt;\n&lt;li&gt;Text matching, especially for filtering/subsetting, and often across multiple objects/tables (like matching part of the query text to part of the citation text).&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;We will need to subset the original repeatedly to get the right data for different projects. (So the entire set must be stored for reuse, we can\u2019t filter it first and just store the part we need.)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Restrictions and Resources&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We &lt;em&gt;might&lt;/em&gt; be able to get space and compute time on our institutional AWS. &lt;/li&gt;\n&lt;li&gt;Otherwise, due to data sensitivity, everything has to stay in house. Our main resources are our laptops.&lt;/li&gt;\n&lt;li&gt;We can manage our own programming libraries through conda. Any other new software we want to install has to go through security approval. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Good lord, I hope you redditors have advice on how to tackle this. I&amp;#39;m just a librarian-turned-data-analyst doing my best out here. Thanks for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "zsuk2o", "is_robot_indexable": true, "report_reasons": null, "author": "GreatMotherPeachy", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsuk2o/help_me_figure_out_etl_and_storage_for_user/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsuk2o/help_me_figure_out_etl_and_storage_for_user/", "subreddit_subscribers": 83869, "created_utc": 1671737279.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "By UI changes I mean stuff like adding an export button to a web app to download data from a table. Is it normal for this to be part of a data engineer's job?", "author_fullname": "t2_s5w2bngt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any data engineers out there whose job scope involves making UI changes to company's product", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsip3l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671704226.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;By UI changes I mean stuff like adding an export button to a web app to download data from a table. Is it normal for this to be part of a data engineer&amp;#39;s job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "zsip3l", "is_robot_indexable": true, "report_reasons": null, "author": "Global_Service_1094", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsip3l/any_data_engineers_out_there_whose_job_scope/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsip3l/any_data_engineers_out_there_whose_job_scope/", "subreddit_subscribers": 83869, "created_utc": 1671704226.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is an article about Airflow and how to install it using docker\n\n https://link.medium.com/pkGfRa24Xvb", "author_fullname": "t2_7ssutue8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow article", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zsn4z0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.14, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671718728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is an article about Airflow and how to install it using docker&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://link.medium.com/pkGfRa24Xvb\"&gt;https://link.medium.com/pkGfRa24Xvb&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/obVLieh4RXVSzC8EASin0xFYNyY2ULLh6_eMAbCm4eQ.jpg?auto=webp&amp;s=8a9794e68000f6f1a66e411ec3869c598b227b25", "width": 1200, "height": 507}, "resolutions": [{"url": "https://external-preview.redd.it/obVLieh4RXVSzC8EASin0xFYNyY2ULLh6_eMAbCm4eQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=60dc9859cacd77f1a614679a444f3bbda4e7cd36", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/obVLieh4RXVSzC8EASin0xFYNyY2ULLh6_eMAbCm4eQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a86f5fb899e036942c1ddd8bc220e5a1e3bb2a0", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/obVLieh4RXVSzC8EASin0xFYNyY2ULLh6_eMAbCm4eQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9883612a17227227641d124b8a8bb601205cf10", "width": 320, "height": 135}, {"url": "https://external-preview.redd.it/obVLieh4RXVSzC8EASin0xFYNyY2ULLh6_eMAbCm4eQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6bf38b10263bcff9a685ade70a08ae4e4c8d50bc", "width": 640, "height": 270}, {"url": "https://external-preview.redd.it/obVLieh4RXVSzC8EASin0xFYNyY2ULLh6_eMAbCm4eQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e6aa77728cc5f81dd63ef2b772235e1e130921d", "width": 960, "height": 405}, {"url": "https://external-preview.redd.it/obVLieh4RXVSzC8EASin0xFYNyY2ULLh6_eMAbCm4eQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98e6d1c72e2180f3351ef66d8b14f2f312141b80", "width": 1080, "height": 456}], "variants": {}, "id": "7E7HfHVgsnKu8czZS6eOzvGqIfDqgWZI-axvAvWy5Cw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "zsn4z0", "is_robot_indexable": true, "report_reasons": null, "author": "Ansam93", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/zsn4z0/airflow_article/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/zsn4z0/airflow_article/", "subreddit_subscribers": 83869, "created_utc": 1671718728.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}