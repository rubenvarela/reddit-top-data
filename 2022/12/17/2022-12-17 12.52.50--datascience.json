{"kind": "Listing", "data": {"after": "t3_znd9ic", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Just venting.\n\nI live in a Midwestern city and I have been looking for an onsite data science/statistician role. I'm that weirdo that actually hates remote work and I've been doing it for 2.5 years and I don't want to do it anymore! I thought my willingness to work in person would make my job search easy and quick.\n\nI'm a week into my full-tilt job search and so far all of the local jobs I'm qualified for don't pay what I'm looking for... and then recruiters contact me about remote jobs that pay way more \ud83d\ude2d.\n\nMy partner keeps encouraging me to prioritize finding an onsite job instead of giving into the temptation of picking a higher-paying remote job, but it's tough. I start thinking \"well, I'm already working remotely... And at least here I'd make significantly more money...\" But frankly if I can't find an onsite position then I should just stay where I am. \n\nWhere are all the good paying local jobs at?", "author_fullname": "t2_efud6jd8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "All the good paying jobs are remote", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zng6bp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 193, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 193, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671201594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just venting.&lt;/p&gt;\n\n&lt;p&gt;I live in a Midwestern city and I have been looking for an onsite data science/statistician role. I&amp;#39;m that weirdo that actually hates remote work and I&amp;#39;ve been doing it for 2.5 years and I don&amp;#39;t want to do it anymore! I thought my willingness to work in person would make my job search easy and quick.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a week into my full-tilt job search and so far all of the local jobs I&amp;#39;m qualified for don&amp;#39;t pay what I&amp;#39;m looking for... and then recruiters contact me about remote jobs that pay way more \ud83d\ude2d.&lt;/p&gt;\n\n&lt;p&gt;My partner keeps encouraging me to prioritize finding an onsite job instead of giving into the temptation of picking a higher-paying remote job, but it&amp;#39;s tough. I start thinking &amp;quot;well, I&amp;#39;m already working remotely... And at least here I&amp;#39;d make significantly more money...&amp;quot; But frankly if I can&amp;#39;t find an onsite position then I should just stay where I am. &lt;/p&gt;\n\n&lt;p&gt;Where are all the good paying local jobs at?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "zng6bp", "is_robot_indexable": true, "report_reasons": null, "author": "DisgustingCantaloupe", "discussion_type": null, "num_comments": 144, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zng6bp/all_the_good_paying_jobs_are_remote/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zng6bp/all_the_good_paying_jobs_are_remote/", "subreddit_subscribers": 827672, "created_utc": 1671201594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Not including ANNs. I\u2019m still in grad school but I\u2019ve yet to come across a situation where RFs, Decision Trees, Bayesian Classifiers, etc. have ever performed better than xgbs on test data. In what situation would you think XGBoost would not outperform all the other supervised-learning models for non-sequential data?", "author_fullname": "t2_524iawau", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone come across a model that outperformed XGBoost?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zne1by", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671195324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not including ANNs. I\u2019m still in grad school but I\u2019ve yet to come across a situation where RFs, Decision Trees, Bayesian Classifiers, etc. have ever performed better than xgbs on test data. In what situation would you think XGBoost would not outperform all the other supervised-learning models for non-sequential data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zne1by", "is_robot_indexable": true, "report_reasons": null, "author": "bassabyss", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zne1by/has_anyone_come_across_a_model_that_outperformed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zne1by/has_anyone_come_across_a_model_that_outperformed/", "subreddit_subscribers": 827672, "created_utc": 1671195324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you too spend more time configuring tooling and troubleshooting package issues than you do working?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znp9do", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_d2xe0c33", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "program", "selftext": "The ecosystem is full of amazing tools that make the developer's job considerably simpler. Typescript, npm, pnpm, parcel, webpack, node, babel... yet getting them to work together is quite difficult.\n\nTypescript is great on its own, but needing to deal with implicit type inclusion is a pain. You do not want DOM types in your Node library? You've just blocked the import of u/ types! You want to utilize ES6 imports? Yes, it suddenly doesn't work because some package uses commonjs need s somewhere down the node modules tree. All of the solutions are outdated stackoverflow answers that no longer apply or function, and the problem is handled by removing and reinstalling node modules.\n\nDo you want to include libraries in your Chrome web extension? Simply copy and paste this &gt;200-line webpack configuration. Wait, you want to utilize a tool like sass or typescript as well? Then either understand the intricacies of webpack or simply use Parcel. However, webextension manifest v3 is not supported.\n\nPNPM is also a handy tool if you don't want to redownload hundreds of gigabytes of npm packages every time you run npm install. The disadvantage is that you must always Google solutions for employing it in your projects. The same is true for yarn.\n\nAnd the list of issues goes on and on. With each new tool and library, the number of workarounds grows and the situation becomes more difficult.\n\nEverything appears to be easy on the surface, yet it's a huge tangle that will eventually break. Nobody explains how things operate or how to set them up; instead, they offer a template, copypaste boilerplate, or cli tool instead of making it simple to install a library and use it (create-react-app, vue-cli comes to mind). It's simply a big mess, and I'm not sure how to get out of it without going insane. Does anyone else have this problem? How can one get out of this situation?\n\n(I don't intend any disrespect to the tool's creators.)", "author_fullname": "t2_d2xe0c33", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you too spend more time configuring tooling and troubleshooting package issues than you do working?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/program", "hidden": false, "pwls": null, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znp5d6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671224873.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.program", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The ecosystem is full of amazing tools that make the developer&amp;#39;s job considerably simpler. Typescript, npm, pnpm, parcel, webpack, node, babel... yet getting them to work together is quite difficult.&lt;/p&gt;\n\n&lt;p&gt;Typescript is great on its own, but needing to deal with implicit type inclusion is a pain. You do not want DOM types in your Node library? You&amp;#39;ve just blocked the import of u/ types! You want to utilize ES6 imports? Yes, it suddenly doesn&amp;#39;t work because some package uses commonjs need s somewhere down the node modules tree. All of the solutions are outdated stackoverflow answers that no longer apply or function, and the problem is handled by removing and reinstalling node modules.&lt;/p&gt;\n\n&lt;p&gt;Do you want to include libraries in your Chrome web extension? Simply copy and paste this &amp;gt;200-line webpack configuration. Wait, you want to utilize a tool like sass or typescript as well? Then either understand the intricacies of webpack or simply use Parcel. However, webextension manifest v3 is not supported.&lt;/p&gt;\n\n&lt;p&gt;PNPM is also a handy tool if you don&amp;#39;t want to redownload hundreds of gigabytes of npm packages every time you run npm install. The disadvantage is that you must always Google solutions for employing it in your projects. The same is true for yarn.&lt;/p&gt;\n\n&lt;p&gt;And the list of issues goes on and on. With each new tool and library, the number of workarounds grows and the situation becomes more difficult.&lt;/p&gt;\n\n&lt;p&gt;Everything appears to be easy on the surface, yet it&amp;#39;s a huge tangle that will eventually break. Nobody explains how things operate or how to set them up; instead, they offer a template, copypaste boilerplate, or cli tool instead of making it simple to install a library and use it (create-react-app, vue-cli comes to mind). It&amp;#39;s simply a big mess, and I&amp;#39;m not sure how to get out of it without going insane. Does anyone else have this problem? How can one get out of this situation?&lt;/p&gt;\n\n&lt;p&gt;(I don&amp;#39;t intend any disrespect to the tool&amp;#39;s creators.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5737d110-661e-11eb-8a81-0e02b289e523", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2rvy6", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff8717", "id": "znp5d6", "is_robot_indexable": true, "report_reasons": null, "author": "More_Breakfast2084", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/program/comments/znp5d6/do_you_too_spend_more_time_configuring_tooling/", "parent_whitelist_status": null, "stickied": false, "url": "https://old.reddit.com/r/program/comments/znp5d6/do_you_too_spend_more_time_configuring_tooling/", "subreddit_subscribers": 579, "created_utc": 1671224873.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1671225168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.program", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/program/comments/znp5d6/do_you_too_spend_more_time_configuring_tooling/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znp9do", "is_robot_indexable": true, "report_reasons": null, "author": "More_Breakfast2084", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_znp5d6", "author_flair_text_color": null, "permalink": "/r/datascience/comments/znp9do/do_you_too_spend_more_time_configuring_tooling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/program/comments/znp5d6/do_you_too_spend_more_time_configuring_tooling/", "subreddit_subscribers": 827672, "created_utc": 1671225168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm fitting my dataset of counts to a Tweedie distribution.  \n\nThe following attached pictures are outputs from the same Tweedie model with power parameter = 1.  The only difference being one uses the default IRLS optimizer and the other one uses a powell optimizer.  How can the resulting p values and coefficients be so drastically different as a result?  Isn't this changing hte entire result based on what optimizer I use?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/qezvctsgjc6a1.png?width=398&amp;format=png&amp;auto=webp&amp;s=4892fe86eb5c1e1c533bae0a2ac7e518ddd58f4c\n\nhttps://preview.redd.it/gvogvw6fjc6a1.png?width=402&amp;format=png&amp;auto=webp&amp;s=b1235829b0f89063b92e96f2a7d157d5660fcf84", "author_fullname": "t2_8avdky0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does my choice of optimizer when fitting my model in linear regression affect output so much in statsmodels?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": false, "media_metadata": {"qezvctsgjc6a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 68, "x": 108, "u": "https://preview.redd.it/qezvctsgjc6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e178d5a2dd9227c876771488ffd3354451931612"}, {"y": 136, "x": 216, "u": "https://preview.redd.it/qezvctsgjc6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=90179deba8bfed481f1730256b50f62bc6aa5ae9"}, {"y": 201, "x": 320, "u": "https://preview.redd.it/qezvctsgjc6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=06b705e235df91ca760d0fcb3b2186752e9fcac3"}], "s": {"y": 251, "x": 398, "u": "https://preview.redd.it/qezvctsgjc6a1.png?width=398&amp;format=png&amp;auto=webp&amp;s=4892fe86eb5c1e1c533bae0a2ac7e518ddd58f4c"}, "id": "qezvctsgjc6a1"}, "gvogvw6fjc6a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 64, "x": 108, "u": "https://preview.redd.it/gvogvw6fjc6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=20339880f7e119066c70183881ec26358ab31112"}, {"y": 128, "x": 216, "u": "https://preview.redd.it/gvogvw6fjc6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=612399e0ba40685fbac11af5099644b599ef8062"}, {"y": 191, "x": 320, "u": "https://preview.redd.it/gvogvw6fjc6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd22d9cad037f7af7a9e268cedf0e5c0f176c9b5"}], "s": {"y": 240, "x": 402, "u": "https://preview.redd.it/gvogvw6fjc6a1.png?width=402&amp;format=png&amp;auto=webp&amp;s=b1235829b0f89063b92e96f2a7d157d5660fcf84"}, "id": "gvogvw6fjc6a1"}}, "name": "t3_znsn6h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0590K7z0RhEpHNK74oXe46WZsRLqc66PqiID1Y_Kdtg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671234181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m fitting my dataset of counts to a Tweedie distribution.  &lt;/p&gt;\n\n&lt;p&gt;The following attached pictures are outputs from the same Tweedie model with power parameter = 1.  The only difference being one uses the default IRLS optimizer and the other one uses a powell optimizer.  How can the resulting p values and coefficients be so drastically different as a result?  Isn&amp;#39;t this changing hte entire result based on what optimizer I use?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qezvctsgjc6a1.png?width=398&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4892fe86eb5c1e1c533bae0a2ac7e518ddd58f4c\"&gt;https://preview.redd.it/qezvctsgjc6a1.png?width=398&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4892fe86eb5c1e1c533bae0a2ac7e518ddd58f4c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gvogvw6fjc6a1.png?width=402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1235829b0f89063b92e96f2a7d157d5660fcf84\"&gt;https://preview.redd.it/gvogvw6fjc6a1.png?width=402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1235829b0f89063b92e96f2a7d157d5660fcf84&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znsn6h", "is_robot_indexable": true, "report_reasons": null, "author": "Significant-Work-204", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znsn6h/why_does_my_choice_of_optimizer_when_fitting_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znsn6h/why_does_my_choice_of_optimizer_when_fitting_my/", "subreddit_subscribers": 827672, "created_utc": 1671234181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Specifically on feature engineering and feature selection best practices, and particularly on tabular data.", "author_fullname": "t2_k4ol07vh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book recommendation on feature engineering &amp; selection?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znu9n6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671238902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Specifically on feature engineering and feature selection best practices, and particularly on tabular data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znu9n6", "is_robot_indexable": true, "report_reasons": null, "author": "Neosinic", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znu9n6/book_recommendation_on_feature_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znu9n6/book_recommendation_on_feature_engineering/", "subreddit_subscribers": 827672, "created_utc": 1671238902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m 27 - through my 20s I\u2019ve had some industry changes, education pivots etc - but I finally found a bit of a niche in data science, if you consider data viz a vertical. \n\nI was hired as a data viz specialist this summer - finally fully getting away from my biz management background. I\u2019m also a CS grad student (on and off), which also contributes to imposter syndrome (I struggle a bit). Anyways, in many ways, I\u2019m still sort of entry level. \n\nI work in an agile environment with 3-4 data engineers, my boss is an ex sr analyst/engineer, and some salesforce folks. I just feel\u2026stupid. They make these amazing things with data happen in queries, I just make it pretty dashboards and do some side admin work (notifications/comms). I get good feedback, I think I\u2019m liked, and I\u2019m the only data viz person remaining - I was an internal hire and we\u2019ve tried 3 consultants to assist with data viz but were all canned for poor performance. So in a way, I am finding some stride in that I\u2019m *the* person creating part of the experience for the end user. \n\nBut then sometimes I feel capped. I\u2019m not in SQL a lot. I have experience in Data Viz tools (Tableau, Power BI), but am mostly using a Salesforce equivalent. I do some light pseudo-SAQL coding (which I\u2019m not sure is even great) but that\u2019s it. I mean, a lot of my queries are sort of trial and error to get the desired output. It works, I\u2019m not always fully sure why, I\u2019m sure there\u2019s a way to do what I did in 5 less lines. But I don\u2019t know. I also manage the downstream dataflows, but that\u2019s mostly simple joins and reporting to DE when a job fails. \n\nIn some ways I\u2019m learning\u2026I think. But in others, I\u2019m wondering if there\u2019s growth in data viz. I love it. But it feels like I\u2019m taking all the true work of the smart people and just doing the work they don\u2019t have bandwidth too. Kind of like a designer and not a core data person. I could be wrong. Most of the time my boss has most of the ideas and I use them. I go back to other dashboards and tweak what worked there and move it to mine. I just feel dumb.\n\nAnd then, what prompted this, is when I worked with the DE team the last few weeks. My only \u201cdata viz\u201d equivalent is another consultant with like, 20 years experience, but he hates data viz and mostly hides in Salesforce tools and data management work. But even then, he knows a lot. We sync with data engineering on topics like data masking, AWS keys, object creation, schemas in tables\u2026I\u2019m just so lost. It\u2019s above me. Sometimes I try to absorb as much as I can, others I just need not to learn something and overwhelm myself because I\u2019m at peak capacity myself. \n\nBut we talk, they solve it. I didn\u2019t even fully understand what buckets are in AWS - dev, prod, etc. I feel like I\u2019m quiet in meetings and let the smart people talk it out. Because I\u2019m out of SQL, I feel like that skill isn\u2019t as sharp. And we\u2019re hiring another core analyst, so my focus remains on viz. and don\u2019t get me wrong - I love viz. I think it\u2019s sort of my niche - but I do want opportunity to do more, earn more, and more so contribute to my team and be useful. Right now it just feels like I\u2019m running alongside everyone trying to keep up, but not fully \u201cgetting\u201d these concepts of data work we\u2019re doing. And obviously I don\u2019t want to ask and sound stupid during meetings. And my boss has limited bandwidth and can\u2019t just teach me things with all this time he doesn\u2019t have. Even I. SQL, I do have the database but have no idea what to do with it. I was told to look into ROW_NUMBER, NOLOCK, etc - but still feel dumbfounded by simple concepts. \n\nThis is mostly a vent. I just feel like a kinda not smart person benefitting from the work of great people, and being unable to understand these concepts. Yes I\u2019m new in a way, but I want to do more and help more. And I\u2019m just unsure what I\u2019m doing is right or if I get all this feedback because honestly, the only skill I feel I have is just following direction", "author_fullname": "t2_ukrr8abp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I work in Data Viz in my team and am feeling bigtime imposter syndrome around my boss and DEs/DAs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znzgsr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671256429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m 27 - through my 20s I\u2019ve had some industry changes, education pivots etc - but I finally found a bit of a niche in data science, if you consider data viz a vertical. &lt;/p&gt;\n\n&lt;p&gt;I was hired as a data viz specialist this summer - finally fully getting away from my biz management background. I\u2019m also a CS grad student (on and off), which also contributes to imposter syndrome (I struggle a bit). Anyways, in many ways, I\u2019m still sort of entry level. &lt;/p&gt;\n\n&lt;p&gt;I work in an agile environment with 3-4 data engineers, my boss is an ex sr analyst/engineer, and some salesforce folks. I just feel\u2026stupid. They make these amazing things with data happen in queries, I just make it pretty dashboards and do some side admin work (notifications/comms). I get good feedback, I think I\u2019m liked, and I\u2019m the only data viz person remaining - I was an internal hire and we\u2019ve tried 3 consultants to assist with data viz but were all canned for poor performance. So in a way, I am finding some stride in that I\u2019m &lt;em&gt;the&lt;/em&gt; person creating part of the experience for the end user. &lt;/p&gt;\n\n&lt;p&gt;But then sometimes I feel capped. I\u2019m not in SQL a lot. I have experience in Data Viz tools (Tableau, Power BI), but am mostly using a Salesforce equivalent. I do some light pseudo-SAQL coding (which I\u2019m not sure is even great) but that\u2019s it. I mean, a lot of my queries are sort of trial and error to get the desired output. It works, I\u2019m not always fully sure why, I\u2019m sure there\u2019s a way to do what I did in 5 less lines. But I don\u2019t know. I also manage the downstream dataflows, but that\u2019s mostly simple joins and reporting to DE when a job fails. &lt;/p&gt;\n\n&lt;p&gt;In some ways I\u2019m learning\u2026I think. But in others, I\u2019m wondering if there\u2019s growth in data viz. I love it. But it feels like I\u2019m taking all the true work of the smart people and just doing the work they don\u2019t have bandwidth too. Kind of like a designer and not a core data person. I could be wrong. Most of the time my boss has most of the ideas and I use them. I go back to other dashboards and tweak what worked there and move it to mine. I just feel dumb.&lt;/p&gt;\n\n&lt;p&gt;And then, what prompted this, is when I worked with the DE team the last few weeks. My only \u201cdata viz\u201d equivalent is another consultant with like, 20 years experience, but he hates data viz and mostly hides in Salesforce tools and data management work. But even then, he knows a lot. We sync with data engineering on topics like data masking, AWS keys, object creation, schemas in tables\u2026I\u2019m just so lost. It\u2019s above me. Sometimes I try to absorb as much as I can, others I just need not to learn something and overwhelm myself because I\u2019m at peak capacity myself. &lt;/p&gt;\n\n&lt;p&gt;But we talk, they solve it. I didn\u2019t even fully understand what buckets are in AWS - dev, prod, etc. I feel like I\u2019m quiet in meetings and let the smart people talk it out. Because I\u2019m out of SQL, I feel like that skill isn\u2019t as sharp. And we\u2019re hiring another core analyst, so my focus remains on viz. and don\u2019t get me wrong - I love viz. I think it\u2019s sort of my niche - but I do want opportunity to do more, earn more, and more so contribute to my team and be useful. Right now it just feels like I\u2019m running alongside everyone trying to keep up, but not fully \u201cgetting\u201d these concepts of data work we\u2019re doing. And obviously I don\u2019t want to ask and sound stupid during meetings. And my boss has limited bandwidth and can\u2019t just teach me things with all this time he doesn\u2019t have. Even I. SQL, I do have the database but have no idea what to do with it. I was told to look into ROW_NUMBER, NOLOCK, etc - but still feel dumbfounded by simple concepts. &lt;/p&gt;\n\n&lt;p&gt;This is mostly a vent. I just feel like a kinda not smart person benefitting from the work of great people, and being unable to understand these concepts. Yes I\u2019m new in a way, but I want to do more and help more. And I\u2019m just unsure what I\u2019m doing is right or if I get all this feedback because honestly, the only skill I feel I have is just following direction&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znzgsr", "is_robot_indexable": true, "report_reasons": null, "author": "hshzhsnnahsbs", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znzgsr/i_work_in_data_viz_in_my_team_and_am_feeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znzgsr/i_work_in_data_viz_in_my_team_and_am_feeling/", "subreddit_subscribers": 827672, "created_utc": 1671256429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to test the effects of training set size on the accuracy of the final model. The total size of my full dataset is 1500 values. What I want to do is to train with small, medium, and large training sets, and to reduce variability, using three different random splits for a total of nine models.\n\nHow should I go about splitting the data into train and test sets? For each random split, do I take a random 20% test set first, and then from the remaining values randomly generate the differently sized training sets? When looking at the effects of training set size, should the smaller training sets be a subset of the next larger training set or completely random? How do I go about analyzing the results and comparing the nine different models?", "author_fullname": "t2_gr2i2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you properly split a dataset into random train and test sets using cross-validation to test the effects of training set size?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znich0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671207213.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to test the effects of training set size on the accuracy of the final model. The total size of my full dataset is 1500 values. What I want to do is to train with small, medium, and large training sets, and to reduce variability, using three different random splits for a total of nine models.&lt;/p&gt;\n\n&lt;p&gt;How should I go about splitting the data into train and test sets? For each random split, do I take a random 20% test set first, and then from the remaining values randomly generate the differently sized training sets? When looking at the effects of training set size, should the smaller training sets be a subset of the next larger training set or completely random? How do I go about analyzing the results and comparing the nine different models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znich0", "is_robot_indexable": true, "report_reasons": null, "author": "ghostoftheuniverse", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znich0/how_do_you_properly_split_a_dataset_into_random/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znich0/how_do_you_properly_split_a_dataset_into_random/", "subreddit_subscribers": 827672, "created_utc": 1671207213.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As the title says, I\u2019m looking for a python and sql refresher. Maybe a video series or lecture notes/books that I can finish over the weekend. I\u2019ve an interview coming up and I need to refresh my concepts. \n\nThank you.", "author_fullname": "t2_1x543wh3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A good, elaborate refresher for python and SQL for interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znxxpk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671250905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says, I\u2019m looking for a python and sql refresher. Maybe a video series or lecture notes/books that I can finish over the weekend. I\u2019ve an interview coming up and I need to refresh my concepts. &lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znxxpk", "is_robot_indexable": true, "report_reasons": null, "author": "sanket39", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znxxpk/a_good_elaborate_refresher_for_python_and_sql_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znxxpk/a_good_elaborate_refresher_for_python_and_sql_for/", "subreddit_subscribers": 827672, "created_utc": 1671250905.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m adapting one of my old algorithms where adds features based on some weight vector and then recalculates a score to see if adding the new feature helps.  \n\nI\u2019m testing it on the iris dataset with 996 noise features and noticed that sometimes it adds around 50-90 noise features because it increases the accuracy.  Less features is more in this case so I\u2019m wondering if there is any penalty I can add that is on the same scale as the scoring metric (eg accuracy for classification which ranges from [0,1] or negative mean squared error which goes from (-inf,0]) but based on the number of features.  Basically saying \u201cis it really worth it to add this feature?\u201d\n\nI know there\u2019s L1 and L2 penalty for linear models but I don\u2019t think that applies here because if o square the number of features or something it will be largely out of scale with the scoring metric (eg adding a feature has a positive change in accuracy of 0.3 so let\u2019s keep it, another feature only adds to the accuracy by 0.001 so let\u2019s not add it). \n\nAny brainstorming on this would be very helpful so don\u2019t hesitate. I\u2019m currently stuck on this idea.", "author_fullname": "t2_d7ung", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ideas for adding a penalty on a feature selection algorithm?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znh7kc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671204407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m adapting one of my old algorithms where adds features based on some weight vector and then recalculates a score to see if adding the new feature helps.  &lt;/p&gt;\n\n&lt;p&gt;I\u2019m testing it on the iris dataset with 996 noise features and noticed that sometimes it adds around 50-90 noise features because it increases the accuracy.  Less features is more in this case so I\u2019m wondering if there is any penalty I can add that is on the same scale as the scoring metric (eg accuracy for classification which ranges from [0,1] or negative mean squared error which goes from (-inf,0]) but based on the number of features.  Basically saying \u201cis it really worth it to add this feature?\u201d&lt;/p&gt;\n\n&lt;p&gt;I know there\u2019s L1 and L2 penalty for linear models but I don\u2019t think that applies here because if o square the number of features or something it will be largely out of scale with the scoring metric (eg adding a feature has a positive change in accuracy of 0.3 so let\u2019s keep it, another feature only adds to the accuracy by 0.001 so let\u2019s not add it). &lt;/p&gt;\n\n&lt;p&gt;Any brainstorming on this would be very helpful so don\u2019t hesitate. I\u2019m currently stuck on this idea.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znh7kc", "is_robot_indexable": true, "report_reasons": null, "author": "o-rka", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znh7kc/ideas_for_adding_a_penalty_on_a_feature_selection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znh7kc/ideas_for_adding_a_penalty_on_a_feature_selection/", "subreddit_subscribers": 827672, "created_utc": 1671204407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been trying to land my first data science role in canada since past two years, but haven't been successful yet. I was always interested in Machine learning and come from Mechanical engineering background. Here is what I have done so far. \n1. Started with ML course on coursera\n2. Did a data science boot camp, learned python, PostgreSQL, scikit learn\n3. Did 10+ guided and unguided projects\n4. Achieved Data scientist certification and AWS ML specialty certification\n5. Doing a mentorship program, refined resume, LinkedIn and other job boards profiles\n6. Applied to easyapply jobs, applied on websites, asked people for referrals and applied.\nHave been doing it while working full-time. But no luck. It seems a never ending process.", "author_fullname": "t2_cqmt7g11", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for the opportunity/advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znq4vt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671227493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been trying to land my first data science role in canada since past two years, but haven&amp;#39;t been successful yet. I was always interested in Machine learning and come from Mechanical engineering background. Here is what I have done so far. \n1. Started with ML course on coursera\n2. Did a data science boot camp, learned python, PostgreSQL, scikit learn\n3. Did 10+ guided and unguided projects\n4. Achieved Data scientist certification and AWS ML specialty certification\n5. Doing a mentorship program, refined resume, LinkedIn and other job boards profiles\n6. Applied to easyapply jobs, applied on websites, asked people for referrals and applied.\nHave been doing it while working full-time. But no luck. It seems a never ending process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "znq4vt", "is_robot_indexable": true, "report_reasons": null, "author": "PotentialFlow9897", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znq4vt/looking_for_the_opportunityadvice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znq4vt/looking_for_the_opportunityadvice/", "subreddit_subscribers": 827672, "created_utc": 1671227493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone, does anyone know if chatGPT has been trained on Kaggle projects? If so, it should already be pretty good at a lot of DS stuff, right?\n\nIf not, I think it's only a matter of time before they will include that, which could create a very powerful DS personal assistant.\n\nI guess it could be challenging to train it on large datasets specifically, but I'm sure there are some smart ways to make that part more efficient, like only using a sample of each data set. Plus, there is the legal question if Kaggle would allow openAI to use their data.\n\nDo you guys have any thoughts?", "author_fullname": "t2_12hyas", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Was ChatGPT trained on Kaggle and other DS coding platforms?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo2pj1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671269496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, does anyone know if chatGPT has been trained on Kaggle projects? If so, it should already be pretty good at a lot of DS stuff, right?&lt;/p&gt;\n\n&lt;p&gt;If not, I think it&amp;#39;s only a matter of time before they will include that, which could create a very powerful DS personal assistant.&lt;/p&gt;\n\n&lt;p&gt;I guess it could be challenging to train it on large datasets specifically, but I&amp;#39;m sure there are some smart ways to make that part more efficient, like only using a sample of each data set. Plus, there is the legal question if Kaggle would allow openAI to use their data.&lt;/p&gt;\n\n&lt;p&gt;Do you guys have any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo2pj1", "is_robot_indexable": true, "report_reasons": null, "author": "ikke89", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo2pj1/was_chatgpt_trained_on_kaggle_and_other_ds_coding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo2pj1/was_chatgpt_trained_on_kaggle_and_other_ds_coding/", "subreddit_subscribers": 827672, "created_utc": 1671269496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi there,\n\nCan someone please share resources for data science project documentation? \n\nWhat are the data science standards you use? e.g. Project proposal guidelines, EDA, ML guidelines etc.", "author_fullname": "t2_qltli13", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science Project Documentation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zntwkw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671237811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;Can someone please share resources for data science project documentation? &lt;/p&gt;\n\n&lt;p&gt;What are the data science standards you use? e.g. Project proposal guidelines, EDA, ML guidelines etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zntwkw", "is_robot_indexable": true, "report_reasons": null, "author": "ashishtele", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zntwkw/data_science_project_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zntwkw/data_science_project_documentation/", "subreddit_subscribers": 827672, "created_utc": 1671237811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have tabular data where each group of 100 rows represents a deployment of a specific geometry that has certain features measured. For example, I have 10000 deployments stored in a column called \"Dep,\" which ranges from 1 to 10000. When fitting this data to an xgboost model, the model may mistakenly assume that the deployment numbers have an ordering relationship (e.g., that deployment 1 is less than deployment 2, which is less than deployment 3, and so on). However, this is not the case because all deployments are independent of each other. On the other hand, if I remove the \"Dep\" column, the model will not be able to differentiate between the different deployment groups and their unique geometries.\n\nI got the idea of one hot encoding of the column dep, but it's useless as the training dataset has 7000 deployments while the test dataset has 3000. If I do a one hot encoding, the shapes will not match (along with a slew of other issues). Better think of another approach.\n\nWhat is the solution to this issue?", "author_fullname": "t2_7uwwf65z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to prevent my model from mistaking categorical feature for ordinal feature", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znrdn7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671230728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tabular data where each group of 100 rows represents a deployment of a specific geometry that has certain features measured. For example, I have 10000 deployments stored in a column called &amp;quot;Dep,&amp;quot; which ranges from 1 to 10000. When fitting this data to an xgboost model, the model may mistakenly assume that the deployment numbers have an ordering relationship (e.g., that deployment 1 is less than deployment 2, which is less than deployment 3, and so on). However, this is not the case because all deployments are independent of each other. On the other hand, if I remove the &amp;quot;Dep&amp;quot; column, the model will not be able to differentiate between the different deployment groups and their unique geometries.&lt;/p&gt;\n\n&lt;p&gt;I got the idea of one hot encoding of the column dep, but it&amp;#39;s useless as the training dataset has 7000 deployments while the test dataset has 3000. If I do a one hot encoding, the shapes will not match (along with a slew of other issues). Better think of another approach.&lt;/p&gt;\n\n&lt;p&gt;What is the solution to this issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znrdn7", "is_robot_indexable": true, "report_reasons": null, "author": "Hamdi_bks", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znrdn7/how_to_prevent_my_model_from_mistaking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znrdn7/how_to_prevent_my_model_from_mistaking/", "subreddit_subscribers": 827672, "created_utc": 1671230728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What are some common functions/transformations you run on a dataset before training and what libraries do you use, or is it a custom script?", "author_fullname": "t2_6i7on", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Common dataset prep operations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo2tb9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671269962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some common functions/transformations you run on a dataset before training and what libraries do you use, or is it a custom script?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo2tb9", "is_robot_indexable": true, "report_reasons": null, "author": "jy2k", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo2tb9/common_dataset_prep_operations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo2tb9/common_dataset_prep_operations/", "subreddit_subscribers": 827672, "created_utc": 1671269962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Does anyone here have experience working in a data role at a UN organization or a development bank or something similar? What is your day to day like and also how did you get that role?", "author_fullname": "t2_i049nix9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone work in large UN/ Developmental organizations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znts42", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671237428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone here have experience working in a data role at a UN organization or a development bank or something similar? What is your day to day like and also how did you get that role?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znts42", "is_robot_indexable": true, "report_reasons": null, "author": "vizualizing123", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znts42/anyone_work_in_large_un_developmental/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znts42/anyone_work_in_large_un_developmental/", "subreddit_subscribers": 827672, "created_utc": 1671237428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So, I\u2019ve built a program that scrapes housing data (address, location, beds, baths, square footage, etc) from a a local real estate site. I want to grab the same data (within the same city) from a few different sites and compare it all. What would be the best way to structure this project? Should I write multiple functions to pull from each website, or would this be too messy? Would anyone be able to share some examples of projects that pull the same data from multiple sites and compare them? thank you!!", "author_fullname": "t2_b31o1g0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Structuring a web scraper that analyzes data from multiple sites?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znkku4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671212987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I\u2019ve built a program that scrapes housing data (address, location, beds, baths, square footage, etc) from a a local real estate site. I want to grab the same data (within the same city) from a few different sites and compare it all. What would be the best way to structure this project? Should I write multiple functions to pull from each website, or would this be too messy? Would anyone be able to share some examples of projects that pull the same data from multiple sites and compare them? thank you!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znkku4", "is_robot_indexable": true, "report_reasons": null, "author": "treebrat", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znkku4/structuring_a_web_scraper_that_analyzes_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znkku4/structuring_a_web_scraper_that_analyzes_data_from/", "subreddit_subscribers": 827672, "created_utc": 1671212987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to do a new project in R, where I'm working with a lot of data (about 6GB of text) my inefficient code is tying me down a lot and was hoping maybe you guys had some tricks.\n\nI'm supposed to load in large .txt files, each file should be split into shorter parts consisting of 10 words each, they are then put into my data frame with each word being put into one of 10 columns. \n\nThe problem is that my code is slow, like really slow. It takes around 20 minutes to load the first 100000 rows of text into my dataframe, which is less than 1% of my data. I have also noticed that code slows down the further it takes to run, it takes about a second to load in the first 3000 rows, and 15 seconds for the first 10000 rows.   \n\nWhat is the main bottleneck slowing me down? Should I just rewrite the code entirely in another language like python?\n\nMy current code below:\n\n    #setup\n    folder &lt;- \"C:/Users/A_tiny/\"\n    \n    # Get a list of files in the folder\n    files &lt;- list.files(folder)\n    \n    # Create an empty data frame to hold the indexed words\n    df &lt;- data.frame(word1 = character(), word2 = character(), word3 = character(), word4 = character(),\n    word5 = character(), word6 = character(), word7 = character(), word8 = character(),\n    word9 = character(), word10 = character(), index = integer())\n    \n    for (file in files) {\n    # Check if the file is a .epub.txt file\n    if (grepl(\".epub.txt$\", file)) {\n    # Read the .epub.txt file into a string variable\n    epub_txt &lt;- readLines(paste0(folder, file))\n    \n    # Split the string into a vector of words\n    words &lt;- unlist(strsplit(epub_txt, \" \"))\n    \n    # Loop through the vector of words and add the next 10 words to the data frame, skipping 10 words at a time\n    for (i in seq(1, length(words), by=10)) {\n      # Create a data frame with the next 10 words and the index\n      df_temp &lt;- data.frame(word1 = words[i], word2 = words[i+1], word3 = words[i+2], word4 = words[i+3],\n                            word5 = words[i+4], word6 = words[i+5], word7 = words[i+6], word8 = words[i+7],\n                            word9 = words[i+8], word10 = words[i+9], index = i)\n      # Add the data frame to the main data frame\n      df &lt;- rbind(df, df_temp)\n    }\n      }\n    }", "author_fullname": "t2_gx7hn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The best approach for rewriting my code more efficiently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zo4p8z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671277772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to do a new project in R, where I&amp;#39;m working with a lot of data (about 6GB of text) my inefficient code is tying me down a lot and was hoping maybe you guys had some tricks.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m supposed to load in large .txt files, each file should be split into shorter parts consisting of 10 words each, they are then put into my data frame with each word being put into one of 10 columns. &lt;/p&gt;\n\n&lt;p&gt;The problem is that my code is slow, like really slow. It takes around 20 minutes to load the first 100000 rows of text into my dataframe, which is less than 1% of my data. I have also noticed that code slows down the further it takes to run, it takes about a second to load in the first 3000 rows, and 15 seconds for the first 10000 rows.   &lt;/p&gt;\n\n&lt;p&gt;What is the main bottleneck slowing me down? Should I just rewrite the code entirely in another language like python?&lt;/p&gt;\n\n&lt;p&gt;My current code below:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#setup\nfolder &amp;lt;- &amp;quot;C:/Users/A_tiny/&amp;quot;\n\n# Get a list of files in the folder\nfiles &amp;lt;- list.files(folder)\n\n# Create an empty data frame to hold the indexed words\ndf &amp;lt;- data.frame(word1 = character(), word2 = character(), word3 = character(), word4 = character(),\nword5 = character(), word6 = character(), word7 = character(), word8 = character(),\nword9 = character(), word10 = character(), index = integer())\n\nfor (file in files) {\n# Check if the file is a .epub.txt file\nif (grepl(&amp;quot;.epub.txt$&amp;quot;, file)) {\n# Read the .epub.txt file into a string variable\nepub_txt &amp;lt;- readLines(paste0(folder, file))\n\n# Split the string into a vector of words\nwords &amp;lt;- unlist(strsplit(epub_txt, &amp;quot; &amp;quot;))\n\n# Loop through the vector of words and add the next 10 words to the data frame, skipping 10 words at a time\nfor (i in seq(1, length(words), by=10)) {\n  # Create a data frame with the next 10 words and the index\n  df_temp &amp;lt;- data.frame(word1 = words[i], word2 = words[i+1], word3 = words[i+2], word4 = words[i+3],\n                        word5 = words[i+4], word6 = words[i+5], word7 = words[i+6], word8 = words[i+7],\n                        word9 = words[i+8], word10 = words[i+9], index = i)\n  # Add the data frame to the main data frame\n  df &amp;lt;- rbind(df, df_temp)\n}\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo4p8z", "is_robot_indexable": true, "report_reasons": null, "author": "_Just7_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo4p8z/the_best_approach_for_rewriting_my_code_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo4p8z/the_best_approach_for_rewriting_my_code_more/", "subreddit_subscribers": 827672, "created_utc": 1671277772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7v124vu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hello everyone, is there any ebook for data science for free?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo3pac", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "spoiler", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671273625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": true, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo3pac", "is_robot_indexable": true, "report_reasons": null, "author": "Same_Estimate8383", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo3pac/hello_everyone_is_there_any_ebook_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo3pac/hello_everyone_is_there_any_ebook_for_data/", "subreddit_subscribers": 827672, "created_utc": 1671273625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_895cqxfg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Network Graphs of all Goals &amp; Goal Chances created by WC2022 finalists, visualized.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_zo16rm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/i6VMGDsopWj6_KTX59CBPBDTTP79_O1tnicjhjnaaXo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671263108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "wcxgnetworks.streamlit.app", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://wcxgnetworks.streamlit.app/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?auto=webp&amp;s=c1002e8b9aefe041811736d6a215bb038eb7aa8b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a90792efc4e26380ce1f4033e8c0c517d804810d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=78a5ebc886dd56810d6c36026f6e6552966517a4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba6d7ad1934366ab73f23c0cae3d61a24560f20c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ab934a1101b6c1e200b46b117784bdc9b7529dcb", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=798f95e131655041677300c64511006ddde07b99", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3f05f99a6150e14c8df1b370cc580e98b0bb350c", "width": 1080, "height": 567}], "variants": {}, "id": "PIe-3OpMNP61TpZzSjrFl3aA2rFm7yVHhU-B_BoSKhk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo16rm", "is_robot_indexable": true, "report_reasons": null, "author": "noimgonnalie", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo16rm/network_graphs_of_all_goals_goal_chances_created/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://wcxgnetworks.streamlit.app/", "subreddit_subscribers": 827672, "created_utc": 1671263108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I made a scorecard using WOE and logistic regression. I am also binning the feature (eg. age: 10-20,20-30,...). After i calculated coefficient from logistic regression model we can make a scorecard from it (eg. age 10-20: 10 pts, 20-30: 20 pts ...). Now I am trying to use other model like XGBoost or neural network to make the same scorecard like that. But with logistic regression, I have coefficient and intercept to calculate score for each bin but tree model and neural network doesn't have it. So I want to ask for the formula, book or any resources about this problem. Thanks for reading! Hope you have a good day.", "author_fullname": "t2_9axjxbxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scorecard in credit scoring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo03ou", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671258812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made a scorecard using WOE and logistic regression. I am also binning the feature (eg. age: 10-20,20-30,...). After i calculated coefficient from logistic regression model we can make a scorecard from it (eg. age 10-20: 10 pts, 20-30: 20 pts ...). Now I am trying to use other model like XGBoost or neural network to make the same scorecard like that. But with logistic regression, I have coefficient and intercept to calculate score for each bin but tree model and neural network doesn&amp;#39;t have it. So I want to ask for the formula, book or any resources about this problem. Thanks for reading! Hope you have a good day.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo03ou", "is_robot_indexable": true, "report_reasons": null, "author": "Previous_Aside_8863", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo03ou/scorecard_in_credit_scoring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo03ou/scorecard_in_credit_scoring/", "subreddit_subscribers": 827672, "created_utc": 1671258812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello everyone, I have an assignment due and could really use a hand!\n\n&amp;#x200B;\n\nI have data for advertising effectiveness. I want to find out which advertising channel has the biggest impact on a company's website visits. I have been told to test for seasonality using day and month dummies. \n\nI have 7 columns total:\n\ndate, website visits, channel 1, ch 2, ch 3, ch4, ch total\n\nwhere channel 1 = number of people who saw the ad \n\n&amp;#x200B;\n\nWould this be considered multivariate time series, or panel data?\n\nThanks in advance! (And apologies if this is the wrong place)", "author_fullname": "t2_4dhmu3pg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this data multivariate time series or panel?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znhx84", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671206168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I have an assignment due and could really use a hand!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have data for advertising effectiveness. I want to find out which advertising channel has the biggest impact on a company&amp;#39;s website visits. I have been told to test for seasonality using day and month dummies. &lt;/p&gt;\n\n&lt;p&gt;I have 7 columns total:&lt;/p&gt;\n\n&lt;p&gt;date, website visits, channel 1, ch 2, ch 3, ch4, ch total&lt;/p&gt;\n\n&lt;p&gt;where channel 1 = number of people who saw the ad &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Would this be considered multivariate time series, or panel data?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance! (And apologies if this is the wrong place)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znhx84", "is_robot_indexable": true, "report_reasons": null, "author": "rushy13", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znhx84/is_this_data_multivariate_time_series_or_panel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znhx84/is_this_data_multivariate_time_series_or_panel/", "subreddit_subscribers": 827672, "created_utc": 1671206168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_kgkprqpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting a Data Science Project? These Ingredients will Help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_zndn13", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/7EN5jAHdSkAXWMXJq1E2vjDCy4k7iZXs9tK4vugYip0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671194047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dasca.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.dasca.org/world-of-big-data/article/starting-a-data-science-project-these-ingredients-will-help", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yTl3RMsq70N7CN9iPKm7ifPsRbIw0db7G19Ubt3LnPs.jpg?auto=webp&amp;s=3826f8440a54d8a96394572255da998955f4719f", "width": 800, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/yTl3RMsq70N7CN9iPKm7ifPsRbIw0db7G19Ubt3LnPs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b809bd8bb05d39f42759d1a6bea992058ba1e7c7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/yTl3RMsq70N7CN9iPKm7ifPsRbIw0db7G19Ubt3LnPs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=57ad59d4a8aa471ff28087d894a4b0e8d0f1221d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/yTl3RMsq70N7CN9iPKm7ifPsRbIw0db7G19Ubt3LnPs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e547b13576b0ddde410c9de304f2a3a61815e7ad", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/yTl3RMsq70N7CN9iPKm7ifPsRbIw0db7G19Ubt3LnPs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=42b479f29d5a65d63f70b168b706ce4365c3b559", "width": 640, "height": 336}], "variants": {}, "id": "nn4AmvDB0PE9hW5F8cfbs7FaGIi-f1fht_uawBwFHDU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zndn13", "is_robot_indexable": true, "report_reasons": null, "author": "Emily-joe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zndn13/starting_a_data_science_project_these_ingredients/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dasca.org/world-of-big-data/article/starting-a-data-science-project-these-ingredients-will-help", "subreddit_subscribers": 827672, "created_utc": 1671194047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone. I have a DS technical interview coming up. I asked the recruiter and she said it won\u2019t be a coding technical interview but they want to know more about how I would work through problems. Does anyone have any ways on preparing for this? This is my first DS interview and I\u2019m about to be a new grad so I have no idea what to expect \ud83d\ude2d", "author_fullname": "t2_6kryb76y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Technical Interview - New Grad", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znkvet", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671213710.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I have a DS technical interview coming up. I asked the recruiter and she said it won\u2019t be a coding technical interview but they want to know more about how I would work through problems. Does anyone have any ways on preparing for this? This is my first DS interview and I\u2019m about to be a new grad so I have no idea what to expect \ud83d\ude2d&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znkvet", "is_robot_indexable": true, "report_reasons": null, "author": "jadeflowersxox", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znkvet/technical_interview_new_grad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znkvet/technical_interview_new_grad/", "subreddit_subscribers": 827672, "created_utc": 1671213710.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I  am trying to make a project to show my business analytics ability to  use SQL and Python. I am trying to build a pipeline of aggregating data  into an SQL database and then analysing them in Python to make forecasts  with regression ML techniques. I was wondering if there is a datasets  that can help me with this, I already know about the Sakila database,  but is there any better one?", "author_fullname": "t2_2knag8t3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Business datasets for analytics projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znda9o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671192896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I  am trying to make a project to show my business analytics ability to  use SQL and Python. I am trying to build a pipeline of aggregating data  into an SQL database and then analysing them in Python to make forecasts  with regression ML techniques. I was wondering if there is a datasets  that can help me with this, I already know about the Sakila database,  but is there any better one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znda9o", "is_robot_indexable": true, "report_reasons": null, "author": "lordgriefter", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znda9o/business_datasets_for_analytics_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znda9o/business_datasets_for_analytics_projects/", "subreddit_subscribers": 827672, "created_utc": 1671192896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi! I have a Bachelor in Media &amp; Economics (Statistics) have 8 years of work experience in Market Research. For the last four years I have been working in Tech at one of the Big Five, running large scale quantitative &amp; qualitative research projects, analysing the data and make them palatable for stakeholders that don't get data. I like the department I am in, my team, the perks, etc. The compensation could be better but it is competitive.\n\nAs I grow in my career I am moving more and more towards a strategic storytelling role, that does not align with my interest and skills. I know that the only way to succeed in my current role is by leaning into this way of thinking. I do enjoy the data analysis part of my role, which unfortunately I am doing less of.\n\nI have come to a point now where I will have to lean into one or the other to grow in my career. My options:\n\n1. My current job is pretty relaxed, I am working flexible hours but will not grow if I don't do more strategic planning and content creation. I can do courses, mentoring, etc. to learn more about storytelling, writing documents and improve my strategic thinking. However, I feel it will always be harder for me than for some of my colleagues having different skill sets.\n2. My alternative is learning how to code. I have done SQL coding and a beginners R course in the past. I enjoyed them and it is the way my brain works. I won't be able to pivot in my current role/ department and would probably have to start my career again.\n\nI worry that I romanticise a career in data science over my current role as the grass is always greener. Can you please let me know your opinion and if you think data science is the way to the light for me?  If so, how / where should I start?\n\nI am UK based if this matters at all.", "author_fullname": "t2_8bi32rna", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am at a crossroads in my career - should I continue in my strategic role at the Big Five or retrain as a data scientist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znd9ic", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671192828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I have a Bachelor in Media &amp;amp; Economics (Statistics) have 8 years of work experience in Market Research. For the last four years I have been working in Tech at one of the Big Five, running large scale quantitative &amp;amp; qualitative research projects, analysing the data and make them palatable for stakeholders that don&amp;#39;t get data. I like the department I am in, my team, the perks, etc. The compensation could be better but it is competitive.&lt;/p&gt;\n\n&lt;p&gt;As I grow in my career I am moving more and more towards a strategic storytelling role, that does not align with my interest and skills. I know that the only way to succeed in my current role is by leaning into this way of thinking. I do enjoy the data analysis part of my role, which unfortunately I am doing less of.&lt;/p&gt;\n\n&lt;p&gt;I have come to a point now where I will have to lean into one or the other to grow in my career. My options:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;My current job is pretty relaxed, I am working flexible hours but will not grow if I don&amp;#39;t do more strategic planning and content creation. I can do courses, mentoring, etc. to learn more about storytelling, writing documents and improve my strategic thinking. However, I feel it will always be harder for me than for some of my colleagues having different skill sets.&lt;/li&gt;\n&lt;li&gt;My alternative is learning how to code. I have done SQL coding and a beginners R course in the past. I enjoyed them and it is the way my brain works. I won&amp;#39;t be able to pivot in my current role/ department and would probably have to start my career again.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I worry that I romanticise a career in data science over my current role as the grass is always greener. Can you please let me know your opinion and if you think data science is the way to the light for me?  If so, how / where should I start?&lt;/p&gt;\n\n&lt;p&gt;I am UK based if this matters at all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znd9ic", "is_robot_indexable": true, "report_reasons": null, "author": "katrincas54", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znd9ic/i_am_at_a_crossroads_in_my_career_should_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znd9ic/i_am_at_a_crossroads_in_my_career_should_i/", "subreddit_subscribers": 827672, "created_utc": 1671192828.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}