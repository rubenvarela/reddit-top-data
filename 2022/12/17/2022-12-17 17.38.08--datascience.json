{"kind": "Listing", "data": {"after": "t3_zo03ou", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7tk61jlx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Offend a data scientist in one tweet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 133, "top_awarded_type": null, "hide_score": false, "name": "t3_zo5bwf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 600, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 600, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BSxLRIlq1njHZP5Z_wamCF9V9SfkDjm9KyuGwG8B2oM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671280215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/t7n4hi55uh6a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?auto=webp&amp;s=bac6161094a9a79489f52437166b8dcb7f64d3b6", "width": 1170, "height": 1113}, "resolutions": [{"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0808045bdcd76aa0c585fab3f629abbd08b9eb4a", "width": 108, "height": 102}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d062ac3047031d4f2a8bb9edd39a1395fa6b71b", "width": 216, "height": 205}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=960fe5dbb627fe1c085be0b1f0cd2cc981a065fc", "width": 320, "height": 304}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=103f470deb4cdda1eb9ce38819483792ba3ea0b8", "width": 640, "height": 608}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7628380a74c4952d35e2d439d930c7d9b3fa6cf9", "width": 960, "height": 913}, {"url": "https://preview.redd.it/t7n4hi55uh6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=41fbefb81a833b413faf3f777ba8756ea5939573", "width": 1080, "height": 1027}], "variants": {}, "id": "cF2H-4KHwkY2zala2RSzU8criZ8swOdmyibBo-iHicE"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo5bwf", "is_robot_indexable": true, "report_reasons": null, "author": "datasciencepro", "discussion_type": null, "num_comments": 86, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo5bwf/offend_a_data_scientist_in_one_tweet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/t7n4hi55uh6a1.jpg", "subreddit_subscribers": 827738, "created_utc": 1671280215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_f900b52f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zo5cll", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 72, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 72, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/s0ynn7G8nXbTR_uXt90gEhSZHUL0mLo7H8bhVRTMixY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671280285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/49ppdolcuh6a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?auto=webp&amp;s=a7a64c1a98fd1a74d3e4355ebccf288513ca89bb", "width": 1080, "height": 1920}, "resolutions": [{"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=30310c228dfd08348c71abb5fb5527956974f114", "width": 108, "height": 192}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=913abe79dc9b0a53473b7089eccc385ca0a071e2", "width": 216, "height": 384}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d9eb5d70911cce318caf1b6c5c8eae9f1ebd5b46", "width": 320, "height": 568}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=12e7b553a6788b9cf808a1629bbe8594e5c4e1e4", "width": 640, "height": 1137}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=afbf3adf5602589e4b4e263d6ff360a7a427131d", "width": 960, "height": 1706}, {"url": "https://preview.redd.it/49ppdolcuh6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf90a16c987c9f4074158f0ba3347e2f0e27f4f5", "width": 1080, "height": 1920}], "variants": {}, "id": "W-uSuaiYuKbO5KZaKtG_pZhr1FKoXou0FiZEo6YQpfY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo5cll", "is_robot_indexable": true, "report_reasons": null, "author": "DwightScott69", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo5cll/thoughts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/49ppdolcuh6a1.jpg", "subreddit_subscribers": 827738, "created_utc": 1671280285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7tk61jlx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\u201cIt is the more personal pain of seeing a 5% GPU utilization number in production. I am offended by it.\u201d\u2014John Carmack, reflecting on his resignation from Facebook", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_zo58mo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cZiwfemgjVRIOmQFBOybgJVQoD5M-M0xI6NAsbjc-4E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671279873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/rnwr1o94th6a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/rnwr1o94th6a1.jpg?auto=webp&amp;s=d07d14c9ee35eaf63e28dd02d757ea8e64949f46", "width": 1170, "height": 1603}, "resolutions": [{"url": "https://preview.redd.it/rnwr1o94th6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=caf6b8dc8c242d703319b79ff3586a96ebb2bbe5", "width": 108, "height": 147}, {"url": "https://preview.redd.it/rnwr1o94th6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bcd31e790b7b0159f03dc8186fda740ca3621204", "width": 216, "height": 295}, {"url": "https://preview.redd.it/rnwr1o94th6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8a92720b749c95b71568fa74afcd504a7c86958c", "width": 320, "height": 438}, {"url": "https://preview.redd.it/rnwr1o94th6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1972834c67749dd15b1a2dbd162a30e2971377bb", "width": 640, "height": 876}, {"url": "https://preview.redd.it/rnwr1o94th6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f43bc9ee4f3dfe9a636c4e8f68244953f15da9ed", "width": 960, "height": 1315}, {"url": "https://preview.redd.it/rnwr1o94th6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5ae96260b0c1da53ff273796d7466de31ab13050", "width": 1080, "height": 1479}], "variants": {}, "id": "1vYv3yM3-0HM6AnWvjVL6rh8OPhjinsQ9rtfgBclYao"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo58mo", "is_robot_indexable": true, "report_reasons": null, "author": "datasciencepro", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo58mo/it_is_the_more_personal_pain_of_seeing_a_5_gpu/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/rnwr1o94th6a1.jpg", "subreddit_subscribers": 827738, "created_utc": 1671279873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm fitting my dataset of counts to a Tweedie distribution.  \n\nThe following attached pictures are outputs from the same Tweedie model with power parameter = 1.  The only difference being one uses the default IRLS optimizer and the other one uses a powell optimizer.  How can the resulting p values and coefficients be so drastically different as a result?  Isn't this changing hte entire result based on what optimizer I use?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/qezvctsgjc6a1.png?width=398&amp;format=png&amp;auto=webp&amp;s=4892fe86eb5c1e1c533bae0a2ac7e518ddd58f4c\n\nhttps://preview.redd.it/gvogvw6fjc6a1.png?width=402&amp;format=png&amp;auto=webp&amp;s=b1235829b0f89063b92e96f2a7d157d5660fcf84", "author_fullname": "t2_8avdky0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does my choice of optimizer when fitting my model in linear regression affect output so much in statsmodels?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 88, "top_awarded_type": null, "hide_score": false, "media_metadata": {"qezvctsgjc6a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 68, "x": 108, "u": "https://preview.redd.it/qezvctsgjc6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e178d5a2dd9227c876771488ffd3354451931612"}, {"y": 136, "x": 216, "u": "https://preview.redd.it/qezvctsgjc6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=90179deba8bfed481f1730256b50f62bc6aa5ae9"}, {"y": 201, "x": 320, "u": "https://preview.redd.it/qezvctsgjc6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=06b705e235df91ca760d0fcb3b2186752e9fcac3"}], "s": {"y": 251, "x": 398, "u": "https://preview.redd.it/qezvctsgjc6a1.png?width=398&amp;format=png&amp;auto=webp&amp;s=4892fe86eb5c1e1c533bae0a2ac7e518ddd58f4c"}, "id": "qezvctsgjc6a1"}, "gvogvw6fjc6a1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 64, "x": 108, "u": "https://preview.redd.it/gvogvw6fjc6a1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=20339880f7e119066c70183881ec26358ab31112"}, {"y": 128, "x": 216, "u": "https://preview.redd.it/gvogvw6fjc6a1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=612399e0ba40685fbac11af5099644b599ef8062"}, {"y": 191, "x": 320, "u": "https://preview.redd.it/gvogvw6fjc6a1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd22d9cad037f7af7a9e268cedf0e5c0f176c9b5"}], "s": {"y": 240, "x": 402, "u": "https://preview.redd.it/gvogvw6fjc6a1.png?width=402&amp;format=png&amp;auto=webp&amp;s=b1235829b0f89063b92e96f2a7d157d5660fcf84"}, "id": "gvogvw6fjc6a1"}}, "name": "t3_znsn6h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0590K7z0RhEpHNK74oXe46WZsRLqc66PqiID1Y_Kdtg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671234181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m fitting my dataset of counts to a Tweedie distribution.  &lt;/p&gt;\n\n&lt;p&gt;The following attached pictures are outputs from the same Tweedie model with power parameter = 1.  The only difference being one uses the default IRLS optimizer and the other one uses a powell optimizer.  How can the resulting p values and coefficients be so drastically different as a result?  Isn&amp;#39;t this changing hte entire result based on what optimizer I use?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qezvctsgjc6a1.png?width=398&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4892fe86eb5c1e1c533bae0a2ac7e518ddd58f4c\"&gt;https://preview.redd.it/qezvctsgjc6a1.png?width=398&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4892fe86eb5c1e1c533bae0a2ac7e518ddd58f4c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gvogvw6fjc6a1.png?width=402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1235829b0f89063b92e96f2a7d157d5660fcf84\"&gt;https://preview.redd.it/gvogvw6fjc6a1.png?width=402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1235829b0f89063b92e96f2a7d157d5660fcf84&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znsn6h", "is_robot_indexable": true, "report_reasons": null, "author": "Significant-Work-204", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znsn6h/why_does_my_choice_of_optimizer_when_fitting_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znsn6h/why_does_my_choice_of_optimizer_when_fitting_my/", "subreddit_subscribers": 827738, "created_utc": 1671234181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you too spend more time configuring tooling and troubleshooting package issues than you do working?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znp9do", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_d2xe0c33", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "program", "selftext": "The ecosystem is full of amazing tools that make the developer's job considerably simpler. Typescript, npm, pnpm, parcel, webpack, node, babel... yet getting them to work together is quite difficult.\n\nTypescript is great on its own, but needing to deal with implicit type inclusion is a pain. You do not want DOM types in your Node library? You've just blocked the import of u/ types! You want to utilize ES6 imports? Yes, it suddenly doesn't work because some package uses commonjs need s somewhere down the node modules tree. All of the solutions are outdated stackoverflow answers that no longer apply or function, and the problem is handled by removing and reinstalling node modules.\n\nDo you want to include libraries in your Chrome web extension? Simply copy and paste this &gt;200-line webpack configuration. Wait, you want to utilize a tool like sass or typescript as well? Then either understand the intricacies of webpack or simply use Parcel. However, webextension manifest v3 is not supported.\n\nPNPM is also a handy tool if you don't want to redownload hundreds of gigabytes of npm packages every time you run npm install. The disadvantage is that you must always Google solutions for employing it in your projects. The same is true for yarn.\n\nAnd the list of issues goes on and on. With each new tool and library, the number of workarounds grows and the situation becomes more difficult.\n\nEverything appears to be easy on the surface, yet it's a huge tangle that will eventually break. Nobody explains how things operate or how to set them up; instead, they offer a template, copypaste boilerplate, or cli tool instead of making it simple to install a library and use it (create-react-app, vue-cli comes to mind). It's simply a big mess, and I'm not sure how to get out of it without going insane. Does anyone else have this problem? How can one get out of this situation?\n\n(I don't intend any disrespect to the tool's creators.)", "author_fullname": "t2_d2xe0c33", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you too spend more time configuring tooling and troubleshooting package issues than you do working?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/program", "hidden": false, "pwls": null, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znp5d6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671224873.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.program", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The ecosystem is full of amazing tools that make the developer&amp;#39;s job considerably simpler. Typescript, npm, pnpm, parcel, webpack, node, babel... yet getting them to work together is quite difficult.&lt;/p&gt;\n\n&lt;p&gt;Typescript is great on its own, but needing to deal with implicit type inclusion is a pain. You do not want DOM types in your Node library? You&amp;#39;ve just blocked the import of u/ types! You want to utilize ES6 imports? Yes, it suddenly doesn&amp;#39;t work because some package uses commonjs need s somewhere down the node modules tree. All of the solutions are outdated stackoverflow answers that no longer apply or function, and the problem is handled by removing and reinstalling node modules.&lt;/p&gt;\n\n&lt;p&gt;Do you want to include libraries in your Chrome web extension? Simply copy and paste this &amp;gt;200-line webpack configuration. Wait, you want to utilize a tool like sass or typescript as well? Then either understand the intricacies of webpack or simply use Parcel. However, webextension manifest v3 is not supported.&lt;/p&gt;\n\n&lt;p&gt;PNPM is also a handy tool if you don&amp;#39;t want to redownload hundreds of gigabytes of npm packages every time you run npm install. The disadvantage is that you must always Google solutions for employing it in your projects. The same is true for yarn.&lt;/p&gt;\n\n&lt;p&gt;And the list of issues goes on and on. With each new tool and library, the number of workarounds grows and the situation becomes more difficult.&lt;/p&gt;\n\n&lt;p&gt;Everything appears to be easy on the surface, yet it&amp;#39;s a huge tangle that will eventually break. Nobody explains how things operate or how to set them up; instead, they offer a template, copypaste boilerplate, or cli tool instead of making it simple to install a library and use it (create-react-app, vue-cli comes to mind). It&amp;#39;s simply a big mess, and I&amp;#39;m not sure how to get out of it without going insane. Does anyone else have this problem? How can one get out of this situation?&lt;/p&gt;\n\n&lt;p&gt;(I don&amp;#39;t intend any disrespect to the tool&amp;#39;s creators.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5737d110-661e-11eb-8a81-0e02b289e523", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2rvy6", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff8717", "id": "znp5d6", "is_robot_indexable": true, "report_reasons": null, "author": "More_Breakfast2084", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/program/comments/znp5d6/do_you_too_spend_more_time_configuring_tooling/", "parent_whitelist_status": null, "stickied": false, "url": "https://old.reddit.com/r/program/comments/znp5d6/do_you_too_spend_more_time_configuring_tooling/", "subreddit_subscribers": 580, "created_utc": 1671224873.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1671225168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.program", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/program/comments/znp5d6/do_you_too_spend_more_time_configuring_tooling/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znp9do", "is_robot_indexable": true, "report_reasons": null, "author": "More_Breakfast2084", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_znp5d6", "author_flair_text_color": null, "permalink": "/r/datascience/comments/znp9do/do_you_too_spend_more_time_configuring_tooling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/program/comments/znp5d6/do_you_too_spend_more_time_configuring_tooling/", "subreddit_subscribers": 827738, "created_utc": 1671225168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Specifically on feature engineering and feature selection best practices, and particularly on tabular data.", "author_fullname": "t2_k4ol07vh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book recommendation on feature engineering &amp; selection?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znu9n6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671238902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Specifically on feature engineering and feature selection best practices, and particularly on tabular data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znu9n6", "is_robot_indexable": true, "report_reasons": null, "author": "Neosinic", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znu9n6/book_recommendation_on_feature_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znu9n6/book_recommendation_on_feature_engineering/", "subreddit_subscribers": 827738, "created_utc": 1671238902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m 27 - through my 20s I\u2019ve had some industry changes, education pivots etc - but I finally found a bit of a niche in data science, if you consider data viz a vertical. \n\nI was hired as a data viz specialist this summer - finally fully getting away from my biz management background. I\u2019m also a CS grad student (on and off), which also contributes to imposter syndrome (I struggle a bit). Anyways, in many ways, I\u2019m still sort of entry level. \n\nI work in an agile environment with 3-4 data engineers, my boss is an ex sr analyst/engineer, and some salesforce folks. I just feel\u2026stupid. They make these amazing things with data happen in queries, I just make it pretty dashboards and do some side admin work (notifications/comms). I get good feedback, I think I\u2019m liked, and I\u2019m the only data viz person remaining - I was an internal hire and we\u2019ve tried 3 consultants to assist with data viz but were all canned for poor performance. So in a way, I am finding some stride in that I\u2019m *the* person creating part of the experience for the end user. \n\nBut then sometimes I feel capped. I\u2019m not in SQL a lot. I have experience in Data Viz tools (Tableau, Power BI), but am mostly using a Salesforce equivalent. I do some light pseudo-SAQL coding (which I\u2019m not sure is even great) but that\u2019s it. I mean, a lot of my queries are sort of trial and error to get the desired output. It works, I\u2019m not always fully sure why, I\u2019m sure there\u2019s a way to do what I did in 5 less lines. But I don\u2019t know. I also manage the downstream dataflows, but that\u2019s mostly simple joins and reporting to DE when a job fails. \n\nIn some ways I\u2019m learning\u2026I think. But in others, I\u2019m wondering if there\u2019s growth in data viz. I love it. But it feels like I\u2019m taking all the true work of the smart people and just doing the work they don\u2019t have bandwidth too. Kind of like a designer and not a core data person. I could be wrong. Most of the time my boss has most of the ideas and I use them. I go back to other dashboards and tweak what worked there and move it to mine. I just feel dumb.\n\nAnd then, what prompted this, is when I worked with the DE team the last few weeks. My only \u201cdata viz\u201d equivalent is another consultant with like, 20 years experience, but he hates data viz and mostly hides in Salesforce tools and data management work. But even then, he knows a lot. We sync with data engineering on topics like data masking, AWS keys, object creation, schemas in tables\u2026I\u2019m just so lost. It\u2019s above me. Sometimes I try to absorb as much as I can, others I just need not to learn something and overwhelm myself because I\u2019m at peak capacity myself. \n\nBut we talk, they solve it. I didn\u2019t even fully understand what buckets are in AWS - dev, prod, etc. I feel like I\u2019m quiet in meetings and let the smart people talk it out. Because I\u2019m out of SQL, I feel like that skill isn\u2019t as sharp. And we\u2019re hiring another core analyst, so my focus remains on viz. and don\u2019t get me wrong - I love viz. I think it\u2019s sort of my niche - but I do want opportunity to do more, earn more, and more so contribute to my team and be useful. Right now it just feels like I\u2019m running alongside everyone trying to keep up, but not fully \u201cgetting\u201d these concepts of data work we\u2019re doing. And obviously I don\u2019t want to ask and sound stupid during meetings. And my boss has limited bandwidth and can\u2019t just teach me things with all this time he doesn\u2019t have. Even I. SQL, I do have the database but have no idea what to do with it. I was told to look into ROW_NUMBER, NOLOCK, etc - but still feel dumbfounded by simple concepts. \n\nThis is mostly a vent. I just feel like a kinda not smart person benefitting from the work of great people, and being unable to understand these concepts. Yes I\u2019m new in a way, but I want to do more and help more. And I\u2019m just unsure what I\u2019m doing is right or if I get all this feedback because honestly, the only skill I feel I have is just following direction", "author_fullname": "t2_ukrr8abp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I work in Data Viz in my team and am feeling bigtime imposter syndrome around my boss and DEs/DAs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znzgsr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671256429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m 27 - through my 20s I\u2019ve had some industry changes, education pivots etc - but I finally found a bit of a niche in data science, if you consider data viz a vertical. &lt;/p&gt;\n\n&lt;p&gt;I was hired as a data viz specialist this summer - finally fully getting away from my biz management background. I\u2019m also a CS grad student (on and off), which also contributes to imposter syndrome (I struggle a bit). Anyways, in many ways, I\u2019m still sort of entry level. &lt;/p&gt;\n\n&lt;p&gt;I work in an agile environment with 3-4 data engineers, my boss is an ex sr analyst/engineer, and some salesforce folks. I just feel\u2026stupid. They make these amazing things with data happen in queries, I just make it pretty dashboards and do some side admin work (notifications/comms). I get good feedback, I think I\u2019m liked, and I\u2019m the only data viz person remaining - I was an internal hire and we\u2019ve tried 3 consultants to assist with data viz but were all canned for poor performance. So in a way, I am finding some stride in that I\u2019m &lt;em&gt;the&lt;/em&gt; person creating part of the experience for the end user. &lt;/p&gt;\n\n&lt;p&gt;But then sometimes I feel capped. I\u2019m not in SQL a lot. I have experience in Data Viz tools (Tableau, Power BI), but am mostly using a Salesforce equivalent. I do some light pseudo-SAQL coding (which I\u2019m not sure is even great) but that\u2019s it. I mean, a lot of my queries are sort of trial and error to get the desired output. It works, I\u2019m not always fully sure why, I\u2019m sure there\u2019s a way to do what I did in 5 less lines. But I don\u2019t know. I also manage the downstream dataflows, but that\u2019s mostly simple joins and reporting to DE when a job fails. &lt;/p&gt;\n\n&lt;p&gt;In some ways I\u2019m learning\u2026I think. But in others, I\u2019m wondering if there\u2019s growth in data viz. I love it. But it feels like I\u2019m taking all the true work of the smart people and just doing the work they don\u2019t have bandwidth too. Kind of like a designer and not a core data person. I could be wrong. Most of the time my boss has most of the ideas and I use them. I go back to other dashboards and tweak what worked there and move it to mine. I just feel dumb.&lt;/p&gt;\n\n&lt;p&gt;And then, what prompted this, is when I worked with the DE team the last few weeks. My only \u201cdata viz\u201d equivalent is another consultant with like, 20 years experience, but he hates data viz and mostly hides in Salesforce tools and data management work. But even then, he knows a lot. We sync with data engineering on topics like data masking, AWS keys, object creation, schemas in tables\u2026I\u2019m just so lost. It\u2019s above me. Sometimes I try to absorb as much as I can, others I just need not to learn something and overwhelm myself because I\u2019m at peak capacity myself. &lt;/p&gt;\n\n&lt;p&gt;But we talk, they solve it. I didn\u2019t even fully understand what buckets are in AWS - dev, prod, etc. I feel like I\u2019m quiet in meetings and let the smart people talk it out. Because I\u2019m out of SQL, I feel like that skill isn\u2019t as sharp. And we\u2019re hiring another core analyst, so my focus remains on viz. and don\u2019t get me wrong - I love viz. I think it\u2019s sort of my niche - but I do want opportunity to do more, earn more, and more so contribute to my team and be useful. Right now it just feels like I\u2019m running alongside everyone trying to keep up, but not fully \u201cgetting\u201d these concepts of data work we\u2019re doing. And obviously I don\u2019t want to ask and sound stupid during meetings. And my boss has limited bandwidth and can\u2019t just teach me things with all this time he doesn\u2019t have. Even I. SQL, I do have the database but have no idea what to do with it. I was told to look into ROW_NUMBER, NOLOCK, etc - but still feel dumbfounded by simple concepts. &lt;/p&gt;\n\n&lt;p&gt;This is mostly a vent. I just feel like a kinda not smart person benefitting from the work of great people, and being unable to understand these concepts. Yes I\u2019m new in a way, but I want to do more and help more. And I\u2019m just unsure what I\u2019m doing is right or if I get all this feedback because honestly, the only skill I feel I have is just following direction&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znzgsr", "is_robot_indexable": true, "report_reasons": null, "author": "hshzhsnnahsbs", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znzgsr/i_work_in_data_viz_in_my_team_and_am_feeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znzgsr/i_work_in_data_viz_in_my_team_and_am_feeling/", "subreddit_subscribers": 827738, "created_utc": 1671256429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been trying to land my first data science role in canada since past two years, but haven't been successful yet. I was always interested in Machine learning and come from Mechanical engineering background. Here is what I have done so far. \n1. Started with ML course on coursera\n2. Did a data science boot camp, learned python, PostgreSQL, scikit learn\n3. Did 10+ guided and unguided projects\n4. Achieved Data scientist certification and AWS ML specialty certification\n5. Doing a mentorship program, refined resume, LinkedIn and other job boards profiles\n6. Applied to easyapply jobs, applied on websites, asked people for referrals and applied.\nHave been doing it while working full-time. But no luck. It seems a never ending process.", "author_fullname": "t2_cqmt7g11", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for the opportunity/advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znq4vt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671227493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been trying to land my first data science role in canada since past two years, but haven&amp;#39;t been successful yet. I was always interested in Machine learning and come from Mechanical engineering background. Here is what I have done so far. \n1. Started with ML course on coursera\n2. Did a data science boot camp, learned python, PostgreSQL, scikit learn\n3. Did 10+ guided and unguided projects\n4. Achieved Data scientist certification and AWS ML specialty certification\n5. Doing a mentorship program, refined resume, LinkedIn and other job boards profiles\n6. Applied to easyapply jobs, applied on websites, asked people for referrals and applied.\nHave been doing it while working full-time. But no luck. It seems a never ending process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "znq4vt", "is_robot_indexable": true, "report_reasons": null, "author": "PotentialFlow9897", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znq4vt/looking_for_the_opportunityadvice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znq4vt/looking_for_the_opportunityadvice/", "subreddit_subscribers": 827738, "created_utc": 1671227493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What are some common functions/transformations you run on a dataset before training and what libraries do you use, or is it a custom script?", "author_fullname": "t2_6i7on", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Common dataset prep operations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo2tb9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671269962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some common functions/transformations you run on a dataset before training and what libraries do you use, or is it a custom script?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo2tb9", "is_robot_indexable": true, "report_reasons": null, "author": "jy2k", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo2tb9/common_dataset_prep_operations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo2tb9/common_dataset_prep_operations/", "subreddit_subscribers": 827738, "created_utc": 1671269962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi there,\n\nCan someone please share resources for data science project documentation? \n\nWhat are the data science standards you use? e.g. Project proposal guidelines, EDA, ML guidelines etc.", "author_fullname": "t2_qltli13", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science Project Documentation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zntwkw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671237811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;Can someone please share resources for data science project documentation? &lt;/p&gt;\n\n&lt;p&gt;What are the data science standards you use? e.g. Project proposal guidelines, EDA, ML guidelines etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zntwkw", "is_robot_indexable": true, "report_reasons": null, "author": "ashishtele", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zntwkw/data_science_project_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zntwkw/data_science_project_documentation/", "subreddit_subscribers": 827738, "created_utc": 1671237811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have tabular data where each group of 100 rows represents a deployment of a specific geometry that has certain features measured. For example, I have 10000 deployments stored in a column called \"Dep,\" which ranges from 1 to 10000. When fitting this data to an xgboost model, the model may mistakenly assume that the deployment numbers have an ordering relationship (e.g., that deployment 1 is less than deployment 2, which is less than deployment 3, and so on). However, this is not the case because all deployments are independent of each other. On the other hand, if I remove the \"Dep\" column, the model will not be able to differentiate between the different deployment groups and their unique geometries.\n\nI got the idea of one hot encoding of the column dep, but it's useless as the training dataset has 7000 deployments while the test dataset has 3000. If I do a one hot encoding, the shapes will not match (along with a slew of other issues). Better think of another approach.\n\nWhat is the solution to this issue?", "author_fullname": "t2_7uwwf65z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to prevent my model from mistaking categorical feature for ordinal feature", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znrdn7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671230728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tabular data where each group of 100 rows represents a deployment of a specific geometry that has certain features measured. For example, I have 10000 deployments stored in a column called &amp;quot;Dep,&amp;quot; which ranges from 1 to 10000. When fitting this data to an xgboost model, the model may mistakenly assume that the deployment numbers have an ordering relationship (e.g., that deployment 1 is less than deployment 2, which is less than deployment 3, and so on). However, this is not the case because all deployments are independent of each other. On the other hand, if I remove the &amp;quot;Dep&amp;quot; column, the model will not be able to differentiate between the different deployment groups and their unique geometries.&lt;/p&gt;\n\n&lt;p&gt;I got the idea of one hot encoding of the column dep, but it&amp;#39;s useless as the training dataset has 7000 deployments while the test dataset has 3000. If I do a one hot encoding, the shapes will not match (along with a slew of other issues). Better think of another approach.&lt;/p&gt;\n\n&lt;p&gt;What is the solution to this issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znrdn7", "is_robot_indexable": true, "report_reasons": null, "author": "Hamdi_bks", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znrdn7/how_to_prevent_my_model_from_mistaking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znrdn7/how_to_prevent_my_model_from_mistaking/", "subreddit_subscribers": 827738, "created_utc": 1671230728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve seen companies hiring for all 4 at once at a single location so I\u2019m curious how the responsibilities would be divided. I feel there still lacks universal definitions for these roles.", "author_fullname": "t2_524iawau", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If a company has a Data Engineer, Analyst, and a ML Engineer, what does the Data Scientist do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zo99yd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671292687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve seen companies hiring for all 4 at once at a single location so I\u2019m curious how the responsibilities would be divided. I feel there still lacks universal definitions for these roles.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo99yd", "is_robot_indexable": true, "report_reasons": null, "author": "bassabyss", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo99yd/if_a_company_has_a_data_engineer_analyst_and_a_ml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo99yd/if_a_company_has_a_data_engineer_analyst_and_a_ml/", "subreddit_subscribers": 827738, "created_utc": 1671292687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_v0ygeiay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hahaha So True :D", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "name": "t3_zo68qb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0hgTEkWJrZnRhbKv2IYUp6vioPwA40PnTS2j2mE2OcQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671283517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9aub6z0dmg6a1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9aub6z0dmg6a1.jpg?auto=webp&amp;s=15a67e500072a2c19197ed36783b7fb790adcf67", "width": 2048, "height": 1260}, "resolutions": [{"url": "https://preview.redd.it/9aub6z0dmg6a1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=de3ee1551a23af623a7854f3e74ed30c646fd777", "width": 108, "height": 66}, {"url": "https://preview.redd.it/9aub6z0dmg6a1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4559d71205f1499662614553d394d2619c7ce6fa", "width": 216, "height": 132}, {"url": "https://preview.redd.it/9aub6z0dmg6a1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=68e889a196c7be125232e928bb25373c327a9cab", "width": 320, "height": 196}, {"url": "https://preview.redd.it/9aub6z0dmg6a1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a60121c896257754eb4d5c49418be81ce2c88496", "width": 640, "height": 393}, {"url": "https://preview.redd.it/9aub6z0dmg6a1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=37923d3c406eded1eb591991f21dbf947972e8a9", "width": 960, "height": 590}, {"url": "https://preview.redd.it/9aub6z0dmg6a1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=20567243f3f509e92ddd6080dd3719a559e64c5a", "width": 1080, "height": 664}], "variants": {}, "id": "Awi2OMWCbn_Rz0uc9MDfWC0GxOblB-rL6sCIJ1Dtzr8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo68qb", "is_robot_indexable": true, "report_reasons": null, "author": "Slayd3r_07", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo68qb/hahaha_so_true_d/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9aub6z0dmg6a1.jpg", "subreddit_subscribers": 827738, "created_utc": 1671283517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So, I\u2019ve built a program that scrapes housing data (address, location, beds, baths, square footage, etc) from a a local real estate site. I want to grab the same data (within the same city) from a few different sites and compare it all. What would be the best way to structure this project? Should I write multiple functions to pull from each website, or would this be too messy? Would anyone be able to share some examples of projects that pull the same data from multiple sites and compare them? thank you!!", "author_fullname": "t2_b31o1g0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Structuring a web scraper that analyzes data from multiple sites?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znkku4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671212987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I\u2019ve built a program that scrapes housing data (address, location, beds, baths, square footage, etc) from a a local real estate site. I want to grab the same data (within the same city) from a few different sites and compare it all. What would be the best way to structure this project? Should I write multiple functions to pull from each website, or would this be too messy? Would anyone be able to share some examples of projects that pull the same data from multiple sites and compare them? thank you!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znkku4", "is_robot_indexable": true, "report_reasons": null, "author": "treebrat", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znkku4/structuring_a_web_scraper_that_analyzes_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znkku4/structuring_a_web_scraper_that_analyzes_data_from/", "subreddit_subscribers": 827738, "created_utc": 1671212987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My sibling has a career in data science and machine learning and they are really into it, it is their hobby too. Any ideas what I cam get them?\n\nThey won\u2019t like knick knacks like a mug with an AI generated dog, or a t shirt. They would just throw that out. What would be a great gift is something (not a book) that allows them to explore, learn, or experiment with ai more.", "author_fullname": "t2_qgco0kmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What can I get my sibling who is an AI data scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zoagmh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671295838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My sibling has a career in data science and machine learning and they are really into it, it is their hobby too. Any ideas what I cam get them?&lt;/p&gt;\n\n&lt;p&gt;They won\u2019t like knick knacks like a mug with an AI generated dog, or a t shirt. They would just throw that out. What would be a great gift is something (not a book) that allows them to explore, learn, or experiment with ai more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoagmh", "is_robot_indexable": true, "report_reasons": null, "author": "CoupleConsistent5378", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoagmh/what_can_i_get_my_sibling_who_is_an_ai_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoagmh/what_can_i_get_my_sibling_who_is_an_ai_data/", "subreddit_subscribers": 827738, "created_utc": 1671295838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7abe1yig", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Huge breakthroughs, tiny changes: the next decade of artificial intelligence", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "name": "t3_zoabrd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/h9wszxT8KjtltYb00iE_8odHsTAkBwXqHc2sWYYj6l4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671295475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "rolandwrites.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.rolandwrites.com/blog/huge-breakthroughs-tiny-changes-ai", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4DWidaaJsNDNYxuIaEpmkphV99YAhfwnrtxQm43Gmfk.jpg?auto=webp&amp;s=c39b256e4ac24e0ae7d86e77331f851b6a6ad911", "width": 1600, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/4DWidaaJsNDNYxuIaEpmkphV99YAhfwnrtxQm43Gmfk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e024dbc16ea78543d12a46d1555159cfcbde1a03", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4DWidaaJsNDNYxuIaEpmkphV99YAhfwnrtxQm43Gmfk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=677a06999f0445ccb317f59c84256315f193b2d7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4DWidaaJsNDNYxuIaEpmkphV99YAhfwnrtxQm43Gmfk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=76d5ee3adc414bd21572015a054b5aeb59c02467", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4DWidaaJsNDNYxuIaEpmkphV99YAhfwnrtxQm43Gmfk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=753bf59972bcd0ca8f6c64a7da0d3f267d171efd", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4DWidaaJsNDNYxuIaEpmkphV99YAhfwnrtxQm43Gmfk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f312ee3beea7f309af99398d8ec9bc06c16a2f41", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4DWidaaJsNDNYxuIaEpmkphV99YAhfwnrtxQm43Gmfk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7e8ec9911123f8ca1a467d81de5ce9a9d13f788d", "width": 1080, "height": 540}], "variants": {}, "id": "HEjKRYAph-sQDNtKSWtNbAKL8QzppwZ2vqyrlf225SY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoabrd", "is_robot_indexable": true, "report_reasons": null, "author": "prwhite18", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoabrd/huge_breakthroughs_tiny_changes_the_next_decade/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.rolandwrites.com/blog/huge-breakthroughs-tiny-changes-ai", "subreddit_subscribers": 827738, "created_utc": 1671295475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to set up an unsupervised learning project for anomaly detection and I'm learning along the way. Since I haven't got any previous experience with unsupervised experiments, I have what is probably a very basic question with regard to setting up my experiment: is the basic approach for unsupervised learning broadly comparable to that of supervised learning?\n\nMy intent is to go with a data split of 70:15:15 (train/test/holdout) and do an EDA on the data. I want to check how meaningful certain features are, whether they correlate etc.. Then I'll drop features and/or engineer derived features and see how they cluster and whether PCA will result in any insight with regard to feature relevance?\n\nSince I'll likely use the insight of the PCA to inform the feature engineering, I think a holdout data set is necessary - at least in theory. However, since anomalies are both \\_anomalous\\_ and sparse, I'm not really sure whether this can actually be practically implemented.\n\nAny pointers to best practices or or design philosophies would be much appreciate by this unsupervised newbie. ;)", "author_fullname": "t2_4j7ujk5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I also use a train/test/holdout split in unsupervised learning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zoaan4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671295389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to set up an unsupervised learning project for anomaly detection and I&amp;#39;m learning along the way. Since I haven&amp;#39;t got any previous experience with unsupervised experiments, I have what is probably a very basic question with regard to setting up my experiment: is the basic approach for unsupervised learning broadly comparable to that of supervised learning?&lt;/p&gt;\n\n&lt;p&gt;My intent is to go with a data split of 70:15:15 (train/test/holdout) and do an EDA on the data. I want to check how meaningful certain features are, whether they correlate etc.. Then I&amp;#39;ll drop features and/or engineer derived features and see how they cluster and whether PCA will result in any insight with regard to feature relevance?&lt;/p&gt;\n\n&lt;p&gt;Since I&amp;#39;ll likely use the insight of the PCA to inform the feature engineering, I think a holdout data set is necessary - at least in theory. However, since anomalies are both _anomalous_ and sparse, I&amp;#39;m not really sure whether this can actually be practically implemented.&lt;/p&gt;\n\n&lt;p&gt;Any pointers to best practices or or design philosophies would be much appreciate by this unsupervised newbie. ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zoaan4", "is_robot_indexable": true, "report_reasons": null, "author": "norfkens2", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zoaan4/do_i_also_use_a_traintestholdout_split_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zoaan4/do_i_also_use_a_traintestholdout_split_in/", "subreddit_subscribers": 827738, "created_utc": 1671295389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Check out out, would you use this?\n\nhttps://github.com/aws-samples/distributed-compute-on-aws-with-cross-regional-dask", "author_fullname": "t2_v1f56kco", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cross Regional Dask on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_zo8v5e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1671291531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check out out, would you use this?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/aws-samples/distributed-compute-on-aws-with-cross-regional-dask\"&gt;https://github.com/aws-samples/distributed-compute-on-aws-with-cross-regional-dask&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?auto=webp&amp;s=2007921aefa32104b16120fdcda67f25052c18d3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0d2363e91de15e7251f6ad01a45ba9d7cd5b36c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef2d65fc0163c885d031bf3b3742c1c90e71a245", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c83d83b48aaaca832314c54b7e208d9414432ff", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8ef536a4b30cac8de2b8c1256e74a37aece58c8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=23049cb7c3f55d216d707f36a48e1fda6e7a758f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/XnREhoYY5KzGE33XRDHLRM-mwYUO0wPCFfNSV8btGr0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02a1f497a8851982103795c3df6c1b2e3029757e", "width": 1080, "height": 540}], "variants": {}, "id": "d7SCmwvPs0_VsnLQQ01R-H3jpt4LRVP6Zmmmk5Icd0c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo8v5e", "is_robot_indexable": true, "report_reasons": null, "author": "oconpa", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo8v5e/cross_regional_dask_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo8v5e/cross_regional_dask_on_aws/", "subreddit_subscribers": 827738, "created_utc": 1671291531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,  \nCurrently I am researching extreme learning machines, and - after I calculated the beta weights  - I would like to optimize the alpha weights (similarly with the Moore-Penrose pseudo inverse) in order to receive better accuracy.  \nDuring this backward calculation, I use the inverse of the previously applied activation function. In case of the inverse sigmoid and tanh (logit and arctan), I receive NaN values, although with the inverse leaky ReLU, on certain datasets, the method yields better accuracy. Unfortunately, in most cases, the accuracy drops (sometimes significantly) with these new optimized alpha weights. What do you think, what is the reason for this?", "author_fullname": "t2_kq8l2zbu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inverse activation function", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo7wjc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671288767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;br/&gt;\nCurrently I am researching extreme learning machines, and - after I calculated the beta weights  - I would like to optimize the alpha weights (similarly with the Moore-Penrose pseudo inverse) in order to receive better accuracy.&lt;br/&gt;\nDuring this backward calculation, I use the inverse of the previously applied activation function. In case of the inverse sigmoid and tanh (logit and arctan), I receive NaN values, although with the inverse leaky ReLU, on certain datasets, the method yields better accuracy. Unfortunately, in most cases, the accuracy drops (sometimes significantly) with these new optimized alpha weights. What do you think, what is the reason for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo7wjc", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical-Plane3730", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo7wjc/inverse_activation_function/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo7wjc/inverse_activation_function/", "subreddit_subscribers": 827738, "created_utc": 1671288767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to do a new project in R, where I'm working with a lot of data (about 6GB of text) my inefficient code is tying me down a lot and was hoping maybe you guys had some tricks.\n\nI'm supposed to load in large .txt files, each file should be split into shorter parts consisting of 10 words each, they are then put into my data frame with each word being put into one of 10 columns. \n\nThe problem is that my code is slow, like really slow. It takes around 20 minutes to load the first 100000 rows of text into my dataframe, which is less than 1% of my data. I have also noticed that code slows down the further it takes to run, it takes about a second to load in the first 3000 rows, and 15 seconds for the first 10000 rows.   \n\nWhat is the main bottleneck slowing me down? Should I just rewrite the code entirely in another language like python?\n\nMy current code below:\n\n    #setup\n    folder &lt;- \"C:/Users/A_tiny/\"\n    \n    # Get a list of files in the folder\n    files &lt;- list.files(folder)\n    \n    # Create an empty data frame to hold the indexed words\n    df &lt;- data.frame(word1 = character(), word2 = character(), word3 = character(), word4 = character(),\n    word5 = character(), word6 = character(), word7 = character(), word8 = character(),\n    word9 = character(), word10 = character(), index = integer())\n    \n    for (file in files) {\n    # Check if the file is a .epub.txt file\n    if (grepl(\".epub.txt$\", file)) {\n    # Read the .epub.txt file into a string variable\n    epub_txt &lt;- readLines(paste0(folder, file))\n    \n    # Split the string into a vector of words\n    words &lt;- unlist(strsplit(epub_txt, \" \"))\n    \n    # Loop through the vector of words and add the next 10 words to the data frame, skipping 10 words at a time\n    for (i in seq(1, length(words), by=10)) {\n      # Create a data frame with the next 10 words and the index\n      df_temp &lt;- data.frame(word1 = words[i], word2 = words[i+1], word3 = words[i+2], word4 = words[i+3],\n                            word5 = words[i+4], word6 = words[i+5], word7 = words[i+6], word8 = words[i+7],\n                            word9 = words[i+8], word10 = words[i+9], index = i)\n      # Add the data frame to the main data frame\n      df &lt;- rbind(df, df_temp)\n    }\n      }\n    }", "author_fullname": "t2_gx7hn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The best approach for rewriting my code more efficiently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo4p8z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671277772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to do a new project in R, where I&amp;#39;m working with a lot of data (about 6GB of text) my inefficient code is tying me down a lot and was hoping maybe you guys had some tricks.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m supposed to load in large .txt files, each file should be split into shorter parts consisting of 10 words each, they are then put into my data frame with each word being put into one of 10 columns. &lt;/p&gt;\n\n&lt;p&gt;The problem is that my code is slow, like really slow. It takes around 20 minutes to load the first 100000 rows of text into my dataframe, which is less than 1% of my data. I have also noticed that code slows down the further it takes to run, it takes about a second to load in the first 3000 rows, and 15 seconds for the first 10000 rows.   &lt;/p&gt;\n\n&lt;p&gt;What is the main bottleneck slowing me down? Should I just rewrite the code entirely in another language like python?&lt;/p&gt;\n\n&lt;p&gt;My current code below:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#setup\nfolder &amp;lt;- &amp;quot;C:/Users/A_tiny/&amp;quot;\n\n# Get a list of files in the folder\nfiles &amp;lt;- list.files(folder)\n\n# Create an empty data frame to hold the indexed words\ndf &amp;lt;- data.frame(word1 = character(), word2 = character(), word3 = character(), word4 = character(),\nword5 = character(), word6 = character(), word7 = character(), word8 = character(),\nword9 = character(), word10 = character(), index = integer())\n\nfor (file in files) {\n# Check if the file is a .epub.txt file\nif (grepl(&amp;quot;.epub.txt$&amp;quot;, file)) {\n# Read the .epub.txt file into a string variable\nepub_txt &amp;lt;- readLines(paste0(folder, file))\n\n# Split the string into a vector of words\nwords &amp;lt;- unlist(strsplit(epub_txt, &amp;quot; &amp;quot;))\n\n# Loop through the vector of words and add the next 10 words to the data frame, skipping 10 words at a time\nfor (i in seq(1, length(words), by=10)) {\n  # Create a data frame with the next 10 words and the index\n  df_temp &amp;lt;- data.frame(word1 = words[i], word2 = words[i+1], word3 = words[i+2], word4 = words[i+3],\n                        word5 = words[i+4], word6 = words[i+5], word7 = words[i+6], word8 = words[i+7],\n                        word9 = words[i+8], word10 = words[i+9], index = i)\n  # Add the data frame to the main data frame\n  df &amp;lt;- rbind(df, df_temp)\n}\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo4p8z", "is_robot_indexable": true, "report_reasons": null, "author": "_Just7_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo4p8z/the_best_approach_for_rewriting_my_code_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo4p8z/the_best_approach_for_rewriting_my_code_more/", "subreddit_subscribers": 827738, "created_utc": 1671277772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_895cqxfg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Network Graphs of all Goals &amp; Goal Chances created by WC2022 finalists, visualized.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_zo16rm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/i6VMGDsopWj6_KTX59CBPBDTTP79_O1tnicjhjnaaXo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1671263108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "wcxgnetworks.streamlit.app", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://wcxgnetworks.streamlit.app/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?auto=webp&amp;s=c1002e8b9aefe041811736d6a215bb038eb7aa8b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a90792efc4e26380ce1f4033e8c0c517d804810d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=78a5ebc886dd56810d6c36026f6e6552966517a4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba6d7ad1934366ab73f23c0cae3d61a24560f20c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ab934a1101b6c1e200b46b117784bdc9b7529dcb", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=798f95e131655041677300c64511006ddde07b99", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/8yeHf3CHsj2mSS5DM0nc3LoxSXjFYqbTk4hgukVPLmg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3f05f99a6150e14c8df1b370cc580e98b0bb350c", "width": 1080, "height": 567}], "variants": {}, "id": "PIe-3OpMNP61TpZzSjrFl3aA2rFm7yVHhU-B_BoSKhk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo16rm", "is_robot_indexable": true, "report_reasons": null, "author": "noimgonnalie", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo16rm/network_graphs_of_all_goals_goal_chances_created/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://wcxgnetworks.streamlit.app/", "subreddit_subscribers": 827738, "created_utc": 1671263108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Does anyone here have experience working in a data role at a UN organization or a development bank or something similar? What is your day to day like and also how did you get that role?", "author_fullname": "t2_i049nix9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone work in large UN/ Developmental organizations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_znts42", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671237428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone here have experience working in a data role at a UN organization or a development bank or something similar? What is your day to day like and also how did you get that role?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "znts42", "is_robot_indexable": true, "report_reasons": null, "author": "vizualizing123", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/znts42/anyone_work_in_large_un_developmental/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/znts42/anyone_work_in_large_un_developmental/", "subreddit_subscribers": 827738, "created_utc": 1671237428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi community! I am a senior in college with Mathematics major. I am based in NYC. I have an avid internet in pursuing a career in Data Science. In the past I have participated in a year long data science fellowship run by my University. I have also participated in a Data Science summer school run by a major technology company. I have been persistently applying to related roles but I haven\u2019t had offers or fair amount of interviews either (I am in a student visa currently). Most of the Data Science jobs ask for at least Master\u2019s degree or fair amount of years of experience. Now my concern is that if I should pursue a MS in Data Science.", "author_fullname": "t2_3lz0pnks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Master\u2019s degree?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo5ma2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671281284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi community! I am a senior in college with Mathematics major. I am based in NYC. I have an avid internet in pursuing a career in Data Science. In the past I have participated in a year long data science fellowship run by my University. I have also participated in a Data Science summer school run by a major technology company. I have been persistently applying to related roles but I haven\u2019t had offers or fair amount of interviews either (I am in a student visa currently). Most of the Data Science jobs ask for at least Master\u2019s degree or fair amount of years of experience. Now my concern is that if I should pursue a MS in Data Science.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo5ma2", "is_robot_indexable": true, "report_reasons": null, "author": "m30aru", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo5ma2/masters_degree/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo5ma2/masters_degree/", "subreddit_subscribers": 827738, "created_utc": 1671281284.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone, does anyone know if chatGPT has been trained on Kaggle projects? If so, it should already be pretty good at a lot of DS stuff, right?\n\nIf not, I think it's only a matter of time before they will include that, which could create a very powerful DS personal assistant.\n\nI guess it could be challenging to train it on large datasets specifically, but I'm sure there are some smart ways to make that part more efficient, like only using a sample of each data set. Plus, there is the legal question if Kaggle would allow openAI to use their data.\n\nDo you guys have any thoughts?", "author_fullname": "t2_12hyas", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Was ChatGPT trained on Kaggle and other DS coding platforms?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo2pj1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671269496.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, does anyone know if chatGPT has been trained on Kaggle projects? If so, it should already be pretty good at a lot of DS stuff, right?&lt;/p&gt;\n\n&lt;p&gt;If not, I think it&amp;#39;s only a matter of time before they will include that, which could create a very powerful DS personal assistant.&lt;/p&gt;\n\n&lt;p&gt;I guess it could be challenging to train it on large datasets specifically, but I&amp;#39;m sure there are some smart ways to make that part more efficient, like only using a sample of each data set. Plus, there is the legal question if Kaggle would allow openAI to use their data.&lt;/p&gt;\n\n&lt;p&gt;Do you guys have any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo2pj1", "is_robot_indexable": true, "report_reasons": null, "author": "ikke89", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo2pj1/was_chatgpt_trained_on_kaggle_and_other_ds_coding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo2pj1/was_chatgpt_trained_on_kaggle_and_other_ds_coding/", "subreddit_subscribers": 827738, "created_utc": 1671269496.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I made a scorecard using WOE and logistic regression. I am also binning the feature (eg. age: 10-20,20-30,...). After i calculated coefficient from logistic regression model we can make a scorecard from it (eg. age 10-20: 10 pts, 20-30: 20 pts ...). Now I am trying to use other model like XGBoost or neural network to make the same scorecard like that. But with logistic regression, I have coefficient and intercept to calculate score for each bin but tree model and neural network doesn't have it. So I want to ask for the formula, book or any resources about this problem. Thanks for reading! Hope you have a good day.", "author_fullname": "t2_9axjxbxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scorecard in credit scoring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_zo03ou", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1671258812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made a scorecard using WOE and logistic regression. I am also binning the feature (eg. age: 10-20,20-30,...). After i calculated coefficient from logistic regression model we can make a scorecard from it (eg. age 10-20: 10 pts, 20-30: 20 pts ...). Now I am trying to use other model like XGBoost or neural network to make the same scorecard like that. But with logistic regression, I have coefficient and intercept to calculate score for each bin but tree model and neural network doesn&amp;#39;t have it. So I want to ask for the formula, book or any resources about this problem. Thanks for reading! Hope you have a good day.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "zo03ou", "is_robot_indexable": true, "report_reasons": null, "author": "Previous_Aside_8863", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/zo03ou/scorecard_in_credit_scoring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/zo03ou/scorecard_in_credit_scoring/", "subreddit_subscribers": 827738, "created_utc": 1671258812.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}