{"kind": "Listing", "data": {"after": "t3_ygpre6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m using R in grad school, as required, and Python at work, due to availability.  I\u2019m reasonably comfortable with both, but switching back and forth within the same day is a bit rough. At what level of fluency will it get easier?  Any suggestions?  Just keep at it?", "author_fullname": "t2_qtt2lnfp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Difficulty transitioning between R and Python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yg38or", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 178, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 178, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666998046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m using R in grad school, as required, and Python at work, due to availability.  I\u2019m reasonably comfortable with both, but switching back and forth within the same day is a bit rough. At what level of fluency will it get easier?  Any suggestions?  Just keep at it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yg38or", "is_robot_indexable": true, "report_reasons": null, "author": "WolfShort7481", "discussion_type": null, "num_comments": 169, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yg38or/difficulty_transitioning_between_r_and_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yg38or/difficulty_transitioning_between_r_and_python/", "subreddit_subscribers": 816088, "created_utc": 1666998046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_2ujnok1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why has nobody ported ggplot to Python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ygnkd9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 74, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 74, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667059819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ygnkd9", "is_robot_indexable": true, "report_reasons": null, "author": "is_this_the_place", "discussion_type": null, "num_comments": 59, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ygnkd9/why_has_nobody_ported_ggplot_to_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ygnkd9/why_has_nobody_ported_ggplot_to_python/", "subreddit_subscribers": 816088, "created_utc": 1667059819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been using sklearn for a long time and I understand what it's doing, I know which techniques are appropriate for which tasks.There's no shortage of Sklearn tutorials and data science courses to learn the concepts and see examples. \n\nBut there's all these syntax details within sklearn that are harder for me than they should be. With other libraries like Pandas I can write new code without a reference, or look at readthedocs for quick reference. But with Sklearn I'm googling, copy pasting code, changing it, getting errors and trying random shit until the errors stop. I do this a lot without improving my understanding.\n\nObviously I'm missing core knowledge but I don't know what it's called to even look for it. If I look something up in Sklearn readthedocs I don't know if I should something.somethingelse()\n Or I should\nsomething(somethingelse)\nOr I should \nmything=something()\nmything(somethingelse)\n\nThen there's words like estimator, object, method, constructor, class, pipeline, transformer. I look them up in the glossary, but I am no closer to grasping their specialness. I know a Pipeline when I see it, but despite reading tutorial after tutorial on Pipelines I cannot find an answer to what makes a Pipeline a different beast from a bunch of functions. For that matter, a lot of this stuff sounds like functions but...aren't? Does this have something to do with the functional versus object oriented programming debate? My limited understanding of this debate is that functional programming is logical and object oriented makes me disoriented.\n\nIs there a book everyone read that I didn't hear about??? I want to be able to look at readthedocs and have that convey to my brain what to type in Python, without resorting to monkey business. How do I get my brain there?", "author_fullname": "t2_tryxm2kz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I stop using Sklearn like a monkey?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yg8tv9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667014375.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using sklearn for a long time and I understand what it&amp;#39;s doing, I know which techniques are appropriate for which tasks.There&amp;#39;s no shortage of Sklearn tutorials and data science courses to learn the concepts and see examples. &lt;/p&gt;\n\n&lt;p&gt;But there&amp;#39;s all these syntax details within sklearn that are harder for me than they should be. With other libraries like Pandas I can write new code without a reference, or look at readthedocs for quick reference. But with Sklearn I&amp;#39;m googling, copy pasting code, changing it, getting errors and trying random shit until the errors stop. I do this a lot without improving my understanding.&lt;/p&gt;\n\n&lt;p&gt;Obviously I&amp;#39;m missing core knowledge but I don&amp;#39;t know what it&amp;#39;s called to even look for it. If I look something up in Sklearn readthedocs I don&amp;#39;t know if I should something.somethingelse()\n Or I should\nsomething(somethingelse)\nOr I should \nmything=something()\nmything(somethingelse)&lt;/p&gt;\n\n&lt;p&gt;Then there&amp;#39;s words like estimator, object, method, constructor, class, pipeline, transformer. I look them up in the glossary, but I am no closer to grasping their specialness. I know a Pipeline when I see it, but despite reading tutorial after tutorial on Pipelines I cannot find an answer to what makes a Pipeline a different beast from a bunch of functions. For that matter, a lot of this stuff sounds like functions but...aren&amp;#39;t? Does this have something to do with the functional versus object oriented programming debate? My limited understanding of this debate is that functional programming is logical and object oriented makes me disoriented.&lt;/p&gt;\n\n&lt;p&gt;Is there a book everyone read that I didn&amp;#39;t hear about??? I want to be able to look at readthedocs and have that convey to my brain what to type in Python, without resorting to monkey business. How do I get my brain there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yg8tv9", "is_robot_indexable": true, "report_reasons": null, "author": "hotdatassss", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yg8tv9/how_can_i_stop_using_sklearn_like_a_monkey/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yg8tv9/how_can_i_stop_using_sklearn_like_a_monkey/", "subreddit_subscribers": 816088, "created_utc": 1667014375.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "hi. i am learning how to code in python and i think i am getting a point where i can start thinking up my own projects. i have no traditional college exp. so i am reliying on getting into the market with a good portfolio and a good interview (my family completely cut me off so i cant network). so i am thinking of getting all the large shows that came out recently and finding a relationship between the average rating of these shows and the global weather for that day. for example. do ppl rate shows worst during cold days than hot days and visa versa? and making a chart to see if there is a high correlation. it is not my first time attempting to do a ds project for fun but now i want to add this to my portfolio. is this a good project thats jobs wants to see on ur portolio?\n\nthx in advance.", "author_fullname": "t2_d78uvi0f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "would a movie rating tracker be a good python data science project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yg7pvq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667010916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi. i am learning how to code in python and i think i am getting a point where i can start thinking up my own projects. i have no traditional college exp. so i am reliying on getting into the market with a good portfolio and a good interview (my family completely cut me off so i cant network). so i am thinking of getting all the large shows that came out recently and finding a relationship between the average rating of these shows and the global weather for that day. for example. do ppl rate shows worst during cold days than hot days and visa versa? and making a chart to see if there is a high correlation. it is not my first time attempting to do a ds project for fun but now i want to add this to my portfolio. is this a good project thats jobs wants to see on ur portolio?&lt;/p&gt;\n\n&lt;p&gt;thx in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yg7pvq", "is_robot_indexable": true, "report_reasons": null, "author": "MsGoogleEyes", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yg7pvq/would_a_movie_rating_tracker_be_a_good_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yg7pvq/would_a_movie_rating_tracker_be_a_good_python/", "subreddit_subscribers": 816088, "created_utc": 1667010916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work for a university. In my experience, it's been a common phenomenon that any new business unit (these are mostly non professors) that we pitch our existing work to show how others are using predictive models, they immediately start obsessing over the model (P-value, attributes, change of coef over time - irrespective of the whether it's regression/tree/nn) rather than how the a model might help/fit in their work. Very curious to know is this a common experience for other here? \n\nI'm seriously pissed at these P-value warriors - who considers P-value is everything and 0.05 is the heavenly number that'll show them the way to salvation \ud83d\ude24", "author_fullname": "t2_9axqyq8u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Half ranting) How often do you find business users more worried about the stats i.e. \"p value\" than how a model will be useful for their work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yg0c7s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666990588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a university. In my experience, it&amp;#39;s been a common phenomenon that any new business unit (these are mostly non professors) that we pitch our existing work to show how others are using predictive models, they immediately start obsessing over the model (P-value, attributes, change of coef over time - irrespective of the whether it&amp;#39;s regression/tree/nn) rather than how the a model might help/fit in their work. Very curious to know is this a common experience for other here? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m seriously pissed at these P-value warriors - who considers P-value is everything and 0.05 is the heavenly number that&amp;#39;ll show them the way to salvation \ud83d\ude24&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yg0c7s", "is_robot_indexable": true, "report_reasons": null, "author": "Difficult-Big-3890", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yg0c7s/half_ranting_how_often_do_you_find_business_users/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yg0c7s/half_ranting_how_often_do_you_find_business_users/", "subreddit_subscribers": 816088, "created_utc": 1666990588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I lurked here for a bit before coming onto reddit. I am early in my grad program (MS Data Analytics), and I just finished my first week of my part-time, fully remote work, data science internship at a large company. The company is much larger than my previous job, which was a part-time bookkeeping at a small church. And I feel kind of lost and anxious. I wasn't able to ask as many questions as I wanted to this first week. It looks like everyone else is pretty busy, as they immediately jump off the call when the scheduled end time hits. Today I attended a meeting with my direct manager and two other team members (one of whom was assigned to me as my mentor of sorts) to discuss requirements for a project they want me to work on, and it was all over the place. I wasn't able to follow any of it aside from the first few minutes, and the rest of the meeting was the two other team members and my direct manager talking about something that happened a year ago. \n\nI have been reassured by my direct manager that I'm basically going to be useless for the first few weeks, because their IT is notoriously slow and they have yet to install everything on my work computer I need. But it's doing little for my nerves. I do try to keep myself busy. I do make sure I tried a couple of things before I ask questions; I know these people are busy. I haven't even finished looking through everything they sent me yet. I've never received so many emails in my entire life continuously. Is it normal to feel like everything is already piling on?", "author_fullname": "t2_72483rbj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feeling mostly anxious about my new internship", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yg94a1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667015264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I lurked here for a bit before coming onto reddit. I am early in my grad program (MS Data Analytics), and I just finished my first week of my part-time, fully remote work, data science internship at a large company. The company is much larger than my previous job, which was a part-time bookkeeping at a small church. And I feel kind of lost and anxious. I wasn&amp;#39;t able to ask as many questions as I wanted to this first week. It looks like everyone else is pretty busy, as they immediately jump off the call when the scheduled end time hits. Today I attended a meeting with my direct manager and two other team members (one of whom was assigned to me as my mentor of sorts) to discuss requirements for a project they want me to work on, and it was all over the place. I wasn&amp;#39;t able to follow any of it aside from the first few minutes, and the rest of the meeting was the two other team members and my direct manager talking about something that happened a year ago. &lt;/p&gt;\n\n&lt;p&gt;I have been reassured by my direct manager that I&amp;#39;m basically going to be useless for the first few weeks, because their IT is notoriously slow and they have yet to install everything on my work computer I need. But it&amp;#39;s doing little for my nerves. I do try to keep myself busy. I do make sure I tried a couple of things before I ask questions; I know these people are busy. I haven&amp;#39;t even finished looking through everything they sent me yet. I&amp;#39;ve never received so many emails in my entire life continuously. Is it normal to feel like everything is already piling on?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yg94a1", "is_robot_indexable": true, "report_reasons": null, "author": "currentgradstudent", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yg94a1/feeling_mostly_anxious_about_my_new_internship/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yg94a1/feeling_mostly_anxious_about_my_new_internship/", "subreddit_subscribers": 816088, "created_utc": 1667015264.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Also what would you call it if out 50 data points, only a handful had pairwise labels? Sparse classifier? Semi supervised?", "author_fullname": "t2_10skoo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you call a classifier that learns from pairwise labels? As in, \"I don't know what these two points of data are, I just know they belong to the same class\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ygkibg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667052275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Also what would you call it if out 50 data points, only a handful had pairwise labels? Sparse classifier? Semi supervised?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ygkibg", "is_robot_indexable": true, "report_reasons": null, "author": "CommunismDoesntWork", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ygkibg/what_do_you_call_a_classifier_that_learns_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ygkibg/what_do_you_call_a_classifier_that_learns_from/", "subreddit_subscribers": 816088, "created_utc": 1667052275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Currently in undergrad studying Industrial Engineering. Have two corporate-level Data Science related projects under my belt.\nJust wondering, when applying to DS and DA roles, what is the best way of marketing myself as an IE who is proficient in DS?", "author_fullname": "t2_5cxl4ri5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Industrial Eng Student Applying to DS/DA roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ygp9xr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667063893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently in undergrad studying Industrial Engineering. Have two corporate-level Data Science related projects under my belt.\nJust wondering, when applying to DS and DA roles, what is the best way of marketing myself as an IE who is proficient in DS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "ygp9xr", "is_robot_indexable": true, "report_reasons": null, "author": "ibuildneatstuff", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ygp9xr/industrial_eng_student_applying_to_dsda_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ygp9xr/industrial_eng_student_applying_to_dsda_roles/", "subreddit_subscribers": 816088, "created_utc": 1667063893.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Actually the weka tool can do the preprocessing stuff and far more faster and also it is easy to use whereas if we use the python for it , it sure takes sometime , BUT MY REAL QUESTION IS DO DATA SCIENTISTS USE ANY TOOLS THROUGH OUT THE DATA PROCESSING  PROCESS ?", "author_fullname": "t2_nljr59px", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "do data scientists use any tools for preprocessing ex:-weka or do they just use the python libraries to do it ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ygm3lb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667056230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Actually the weka tool can do the preprocessing stuff and far more faster and also it is easy to use whereas if we use the python for it , it sure takes sometime , BUT MY REAL QUESTION IS DO DATA SCIENTISTS USE ANY TOOLS THROUGH OUT THE DATA PROCESSING  PROCESS ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ygm3lb", "is_robot_indexable": true, "report_reasons": null, "author": "Inner_Escape3611", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ygm3lb/do_data_scientists_use_any_tools_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ygm3lb/do_data_scientists_use_any_tools_for/", "subreddit_subscribers": 816088, "created_utc": 1667056230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_6q7a2p0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anything you wish you\u2019d learned or done before your first job/internship?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yg3962", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666998076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yg3962", "is_robot_indexable": true, "report_reasons": null, "author": "jeffrey_56", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yg3962/anything_you_wish_youd_learned_or_done_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yg3962/anything_you_wish_youd_learned_or_done_before/", "subreddit_subscribers": 816088, "created_utc": 1666998076.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Last DS project I did was 5 month's ago , due to some problems I couldn't practice data science for a long time now , what's the best way to come back ? should I take a course to refresh concepts or start a project  right ahead ?", "author_fullname": "t2_kmj1xtch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to refresh your data science skills ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yg03on", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666989969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last DS project I did was 5 month&amp;#39;s ago , due to some problems I couldn&amp;#39;t practice data science for a long time now , what&amp;#39;s the best way to come back ? should I take a course to refresh concepts or start a project  right ahead ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yg03on", "is_robot_indexable": true, "report_reasons": null, "author": "beselaa", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yg03on/how_to_refresh_your_data_science_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yg03on/how_to_refresh_your_data_science_skills/", "subreddit_subscribers": 816088, "created_utc": 1666989969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I have to take important decisions in the next months and would like to know your opinion on which fields related to big data have the best perspectives in the present and following years. Genetics, AutoML...", "author_fullname": "t2_dr6xhswp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most promising fields in data science for the next years?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ygsl59", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667072088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have to take important decisions in the next months and would like to know your opinion on which fields related to big data have the best perspectives in the present and following years. Genetics, AutoML...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ygsl59", "is_robot_indexable": true, "report_reasons": null, "author": "estrellaDeOriente", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ygsl59/most_promising_fields_in_data_science_for_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ygsl59/most_promising_fields_in_data_science_for_the/", "subreddit_subscribers": 816088, "created_utc": 1667072088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_5msa2w9e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data science bootcamp", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ygs0xm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667070655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ygs0xm", "is_robot_indexable": true, "report_reasons": null, "author": "broodingbrownie", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ygs0xm/data_science_bootcamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ygs0xm/data_science_bootcamp/", "subreddit_subscribers": 816088, "created_utc": 1667070655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am trying to make a object detection model (this post isn't about that) I found a great dataset to use for training. It is called 'Open images' and it is by Google. The data set is huge coming in at over 520GB with all the photos and labels. \n\nMy computer does not have enough store storage for nor is my internet good enough to download and them upload it else where. \n\nIs there a way to just \\`images = [example.com/myimages/\\`](https://example.com/myimages/`) and \\`csv = [example.com/imageannotations.csv\\`](https://example.com/imageannotations.csv`) without downloading the data locally? (the syntax will obviously different for loading them in)\n\nAlso will just downloading the train csv be enough or do I need to have the corresponding images locally as well? I am having a hard time telling what files I need to download from the list.\n\nDataset - [https://storage.googleapis.com/openimages/web/factsfigures\\_v7.html](https://storage.googleapis.com/openimages/web/factsfigures_v7.html)", "author_fullname": "t2_9lvsgisb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I work on remote datasets without storing them locally?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ygrupk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667070208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to make a object detection model (this post isn&amp;#39;t about that) I found a great dataset to use for training. It is called &amp;#39;Open images&amp;#39; and it is by Google. The data set is huge coming in at over 520GB with all the photos and labels. &lt;/p&gt;\n\n&lt;p&gt;My computer does not have enough store storage for nor is my internet good enough to download and them upload it else where. &lt;/p&gt;\n\n&lt;p&gt;Is there a way to just `images = &lt;a href=\"https://example.com/myimages/%60\"&gt;example.com/myimages/`&lt;/a&gt; and `csv = &lt;a href=\"https://example.com/imageannotations.csv%60\"&gt;example.com/imageannotations.csv`&lt;/a&gt; without downloading the data locally? (the syntax will obviously different for loading them in)&lt;/p&gt;\n\n&lt;p&gt;Also will just downloading the train csv be enough or do I need to have the corresponding images locally as well? I am having a hard time telling what files I need to download from the list.&lt;/p&gt;\n\n&lt;p&gt;Dataset - &lt;a href=\"https://storage.googleapis.com/openimages/web/factsfigures_v7.html\"&gt;https://storage.googleapis.com/openimages/web/factsfigures_v7.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ygrupk", "is_robot_indexable": true, "report_reasons": null, "author": "Soumil30", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ygrupk/how_do_i_work_on_remote_datasets_without_storing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ygrupk/how_do_i_work_on_remote_datasets_without_storing/", "subreddit_subscribers": 816088, "created_utc": 1667070208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Currently working on a project where each data point is assigned any number of \u2018tags\u2019, from 0 up to 10. Right now I\u2019m pulling them straight from an API that doesn\u2019t have great functionality so I can\u2019t filter for tags themselves. All I can do is download jsons that contains lists of tags for each datapoint. If I\u2019m interested in analyzing tags, what\u2019s the best way of structuring the data? It seems like I\u2019ll probably need to construct some sort of relational database, but I\u2019m not a data engineer by training and was wondering if there\u2019s a simpler method that I\u2019m missing.", "author_fullname": "t2_l4rahadg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dealing with \u2018Tagged\u2019 Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ygrc5r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667068937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently working on a project where each data point is assigned any number of \u2018tags\u2019, from 0 up to 10. Right now I\u2019m pulling them straight from an API that doesn\u2019t have great functionality so I can\u2019t filter for tags themselves. All I can do is download jsons that contains lists of tags for each datapoint. If I\u2019m interested in analyzing tags, what\u2019s the best way of structuring the data? It seems like I\u2019ll probably need to construct some sort of relational database, but I\u2019m not a data engineer by training and was wondering if there\u2019s a simpler method that I\u2019m missing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ygrc5r", "is_robot_indexable": true, "report_reasons": null, "author": "heartofcinders", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ygrc5r/dealing_with_tagged_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ygrc5r/dealing_with_tagged_data/", "subreddit_subscribers": 816088, "created_utc": 1667068937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Data Science feels boring once you do it for your job, limits your scope for exploration. I\u2019ve felt that side projects are not only meant as a way to make more income, but more as a means to explore beyond and gain more knowledge, while keeping the job exciting enough to pursue. What are some other perspectives to this?", "author_fullname": "t2_fgpnuff3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Side projects make the day job more exciting.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ygr2py", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667068285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data Science feels boring once you do it for your job, limits your scope for exploration. I\u2019ve felt that side projects are not only meant as a way to make more income, but more as a means to explore beyond and gain more knowledge, while keeping the job exciting enough to pursue. What are some other perspectives to this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ygr2py", "is_robot_indexable": true, "report_reasons": null, "author": "metalvendetta", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ygr2py/side_projects_make_the_day_job_more_exciting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ygr2py/side_projects_make_the_day_job_more_exciting/", "subreddit_subscribers": 816088, "created_utc": 1667068285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a dataframe with weather data from 2015-2021, where the columns are different weather variables and the rows are date/time. The purpose is to use the weather data to predict the number of people on the beach at a given time.\n\nThe training data consists of about 40 000 rows, and under 0.5% of each column have missing values. The missing values spreads out pretty evenly across the years and throughout the day. The number is roughly the same for every variable.\n\nThe question is, is it fair to categorize the missing values as Missing Completely at Random, as it doesn't seem to be any pattern/bias to the missing values?  Is it ok to just simply drop every row with missing values? When I do that, I only drop about 1.7% of the total rows. \n\n&amp;#x200B;\n\nI ask because I read everywhere that you shouldn't really drop missing values, but impute it. But in this case, I can't see why this would be a problem.", "author_fullname": "t2_b7z7a6t4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drop every row with missing values when the percentage is low?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ygqic5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667066863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a dataframe with weather data from 2015-2021, where the columns are different weather variables and the rows are date/time. The purpose is to use the weather data to predict the number of people on the beach at a given time.&lt;/p&gt;\n\n&lt;p&gt;The training data consists of about 40 000 rows, and under 0.5% of each column have missing values. The missing values spreads out pretty evenly across the years and throughout the day. The number is roughly the same for every variable.&lt;/p&gt;\n\n&lt;p&gt;The question is, is it fair to categorize the missing values as Missing Completely at Random, as it doesn&amp;#39;t seem to be any pattern/bias to the missing values?  Is it ok to just simply drop every row with missing values? When I do that, I only drop about 1.7% of the total rows. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I ask because I read everywhere that you shouldn&amp;#39;t really drop missing values, but impute it. But in this case, I can&amp;#39;t see why this would be a problem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ygqic5", "is_robot_indexable": true, "report_reasons": null, "author": "Outside-Werewolf-983", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ygqic5/drop_every_row_with_missing_values_when_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ygqic5/drop_every_row_with_missing_values_when_the/", "subreddit_subscribers": 816088, "created_utc": 1667066863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I just got results for my model and let's just say accuracy, precision, and recall are at 75 with AUC at 80.  I've balanced my data sets so let's say out of 10, it classified 7 correctly.  Of the 7, let's say it correctly classified 3 as red and 4 as blue.  The other 3/10 were miss classified.  How does this translate if in reality, I have 10,000 blue and 100 red?", "author_fullname": "t2_cn54oiy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Balanced data sets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yg4tym", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667002460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just got results for my model and let&amp;#39;s just say accuracy, precision, and recall are at 75 with AUC at 80.  I&amp;#39;ve balanced my data sets so let&amp;#39;s say out of 10, it classified 7 correctly.  Of the 7, let&amp;#39;s say it correctly classified 3 as red and 4 as blue.  The other 3/10 were miss classified.  How does this translate if in reality, I have 10,000 blue and 100 red?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yg4tym", "is_robot_indexable": true, "report_reasons": null, "author": "BlackLotus8888", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yg4tym/balanced_data_sets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yg4tym/balanced_data_sets/", "subreddit_subscribers": 816088, "created_utc": 1667002460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I got into data science in college, but since I was almost done with my marketing degree I decided to get a job in marketing instead and get a stats minor. Now I'm getting back into it because my favorite classes in college were always applied statistics (econometrics/business analytics/applied regression). I took the ML Google Developer Crash Course a year ago, but feel like I lack the numpy/pandas/tensorflow foundation required for a standard data science job (I've been able to rely on SPSS for a few years now)\n\nHere's an example of qualifications I want to acquire:\n\n* Experience building and testing ML models or building applied multivariate statistical models, particularly for data classification\n* Experience with self-supervised and semi-supervised learning.\n* Proficiency with Python and/or related analytical programming languages\n* Familiarity with Machine Learning and data processing related Python packages such as Scikit-learn, Pandas, Numpy, or Tensorflow.\n\nAre there any good courses for learning this niche of data science/programming? Theres so many out there its hard to find the good ones.", "author_fullname": "t2_4em4ozqw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Courses to return to basics after a while off", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yg1woc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666994570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got into data science in college, but since I was almost done with my marketing degree I decided to get a job in marketing instead and get a stats minor. Now I&amp;#39;m getting back into it because my favorite classes in college were always applied statistics (econometrics/business analytics/applied regression). I took the ML Google Developer Crash Course a year ago, but feel like I lack the numpy/pandas/tensorflow foundation required for a standard data science job (I&amp;#39;ve been able to rely on SPSS for a few years now)&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an example of qualifications I want to acquire:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Experience building and testing ML models or building applied multivariate statistical models, particularly for data classification&lt;/li&gt;\n&lt;li&gt;Experience with self-supervised and semi-supervised learning.&lt;/li&gt;\n&lt;li&gt;Proficiency with Python and/or related analytical programming languages&lt;/li&gt;\n&lt;li&gt;Familiarity with Machine Learning and data processing related Python packages such as Scikit-learn, Pandas, Numpy, or Tensorflow.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Are there any good courses for learning this niche of data science/programming? Theres so many out there its hard to find the good ones.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yg1woc", "is_robot_indexable": true, "report_reasons": null, "author": "Lintaar", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yg1woc/courses_to_return_to_basics_after_a_while_off/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yg1woc/courses_to_return_to_basics_after_a_while_off/", "subreddit_subscribers": 816088, "created_utc": 1666994570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all,\n\nHopefully this isn't the wrong place for this. Currently working on a clustering problem. Trying to identify strings (email addresses) with a high degree of similarity. Attempting to use scipy's implementation of hierarchical clustering. To prepare the string data for use with the algorithm I transformed the strings to ascii, and then have tried a few different methods to induce normality (log and boxcox). The issue is I'm losing information through the transformation process so I can't convert the data back to the proper email strings after I've clustered them. \n\nAny recommendations for how to deal with this problem? Better approaches to solving the overall problem are also very welcome!", "author_fullname": "t2_gj1m6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "clustering string methodologies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yg15gk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666992656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;Hopefully this isn&amp;#39;t the wrong place for this. Currently working on a clustering problem. Trying to identify strings (email addresses) with a high degree of similarity. Attempting to use scipy&amp;#39;s implementation of hierarchical clustering. To prepare the string data for use with the algorithm I transformed the strings to ascii, and then have tried a few different methods to induce normality (log and boxcox). The issue is I&amp;#39;m losing information through the transformation process so I can&amp;#39;t convert the data back to the proper email strings after I&amp;#39;ve clustered them. &lt;/p&gt;\n\n&lt;p&gt;Any recommendations for how to deal with this problem? Better approaches to solving the overall problem are also very welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yg15gk", "is_robot_indexable": true, "report_reasons": null, "author": "JimBeanery", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yg15gk/clustering_string_methodologies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yg15gk/clustering_string_methodologies/", "subreddit_subscribers": 816088, "created_utc": 1666992656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was reading this section:[https://jupyterbook.org/en/stable/content/references.html#reference-tables](https://jupyterbook.org/en/stable/content/references.html#reference-tables)\n\nI want to create automatic references/indexes for some printed pandas df but I don't want to paste the text in the cell to use the {table} chunk. Is there a way I can directly reference the output of some Python code?\n\nEDIT: I can't render/reproduce the glue examples from this page: [https://jupyterbook.org/en/stable/content/executable/output-insert.html#the-glue-figure-directive](https://jupyterbook.org/en/stable/content/executable/output-insert.html#the-glue-figure-directive)", "author_fullname": "t2_1qt8rl2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to cross-reference the output of a pandas dataframe in a Jupyter Notebook?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yg0lie", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666994065.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666991238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was reading this section:&lt;a href=\"https://jupyterbook.org/en/stable/content/references.html#reference-tables\"&gt;https://jupyterbook.org/en/stable/content/references.html#reference-tables&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I want to create automatic references/indexes for some printed pandas df but I don&amp;#39;t want to paste the text in the cell to use the {table} chunk. Is there a way I can directly reference the output of some Python code?&lt;/p&gt;\n\n&lt;p&gt;EDIT: I can&amp;#39;t render/reproduce the glue examples from this page: &lt;a href=\"https://jupyterbook.org/en/stable/content/executable/output-insert.html#the-glue-figure-directive\"&gt;https://jupyterbook.org/en/stable/content/executable/output-insert.html#the-glue-figure-directive&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yg0lie", "is_robot_indexable": true, "report_reasons": null, "author": "bayesianwannabe1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yg0lie/is_there_a_way_to_crossreference_the_output_of_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yg0lie/is_there_a_way_to_crossreference_the_output_of_a/", "subreddit_subscribers": 816088, "created_utc": 1666991238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What was it and what was your manager's reaction?", "author_fullname": "t2_jjuvhqid", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you ever committed a big or small mistake at work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yg09c7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666990379.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What was it and what was your manager&amp;#39;s reaction?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yg09c7", "is_robot_indexable": true, "report_reasons": null, "author": "VeliVoy", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yg09c7/have_you_ever_committed_a_big_or_small_mistake_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yg09c7/have_you_ever_committed_a_big_or_small_mistake_at/", "subreddit_subscribers": 816088, "created_utc": 1666990379.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "The model part does not some difficult whatsoever by time consuming considering the \u201cask\u201d. Also I was caught off guard after the HR guy asked to do a SQL assessment. Have any of you been blind sided like that in the past or did HR just make a mistake? \nIt\u2019s for a senior lead analyst at an insurance company", "author_fullname": "t2_a35f24qv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Was asked to do a SQL assessment for a job interview and it\u2019s turned into a model development and presentation\u2026 anyone seen this in the past", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yg06y6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "spoiler", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666990207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The model part does not some difficult whatsoever by time consuming considering the \u201cask\u201d. Also I was caught off guard after the HR guy asked to do a SQL assessment. Have any of you been blind sided like that in the past or did HR just make a mistake? \nIt\u2019s for a senior lead analyst at an insurance company&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": true, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yg06y6", "is_robot_indexable": true, "report_reasons": null, "author": "hartwickw", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yg06y6/was_asked_to_do_a_sql_assessment_for_a_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yg06y6/was_asked_to_do_a_sql_assessment_for_a_job/", "subreddit_subscribers": 816088, "created_utc": 1666990207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Basically I have accepted an internship as a Decision Support Analyst. It's Descriptive Analytics where I will be writing/executing SQL queries and working with data management and reporting to the team. In the future I hope to seek a data analyst intern position. It's also a government job, would this intern be a strong foundation for me to land a future data analyst internship since it overlaps in some aspects of skills required (SQL)?", "author_fullname": "t2_7i7z4bhx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my internship a strong foundation towards a future Data Analyst Intern?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ygr0vs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667068162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically I have accepted an internship as a Decision Support Analyst. It&amp;#39;s Descriptive Analytics where I will be writing/executing SQL queries and working with data management and reporting to the team. In the future I hope to seek a data analyst intern position. It&amp;#39;s also a government job, would this intern be a strong foundation for me to land a future data analyst internship since it overlaps in some aspects of skills required (SQL)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ygr0vs", "is_robot_indexable": true, "report_reasons": null, "author": "Defiant_Delivery2106", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ygr0vs/is_my_internship_a_strong_foundation_towards_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ygr0vs/is_my_internship_a_strong_foundation_towards_a/", "subreddit_subscribers": 816088, "created_utc": 1667068162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm looking for an efficient way to solve this problem (in Python):\n\nThere are N samples, and they are ordered (e.g. by timestamp.) Each sample has M different features measured. I want to pick the best timestamp to split the data into \"before\" and \"after\" in a way that minimizes the RMSE of the overall dataset once you subtract the mean values from the \"before\" and \"after\" groups.\n\nFor example, say there is a machine that takes Skittles of five colors and fills a bag by mixing them. At some point, its performance changed - say, it used to give 5% too many yellows, and now it gives the right number of yellows and purples but 10% too few reds. I have 200 bags of Skittles with an associated timestamp for each and the count for each bag of how many Skittles of each of the five colors are in that bag, and I have to determine my best guess of when the performance of the filling machine shifted.\n\nI think the brute force method is reasonably straightforward: with N samples, there are N-1 possible \"break points\" to split \"before\" vs \"after\" datasets. So, I could loop through each break point, take the mean value for each color in the i \"before\" data points, take the mean value for each color in the N-i \"after\" data points, and then calculate RMSE of the overall dataset for that break point. After calculating N-1 RMSE values at every possible break point, I choose the break point with the lowest RMSE.\n\nHowever, that doesn't seem very efficient. I think this is basically a decision tree, but the most bare-bones version of a decision tree possible: there is only one predictor (timestamp) and I only need to make a single branch in the tree. (The only thing making it slightly more complicated is that there are multiple response variables instead of just one response, but I think that shouldn't be hard to deal with if we say the responses are already scaled appropriately.) So I would assume that there are already very well-optimized tools to solve this sort of problem, that will be more efficient than the brute-force solution I outlined above.\n\nAny suggestions on a good way to do a single split/partition like this in Python?", "author_fullname": "t2_3cw8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to do \"decision tree\" with just two leaves, with just one continuous predictor, and multiple responses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ygpre6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667065066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for an efficient way to solve this problem (in Python):&lt;/p&gt;\n\n&lt;p&gt;There are N samples, and they are ordered (e.g. by timestamp.) Each sample has M different features measured. I want to pick the best timestamp to split the data into &amp;quot;before&amp;quot; and &amp;quot;after&amp;quot; in a way that minimizes the RMSE of the overall dataset once you subtract the mean values from the &amp;quot;before&amp;quot; and &amp;quot;after&amp;quot; groups.&lt;/p&gt;\n\n&lt;p&gt;For example, say there is a machine that takes Skittles of five colors and fills a bag by mixing them. At some point, its performance changed - say, it used to give 5% too many yellows, and now it gives the right number of yellows and purples but 10% too few reds. I have 200 bags of Skittles with an associated timestamp for each and the count for each bag of how many Skittles of each of the five colors are in that bag, and I have to determine my best guess of when the performance of the filling machine shifted.&lt;/p&gt;\n\n&lt;p&gt;I think the brute force method is reasonably straightforward: with N samples, there are N-1 possible &amp;quot;break points&amp;quot; to split &amp;quot;before&amp;quot; vs &amp;quot;after&amp;quot; datasets. So, I could loop through each break point, take the mean value for each color in the i &amp;quot;before&amp;quot; data points, take the mean value for each color in the N-i &amp;quot;after&amp;quot; data points, and then calculate RMSE of the overall dataset for that break point. After calculating N-1 RMSE values at every possible break point, I choose the break point with the lowest RMSE.&lt;/p&gt;\n\n&lt;p&gt;However, that doesn&amp;#39;t seem very efficient. I think this is basically a decision tree, but the most bare-bones version of a decision tree possible: there is only one predictor (timestamp) and I only need to make a single branch in the tree. (The only thing making it slightly more complicated is that there are multiple response variables instead of just one response, but I think that shouldn&amp;#39;t be hard to deal with if we say the responses are already scaled appropriately.) So I would assume that there are already very well-optimized tools to solve this sort of problem, that will be more efficient than the brute-force solution I outlined above.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions on a good way to do a single split/partition like this in Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ygpre6", "is_robot_indexable": true, "report_reasons": null, "author": "Pandaemonium", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ygpre6/best_way_to_do_decision_tree_with_just_two_leaves/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ygpre6/best_way_to_do_decision_tree_with_just_two_leaves/", "subreddit_subscribers": 816088, "created_utc": 1667065066.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}