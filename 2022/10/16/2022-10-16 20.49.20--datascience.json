{"kind": "Listing", "data": {"after": null, "dist": 13, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all,\n\nAs a meager software engineer mortal, I've always been curious about how concepts/algorithms work in machine learning. I am starting a series of posts, documenting my understanding of these concepts.\n\nHere's my first attempt; it's on Gini impurity, a core concept fundamental in how decision trees are built.\n\n[https://gradiently.io/gini-impurity/](https://gradiently.io/gini-impurity/)", "author_fullname": "t2_6dnrwg8t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An Explanation of Gini Impurity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y50l0u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 53, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 53, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665873125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;As a meager software engineer mortal, I&amp;#39;ve always been curious about how concepts/algorithms work in machine learning. I am starting a series of posts, documenting my understanding of these concepts.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s my first attempt; it&amp;#39;s on Gini impurity, a core concept fundamental in how decision trees are built.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://gradiently.io/gini-impurity/\"&gt;https://gradiently.io/gini-impurity/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y50l0u", "is_robot_indexable": true, "report_reasons": null, "author": "Ill-Tomato-8400", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y50l0u/an_explanation_of_gini_impurity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y50l0u/an_explanation_of_gini_impurity/", "subreddit_subscribers": 813860, "created_utc": 1665873125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "the youtube video for the  How to Win a Data Science Competition Learn from Top Kagglers has been taken down as the youtube channel was also deleted, i assume that one of the reasons is due to them being russian which is why their course was also taken down from coursera. I was at the part wherein they talked about their solutions in past competitions which is the final hour i'd say and then the channel got deleted. So if it is ok and if it is possible please give a link of the video as i'd like to finish it\n\nEDIT: i know that the videos are also in bilibili but they don't have english subtitles like in youtube and i'm having a hard time understanding some of the words they're saying due to their accent", "author_fullname": "t2_7bstgl3u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "does anyone have a copy of the How to Win a Data Science Competition Learn from Top Kagglers video from youtube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5g0hh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 56, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 56, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665924515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;the youtube video for the  How to Win a Data Science Competition Learn from Top Kagglers has been taken down as the youtube channel was also deleted, i assume that one of the reasons is due to them being russian which is why their course was also taken down from coursera. I was at the part wherein they talked about their solutions in past competitions which is the final hour i&amp;#39;d say and then the channel got deleted. So if it is ok and if it is possible please give a link of the video as i&amp;#39;d like to finish it&lt;/p&gt;\n\n&lt;p&gt;EDIT: i know that the videos are also in bilibili but they don&amp;#39;t have english subtitles like in youtube and i&amp;#39;m having a hard time understanding some of the words they&amp;#39;re saying due to their accent&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5g0hh", "is_robot_indexable": true, "report_reasons": null, "author": "Impossible_Squirrel5", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5g0hh/does_anyone_have_a_copy_of_the_how_to_win_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5g0hh/does_anyone_have_a_copy_of_the_how_to_win_a_data/", "subreddit_subscribers": 813860, "created_utc": 1665924515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_b7z7a6t4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visualization of a column of a dataframe where the values are the number of cars driving in a certain area at a given time. Any tips on how to proceed in regards to outliers? Should I remove them? Which method should I use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 56, "top_awarded_type": null, "hide_score": false, "name": "t3_y5czuz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PZmKJh7GKN5Aqr9AWJl56onaSFdXKisOm-KWFmOEb2Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1665914384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/23df2xnj45u91.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/23df2xnj45u91.png?auto=webp&amp;s=d1d965d8f1c3e24f8638d5ed654bd7a7834f5f5b", "width": 1439, "height": 578}, "resolutions": [{"url": "https://preview.redd.it/23df2xnj45u91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0abd28d1c4e3c1b6b26d57ba754e08a3be797336", "width": 108, "height": 43}, {"url": "https://preview.redd.it/23df2xnj45u91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b63ccfa4adce257b81f3b5b21a11b630b0ecda2", "width": 216, "height": 86}, {"url": "https://preview.redd.it/23df2xnj45u91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a3fbf41ca2c100dd5fe2c1d6fbff449b87d5d76", "width": 320, "height": 128}, {"url": "https://preview.redd.it/23df2xnj45u91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=55c522e2c97ab55db2c878228a3b214cd6df9a3f", "width": 640, "height": 257}, {"url": "https://preview.redd.it/23df2xnj45u91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c54c0affbd933536c327a31bc0d88d496adc1dd7", "width": 960, "height": 385}, {"url": "https://preview.redd.it/23df2xnj45u91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0450942eb931bd0485f22732a21cef1a93fd860a", "width": 1080, "height": 433}], "variants": {}, "id": "sU5-iEpPBGh0qA_qLTEEhlH8k0ii3ALKLch77ad5CSI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5czuz", "is_robot_indexable": true, "report_reasons": null, "author": "Outside-Werewolf-983", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5czuz/visualization_of_a_column_of_a_dataframe_where/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/23df2xnj45u91.png", "subreddit_subscribers": 813860, "created_utc": 1665914384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I know there are a lot of posts about resources, but I don't recall seeing this specifically. \n\nI'm looking for for resources for Bayes statistics.   Everything I've found is either simple examples meant to introduce a concept... or very theoretical without many examples. \n\nI need something in the middle, examples that are complex and varied enough to get into details.  Maybe even something that progresses in difficult.  Currently going over MCMC, but I actually thing it's more basic Bayes stuff that I'm getting hung up on. \n\nI might be a bit weird... i'm good at math, but not strong.   What i mean is: I went up through Calc II in college and didn't have issues with it, but between a lack of application or practice, I've been struggling with the stats/math side of DS classes. \n\nIn honestly considering dropping this class, just so I can study and retake it next term.  Any assistance is appreciated!", "author_fullname": "t2_7t6en9d2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bayes examples and study help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y55z3b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665889307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know there are a lot of posts about resources, but I don&amp;#39;t recall seeing this specifically. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for for resources for Bayes statistics.   Everything I&amp;#39;ve found is either simple examples meant to introduce a concept... or very theoretical without many examples. &lt;/p&gt;\n\n&lt;p&gt;I need something in the middle, examples that are complex and varied enough to get into details.  Maybe even something that progresses in difficult.  Currently going over MCMC, but I actually thing it&amp;#39;s more basic Bayes stuff that I&amp;#39;m getting hung up on. &lt;/p&gt;\n\n&lt;p&gt;I might be a bit weird... i&amp;#39;m good at math, but not strong.   What i mean is: I went up through Calc II in college and didn&amp;#39;t have issues with it, but between a lack of application or practice, I&amp;#39;ve been struggling with the stats/math side of DS classes. &lt;/p&gt;\n\n&lt;p&gt;In honestly considering dropping this class, just so I can study and retake it next term.  Any assistance is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y55z3b", "is_robot_indexable": true, "report_reasons": null, "author": "Cryptic-Squid", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y55z3b/bayes_examples_and_study_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y55z3b/bayes_examples_and_study_help/", "subreddit_subscribers": 813860, "created_utc": 1665889307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Id assume that if you wanted to predict on certain features, youd train the model using only those - however the instructions are explicit. Is there a standard way of dealing with missing features like this? \n\nEg: features for model trained on x1, x2, x3, x4, x5\nFeatures for data to be predicted on: x1, x2, x3", "author_fullname": "t2_gm8b3iu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have an assignment to train a model with specific features, then use it to predict on a dataset containing a subset of the features, is this normal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5lodf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665939210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Id assume that if you wanted to predict on certain features, youd train the model using only those - however the instructions are explicit. Is there a standard way of dealing with missing features like this? &lt;/p&gt;\n\n&lt;p&gt;Eg: features for model trained on x1, x2, x3, x4, x5\nFeatures for data to be predicted on: x1, x2, x3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5lodf", "is_robot_indexable": true, "report_reasons": null, "author": "poppycocknbalderdash", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5lodf/i_have_an_assignment_to_train_a_model_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5lodf/i_have_an_assignment_to_train_a_model_with/", "subreddit_subscribers": 813860, "created_utc": 1665939210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm wondering if traditional statistics/EDA/visualization really scales to \"big data\" and whether it is really useful to process an entire \"big dataset\" for EDA vs just sub-sampling.\n\nI realize of course that deep learning **DOES** scale to big data, **and I'm in no way doubting that**. I'm just asking what other data science technique **ALSO** scale to big data (e.g. billions of samples)?\n\nP.S. I'm also asking in the context of map-reduce. That design paradigm seems to imply that data-processing on that scale is useful outside of the context of Deep learning.   \nI'm wondering if I could get practical examples where that is obviously true (preferably in EDA/statistics but other examples are welcome as well).", "author_fullname": "t2_qrw52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EDA on \"big data\": is it sufficient to just sub-sample?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y5pi8e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665948483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering if traditional statistics/EDA/visualization really scales to &amp;quot;big data&amp;quot; and whether it is really useful to process an entire &amp;quot;big dataset&amp;quot; for EDA vs just sub-sampling.&lt;/p&gt;\n\n&lt;p&gt;I realize of course that deep learning &lt;strong&gt;DOES&lt;/strong&gt; scale to big data, &lt;strong&gt;and I&amp;#39;m in no way doubting that&lt;/strong&gt;. I&amp;#39;m just asking what other data science technique &lt;strong&gt;ALSO&lt;/strong&gt; scale to big data (e.g. billions of samples)?&lt;/p&gt;\n\n&lt;p&gt;P.S. I&amp;#39;m also asking in the context of map-reduce. That design paradigm seems to imply that data-processing on that scale is useful outside of the context of Deep learning.&lt;br/&gt;\nI&amp;#39;m wondering if I could get practical examples where that is obviously true (preferably in EDA/statistics but other examples are welcome as well).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5pi8e", "is_robot_indexable": true, "report_reasons": null, "author": "Udon_noodles", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5pi8e/eda_on_big_data_is_it_sufficient_to_just_subsample/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5pi8e/eda_on_big_data_is_it_sufficient_to_just_subsample/", "subreddit_subscribers": 813860, "created_utc": 1665948483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Q]: Can one apply multiple linear regression to the means of simulations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y5oykw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_28xoyq0t", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "statistics", "selftext": "My problem is sort of hard to explain so I used code to demonstrate. I am simulating different temporal sampling methods from time series data. From this I am looking to see how the sampled data can be used to estimate the annual total from 30-minute data. See this example using R:\n\nExample data:\n\n`set.seed(123)`\n\n`df = bind_rows(tibble(date =seq(as.POSIXct(\"2022-01-01\", tz = \"UTC\"),as.POSIXct(\"2022-12-31\", tz = \"UTC\"),by = \"30 min\"), val=rnorm((17473))%&gt;% cumsum(.), group = \"North\",a = 56, b=96, c=0.68), tibble(date =seq(as.POSIXct(\"2022-01-01\", tz = \"UTC\"),as.POSIXct(\"2022-12-31\", tz = \"UTC\"),by = \"30 min\"), val=rnorm((17473))%&gt;% cumsum(.), group = \"East\",a = 85, b=46, c=0.25), tibble(date =seq(as.POSIXct(\"2022-01-01\", tz = \"UTC\"),as.POSIXct(\"2022-12-31\", tz = \"UTC\"),by = \"30 min\"), val=rnorm((17473))%&gt;% cumsum(.), group = \"South\",a = 60, b=100, c=0.99)) %&gt;% mutate(val = abs(val))`\n\nI calculate the annual sum:\n\n`df_totals= df %&gt;% group_by(group) %&gt;% summarise(annual_amount = mean(val)*(48*365))`\n\nThe loop looks something like this:\n\n`for(j in unique(df$group)){`  \n`df_sub &lt;- df %&gt;% filter(group == j)`  \n`qe &lt;- NULL`  \n`me &lt;- NULL`  \n`we &lt;- NULL`  \n`print(j)`  \n`for(k in 1:25){`  \n`print(k)`  \n`qtemp=tibble()`  \n`for(i in unique(quarters(df_sub$date))){`  \n`q &lt;- df_sub[sample(which(quarters(df_sub$date) == i),1,replace = T),]`  \n`qtemp &lt;- bind_rows(qtemp,q)`  \n`print(\"Quarter Data Report\")`  \n`}`  \n`qe[k] = mean(qtemp$val)*17520`  \n`mtemp=tibble()`  \n`for(i in unique(format(df_sub$date,\"%m\"))){`  \n`m &lt;- df_sub[sample(which(format(df_sub$date,\"%m\") == i),1,replace = T),]`  \n`mtemp &lt;- bind_rows(mtemp,m)`  \n`print(\"Monthly Data Report\")`  \n`}`  \n`me[k] = mean(mtemp$val)*17520`  \n`wtemp=tibble()`  \n`for(i in unique(format(df_sub$date,\"%U\"))){`  \n`w &lt;- df_sub[sample(which(format(df_sub$date,\"%U\") == i),1,replace = T),]`  \n`wtemp &lt;- bind_rows(wtemp,w)`  \n`print(\"Weekly Data Report\")`  \n`}`  \n`we[k] = mean(wtemp$val)*17520`  \n`}`  \n`qe_all = qe_all %&gt;% bind_rows(tibble(est = qe, sample = \"Quarterly\", group =unique(qtemp$group), a = unique(qtemp$a), b = unique(qtemp$b), c = unique(qtemp$c)))`   \n`me_all = me_all %&gt;% bind_rows(tibble(est = me, sample = \"Monthly\", group = unique(mtemp$group), a = unique(mtemp$a), b = unique(mtemp$b), c = unique(mtemp$c)))`   \n`we_all = we_all %&gt;% bind_rows(tibble(est = we, sample = \"Weekly\", group = unique(wtemp$group), a = unique(wtemp$a), b = unique(wtemp$b), c = unique(wtemp$c))) rm(qe,me,we)`  \n`}`\n\n`ests &lt;- ests %&gt;% bind_rows(qe_all, me_all, we_all) %&gt;% left_join(df_totals, by = \"group\") %&gt;% mutate(error = est/annual_amount)`\n\nIn reality the error is much more variable between group and a, b, c.\n\nI am trying to understand how the independent variables a, b, and c affect the dependent variable error for each digital simulated sampling method (Quarterly, Monthly, Weekly). So, there will be 3 regression models. Where y is error, with predictors a, b, and c. In my real data, I am simulating 1000+ times. It doesn't seem correct to use 1000+ observations of y. But what if I took the mean of y (error) for each group (North, East, and South).\n\nHere are my questions, for which I would be grateful to get feedback:\n\n* Is multiple linear regression the best approach here?\n* Would a machine learning method be better, such as random forest?\n* For each regression model, is it reasonable to take the mean error for each group and use the mean error as the dependent variable?\n* Is it an issue for dependent variable to be a percentage in the regression (it's not necessarily constrained between 0 and 1)?\n\nIf I were to use the means, data would look like this, and three regression models would be fit by sample:\n\n`ests %&gt;% group_by(sample, group) %&gt;% summarise(a = unique(a), b = unique(b), c = unique(c), error = mean(error))`\n\nApologies for a long-winded question!", "author_fullname": "t2_28xoyq0t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Q]: Can one apply multiple linear regression to the means of simulations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/statistics", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y4apku", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665798178.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665796362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.statistics", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My problem is sort of hard to explain so I used code to demonstrate. I am simulating different temporal sampling methods from time series data. From this I am looking to see how the sampled data can be used to estimate the annual total from 30-minute data. See this example using R:&lt;/p&gt;\n\n&lt;p&gt;Example data:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;set.seed(123)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;df = bind_rows(tibble(date =seq(as.POSIXct(&amp;quot;2022-01-01&amp;quot;, tz = &amp;quot;UTC&amp;quot;),as.POSIXct(&amp;quot;2022-12-31&amp;quot;, tz = &amp;quot;UTC&amp;quot;),by = &amp;quot;30 min&amp;quot;), val=rnorm((17473))%&amp;gt;% cumsum(.), group = &amp;quot;North&amp;quot;,a = 56, b=96, c=0.68), tibble(date =seq(as.POSIXct(&amp;quot;2022-01-01&amp;quot;, tz = &amp;quot;UTC&amp;quot;),as.POSIXct(&amp;quot;2022-12-31&amp;quot;, tz = &amp;quot;UTC&amp;quot;),by = &amp;quot;30 min&amp;quot;), val=rnorm((17473))%&amp;gt;% cumsum(.), group = &amp;quot;East&amp;quot;,a = 85, b=46, c=0.25), tibble(date =seq(as.POSIXct(&amp;quot;2022-01-01&amp;quot;, tz = &amp;quot;UTC&amp;quot;),as.POSIXct(&amp;quot;2022-12-31&amp;quot;, tz = &amp;quot;UTC&amp;quot;),by = &amp;quot;30 min&amp;quot;), val=rnorm((17473))%&amp;gt;% cumsum(.), group = &amp;quot;South&amp;quot;,a = 60, b=100, c=0.99)) %&amp;gt;% mutate(val = abs(val))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I calculate the annual sum:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;df_totals= df %&amp;gt;% group_by(group) %&amp;gt;% summarise(annual_amount = mean(val)*(48*365))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;The loop looks something like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;for(j in unique(df$group)){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;df_sub &amp;lt;- df %&amp;gt;% filter(group == j)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qe &amp;lt;- NULL&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;me &amp;lt;- NULL&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;we &amp;lt;- NULL&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(j)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;for(k in 1:25){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(k)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qtemp=tibble()&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;for(i in unique(quarters(df_sub$date))){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;q &amp;lt;- df_sub[sample(which(quarters(df_sub$date) == i),1,replace = T),]&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qtemp &amp;lt;- bind_rows(qtemp,q)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(&amp;quot;Quarter Data Report&amp;quot;)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qe[k] = mean(qtemp$val)*17520&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;mtemp=tibble()&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;for(i in unique(format(df_sub$date,&amp;quot;%m&amp;quot;))){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;m &amp;lt;- df_sub[sample(which(format(df_sub$date,&amp;quot;%m&amp;quot;) == i),1,replace = T),]&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;mtemp &amp;lt;- bind_rows(mtemp,m)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(&amp;quot;Monthly Data Report&amp;quot;)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;me[k] = mean(mtemp$val)*17520&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;wtemp=tibble()&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;for(i in unique(format(df_sub$date,&amp;quot;%U&amp;quot;))){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;w &amp;lt;- df_sub[sample(which(format(df_sub$date,&amp;quot;%U&amp;quot;) == i),1,replace = T),]&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;wtemp &amp;lt;- bind_rows(wtemp,w)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(&amp;quot;Weekly Data Report&amp;quot;)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;we[k] = mean(wtemp$val)*17520&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qe_all = qe_all %&amp;gt;% bind_rows(tibble(est = qe, sample = &amp;quot;Quarterly&amp;quot;, group =unique(qtemp$group), a = unique(qtemp$a), b = unique(qtemp$b), c = unique(qtemp$c)))&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;me_all = me_all %&amp;gt;% bind_rows(tibble(est = me, sample = &amp;quot;Monthly&amp;quot;, group = unique(mtemp$group), a = unique(mtemp$a), b = unique(mtemp$b), c = unique(mtemp$c)))&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;we_all = we_all %&amp;gt;% bind_rows(tibble(est = we, sample = &amp;quot;Weekly&amp;quot;, group = unique(wtemp$group), a = unique(wtemp$a), b = unique(wtemp$b), c = unique(wtemp$c))) rm(qe,me,we)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ests &amp;lt;- ests %&amp;gt;% bind_rows(qe_all, me_all, we_all) %&amp;gt;% left_join(df_totals, by = &amp;quot;group&amp;quot;) %&amp;gt;% mutate(error = est/annual_amount)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;In reality the error is much more variable between group and a, b, c.&lt;/p&gt;\n\n&lt;p&gt;I am trying to understand how the independent variables a, b, and c affect the dependent variable error for each digital simulated sampling method (Quarterly, Monthly, Weekly). So, there will be 3 regression models. Where y is error, with predictors a, b, and c. In my real data, I am simulating 1000+ times. It doesn&amp;#39;t seem correct to use 1000+ observations of y. But what if I took the mean of y (error) for each group (North, East, and South).&lt;/p&gt;\n\n&lt;p&gt;Here are my questions, for which I would be grateful to get feedback:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is multiple linear regression the best approach here?&lt;/li&gt;\n&lt;li&gt;Would a machine learning method be better, such as random forest?&lt;/li&gt;\n&lt;li&gt;For each regression model, is it reasonable to take the mean error for each group and use the mean error as the dependent variable?&lt;/li&gt;\n&lt;li&gt;Is it an issue for dependent variable to be a percentage in the regression (it&amp;#39;s not necessarily constrained between 0 and 1)?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If I were to use the means, data would look like this, and three regression models would be fit by sample:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ests %&amp;gt;% group_by(sample, group) %&amp;gt;% summarise(a = unique(a), b = unique(b), c = unique(c), error = mean(error))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Apologies for a long-winded question!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qhfi", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y4apku", "is_robot_indexable": true, "report_reasons": null, "author": "squishytea", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/statistics/comments/y4apku/q_can_one_apply_multiple_linear_regression_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/statistics/comments/y4apku/q_can_one_apply_multiple_linear_regression_to_the/", "subreddit_subscribers": 521882, "created_utc": 1665796362.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1665947169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.statistics", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/statistics/comments/y4apku/q_can_one_apply_multiple_linear_regression_to_the/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5oykw", "is_robot_indexable": true, "report_reasons": null, "author": "squishytea", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_y4apku", "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5oykw/q_can_one_apply_multiple_linear_regression_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/statistics/comments/y4apku/q_can_one_apply_multiple_linear_regression_to_the/", "subreddit_subscribers": 813860, "created_utc": 1665947169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have given data for users which is right skewed with a long tail, meaning high gmv is driven by few users. Now I have 2 cohorts of users for whom I want to compare gmv distribution. My first instinct was to go for t-test but it has an assumption of normality.  Though I also found I  my readings that if my sample size is large enough (typically &gt; 100) central limit theorem would kick in and the difference in mean should be normally distributed so I should be able to apply t-test on my raw data. \n\nBut there is no literature on effect size calculation if my data is skewed, I am thinking of Cohen's D and since it also assumes normality, perform log normal transformation on my data and perform t test and Cohen's D on that. \n\nFrom my reading transformed t-test p value is applicable for raw data as well but not sure about Cohen's D. \n\nAny guidance on how this kind of analysis is usually done would be really helpful.", "author_fullname": "t2_6ys5mu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Cohen's D valid for effect size on log transformed data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5na1y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665943153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have given data for users which is right skewed with a long tail, meaning high gmv is driven by few users. Now I have 2 cohorts of users for whom I want to compare gmv distribution. My first instinct was to go for t-test but it has an assumption of normality.  Though I also found I  my readings that if my sample size is large enough (typically &amp;gt; 100) central limit theorem would kick in and the difference in mean should be normally distributed so I should be able to apply t-test on my raw data. &lt;/p&gt;\n\n&lt;p&gt;But there is no literature on effect size calculation if my data is skewed, I am thinking of Cohen&amp;#39;s D and since it also assumes normality, perform log normal transformation on my data and perform t test and Cohen&amp;#39;s D on that. &lt;/p&gt;\n\n&lt;p&gt;From my reading transformed t-test p value is applicable for raw data as well but not sure about Cohen&amp;#39;s D. &lt;/p&gt;\n\n&lt;p&gt;Any guidance on how this kind of analysis is usually done would be really helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5na1y", "is_robot_indexable": true, "report_reasons": null, "author": "user19911506", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5na1y/is_cohens_d_valid_for_effect_size_on_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5na1y/is_cohens_d_valid_for_effect_size_on_log/", "subreddit_subscribers": 813860, "created_utc": 1665943153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone. Working on understanding entropy and toying some R code around it. \n\nOne thing Im struggling to understand; when your using entropy to build classification models, why try and minimize entropy instead of maximizing probability? \n\nThe root of the entropy formula is inverse probability, so why minimize this instead of maximizing probability. Is it a pohtato potahto kind of thing? Does it make a difference?", "author_fullname": "t2_ix20cupc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why minimize entropy vs maximize probability when building classification models?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5ham7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665928155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. Working on understanding entropy and toying some R code around it. &lt;/p&gt;\n\n&lt;p&gt;One thing Im struggling to understand; when your using entropy to build classification models, why try and minimize entropy instead of maximizing probability? &lt;/p&gt;\n\n&lt;p&gt;The root of the entropy formula is inverse probability, so why minimize this instead of maximizing probability. Is it a pohtato potahto kind of thing? Does it make a difference?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5ham7", "is_robot_indexable": true, "report_reasons": null, "author": "crustyporuc", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5ham7/why_minimize_entropy_vs_maximize_probability_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5ham7/why_minimize_entropy_vs_maximize_probability_when/", "subreddit_subscribers": 813860, "created_utc": 1665928155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello Data Enthusiasts,\n\nWhat kind of projects should i be focusing on or doing more of from a entry level job perspective? Also, what do recruiters mostly look for entry level graduates in data science field?\n\nAny projects ideas would be helpful?", "author_fullname": "t2_bsfosv4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Projects relating to DS/ML?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y58scn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.42, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665898781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Data Enthusiasts,&lt;/p&gt;\n\n&lt;p&gt;What kind of projects should i be focusing on or doing more of from a entry level job perspective? Also, what do recruiters mostly look for entry level graduates in data science field?&lt;/p&gt;\n\n&lt;p&gt;Any projects ideas would be helpful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y58scn", "is_robot_indexable": true, "report_reasons": null, "author": "Adventurous-Grab-20", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y58scn/projects_relating_to_dsml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y58scn/projects_relating_to_dsml/", "subreddit_subscribers": 813860, "created_utc": 1665898781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi I am looking for opportunities in Data Science field. Any opportunity will be highly appreciated. Please reach out to me on samir_harris3@hotmail.com\n\nhttps://www.linkedin.com/in/emmanuel-h-87312918", "author_fullname": "t2_mcbs6ugc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for DataScience Internship", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5jv0s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.13, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665934733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am looking for opportunities in Data Science field. Any opportunity will be highly appreciated. Please reach out to me on &lt;a href=\"mailto:samir_harris3@hotmail.com\"&gt;samir_harris3@hotmail.com&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.linkedin.com/in/emmanuel-h-87312918\"&gt;https://www.linkedin.com/in/emmanuel-h-87312918&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5jv0s", "is_robot_indexable": true, "report_reasons": null, "author": "Necessary-Comfort-89", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5jv0s/looking_for_datascience_internship/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5jv0s/looking_for_datascience_internship/", "subreddit_subscribers": 813860, "created_utc": 1665934733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am Japanese, so please forgive me if my grammar may be incorrect.  \n\n\nI would like to know how to reward DAOs not on a task basis, but by calculating their contribution through data analysis.  \nIf we can create that method with high quality, I believe we can create a new system of economy that will be upward compatible with capitalism.  \nWe call it freeism.  \n\n\nAbout freeism\n\nWe came up with the \"freeism\" system as an alternative economic mechanism to capitalism.\n\nIt gives a new alternative to economic mechanisms that have had only limited choices, such as capitalism and socialism.\n\nI know that many people are already thinking about this kind of thing, but I would like to implement it as a part of the social structure, not as a theoretical theory, and eventually create a society in which the entire economy revolves around freeism instead of capitalism.\n\n&amp;#x200B;\n\nBasic Mechanism of Freeism\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThe following is an explanation of the basic mechanism of FREELISM.\n\n&amp;#x200B;\n\nThe supplier is asked to provide all types of goods and services for free, and non-transferable points (SBT?) (called contribution points) are given to those who provide them.\n\nThere will be multiple ways to calculate contribution points, and we will also create a mechanism to provide them.\n\nThe mechanism is described below.\n\nFor products or services that are not available to all who want them (hereinafter referred to as \"limited edition products\"), the points (hereinafter referred to as \"quota\") will be set as the square root of contribution points, and those who offer to use the most quota will be given priority in using them.\n\nInstead of consuming contribution points as compensation for obtaining goods or services, they should only be accounted for so that they cannot be used to obtain other limited items.\n\nTherefore, contribution points do not decrease even if a limited item is obtained but accumulate (exceptions apply. See below)\n\nNecessity\n\nThe necessity of the quota is described below.\n\nThe method of calculating the quota is also described below.\n\nThis type of economic mechanism is called \"freeism.\n\nFreeism is an economic model that can be finally established by using technologies such as data analysis, blockchain, web apps, etc.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nDig a little deeper into the mechanism of FREESISM\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThis section explains each of the mechanisms that make up FREESISM.\n\n&amp;#x200B;\n\nContribution rules\n\nThe rules of contribution are the laws that govern life in a freeism society.\n\nOr the country in which the law is located.\n\nAt first, only one rule exists, but just as there are multiple countries, multiple rules of contribution can be created.\n\nThe contribution rules should be easy for anyone to create.\n\nI want the rules of contribution to be divided by ideology and taste (only some people emigrate based on taxation, culture, and taste, but for the most part, the current country is determined by birth and upbringing, and there are people with different tastes, values, and ideology in the same country, which creates conflict).\n\nThat is why we create the right of non-interference as the right not to interfere with others.\n\nEach contribution rule has an end goal.\n\nThe end goal is discussed below.\n\n&amp;#x200B;\n\nThe FREEISM Platform\n\nThe freeism platform is a mechanism to manage the rules of contribution.\n\nThe rules of contribution play a legal role.\n\nThe freeism platform becomes a higher-level entity like the Constitution OR the UN OR the federal government.\n\nThe freeism platform also has rules, and within the scope of those rules, they get to make the rules of contribution.\n\nThere will be multiple freeism platforms, and people will be able to join any freeism platform they want.\n\nA freeism platform may also have an end goal.\n\nThe end goal is described below.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nFinal Goal\n\nThe end goal is the goal that each freeism platform, each contribution rule, and each project (like a company) is aiming for.\n\nIt can be set by vote, decided at the establishment stage, or left to the decision-makers to set.\n\nSet goals that you want to achieve, not goals or KPIs to achieve subdivided goals.\n\nFor example, \"increase in happiness,\" \"degree of health,\" \"increase in productivity,\" \"increase in the number of rational decisions made,\" and \"crime rate.\"\n\nIt is unclear if we would set \"lower crime rate\" as a goal since even \"lower crime rate\" might be a goal to achieve higher levels of happiness. We want to set a goal that we are ultimately aiming for.\n\nFor FREEISM, we should set a final goal for the FREEISM platform, a final goal for the contribution rule, and a final goal for the project, and then calculate the contributions based on how much they have helped to bring us closer to or achieve that final goal.\n\nExamples of final goals\n\nIncrease in productivity (working hours required to obtain goods and services) within the limits of the law\n\nIncrease in happiness within the limits of the law\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nContribution points\n\nRefers to points required to obtain priority for limited items.\n\nContribution points are non-transferable.\n\nHowever, loans are possible under some contribution rules.\n\nCan I use SBT as contribution points?\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nHow to Calculate Contribution Points\n\nContribution points earned are calculated based on \"the degree of contribution to the final goal\" \u2716\ufe0e \"market principle\" \u2716\ufe0e \"different weighting/rules for each contribution rule.\"\n\nUntil now, it was \"market principles\" \u2716\ufe0e \"different weighting/rules for each country or region rule\", but we will add \"degree of contribution to the final goal\" to it.\n\nI think the market principle is encapsulated in the degree of contribution to the final goal. However, if you still want to make further use of the market principle, you can give contribution points based on \"market principle x degree of contribution to the final goal\" or \"market principle only\".\n\nThe degree of contribution to the final goal could be calculated using multiple regression analysis or other data analysis methods or methods to visualize DAO's task-based contribution.\n\nSee below.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nQuota\n\nThe quota is a mechanism to avoid monopolizing the service, and if the service can be provided to everyone, the quota is not used.\n\nEach contribution rule has a different way of calculating the quota, and each person is given a quota weighted by contribution points (contribution points \u2716\ufe0f0.9 = amount of quota, etc.) or the square root of contribution points.\n\nThe amount of quota used will be the price.\n\nThere is a negative correlation: the lower the winning rate of a product or service, the more quota it occupies, and the closer the winning rate is to 100%, the less quota it occupies.\n\nIn FREESISM, the quota is always used while the limited item is in use, and the amount used always reflects the market value, not the amount used when it was acquired.\n\nThe structure of the quota differs for each contribution rule.\n\nThe period you occupy a quota should be the period you own that limited edition item.\n\nIf you give it away or throw it away, you can apply for it on the FREEMISM platform, or the service will automatically detect it and release the use of the quota.\n\nTo prevent \"false applications\" where people apply to give it away when they own it, make it necessary to give it away or throw it away at a predetermined location or app.\n\nCreate an incentive to have it detected through the app, since it should be detected from each app and if not through the app, it will occupy the frame all the time.\n\nMake it impossible for people to report that they have given it up on their own, other than by having the app automatically detect it.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nFREEISM\n\nIn freeism, if other people refer to the products, services, source code, or other deliverables developed by each person, or to know-how and other things such as research findings, they will receive only a portion of the contribution points earned by the person they refer to, such as 0.5 times the contribution points earned by the person they refer to.\n\nThereby.\n\nIf someone only submits an idea and others implement it, the person who submitted the idea will also receive a portion of the contribution points earned by the person who implemented it.\n\nIf another company started a business and failed but was successful in running it by avoiding the strategies of the failed company, then allow the people who worked in the failed company to receive a portion of the contribution points earned by the successful company.\n\nThat way, where it was 0-1 to succeed or fail, maybe we could make it so that even if it fails, it is not 0-1 financially, but there is some benefit.\n\nSince points are just points, unlike the currency, they don't have to be distributed and can be offered to both.\n\nBut it's hard to determine if that's what I was referring to.\n\nI will discuss this later.\n\nWe want a society where everything is open source.\n\nI want to create a society where every no-how, patent, and other thing is open source, by weighting the contribution points that can be earned if others use the no-how, and by weighting the contribution points that can be earned just by making it open source, such as 1.2 times the contribution points that can be earned by making it open source.\n\nEveryone gets it without having to distribute, and being open source could lead to more compensation for sharing without monopolizing rather than monopolizing and competing.\n\nBut if the relative position without distribution determines whether you get a limited product, is it the same as when profits were shared?\n\nIt's more mentally stable because you don't have to fight for a piece of the pie.\n\nThe basic idea is to think in terms of rules of contribution, where the only rule is the degree of contribution to the liberal end goal, but when considering various rules of contribution, we can make a law that regulates each of them in a way that is characteristic to the sound of each rule of contribution.\n\nEven if any action is the wrong action or not, we will stipulate that only actions that have a bad impact on the final goal contribution level are bad.\n\nEven theft would not be a crime if it did not negatively impact anyone.\n\nSomewhere the idea is like a woman who says it's not cheating if she doesn't get caught in a love story about whether or not she qualifies as cheating.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThe position of each\n\nIn freeism, the system is divided into a freeism platform, contribution rules, projects, and other mechanisms.\n\nThe freeism platform is the role of the federal government, the contribution rule is the role of the state government, and the project is the role of the company.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nProject\n\nThe role of the company.\n\nThe idea of freeism is to make everything open, so all the data of companies that are currently hidden will be shared and made available to everyone so that the contribution points earned will be weighted 1.1 times more than before.\n\nOr, by being more open, make the service more technical and reliable, so that it is seen as contributing to the well-being and other end goals.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nPrice\n\nThe lower the winning rate (the percentage of people who apply who win), the more quota is occupied and the more the beneficiary pays.\n\nThe amount of quota used becomes the price, so if the winning rate increases, the price becomes \"completely free\" with zero quota use of zero yen.\n\nIn capitalism, even for products with high demand and high supply, the price is close to the cost, but the beneficiaries bear the burden.\n\nIn FREEMISM, the overall goods and services are cheaper because they are negatively correlated with the winning rate, regardless of cost.\n\nFurthermore, since the added value cannot be added to the price, the price becomes cheaper by that amount, and the beneficiary burden of freeism is better than the beneficiary burden of capitalism.\n\nThe provider of goods and services receives nothing from the beneficiary for providing goods and services, and the beneficiary pays nothing to the provider for acquiring goods and services, which only increases the amount that occupies the quota managed by the freeism platform.\n\nThe provider can offer goods and services to those who cannot afford to pay for them, and if they do, the business will be compensated.\n\n  \nLife of FREEISM\n\nLet's imagine and write about what society would be like in a society where FREESISM is realized.\n\n&amp;#x200B;\n\nDownload or use any of the web apps, native apps, VR or AR apps of the freeism app (which also serves as the freeism platform) that manages all the functions of freeism, from a web app, native app, VR, or AR app, or a browser.\n\nIn the freeism app, choose the contribution rule to which you want to belong.\n\nEnter various information about yourself and link it to the contribution rule you have chosen, and generate an account with the data for that contribution rule.\n\nYou can put the date of the contribution rule you belong to as a property in your wallet for Web3, or in your Google account for Web2, etc.\n\nWith the account containing that data, data is acquired from various applications and the contribution rules are calculated using that data.\n\nFurthermore, according to each contribution rule, you can display 00 content on Twitter, but not 00. If the content is prohibited by each rule of each contribution level, such as \"00 content is allowed to be displayed on Twitter, but 00 is prohibited,\" each rule of each contribution level will be detected from the account and automatically hidden.\n\nWhen you want to get a product or service on each platform of freeism services or existing services (Amazon, Spotify, etc.), you can get the person's contribution points from the API that can get the contribution points of each person managed by that freeism platform, and Then, we can set up an input field in the platform, such as Amazon, where people can input how many quotas they want to offer, and those who offer the highest amount will be given priority to receive?\n\nIs it faster to create a new one?\n\nThose who offer limited items in the above process will receive contribution points for the contribution rules to which they belong.\n\nThere is no such thing as an exchange rate between multiple contribution points, so the more contribution rules that contribute to the ultimate goal of the freeism platform, the larger the percentage to be offered (tentative idea).\n\nCompatibility between freeism platforms is discussed below.", "author_fullname": "t2_dy46uwx1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "freeism\u30fcCapitalism upwardly compatible\u30fc", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y539db", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665880895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am Japanese, so please forgive me if my grammar may be incorrect.  &lt;/p&gt;\n\n&lt;p&gt;I would like to know how to reward DAOs not on a task basis, but by calculating their contribution through data analysis.&lt;br/&gt;\nIf we can create that method with high quality, I believe we can create a new system of economy that will be upward compatible with capitalism.&lt;br/&gt;\nWe call it freeism.  &lt;/p&gt;\n\n&lt;p&gt;About freeism&lt;/p&gt;\n\n&lt;p&gt;We came up with the &amp;quot;freeism&amp;quot; system as an alternative economic mechanism to capitalism.&lt;/p&gt;\n\n&lt;p&gt;It gives a new alternative to economic mechanisms that have had only limited choices, such as capitalism and socialism.&lt;/p&gt;\n\n&lt;p&gt;I know that many people are already thinking about this kind of thing, but I would like to implement it as a part of the social structure, not as a theoretical theory, and eventually create a society in which the entire economy revolves around freeism instead of capitalism.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Basic Mechanism of Freeism&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The following is an explanation of the basic mechanism of FREELISM.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The supplier is asked to provide all types of goods and services for free, and non-transferable points (SBT?) (called contribution points) are given to those who provide them.&lt;/p&gt;\n\n&lt;p&gt;There will be multiple ways to calculate contribution points, and we will also create a mechanism to provide them.&lt;/p&gt;\n\n&lt;p&gt;The mechanism is described below.&lt;/p&gt;\n\n&lt;p&gt;For products or services that are not available to all who want them (hereinafter referred to as &amp;quot;limited edition products&amp;quot;), the points (hereinafter referred to as &amp;quot;quota&amp;quot;) will be set as the square root of contribution points, and those who offer to use the most quota will be given priority in using them.&lt;/p&gt;\n\n&lt;p&gt;Instead of consuming contribution points as compensation for obtaining goods or services, they should only be accounted for so that they cannot be used to obtain other limited items.&lt;/p&gt;\n\n&lt;p&gt;Therefore, contribution points do not decrease even if a limited item is obtained but accumulate (exceptions apply. See below)&lt;/p&gt;\n\n&lt;p&gt;Necessity&lt;/p&gt;\n\n&lt;p&gt;The necessity of the quota is described below.&lt;/p&gt;\n\n&lt;p&gt;The method of calculating the quota is also described below.&lt;/p&gt;\n\n&lt;p&gt;This type of economic mechanism is called &amp;quot;freeism.&lt;/p&gt;\n\n&lt;p&gt;Freeism is an economic model that can be finally established by using technologies such as data analysis, blockchain, web apps, etc.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Dig a little deeper into the mechanism of FREESISM&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This section explains each of the mechanisms that make up FREESISM.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Contribution rules&lt;/p&gt;\n\n&lt;p&gt;The rules of contribution are the laws that govern life in a freeism society.&lt;/p&gt;\n\n&lt;p&gt;Or the country in which the law is located.&lt;/p&gt;\n\n&lt;p&gt;At first, only one rule exists, but just as there are multiple countries, multiple rules of contribution can be created.&lt;/p&gt;\n\n&lt;p&gt;The contribution rules should be easy for anyone to create.&lt;/p&gt;\n\n&lt;p&gt;I want the rules of contribution to be divided by ideology and taste (only some people emigrate based on taxation, culture, and taste, but for the most part, the current country is determined by birth and upbringing, and there are people with different tastes, values, and ideology in the same country, which creates conflict).&lt;/p&gt;\n\n&lt;p&gt;That is why we create the right of non-interference as the right not to interfere with others.&lt;/p&gt;\n\n&lt;p&gt;Each contribution rule has an end goal.&lt;/p&gt;\n\n&lt;p&gt;The end goal is discussed below.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The FREEISM Platform&lt;/p&gt;\n\n&lt;p&gt;The freeism platform is a mechanism to manage the rules of contribution.&lt;/p&gt;\n\n&lt;p&gt;The rules of contribution play a legal role.&lt;/p&gt;\n\n&lt;p&gt;The freeism platform becomes a higher-level entity like the Constitution OR the UN OR the federal government.&lt;/p&gt;\n\n&lt;p&gt;The freeism platform also has rules, and within the scope of those rules, they get to make the rules of contribution.&lt;/p&gt;\n\n&lt;p&gt;There will be multiple freeism platforms, and people will be able to join any freeism platform they want.&lt;/p&gt;\n\n&lt;p&gt;A freeism platform may also have an end goal.&lt;/p&gt;\n\n&lt;p&gt;The end goal is described below.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Final Goal&lt;/p&gt;\n\n&lt;p&gt;The end goal is the goal that each freeism platform, each contribution rule, and each project (like a company) is aiming for.&lt;/p&gt;\n\n&lt;p&gt;It can be set by vote, decided at the establishment stage, or left to the decision-makers to set.&lt;/p&gt;\n\n&lt;p&gt;Set goals that you want to achieve, not goals or KPIs to achieve subdivided goals.&lt;/p&gt;\n\n&lt;p&gt;For example, &amp;quot;increase in happiness,&amp;quot; &amp;quot;degree of health,&amp;quot; &amp;quot;increase in productivity,&amp;quot; &amp;quot;increase in the number of rational decisions made,&amp;quot; and &amp;quot;crime rate.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;It is unclear if we would set &amp;quot;lower crime rate&amp;quot; as a goal since even &amp;quot;lower crime rate&amp;quot; might be a goal to achieve higher levels of happiness. We want to set a goal that we are ultimately aiming for.&lt;/p&gt;\n\n&lt;p&gt;For FREEISM, we should set a final goal for the FREEISM platform, a final goal for the contribution rule, and a final goal for the project, and then calculate the contributions based on how much they have helped to bring us closer to or achieve that final goal.&lt;/p&gt;\n\n&lt;p&gt;Examples of final goals&lt;/p&gt;\n\n&lt;p&gt;Increase in productivity (working hours required to obtain goods and services) within the limits of the law&lt;/p&gt;\n\n&lt;p&gt;Increase in happiness within the limits of the law&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Contribution points&lt;/p&gt;\n\n&lt;p&gt;Refers to points required to obtain priority for limited items.&lt;/p&gt;\n\n&lt;p&gt;Contribution points are non-transferable.&lt;/p&gt;\n\n&lt;p&gt;However, loans are possible under some contribution rules.&lt;/p&gt;\n\n&lt;p&gt;Can I use SBT as contribution points?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How to Calculate Contribution Points&lt;/p&gt;\n\n&lt;p&gt;Contribution points earned are calculated based on &amp;quot;the degree of contribution to the final goal&amp;quot; \u2716\ufe0e &amp;quot;market principle&amp;quot; \u2716\ufe0e &amp;quot;different weighting/rules for each contribution rule.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Until now, it was &amp;quot;market principles&amp;quot; \u2716\ufe0e &amp;quot;different weighting/rules for each country or region rule&amp;quot;, but we will add &amp;quot;degree of contribution to the final goal&amp;quot; to it.&lt;/p&gt;\n\n&lt;p&gt;I think the market principle is encapsulated in the degree of contribution to the final goal. However, if you still want to make further use of the market principle, you can give contribution points based on &amp;quot;market principle x degree of contribution to the final goal&amp;quot; or &amp;quot;market principle only&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;The degree of contribution to the final goal could be calculated using multiple regression analysis or other data analysis methods or methods to visualize DAO&amp;#39;s task-based contribution.&lt;/p&gt;\n\n&lt;p&gt;See below.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Quota&lt;/p&gt;\n\n&lt;p&gt;The quota is a mechanism to avoid monopolizing the service, and if the service can be provided to everyone, the quota is not used.&lt;/p&gt;\n\n&lt;p&gt;Each contribution rule has a different way of calculating the quota, and each person is given a quota weighted by contribution points (contribution points \u2716\ufe0f0.9 = amount of quota, etc.) or the square root of contribution points.&lt;/p&gt;\n\n&lt;p&gt;The amount of quota used will be the price.&lt;/p&gt;\n\n&lt;p&gt;There is a negative correlation: the lower the winning rate of a product or service, the more quota it occupies, and the closer the winning rate is to 100%, the less quota it occupies.&lt;/p&gt;\n\n&lt;p&gt;In FREESISM, the quota is always used while the limited item is in use, and the amount used always reflects the market value, not the amount used when it was acquired.&lt;/p&gt;\n\n&lt;p&gt;The structure of the quota differs for each contribution rule.&lt;/p&gt;\n\n&lt;p&gt;The period you occupy a quota should be the period you own that limited edition item.&lt;/p&gt;\n\n&lt;p&gt;If you give it away or throw it away, you can apply for it on the FREEMISM platform, or the service will automatically detect it and release the use of the quota.&lt;/p&gt;\n\n&lt;p&gt;To prevent &amp;quot;false applications&amp;quot; where people apply to give it away when they own it, make it necessary to give it away or throw it away at a predetermined location or app.&lt;/p&gt;\n\n&lt;p&gt;Create an incentive to have it detected through the app, since it should be detected from each app and if not through the app, it will occupy the frame all the time.&lt;/p&gt;\n\n&lt;p&gt;Make it impossible for people to report that they have given it up on their own, other than by having the app automatically detect it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;FREEISM&lt;/p&gt;\n\n&lt;p&gt;In freeism, if other people refer to the products, services, source code, or other deliverables developed by each person, or to know-how and other things such as research findings, they will receive only a portion of the contribution points earned by the person they refer to, such as 0.5 times the contribution points earned by the person they refer to.&lt;/p&gt;\n\n&lt;p&gt;Thereby.&lt;/p&gt;\n\n&lt;p&gt;If someone only submits an idea and others implement it, the person who submitted the idea will also receive a portion of the contribution points earned by the person who implemented it.&lt;/p&gt;\n\n&lt;p&gt;If another company started a business and failed but was successful in running it by avoiding the strategies of the failed company, then allow the people who worked in the failed company to receive a portion of the contribution points earned by the successful company.&lt;/p&gt;\n\n&lt;p&gt;That way, where it was 0-1 to succeed or fail, maybe we could make it so that even if it fails, it is not 0-1 financially, but there is some benefit.&lt;/p&gt;\n\n&lt;p&gt;Since points are just points, unlike the currency, they don&amp;#39;t have to be distributed and can be offered to both.&lt;/p&gt;\n\n&lt;p&gt;But it&amp;#39;s hard to determine if that&amp;#39;s what I was referring to.&lt;/p&gt;\n\n&lt;p&gt;I will discuss this later.&lt;/p&gt;\n\n&lt;p&gt;We want a society where everything is open source.&lt;/p&gt;\n\n&lt;p&gt;I want to create a society where every no-how, patent, and other thing is open source, by weighting the contribution points that can be earned if others use the no-how, and by weighting the contribution points that can be earned just by making it open source, such as 1.2 times the contribution points that can be earned by making it open source.&lt;/p&gt;\n\n&lt;p&gt;Everyone gets it without having to distribute, and being open source could lead to more compensation for sharing without monopolizing rather than monopolizing and competing.&lt;/p&gt;\n\n&lt;p&gt;But if the relative position without distribution determines whether you get a limited product, is it the same as when profits were shared?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s more mentally stable because you don&amp;#39;t have to fight for a piece of the pie.&lt;/p&gt;\n\n&lt;p&gt;The basic idea is to think in terms of rules of contribution, where the only rule is the degree of contribution to the liberal end goal, but when considering various rules of contribution, we can make a law that regulates each of them in a way that is characteristic to the sound of each rule of contribution.&lt;/p&gt;\n\n&lt;p&gt;Even if any action is the wrong action or not, we will stipulate that only actions that have a bad impact on the final goal contribution level are bad.&lt;/p&gt;\n\n&lt;p&gt;Even theft would not be a crime if it did not negatively impact anyone.&lt;/p&gt;\n\n&lt;p&gt;Somewhere the idea is like a woman who says it&amp;#39;s not cheating if she doesn&amp;#39;t get caught in a love story about whether or not she qualifies as cheating.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The position of each&lt;/p&gt;\n\n&lt;p&gt;In freeism, the system is divided into a freeism platform, contribution rules, projects, and other mechanisms.&lt;/p&gt;\n\n&lt;p&gt;The freeism platform is the role of the federal government, the contribution rule is the role of the state government, and the project is the role of the company.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Project&lt;/p&gt;\n\n&lt;p&gt;The role of the company.&lt;/p&gt;\n\n&lt;p&gt;The idea of freeism is to make everything open, so all the data of companies that are currently hidden will be shared and made available to everyone so that the contribution points earned will be weighted 1.1 times more than before.&lt;/p&gt;\n\n&lt;p&gt;Or, by being more open, make the service more technical and reliable, so that it is seen as contributing to the well-being and other end goals.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Price&lt;/p&gt;\n\n&lt;p&gt;The lower the winning rate (the percentage of people who apply who win), the more quota is occupied and the more the beneficiary pays.&lt;/p&gt;\n\n&lt;p&gt;The amount of quota used becomes the price, so if the winning rate increases, the price becomes &amp;quot;completely free&amp;quot; with zero quota use of zero yen.&lt;/p&gt;\n\n&lt;p&gt;In capitalism, even for products with high demand and high supply, the price is close to the cost, but the beneficiaries bear the burden.&lt;/p&gt;\n\n&lt;p&gt;In FREEMISM, the overall goods and services are cheaper because they are negatively correlated with the winning rate, regardless of cost.&lt;/p&gt;\n\n&lt;p&gt;Furthermore, since the added value cannot be added to the price, the price becomes cheaper by that amount, and the beneficiary burden of freeism is better than the beneficiary burden of capitalism.&lt;/p&gt;\n\n&lt;p&gt;The provider of goods and services receives nothing from the beneficiary for providing goods and services, and the beneficiary pays nothing to the provider for acquiring goods and services, which only increases the amount that occupies the quota managed by the freeism platform.&lt;/p&gt;\n\n&lt;p&gt;The provider can offer goods and services to those who cannot afford to pay for them, and if they do, the business will be compensated.&lt;/p&gt;\n\n&lt;p&gt;Life of FREEISM&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s imagine and write about what society would be like in a society where FREESISM is realized.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Download or use any of the web apps, native apps, VR or AR apps of the freeism app (which also serves as the freeism platform) that manages all the functions of freeism, from a web app, native app, VR, or AR app, or a browser.&lt;/p&gt;\n\n&lt;p&gt;In the freeism app, choose the contribution rule to which you want to belong.&lt;/p&gt;\n\n&lt;p&gt;Enter various information about yourself and link it to the contribution rule you have chosen, and generate an account with the data for that contribution rule.&lt;/p&gt;\n\n&lt;p&gt;You can put the date of the contribution rule you belong to as a property in your wallet for Web3, or in your Google account for Web2, etc.&lt;/p&gt;\n\n&lt;p&gt;With the account containing that data, data is acquired from various applications and the contribution rules are calculated using that data.&lt;/p&gt;\n\n&lt;p&gt;Furthermore, according to each contribution rule, you can display 00 content on Twitter, but not 00. If the content is prohibited by each rule of each contribution level, such as &amp;quot;00 content is allowed to be displayed on Twitter, but 00 is prohibited,&amp;quot; each rule of each contribution level will be detected from the account and automatically hidden.&lt;/p&gt;\n\n&lt;p&gt;When you want to get a product or service on each platform of freeism services or existing services (Amazon, Spotify, etc.), you can get the person&amp;#39;s contribution points from the API that can get the contribution points of each person managed by that freeism platform, and Then, we can set up an input field in the platform, such as Amazon, where people can input how many quotas they want to offer, and those who offer the highest amount will be given priority to receive?&lt;/p&gt;\n\n&lt;p&gt;Is it faster to create a new one?&lt;/p&gt;\n\n&lt;p&gt;Those who offer limited items in the above process will receive contribution points for the contribution rules to which they belong.&lt;/p&gt;\n\n&lt;p&gt;There is no such thing as an exchange rate between multiple contribution points, so the more contribution rules that contribute to the ultimate goal of the freeism platform, the larger the percentage to be offered (tentative idea).&lt;/p&gt;\n\n&lt;p&gt;Compatibility between freeism platforms is discussed below.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y539db", "is_robot_indexable": true, "report_reasons": null, "author": "Traditional-Self1986", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y539db/freeism\u30fccapitalism_upwardly_compatible\u30fc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y539db/freeism\u30fccapitalism_upwardly_compatible\u30fc/", "subreddit_subscribers": 813860, "created_utc": 1665880895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been lurking on this sub for a while now and all too often I see posts from people claiming they feel inadequate and then they go on to describe their stupid impressive background and experience. That's great and all but I'd like to move the spotlight to the rest of us for just a minute. Cheers to my fellow mediocre data scientists who don't work at FAANG companies, aren't pursing a PhD, don't publish papers, haven't won Kaggle competitions, and don't spend every waking hour improving their portfolio. Even though we're nothing special, we still deserve some appreciation every once in a while.\n\n/rant I'll hand it back over to the smart people now", "author_fullname": "t2_rv2brc3u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shout Out to All the Mediocre Data Scientists Out There", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y4zh44", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.34, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665870141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been lurking on this sub for a while now and all too often I see posts from people claiming they feel inadequate and then they go on to describe their stupid impressive background and experience. That&amp;#39;s great and all but I&amp;#39;d like to move the spotlight to the rest of us for just a minute. Cheers to my fellow mediocre data scientists who don&amp;#39;t work at FAANG companies, aren&amp;#39;t pursing a PhD, don&amp;#39;t publish papers, haven&amp;#39;t won Kaggle competitions, and don&amp;#39;t spend every waking hour improving their portfolio. Even though we&amp;#39;re nothing special, we still deserve some appreciation every once in a while.&lt;/p&gt;\n\n&lt;p&gt;/rant I&amp;#39;ll hand it back over to the smart people now&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y4zh44", "is_robot_indexable": true, "report_reasons": null, "author": "Dull_Adagio4038", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y4zh44/shout_out_to_all_the_mediocre_data_scientists_out/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y4zh44/shout_out_to_all_the_mediocre_data_scientists_out/", "subreddit_subscribers": 813860, "created_utc": 1665870141.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}