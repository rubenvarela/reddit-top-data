{"kind": "Listing", "data": {"after": null, "dist": 18, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_cvcebwn5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My mother's Mac, supposedly none of these files can get deleted nor any of her 52k emails dating back to 2006...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_ycre5f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 186, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Editable Flair", "can_mod_post": false, "score": 186, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IfPG5dRClufVWhYPx-IeMzlWy79TyQ5uKbiPXsM5tFY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666658432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/kims7t673wv91.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/kims7t673wv91.jpg?auto=webp&amp;s=fc729010e4ade603795cdcc54e8df18fed1d7eb2", "width": 4000, "height": 2252}, "resolutions": [{"url": "https://preview.redd.it/kims7t673wv91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fa0a621f613372e40c86119f18172b31f51c5cb", "width": 108, "height": 60}, {"url": "https://preview.redd.it/kims7t673wv91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd05ad44e1467c6fcfc8a62e4b5b1df3cd1b5d94", "width": 216, "height": 121}, {"url": "https://preview.redd.it/kims7t673wv91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=63f5f6a3effd5e53cf26e35627299212c429d486", "width": 320, "height": 180}, {"url": "https://preview.redd.it/kims7t673wv91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9483c4887b883363baa1560d973efd859b730114", "width": 640, "height": 360}, {"url": "https://preview.redd.it/kims7t673wv91.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=44e780b38e9a20510ae06237d9c37ec5cd4af67e", "width": 960, "height": 540}, {"url": "https://preview.redd.it/kims7t673wv91.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=88ab3dff171d2989e0f057e10169c9ee67e2124c", "width": 1080, "height": 608}], "variants": {}, "id": "0qw10LpRdhNBwpxSexbj0DlEbrEE_q2wrFGqyPn3l0s"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "ycre5f", "is_robot_indexable": true, "report_reasons": null, "author": "Sons-Father", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ycre5f/my_mothers_mac_supposedly_none_of_these_files_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/kims7t673wv91.jpg", "subreddit_subscribers": 649001, "created_utc": 1666658432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If I store data on an SSD and leave it unplugged in my room is there any risk of data loss or corruption over time?\n\nWhat if it's a hard drive? If I leave a hard drive unplugged is there any chance of data loss over time?", "author_fullname": "t2_5gyro00k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If an SSD sits, unplugged and unpowered, is there any risk if data loss or corruption over time I'd the SSD is left in normal household conditions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ydfx39", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666732353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I store data on an SSD and leave it unplugged in my room is there any risk of data loss or corruption over time?&lt;/p&gt;\n\n&lt;p&gt;What if it&amp;#39;s a hard drive? If I leave a hard drive unplugged is there any chance of data loss over time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ydfx39", "is_robot_indexable": true, "report_reasons": null, "author": "ALT703", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ydfx39/if_an_ssd_sits_unplugged_and_unpowered_is_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ydfx39/if_an_ssd_sits_unplugged_and_unpowered_is_there/", "subreddit_subscribers": 649001, "created_utc": 1666732353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for a good capture card with S-video input.  Initially I was thinking about the AVerMedia CE310B due to its simplicity.  However through a lot of reading I ended up finding out about the Blackmagic Design Intensity Pro 4k, which instead uses breakout cables attached to MORE breakout cables to capture S-Video.\n\nThe Intensity Pro does seem somewhat more capable than the CE310B, but I am worried that introducing so many adapters is going to degrade the signal quality.  After all, the breakout cables are using RCA connectors, which S-Video was deliberately designed to circumvent.  I'm just a bit skeptical and was hoping someone could shed some light on whether or not that would be an issue, and of course I am also open to other capture card recommendations! \n\nThanks! (:", "author_fullname": "t2_648gucfq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For video capture via S-Video...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ydb3kg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666720240.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a good capture card with S-video input.  Initially I was thinking about the AVerMedia CE310B due to its simplicity.  However through a lot of reading I ended up finding out about the Blackmagic Design Intensity Pro 4k, which instead uses breakout cables attached to MORE breakout cables to capture S-Video.&lt;/p&gt;\n\n&lt;p&gt;The Intensity Pro does seem somewhat more capable than the CE310B, but I am worried that introducing so many adapters is going to degrade the signal quality.  After all, the breakout cables are using RCA connectors, which S-Video was deliberately designed to circumvent.  I&amp;#39;m just a bit skeptical and was hoping someone could shed some light on whether or not that would be an issue, and of course I am also open to other capture card recommendations! &lt;/p&gt;\n\n&lt;p&gt;Thanks! (:&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ydb3kg", "is_robot_indexable": true, "report_reasons": null, "author": "PupidStunk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ydb3kg/for_video_capture_via_svideo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ydb3kg/for_video_capture_via_svideo/", "subreddit_subscribers": 649001, "created_utc": 1666720240.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi fellow Data Hoarders. I received this notification from Google today regarding the Google Chat upgrade. I was wondering if any of you know anything about the update, or if anyone has any tricks to exporting messages. Currently, I'm saving threads as PDFs, but it is tedious. I did backup my messages via Takeout, but I was trying to find a more visual, easy-to-access format for the threads (they export as JSON). Any tips? Feeling rather anxious about this data potentially being lost in a week. Thank you so much!\n\nHere is the message from Google:\n\n&gt;Earlier this year, we [announced ](https://notifications.google.com/g/p/APNL1ThA4M_vWch1qWll4fTmKgJRnRegSo7OSz2KcsNTQLvhCSiPyTRXdK-8iw3KiW1RPOmwKAGKgrol_6bu_oS0t4JvYPb2Sbb9QLflw6sYgELh44Z-bkJd_BfLrAhyhxw4tcYWuw181G3u44Wwbg)that all Hangouts users would be upgraded to Google Chat. As the last step of the upgrade, starting November 1, 2022, [Hangouts on the web](https://notifications.google.com/g/p/APNL1Thb3RsqVFS-sM9CmdgGC3N8oGg-htICf5V4q8Tcb_2aqvtyZ7h-Rxax3E71lzbykFhECG40p_rNYD6a7PB0rxcpCTvcgcfDuNaObtpUc_Iy2ZmOcjlsF6Cdd68y) will be upgraded to [Chat on the web](https://notifications.google.com/g/p/APNL1Ti6HZFHNDzqhij1Fe13cHuOxPxCr9JUpy9jIjMxMiOZBuF6hJWCcF-LUWFJU8QrxDn1brFHwQBA5QMM7vcbvZqXXo8G7jbagZWh7GXgUP1AblQ). **We are emailing you because some of your conversations or portions of conversations won\u2019t automatically migrate from Hangouts to Chat.**  \n&gt;  \n&gt;If you wish to keep your Hangouts conversations, we strongly encourage you to use [Google Takeout](https://notifications.google.com/g/p/APNL1TisPZz7XIlb7tAIQX_QXIwCrv65DybNAj5arpVsg7NEPwe0tfWANP7JW_4SaNKr0i01B2O-jRv4PN4YMNOexIsnSMSQIx6Kmd0WI7Kgj1lx-xZOG3h2) and download your data before January 1, 2023 when Hangouts data will be deleted.  \n&gt;  \n&gt;Rest assured that [Hangouts on the web](https://notifications.google.com/g/p/APNL1TjYEKE78LCfrZwZ1Lbp18QtsarZKcTLR2qkqIcshPr-eZSHWd7ZM84aBAt7L_LEmEHUsveKZkvHaq4G_t3W1CNiUpRyy0YXD-KepopZIzgHEaAM3Wjhs7GoJ-kH) and your conversations there will remain accessible in Hangouts until November 1, 2022.  \n&gt;  \n&gt;**Why are users being upgraded?**  \n&gt;  \n&gt;Google Chat offers closer integration with other Google Workspace products and modern features like a gif picker and [Spaces](https://notifications.google.com/g/p/APNL1TiuMR8VVm_1GYnlOSHfewBeOSHjKjAiUzbK32fBc2zDni5kASjxpWLRPkdW6jDTlv0StAae5T6y3gGZJLK5FLRimp0o4gQ4q58dk0qQzPzbgKoP67bJ06dtptZJrh4grt8z1ghti0-lsDg3fStmaLzr0137WiLRGJfMBuekc3Ycek6fSfAce8vkklLm), a dedicated place for topic-based collaboration. In-product notifications about this change have been visible in Hangouts since last year and many users have already made the upgrade to Google Chat. For more information about the upgrade to Google Chat, please visit the [Help Center](https://notifications.google.com/g/p/APNL1TiP_9YEpXQNr0hMzH_qUfk4HfraNdW3qQfS65fGGqGLIFmMXOFyH9aTj5febnloKAhzsQ8tVITISzdze74xO5yNuTEkvH82vS0pdKvRsInERTrNKLWEsVYWgbsnYDWxZp9eCSY).  \n&gt;  \n&gt;**How do I download my data?**  \n&gt;  \n&gt;Starting November 1, 2022, [Hangouts on the web](https://notifications.google.com/g/p/APNL1TgSZFwxTC0cOlOdCumDa_m-hB0JtuVeNegI9iX0COGcVo7vz9kHsGi5Iza9czU7dwhpbFWu6vSAqNf6GRiXIcA9UPW6E_eflqUpFNl2oEzTpu-FosiDAnXRHmHX) will no longer be available. If you wish to keep your Hangouts data, we strongly encourage you to use [Google Takeout](https://notifications.google.com/g/p/APNL1Tip-ZlvFOnNmbHOnBWe-kw3RIWq_fKN1KF3eZxnbH9ziFC7y29k-rlymuq4JSIJpLmLgLN_ya41-9W4wjpWhnA_Piy1HzBk0KYngKr1hCjdm2ItPVV1) and download a copy before January 1, 2023. You can find step by step instructions for Takeout [here](https://notifications.google.com/g/p/APNL1TjZiBBguBncizt9266j3_X2Lx71oYkufuybhN1S-9eN9uM7Pub7YL0S8JG4imSl8UVRwQ8p9NiaT8W9F6KVRpCfOsxYYJhQhPC96A6GazDLnvKPapzACp3oSZmSoA).  \n&gt;  \n&gt;**What happens if I delete a conversation in Hangouts?**  \n&gt;  \n&gt;Deleting conversations from Google Hangouts may delete the corresponding content in Google Chat, so don't delete any conversations you wish to keep.  \n&gt;  \n&gt;**What if I don\u2019t want to move to Chat?**  \n&gt;  \n&gt;If you would prefer not to move to Chat, you can stop using it at any time. You can also [delete conversations](https://notifications.google.com/g/p/APNL1TglGB7dK3t7Jq6wHOP0wKn5zN6uFCH2wYW47RWH19hq0am3AVSMz_735eCiX7jjdRPpcccWD9h97Kx4tHEUOcp6hqJI9SXfefs7RmBgn9fDMYiC-XEQMEkQpHuayPl0SPJd6OIwF7jTxkhpg6A) in Chat or [delete your Google](https://notifications.google.com/g/p/APNL1TjKZlI8Mjo8QRGCIWkyhpfkeXzi2pkcKXJDW68zKk_68HgssAtBaHPbbcPUMH0zFbsrd_lcOkao_Mgsxl2Tt2TYCA3aNFq8BdXz19BhlYtApIbQIPE5f17Pj6c6UrZ4JpI-AoesqP832KSZG9CUD-tiu3U_TQ) account entirely. Please note that deleting your Google account will result in deletion of all the data and content in that account, like emails, files, calendars, photos, etc.", "author_fullname": "t2_1wf3il78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google's Hangouts to Chat Migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ycynyj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1666682779.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow Data Hoarders. I received this notification from Google today regarding the Google Chat upgrade. I was wondering if any of you know anything about the update, or if anyone has any tricks to exporting messages. Currently, I&amp;#39;m saving threads as PDFs, but it is tedious. I did backup my messages via Takeout, but I was trying to find a more visual, easy-to-access format for the threads (they export as JSON). Any tips? Feeling rather anxious about this data potentially being lost in a week. Thank you so much!&lt;/p&gt;\n\n&lt;p&gt;Here is the message from Google:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Earlier this year, we &lt;a href=\"https://notifications.google.com/g/p/APNL1ThA4M_vWch1qWll4fTmKgJRnRegSo7OSz2KcsNTQLvhCSiPyTRXdK-8iw3KiW1RPOmwKAGKgrol_6bu_oS0t4JvYPb2Sbb9QLflw6sYgELh44Z-bkJd_BfLrAhyhxw4tcYWuw181G3u44Wwbg\"&gt;announced &lt;/a&gt;that all Hangouts users would be upgraded to Google Chat. As the last step of the upgrade, starting November 1, 2022, &lt;a href=\"https://notifications.google.com/g/p/APNL1Thb3RsqVFS-sM9CmdgGC3N8oGg-htICf5V4q8Tcb_2aqvtyZ7h-Rxax3E71lzbykFhECG40p_rNYD6a7PB0rxcpCTvcgcfDuNaObtpUc_Iy2ZmOcjlsF6Cdd68y\"&gt;Hangouts on the web&lt;/a&gt; will be upgraded to &lt;a href=\"https://notifications.google.com/g/p/APNL1Ti6HZFHNDzqhij1Fe13cHuOxPxCr9JUpy9jIjMxMiOZBuF6hJWCcF-LUWFJU8QrxDn1brFHwQBA5QMM7vcbvZqXXo8G7jbagZWh7GXgUP1AblQ\"&gt;Chat on the web&lt;/a&gt;. &lt;strong&gt;We are emailing you because some of your conversations or portions of conversations won\u2019t automatically migrate from Hangouts to Chat.&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;If you wish to keep your Hangouts conversations, we strongly encourage you to use &lt;a href=\"https://notifications.google.com/g/p/APNL1TisPZz7XIlb7tAIQX_QXIwCrv65DybNAj5arpVsg7NEPwe0tfWANP7JW_4SaNKr0i01B2O-jRv4PN4YMNOexIsnSMSQIx6Kmd0WI7Kgj1lx-xZOG3h2\"&gt;Google Takeout&lt;/a&gt; and download your data before January 1, 2023 when Hangouts data will be deleted.  &lt;/p&gt;\n\n&lt;p&gt;Rest assured that &lt;a href=\"https://notifications.google.com/g/p/APNL1TjYEKE78LCfrZwZ1Lbp18QtsarZKcTLR2qkqIcshPr-eZSHWd7ZM84aBAt7L_LEmEHUsveKZkvHaq4G_t3W1CNiUpRyy0YXD-KepopZIzgHEaAM3Wjhs7GoJ-kH\"&gt;Hangouts on the web&lt;/a&gt; and your conversations there will remain accessible in Hangouts until November 1, 2022.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why are users being upgraded?&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;Google Chat offers closer integration with other Google Workspace products and modern features like a gif picker and &lt;a href=\"https://notifications.google.com/g/p/APNL1TiuMR8VVm_1GYnlOSHfewBeOSHjKjAiUzbK32fBc2zDni5kASjxpWLRPkdW6jDTlv0StAae5T6y3gGZJLK5FLRimp0o4gQ4q58dk0qQzPzbgKoP67bJ06dtptZJrh4grt8z1ghti0-lsDg3fStmaLzr0137WiLRGJfMBuekc3Ycek6fSfAce8vkklLm\"&gt;Spaces&lt;/a&gt;, a dedicated place for topic-based collaboration. In-product notifications about this change have been visible in Hangouts since last year and many users have already made the upgrade to Google Chat. For more information about the upgrade to Google Chat, please visit the &lt;a href=\"https://notifications.google.com/g/p/APNL1TiP_9YEpXQNr0hMzH_qUfk4HfraNdW3qQfS65fGGqGLIFmMXOFyH9aTj5febnloKAhzsQ8tVITISzdze74xO5yNuTEkvH82vS0pdKvRsInERTrNKLWEsVYWgbsnYDWxZp9eCSY\"&gt;Help Center&lt;/a&gt;.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How do I download my data?&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;Starting November 1, 2022, &lt;a href=\"https://notifications.google.com/g/p/APNL1TgSZFwxTC0cOlOdCumDa_m-hB0JtuVeNegI9iX0COGcVo7vz9kHsGi5Iza9czU7dwhpbFWu6vSAqNf6GRiXIcA9UPW6E_eflqUpFNl2oEzTpu-FosiDAnXRHmHX\"&gt;Hangouts on the web&lt;/a&gt; will no longer be available. If you wish to keep your Hangouts data, we strongly encourage you to use &lt;a href=\"https://notifications.google.com/g/p/APNL1Tip-ZlvFOnNmbHOnBWe-kw3RIWq_fKN1KF3eZxnbH9ziFC7y29k-rlymuq4JSIJpLmLgLN_ya41-9W4wjpWhnA_Piy1HzBk0KYngKr1hCjdm2ItPVV1\"&gt;Google Takeout&lt;/a&gt; and download a copy before January 1, 2023. You can find step by step instructions for Takeout &lt;a href=\"https://notifications.google.com/g/p/APNL1TjZiBBguBncizt9266j3_X2Lx71oYkufuybhN1S-9eN9uM7Pub7YL0S8JG4imSl8UVRwQ8p9NiaT8W9F6KVRpCfOsxYYJhQhPC96A6GazDLnvKPapzACp3oSZmSoA\"&gt;here&lt;/a&gt;.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What happens if I delete a conversation in Hangouts?&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;Deleting conversations from Google Hangouts may delete the corresponding content in Google Chat, so don&amp;#39;t delete any conversations you wish to keep.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What if I don\u2019t want to move to Chat?&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;If you would prefer not to move to Chat, you can stop using it at any time. You can also &lt;a href=\"https://notifications.google.com/g/p/APNL1TglGB7dK3t7Jq6wHOP0wKn5zN6uFCH2wYW47RWH19hq0am3AVSMz_735eCiX7jjdRPpcccWD9h97Kx4tHEUOcp6hqJI9SXfefs7RmBgn9fDMYiC-XEQMEkQpHuayPl0SPJd6OIwF7jTxkhpg6A\"&gt;delete conversations&lt;/a&gt; in Chat or &lt;a href=\"https://notifications.google.com/g/p/APNL1TjKZlI8Mjo8QRGCIWkyhpfkeXzi2pkcKXJDW68zKk_68HgssAtBaHPbbcPUMH0zFbsrd_lcOkao_Mgsxl2Tt2TYCA3aNFq8BdXz19BhlYtApIbQIPE5f17Pj6c6UrZ4JpI-AoesqP832KSZG9CUD-tiu3U_TQ\"&gt;delete your Google&lt;/a&gt; account entirely. Please note that deleting your Google account will result in deletion of all the data and content in that account, like emails, files, calendars, photos, etc.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bSn79kk_Bwv9XbKMPZsmS7J1qyqRjr8jpgpXcxV96_c.jpg?auto=webp&amp;s=3266818b2d24d8b7a157725a1b188ad7f70f2e84", "width": 1300, "height": 733}, "resolutions": [{"url": "https://external-preview.redd.it/bSn79kk_Bwv9XbKMPZsmS7J1qyqRjr8jpgpXcxV96_c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e185cf4c423f555525dd91ab3a2d436f1458e41d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/bSn79kk_Bwv9XbKMPZsmS7J1qyqRjr8jpgpXcxV96_c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5c0baba2b29724b57000f6f688aa4b9f9d894ec3", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/bSn79kk_Bwv9XbKMPZsmS7J1qyqRjr8jpgpXcxV96_c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=03ff0c320bfb510f4f103f32b3dc87d3118ce086", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/bSn79kk_Bwv9XbKMPZsmS7J1qyqRjr8jpgpXcxV96_c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1c63ec6a5664b2b7b8ac3a807cb3bc757d805457", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/bSn79kk_Bwv9XbKMPZsmS7J1qyqRjr8jpgpXcxV96_c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=22d08ec056aa1b6f55ed2b88f66e79182efbd266", "width": 960, "height": 541}, {"url": "https://external-preview.redd.it/bSn79kk_Bwv9XbKMPZsmS7J1qyqRjr8jpgpXcxV96_c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=639ea4a0f35b43eb3f1e84196b2ffd2a42203a9d", "width": 1080, "height": 608}], "variants": {}, "id": "LG-eVHCFVdy9GSDhonuLiDAezBbB2SXGbcmElKypliQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "I'll gigaBITE you", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ycynyj", "is_robot_indexable": true, "report_reasons": null, "author": "YeahILikeGirls", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/ycynyj/googles_hangouts_to_chat_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ycynyj/googles_hangouts_to_chat_migration/", "subreddit_subscribers": 649001, "created_utc": 1666682779.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to use Blu-rays to cold store some files I don't need constant access to, and want to utilize every last byte of the discs, so I'm using 7z archives. However, my initial expectation was that I would be able to open the parts separately, and pull files that are only in that part, so if I need something that's on just one or two discs, I don't need to rip all ten discs. Is this possible? Is it not possible with 7z but is possible with another archive format? And will that archive format split files across 2 parts allowing exact byte amount parts, or will it never split files across multiple archives to retain independent part access? (I'm okay with splitting files to maximize space, just not having to rip 10 discs to access the contents of 2)", "author_fullname": "t2_g457f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Possible to create individually openable 7z parts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yd807q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666712717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to use Blu-rays to cold store some files I don&amp;#39;t need constant access to, and want to utilize every last byte of the discs, so I&amp;#39;m using 7z archives. However, my initial expectation was that I would be able to open the parts separately, and pull files that are only in that part, so if I need something that&amp;#39;s on just one or two discs, I don&amp;#39;t need to rip all ten discs. Is this possible? Is it not possible with 7z but is possible with another archive format? And will that archive format split files across 2 parts allowing exact byte amount parts, or will it never split files across multiple archives to retain independent part access? (I&amp;#39;m okay with splitting files to maximize space, just not having to rip 10 discs to access the contents of 2)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yd807q", "is_robot_indexable": true, "report_reasons": null, "author": "wright96d", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yd807q/possible_to_create_individually_openable_7z_parts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yd807q/possible_to_create_individually_openable_7z_parts/", "subreddit_subscribers": 649001, "created_utc": 1666712717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a small 1U 19\" appliance with this mainboard: [http://n.hanzsung.com/m/prod\\_view.aspx?TypeId=87&amp;Id=204&amp;Fid=t3:87:3&amp;typefid=87](http://n.hanzsung.com/m/prod_view.aspx?TypeId=87&amp;Id=204&amp;Fid=t3:87:3&amp;typefid=87)\n\nI am aware it's not the perfect machine for a file server but I'd still like to use it as personal NAS. Sadly it does not have USB3.0, only 2.0 and it has a PCIe slot as well as an mSATA slot (which currently holds a 128GB SSD).\n\nMirror RAID is absolutely mandatory for me so I am looking for a way to connect two harddrives (2x14TB).\n\nIs there a PCIe (or better, mSATA) to eSATA board I could use?", "author_fullname": "t2_219qd4kh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mini PCIe or mSATA (preferred) to 2x (e)SATA?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ycxcyi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666677579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a small 1U 19&amp;quot; appliance with this mainboard: &lt;a href=\"http://n.hanzsung.com/m/prod_view.aspx?TypeId=87&amp;amp;Id=204&amp;amp;Fid=t3:87:3&amp;amp;typefid=87\"&gt;http://n.hanzsung.com/m/prod_view.aspx?TypeId=87&amp;amp;Id=204&amp;amp;Fid=t3:87:3&amp;amp;typefid=87&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I am aware it&amp;#39;s not the perfect machine for a file server but I&amp;#39;d still like to use it as personal NAS. Sadly it does not have USB3.0, only 2.0 and it has a PCIe slot as well as an mSATA slot (which currently holds a 128GB SSD).&lt;/p&gt;\n\n&lt;p&gt;Mirror RAID is absolutely mandatory for me so I am looking for a way to connect two harddrives (2x14TB).&lt;/p&gt;\n\n&lt;p&gt;Is there a PCIe (or better, mSATA) to eSATA board I could use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "34TB (raw), ZFS, 3 pools, Linux", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ycxcyi", "is_robot_indexable": true, "report_reasons": null, "author": "segdy", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/ycxcyi/mini_pcie_or_msata_preferred_to_2x_esata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ycxcyi/mini_pcie_or_msata_preferred_to_2x_esata/", "subreddit_subscribers": 649001, "created_utc": 1666677579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "YouTube's changed their style a bunch yesterday, and it doesn't look that good. Does anyone have a backup of the old CSS file that I can use?", "author_fullname": "t2_pne6k507", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have a backup of Youtube's CSS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ydff3n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666731111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;YouTube&amp;#39;s changed their style a bunch yesterday, and it doesn&amp;#39;t look that good. Does anyone have a backup of the old CSS file that I can use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ydff3n", "is_robot_indexable": true, "report_reasons": null, "author": "Username8457", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ydff3n/does_anyone_have_a_backup_of_youtubes_css/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ydff3n/does_anyone_have_a_backup_of_youtubes_css/", "subreddit_subscribers": 649001, "created_utc": 1666731111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Most SMART test softwares usually give weird temperature reports about lots of HDD's and SSD'S,but at the same time they give a different value that makes sense up there,\n\nSo what the hell is going on ?", "author_fullname": "t2_5p9rx19x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "why most SMART test softwares give a weird temperature value ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_ydb2mx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/6onEX14-_H02wgeEeYKKBhNzgJ4Yz-sSJmWRPvFB8Z8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666720173.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most SMART test softwares usually give weird temperature reports about lots of HDD&amp;#39;s and SSD&amp;#39;S,but at the same time they give a different value that makes sense up there,&lt;/p&gt;\n\n&lt;p&gt;So what the hell is going on ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/p1qmwhfr61w91.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/p1qmwhfr61w91.jpg?auto=webp&amp;s=e6a2b16114796b5b2998f1b43aba3eb5d556ca8b", "width": 2991, "height": 3821}, "resolutions": [{"url": "https://preview.redd.it/p1qmwhfr61w91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a1b43877c8e8468950c96828955a47f76ca3fbe9", "width": 108, "height": 137}, {"url": "https://preview.redd.it/p1qmwhfr61w91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cfc18cab5cb36c9a528995900016a6048516b880", "width": 216, "height": 275}, {"url": "https://preview.redd.it/p1qmwhfr61w91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1103a6f3543f2a0a1a47572bbf137b7ee26b63ca", "width": 320, "height": 408}, {"url": "https://preview.redd.it/p1qmwhfr61w91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=46418eaa51c8c9a70a07bd98fd2ac9f33345e5e9", "width": 640, "height": 817}, {"url": "https://preview.redd.it/p1qmwhfr61w91.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=881a10b9ed3b3ad03729371926c018beba9a7d50", "width": 960, "height": 1226}, {"url": "https://preview.redd.it/p1qmwhfr61w91.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=66b81d35bb1eca9d0a0bf33ef267f8fab849e24d", "width": 1080, "height": 1379}], "variants": {}, "id": "SiHSwwmNKl70V9q4k39Ni9C0mBVnpl1_0BIA0W2we7c"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ydb2mx", "is_robot_indexable": true, "report_reasons": null, "author": "VladamirLem9781", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ydb2mx/why_most_smart_test_softwares_give_a_weird/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/p1qmwhfr61w91.jpg", "subreddit_subscribers": 649001, "created_utc": 1666720173.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "And all comments on every post?", "author_fullname": "t2_8y02zre8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download all posts in a facebook group?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ycwcmx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Editable Flair", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666673901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;And all comments on every post?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "ycwcmx", "is_robot_indexable": true, "report_reasons": null, "author": "OkAir5443", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ycwcmx/how_to_download_all_posts_in_a_facebook_group/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ycwcmx/how_to_download_all_posts_in_a_facebook_group/", "subreddit_subscribers": 649001, "created_utc": 1666673901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\ndoes anyone have any information on how to restore the folder tree from a NAS Drive?\n\nThe NAS was a Buffalo Linkstation. I died some time ago but the HDD is fine. \n\nI tried different programs but even testdisk couldnt solve it.\n\nMost important to me would be the folder structure, the files not that much (backup but unsorted).", "author_fullname": "t2_1imuno", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS xfs restore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ydadce", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666718490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;does anyone have any information on how to restore the folder tree from a NAS Drive?&lt;/p&gt;\n\n&lt;p&gt;The NAS was a Buffalo Linkstation. I died some time ago but the HDD is fine. &lt;/p&gt;\n\n&lt;p&gt;I tried different programs but even testdisk couldnt solve it.&lt;/p&gt;\n\n&lt;p&gt;Most important to me would be the folder structure, the files not that much (backup but unsorted).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44MB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ydadce", "is_robot_indexable": true, "report_reasons": null, "author": "51ck60y", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/ydadce/nas_xfs_restore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ydadce/nas_xfs_restore/", "subreddit_subscribers": 649001, "created_utc": 1666718490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download DRM protected M3U8 files? (Already tried IDM, youtube-dl, m3u8x and Chrome extensions)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yd6j9x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_idc1kmai", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "Piracy", "selftext": "I subscribed to an online course that lasts only for a month, and screen recording is very tedious. Every time I try downloading a video, it automatically gets blocked.\n\nHow should I go about it? Any genuine advice is appreciated.\n\nThanks!\n\nEdit: I tried using aria2 with yt-dlp, but am getting an error: **Incorrect AES key length (48 bytes)**\n\nI found [this](https://github.com/streamlink/streamlink/issues/2253) similar issue relating to Streamlink, but I am not able to interpret it.", "author_fullname": "t2_idc1kmai", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download DRM protected M3U8 files? (Already tried IDM, youtube-dl, m3u8x and Chrome extensions)", "link_flair_richtext": [{"e": "text", "t": "Question"}], "subreddit_name_prefixed": "r/Piracy", "hidden": false, "pwls": 6, "link_flair_css_class": "Question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yczqu7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": "", "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666716616.0, "author_flair_css_class": "", "author_flair_richtext": [{"e": "text", "t": " "}], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1666687281.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "self.Piracy", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I subscribed to an online course that lasts only for a month, and screen recording is very tedious. Every time I try downloading a video, it automatically gets blocked.&lt;/p&gt;\n\n&lt;p&gt;How should I go about it? Any genuine advice is appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;Edit: I tried using aria2 with yt-dlp, but am getting an error: &lt;strong&gt;Incorrect AES key length (48 bytes)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I found &lt;a href=\"https://github.com/streamlink/streamlink/issues/2253\"&gt;this&lt;/a&gt; similar issue relating to Streamlink, but I am not able to interpret it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?auto=webp&amp;s=f6025f8d6509883b3274e01fe9c9d514d5d3e2ff", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c71c9af0e600d917e3db10ec20ab639e7b44e2c9", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=417ddba27a3913b4f57ed8f02e1432060e2235a2", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=daf38c8108f620da12e208cbc72dda56504527c8", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=493a666cf5c201f5969b38c10bb33a0dad08d295", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c761a318cee046b79ab2ec815683d854d22a788", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f67abd5fc074a5dd5910e6859f4d0ea0e5251345", "width": 1080, "height": 540}], "variants": {}, "id": "xPwMRyHPhKfZituI289sHJW_dVrALYp1BgZtMYQyskI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5e1b454e-6cbb-11e7-aab1-0e24e918e778", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": " ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qmox", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#8ab9f9", "id": "yczqu7", "is_robot_indexable": true, "report_reasons": null, "author": "WishIWasDead2004", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/Piracy/comments/yczqu7/how_to_download_drm_protected_m3u8_files_already/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/Piracy/comments/yczqu7/how_to_download_drm_protected_m3u8_files_already/", "subreddit_subscribers": 973915, "created_utc": 1666687281.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1666708948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Piracy", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/Piracy/comments/yczqu7/how_to_download_drm_protected_m3u8_files_already/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?auto=webp&amp;s=f6025f8d6509883b3274e01fe9c9d514d5d3e2ff", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c71c9af0e600d917e3db10ec20ab639e7b44e2c9", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=417ddba27a3913b4f57ed8f02e1432060e2235a2", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=daf38c8108f620da12e208cbc72dda56504527c8", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=493a666cf5c201f5969b38c10bb33a0dad08d295", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c761a318cee046b79ab2ec815683d854d22a788", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/lZl1xJItP3X7H4glvRkbQ2aZj9syG8-E49QXW5mFLPE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f67abd5fc074a5dd5910e6859f4d0ea0e5251345", "width": 1080, "height": 540}], "variants": {}, "id": "xPwMRyHPhKfZituI289sHJW_dVrALYp1BgZtMYQyskI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yd6j9x", "is_robot_indexable": true, "report_reasons": null, "author": "WishIWasDead2004", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_yczqu7", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yd6j9x/how_to_download_drm_protected_m3u8_files_already/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/Piracy/comments/yczqu7/how_to_download_drm_protected_m3u8_files_already/", "subreddit_subscribers": 649001, "created_utc": 1666708948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using Opendrive for about a year or 2 but find it quite unreliable. Many of my files simply don't make it.  \n\n\nI'm looking to have an offsite backup too. I'm considering another NAS but I'd be happy with an affordable Cloud Storage solution if one exists.\n\nAny solutions for 16tb or more for around $20usd p/month or am I dreaming?  \n\n\nThanks in advance!", "author_fullname": "t2_t8qje", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Videographer W/16tb NAS - Affordable Cloud Storage Solutions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ycx4bu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666676649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using Opendrive for about a year or 2 but find it quite unreliable. Many of my files simply don&amp;#39;t make it.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to have an offsite backup too. I&amp;#39;m considering another NAS but I&amp;#39;d be happy with an affordable Cloud Storage solution if one exists.&lt;/p&gt;\n\n&lt;p&gt;Any solutions for 16tb or more for around $20usd p/month or am I dreaming?  &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ycx4bu", "is_robot_indexable": true, "report_reasons": null, "author": "SaracenRush", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ycx4bu/videographer_w16tb_nas_affordable_cloud_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ycx4bu/videographer_w16tb_nas_affordable_cloud_storage/", "subreddit_subscribers": 649001, "created_utc": 1666676649.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I downloaded the PowerShell extension and dropped in a symbolic link copy to my sync.com folder and it has worked for the past few months.\n\nHowever, I renamed the source folder and now the symlink is broken.\n\nIs there a way to change symlink target source or create a symlink where it acts as a dynamic copy (ie reflects changes in directory)? Don\u2019t want to have to resume everything just because source name is different\u2026 \n\nAlso not sure what the difference of clone/copy is in terms of the right click \u201cDrop As\u201d option\n\nWindows 11 user by the way", "author_fullname": "t2_7n7q0s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Symbolic Links", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yculuu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666668117.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded the PowerShell extension and dropped in a symbolic link copy to my sync.com folder and it has worked for the past few months.&lt;/p&gt;\n\n&lt;p&gt;However, I renamed the source folder and now the symlink is broken.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to change symlink target source or create a symlink where it acts as a dynamic copy (ie reflects changes in directory)? Don\u2019t want to have to resume everything just because source name is different\u2026 &lt;/p&gt;\n\n&lt;p&gt;Also not sure what the difference of clone/copy is in terms of the right click \u201cDrop As\u201d option&lt;/p&gt;\n\n&lt;p&gt;Windows 11 user by the way&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yculuu", "is_robot_indexable": true, "report_reasons": null, "author": "deadlydeity", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yculuu/symbolic_links/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yculuu/symbolic_links/", "subreddit_subscribers": 649001, "created_utc": 1666668117.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am using backblaze b2 as a backup storage option and have been using cyberduck to try and upload my data but it seems to struggle and fail. I am uploading a few TB of files most of which are over 1GB and cyberduck seems to often fail on them without saying so laving unfinished and partial files and forcing me to manually check for what was missed.\n\nI just want an option where I can give it a huge folder and tell it it upload and it will do that with no errors at all. I feel that is not too much to ask, is there a better piece of software to do this?", "author_fullname": "t2_tib2fva3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best intergration for uploading to Backblaze B2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ycop98", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666650742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using backblaze b2 as a backup storage option and have been using cyberduck to try and upload my data but it seems to struggle and fail. I am uploading a few TB of files most of which are over 1GB and cyberduck seems to often fail on them without saying so laving unfinished and partial files and forcing me to manually check for what was missed.&lt;/p&gt;\n\n&lt;p&gt;I just want an option where I can give it a huge folder and tell it it upload and it will do that with no errors at all. I feel that is not too much to ask, is there a better piece of software to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ycop98", "is_robot_indexable": true, "report_reasons": null, "author": "Fragrant_Big3901", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ycop98/best_intergration_for_uploading_to_backblaze_b2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ycop98/best_intergration_for_uploading_to_backblaze_b2/", "subreddit_subscribers": 649001, "created_utc": 1666650742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI\u2019ve got a 6 bay Synology 1621xs+ that I\u2019m moving to from a 4 bay 920+.\n\nI realistically only need a max of about 20-30tb, as I\u2019m currently only using 6.6tb and that amount hasn\u2019t changed in a while. I do full system backups and will eventually create a Plex server as well.\n\nMy issue is: I\u2019m trying to find the most quiet, reliable drives for my NAS, but here\u2019s what my research shows so far\u2026\n\n- from every account I\u2019ve followed, I should stay away from Ironwolf if noise is a concern, and I can attest to that because I tested 14tb Ironwolf Pros in the past and could hear them a full floor up.\n- if I want Helium I need to go 12tb or higher (and I\u2019ve heard 10tb WD Red Plus drives can be loud)\n- I shouldn\u2019t bother with 5400 class drives because they just run at 7200rpm anyway from what I\u2019ve read.\n\nAny accounts about how 10tb WD Red Plus sound? Or 8tb? Or just opinions on which drives to look at? Thanks.", "author_fullname": "t2_15umg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Difficulty with hard drive choice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yco9uu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666649628.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve got a 6 bay Synology 1621xs+ that I\u2019m moving to from a 4 bay 920+.&lt;/p&gt;\n\n&lt;p&gt;I realistically only need a max of about 20-30tb, as I\u2019m currently only using 6.6tb and that amount hasn\u2019t changed in a while. I do full system backups and will eventually create a Plex server as well.&lt;/p&gt;\n\n&lt;p&gt;My issue is: I\u2019m trying to find the most quiet, reliable drives for my NAS, but here\u2019s what my research shows so far\u2026&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;from every account I\u2019ve followed, I should stay away from Ironwolf if noise is a concern, and I can attest to that because I tested 14tb Ironwolf Pros in the past and could hear them a full floor up.&lt;/li&gt;\n&lt;li&gt;if I want Helium I need to go 12tb or higher (and I\u2019ve heard 10tb WD Red Plus drives can be loud)&lt;/li&gt;\n&lt;li&gt;I shouldn\u2019t bother with 5400 class drives because they just run at 7200rpm anyway from what I\u2019ve read.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any accounts about how 10tb WD Red Plus sound? Or 8tb? Or just opinions on which drives to look at? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "yco9uu", "is_robot_indexable": true, "report_reasons": null, "author": "Scrutape", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/yco9uu/difficulty_with_hard_drive_choice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/yco9uu/difficulty_with_hard_drive_choice/", "subreddit_subscribers": 649001, "created_utc": 1666649628.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\n&amp;#x200B;\n\nI have a script that is trying to pull followers for an Instagram account. The account has about 100K followers. When I run the script below, it pulls about 45K of those followers, which is pretty good, but obviously not even half of what I need.\n\n&amp;#x200B;\n\nIs there a way I can setup a script where with each run, it only collects the followers it hasn't previously already extracted? So the next time I run it, it will exclude extracting those first 45K and just work on getting the remaining 55K?\n\n&amp;#x200B;\n\nFor reference, here is the script:\n\n    # Get instance\n    import instaloader\n    \n    L = instaloader.Instaloader()\n    \n    # Login or load session\n    username = \"XXXXX\"\n    password = \"XXXXX\"\n    L.login(username, password)  # (login)\n    \n    # Obtain profile metadata\n    profile =  .Profile.from_username(L.context, 'zheng_max_')\n    \n    # Print list of followees\n    follow_list = []\n    count = 0\n    for followee in profile.get_followers():\n    follow_list.append(followee.username)\n    file = open(\"zheng_max_followers.txt\", \"a+\")\n    file.write(follow_list[count])\n    file.write(\"\\n\")\n    file.close()\n    print(follow_list[count])\n    count = count + 1\n    # (likewise with profile.get_followers())", "author_fullname": "t2_g061pa0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Instagram - Pulling Follower BUT Only Pull Followers That Haven't Been Pulled Yet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ydezzz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666730064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have a script that is trying to pull followers for an Instagram account. The account has about 100K followers. When I run the script below, it pulls about 45K of those followers, which is pretty good, but obviously not even half of what I need.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there a way I can setup a script where with each run, it only collects the followers it hasn&amp;#39;t previously already extracted? So the next time I run it, it will exclude extracting those first 45K and just work on getting the remaining 55K?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;For reference, here is the script:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# Get instance\nimport instaloader\n\nL = instaloader.Instaloader()\n\n# Login or load session\nusername = &amp;quot;XXXXX&amp;quot;\npassword = &amp;quot;XXXXX&amp;quot;\nL.login(username, password)  # (login)\n\n# Obtain profile metadata\nprofile =  .Profile.from_username(L.context, &amp;#39;zheng_max_&amp;#39;)\n\n# Print list of followees\nfollow_list = []\ncount = 0\nfor followee in profile.get_followers():\nfollow_list.append(followee.username)\nfile = open(&amp;quot;zheng_max_followers.txt&amp;quot;, &amp;quot;a+&amp;quot;)\nfile.write(follow_list[count])\nfile.write(&amp;quot;\\n&amp;quot;)\nfile.close()\nprint(follow_list[count])\ncount = count + 1\n# (likewise with profile.get_followers())\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ydezzz", "is_robot_indexable": true, "report_reasons": null, "author": "GoopOnYaGrinch", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ydezzz/instagram_pulling_follower_but_only_pull/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ydezzz/instagram_pulling_follower_but_only_pull/", "subreddit_subscribers": 649001, "created_utc": 1666730064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys,  \n\n\nSo, I have 1tb of various private things (porn, memes etc) I'd like to keep on the cloud for easy access and viewing. I have a bunch of OneDrive space, but I've heard plenty of horror stories regarding people's accounts being shut down due to just private porn and would not like to risk the same fate.  \n\n\nSolution 1: Use cyberduck to encrypt and store in normal cloud. Unfortunately, this means if I simply want to view a file, I would have to use cryptomator to decrypt said files. I've had issues using cryptomator on its own due to sync issues. (Besides unsure if their ios and android app is any good)  \n\n\nSolution 2: Nas? Would be perfect however my technical skills are completely non-existent. I'd be happy to hire someone here locally to set it up but information on it already feels overwhelming.  \n\n\nSolution 3: Just say fuck it and store it unencrypted?   \n\n\nNot sure what you guys would recommend but yeah. Maybe this \"perfect &amp; seamless but also encrypted access\" set up doesn't exist.  \n\n\nThanks!", "author_fullname": "t2_59tutzed", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can't seem to find a solution. Would appreciate advice.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ydcejg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666723557.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,  &lt;/p&gt;\n\n&lt;p&gt;So, I have 1tb of various private things (porn, memes etc) I&amp;#39;d like to keep on the cloud for easy access and viewing. I have a bunch of OneDrive space, but I&amp;#39;ve heard plenty of horror stories regarding people&amp;#39;s accounts being shut down due to just private porn and would not like to risk the same fate.  &lt;/p&gt;\n\n&lt;p&gt;Solution 1: Use cyberduck to encrypt and store in normal cloud. Unfortunately, this means if I simply want to view a file, I would have to use cryptomator to decrypt said files. I&amp;#39;ve had issues using cryptomator on its own due to sync issues. (Besides unsure if their ios and android app is any good)  &lt;/p&gt;\n\n&lt;p&gt;Solution 2: Nas? Would be perfect however my technical skills are completely non-existent. I&amp;#39;d be happy to hire someone here locally to set it up but information on it already feels overwhelming.  &lt;/p&gt;\n\n&lt;p&gt;Solution 3: Just say fuck it and store it unencrypted?   &lt;/p&gt;\n\n&lt;p&gt;Not sure what you guys would recommend but yeah. Maybe this &amp;quot;perfect &amp;amp; seamless but also encrypted access&amp;quot; set up doesn&amp;#39;t exist.  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ydcejg", "is_robot_indexable": true, "report_reasons": null, "author": "MrCaptainCapitalist", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ydcejg/cant_seem_to_find_a_solution_would_appreciate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ydcejg/cant_seem_to_find_a_solution_would_appreciate/", "subreddit_subscribers": 649001, "created_utc": 1666723557.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First of all, I'm in Brazil, so stuff which may be inexpensive elsewhere might be really inaccessible for me.\n\nMy data is divided like this: 1.4 TB of movies and TV shows, mostly on 720p or 1080p, 375GB of copyrighted classes I ripped from places and a few GB of files that are less static than the other two sets.\n\nBacking up the movies and TV shows isn't *really* necessarily, as I could always grab them again, but I'd like to safely store what I ripped, as well as  have versioned syncing for some of my user folders.\n\nAlso, I'll visit my brother in the US by the end of the year, so I might take the opportunity to buy some hardware on black friday and set up a nice NAS to feed my Plex.\n\nFor the backup and storage, I was wondering if I should go with Wasabi + some software (preferably with a GUI, but not strictly necessary).\n\nFor the NAS, I believe that some setup with support for 2 HDDS, one with 4TB and one with 2TB, would meet my requirements for a very long time already. I'd be streaming mostly to my laptop, but also to smartphones and TVs from time to time, so it would be nice if it had transcoding capabilities.\n\nI tried reading through the wiki, but I simply felt overwhelmed by the information and decided I'd rather ask for an informed opinion, so any advice is very much appreciated!", "author_fullname": "t2_s91sz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I could use some help on setting up a NAS for my Plex and a cloud backup for my laptop", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ycr8z5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666658008.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all, I&amp;#39;m in Brazil, so stuff which may be inexpensive elsewhere might be really inaccessible for me.&lt;/p&gt;\n\n&lt;p&gt;My data is divided like this: 1.4 TB of movies and TV shows, mostly on 720p or 1080p, 375GB of copyrighted classes I ripped from places and a few GB of files that are less static than the other two sets.&lt;/p&gt;\n\n&lt;p&gt;Backing up the movies and TV shows isn&amp;#39;t &lt;em&gt;really&lt;/em&gt; necessarily, as I could always grab them again, but I&amp;#39;d like to safely store what I ripped, as well as  have versioned syncing for some of my user folders.&lt;/p&gt;\n\n&lt;p&gt;Also, I&amp;#39;ll visit my brother in the US by the end of the year, so I might take the opportunity to buy some hardware on black friday and set up a nice NAS to feed my Plex.&lt;/p&gt;\n\n&lt;p&gt;For the backup and storage, I was wondering if I should go with Wasabi + some software (preferably with a GUI, but not strictly necessary).&lt;/p&gt;\n\n&lt;p&gt;For the NAS, I believe that some setup with support for 2 HDDS, one with 4TB and one with 2TB, would meet my requirements for a very long time already. I&amp;#39;d be streaming mostly to my laptop, but also to smartphones and TVs from time to time, so it would be nice if it had transcoding capabilities.&lt;/p&gt;\n\n&lt;p&gt;I tried reading through the wiki, but I simply felt overwhelmed by the information and decided I&amp;#39;d rather ask for an informed opinion, so any advice is very much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "ycr8z5", "is_robot_indexable": true, "report_reasons": null, "author": "fabiorzfreitas", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/ycr8z5/i_could_use_some_help_on_setting_up_a_nas_for_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/ycr8z5/i_could_use_some_help_on_setting_up_a_nas_for_my/", "subreddit_subscribers": 649001, "created_utc": 1666658008.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}