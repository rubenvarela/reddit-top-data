{"kind": "Listing", "data": {"after": null, "dist": 18, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Which storage option is better to store data for analysis?", "author_fullname": "t2_pwxlrv27", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you prefer OneDrive, Google Drive, or Dropbox?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yd34mn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666699362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which storage option is better to store data for analysis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yd34mn", "is_robot_indexable": true, "report_reasons": null, "author": "Fast-Group-8501", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yd34mn/do_you_prefer_onedrive_google_drive_or_dropbox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yd34mn/do_you_prefer_onedrive_google_drive_or_dropbox/", "subreddit_subscribers": 815379, "created_utc": 1666699362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "It seems like a significant amount of that for me is used on doing \"nothing productive\".", "author_fullname": "t2_amfdjuba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[PRODUCTIVITY-RELATED] If you work 8 hours a day, how many of those hours are productive hours?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ycxcw3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666677572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems like a significant amount of that for me is used on doing &amp;quot;nothing productive&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ycxcw3", "is_robot_indexable": true, "report_reasons": null, "author": "limedove", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ycxcw3/productivityrelated_if_you_work_8_hours_a_day_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ycxcw3/productivityrelated_if_you_work_8_hours_a_day_how/", "subreddit_subscribers": 815379, "created_utc": 1666677572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently a Computer Science undergraduate student working as a \"Data Science Intern\" for my university. I work with the Oracle Database, using Python (PySpark, Pandas, Numpy) and SQL to handle and analyze big student data. \n\nAre there even Data Scientist who work at Universities and if so what do you do? Do you make Prediction models, or any Machine Learning at all?", "author_fullname": "t2_705udfmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Scientists who work at Universities", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ycyt72", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666683385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a Computer Science undergraduate student working as a &amp;quot;Data Science Intern&amp;quot; for my university. I work with the Oracle Database, using Python (PySpark, Pandas, Numpy) and SQL to handle and analyze big student data. &lt;/p&gt;\n\n&lt;p&gt;Are there even Data Scientist who work at Universities and if so what do you do? Do you make Prediction models, or any Machine Learning at all?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ycyt72", "is_robot_indexable": true, "report_reasons": null, "author": "Huitzilin_760", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ycyt72/data_scientists_who_work_at_universities/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ycyt72/data_scientists_who_work_at_universities/", "subreddit_subscribers": 815379, "created_utc": 1666683385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In almost all the learning material and tutorials i've observed that the training of the model is done everytime the application is executed. I understand that training would be required everytime if the type of data changes but for a real world application do we have to teach it how to run everytime?\n\ne.g. in an application that forecasts some values based on a dataset that grows with time, do i have to train my model each time i request a forecast?", "author_fullname": "t2_n732f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Newbie here. Do people train their models everytime an app runs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ycxtn8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666679395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In almost all the learning material and tutorials i&amp;#39;ve observed that the training of the model is done everytime the application is executed. I understand that training would be required everytime if the type of data changes but for a real world application do we have to teach it how to run everytime?&lt;/p&gt;\n\n&lt;p&gt;e.g. in an application that forecasts some values based on a dataset that grows with time, do i have to train my model each time i request a forecast?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ycxtn8", "is_robot_indexable": true, "report_reasons": null, "author": "buckypimpin", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ycxtn8/newbie_here_do_people_train_their_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ycxtn8/newbie_here_do_people_train_their_models/", "subreddit_subscribers": 815379, "created_utc": 1666679395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "A two-day virtual event packed with demos, AMAs, and hands-on labs created by builders, for builders. Immerse yourself in improving your apps, data pipelines, machine learning workflows, and much more. Join here - [https://www.snowflake.com/build/?utm\\_source=dataops.live&amp;utm\\_medium=partner&amp;utm\\_campaign=na-us-en-&amp;utm\\_content=-evv-build-virtual-](https://www.snowflake.com/build/?utm_source=dataops.live&amp;utm_medium=partner&amp;utm_campaign=na-us-en-&amp;utm_content=-evv-build-virtual-)\n\n[Snowflake Build - DataOps.live](https://preview.redd.it/fm5cmlgn90w91.png?width=1200&amp;format=png&amp;auto=webp&amp;s=4bdf935a447955f927b6d76eb305d549157afaf4)", "author_fullname": "t2_12li6zgs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake BUILD '22 - The Data Cloud Dev Virtual Summit (NOV 15-16)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "media_metadata": {"fm5cmlgn90w91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/fm5cmlgn90w91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5015c70f6cefc72d1c3c1a701790a93dfefac415"}, {"y": 113, "x": 216, "u": "https://preview.redd.it/fm5cmlgn90w91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eeedd9b0471d83dbfa9ec943a53699200e0158c7"}, {"y": 167, "x": 320, "u": "https://preview.redd.it/fm5cmlgn90w91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3235d651c8e431c38560602147c4c2cef425e696"}, {"y": 334, "x": 640, "u": "https://preview.redd.it/fm5cmlgn90w91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5e37d6653e82373f0eb5557956ae787610df4e87"}, {"y": 502, "x": 960, "u": "https://preview.redd.it/fm5cmlgn90w91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e079ff1631f930b9a46a4a6e9d93d2e16ed4d15c"}, {"y": 565, "x": 1080, "u": "https://preview.redd.it/fm5cmlgn90w91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f1aa5a863321fe1e48b1e3121999bd94547664b1"}], "s": {"y": 628, "x": 1200, "u": "https://preview.redd.it/fm5cmlgn90w91.png?width=1200&amp;format=png&amp;auto=webp&amp;s=4bdf935a447955f927b6d76eb305d549157afaf4"}, "id": "fm5cmlgn90w91"}}, "name": "t3_yddsft", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xQ1gfBeCM9pXtU4Z9kxK7o_g4srLxlaPYrCkW3yBSXU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666727048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A two-day virtual event packed with demos, AMAs, and hands-on labs created by builders, for builders. Immerse yourself in improving your apps, data pipelines, machine learning workflows, and much more. Join here - &lt;a href=\"https://www.snowflake.com/build/?utm_source=dataops.live&amp;amp;utm_medium=partner&amp;amp;utm_campaign=na-us-en-&amp;amp;utm_content=-evv-build-virtual-\"&gt;https://www.snowflake.com/build/?utm_source=dataops.live&amp;amp;utm_medium=partner&amp;amp;utm_campaign=na-us-en-&amp;amp;utm_content=-evv-build-virtual-&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fm5cmlgn90w91.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bdf935a447955f927b6d76eb305d549157afaf4\"&gt;Snowflake Build - DataOps.live&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yddsft", "is_robot_indexable": true, "report_reasons": null, "author": "Dkreig", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yddsft/snowflake_build_22_the_data_cloud_dev_virtual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yddsft/snowflake_build_22_the_data_cloud_dev_virtual/", "subreddit_subscribers": 815379, "created_utc": 1666727048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Here is a simplified illustration of my problem. Note, I am using a Linear Mixed Effects model with a binomial link function, as I need to account for another categorical feature, and I need the model to be as interpretable as possible.\n\nSuppose I have 2 features (A and B) that I have found to have importance on predicting a binary outcome, Y.\nFeature A\u2019s type is ordinal values from the set {1,2,3,4,5,6}, with a linear negative correlation with P(Y).\n\nFeature B is a continuous value between 0 and 1, however, it is not definable when Feature A = 1. For all other values of Feature A, Feature B has a weaker but significant linear positive correlation with P(Y).\n\nAny idea how I could deal with this? Options I\u2019ve considered, but not yet tested are\n- Have separate models/mixed effect groups for when Feature A = 1 and when Feature A != 1\n- Set Feature B to some arbitrary(?) extreme value when Feature A = 1\n- Impute Feature B with its mean when Feature A = 1\n\nAny thoughts on those, or alternative ideas would be really appreciated!\n\nThank you", "author_fullname": "t2_s5vml4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a binary classification problem where one feature is not defined for certain values of another. Is it possible to model this with logistic regression?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_ydde4v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666726054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is a simplified illustration of my problem. Note, I am using a Linear Mixed Effects model with a binomial link function, as I need to account for another categorical feature, and I need the model to be as interpretable as possible.&lt;/p&gt;\n\n&lt;p&gt;Suppose I have 2 features (A and B) that I have found to have importance on predicting a binary outcome, Y.\nFeature A\u2019s type is ordinal values from the set {1,2,3,4,5,6}, with a linear negative correlation with P(Y).&lt;/p&gt;\n\n&lt;p&gt;Feature B is a continuous value between 0 and 1, however, it is not definable when Feature A = 1. For all other values of Feature A, Feature B has a weaker but significant linear positive correlation with P(Y).&lt;/p&gt;\n\n&lt;p&gt;Any idea how I could deal with this? Options I\u2019ve considered, but not yet tested are\n- Have separate models/mixed effect groups for when Feature A = 1 and when Feature A != 1\n- Set Feature B to some arbitrary(?) extreme value when Feature A = 1\n- Impute Feature B with its mean when Feature A = 1&lt;/p&gt;\n\n&lt;p&gt;Any thoughts on those, or alternative ideas would be really appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ydde4v", "is_robot_indexable": true, "report_reasons": null, "author": "sniffykix", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ydde4v/i_have_a_binary_classification_problem_where_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ydde4v/i_have_a_binary_classification_problem_where_one/", "subreddit_subscribers": 815379, "created_utc": 1666726054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm training a machine learning model using [YOLOv5 from Ultralytics](https://github.com/ultralytics/yolov5) (arch: YOLOv5s6). The task is to detect and identify laundry symbols. For that, I've scraped and labeled 600 images from Google.\n\nUsing this dataset, I receive a result with an **mAP around 0.6**.\n\nBut **600 images** is a tiny dataset and there are multiple laundry symbols where I have only 1-4 images for training and symbols where I have 100 and more.\n\nSo I started writing a Python script which generates more images of laundry symbols. The script basically takes a background image and adds randomly positioned 1-10 laundry symbols in different colors and rotations. No background is used twice. With that script, **I generated around 6.000 entirely different images with laundry symbols** that every laundry symbol is at least 800 times in the dataset.\n\n**Here are examples of the generated data:** [Link 1](https://i.stack.imgur.com/jaQmF.jpg) [Link 2](https://i.stack.imgur.com/OTZId.jpg)\n\nI combined the scraped and the generated dataset and retrained the model with the same configuration. The result is really bad: the **mAP dropped to 0.15** and the model **overfits**. The confusion matrix told me why: [Confusion matrix](https://i.stack.imgur.com/cBJSF.png)\n\n## Why is the model learning the background instead the objects?\n\nFirst I thought my annotation might be wrong, but the training script from Ultralytics saves a few examples of training batch images - there the boxes are drawn perfectly around the generated symbols.\n\nFor completeness, below are more analytics added about the training:\n\n## More analytics\n\n[Labels](https://i.stack.imgur.com/vrMV1.jpg)  [Curves](https://i.stack.imgur.com/3rKKz.png) [More examples from the dataset](https://i.stack.imgur.com/IunEF.jpg)", "author_fullname": "t2_7y7wl0i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Object detection model learns backgrounds and not objects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ydbkaf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1666721410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m training a machine learning model using &lt;a href=\"https://github.com/ultralytics/yolov5\"&gt;YOLOv5 from Ultralytics&lt;/a&gt; (arch: YOLOv5s6). The task is to detect and identify laundry symbols. For that, I&amp;#39;ve scraped and labeled 600 images from Google.&lt;/p&gt;\n\n&lt;p&gt;Using this dataset, I receive a result with an &lt;strong&gt;mAP around 0.6&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;But &lt;strong&gt;600 images&lt;/strong&gt; is a tiny dataset and there are multiple laundry symbols where I have only 1-4 images for training and symbols where I have 100 and more.&lt;/p&gt;\n\n&lt;p&gt;So I started writing a Python script which generates more images of laundry symbols. The script basically takes a background image and adds randomly positioned 1-10 laundry symbols in different colors and rotations. No background is used twice. With that script, &lt;strong&gt;I generated around 6.000 entirely different images with laundry symbols&lt;/strong&gt; that every laundry symbol is at least 800 times in the dataset.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here are examples of the generated data:&lt;/strong&gt; &lt;a href=\"https://i.stack.imgur.com/jaQmF.jpg\"&gt;Link 1&lt;/a&gt; &lt;a href=\"https://i.stack.imgur.com/OTZId.jpg\"&gt;Link 2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I combined the scraped and the generated dataset and retrained the model with the same configuration. The result is really bad: the &lt;strong&gt;mAP dropped to 0.15&lt;/strong&gt; and the model &lt;strong&gt;overfits&lt;/strong&gt;. The confusion matrix told me why: &lt;a href=\"https://i.stack.imgur.com/cBJSF.png\"&gt;Confusion matrix&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Why is the model learning the background instead the objects?&lt;/h2&gt;\n\n&lt;p&gt;First I thought my annotation might be wrong, but the training script from Ultralytics saves a few examples of training batch images - there the boxes are drawn perfectly around the generated symbols.&lt;/p&gt;\n\n&lt;p&gt;For completeness, below are more analytics added about the training:&lt;/p&gt;\n\n&lt;h2&gt;More analytics&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.stack.imgur.com/vrMV1.jpg\"&gt;Labels&lt;/a&gt;  &lt;a href=\"https://i.stack.imgur.com/3rKKz.png\"&gt;Curves&lt;/a&gt; &lt;a href=\"https://i.stack.imgur.com/IunEF.jpg\"&gt;More examples from the dataset&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ytAh_HoPvhxhbLN8xUANRtY73AQ1EaEakmzjqRoIwWU.jpg?auto=webp&amp;s=11c9b1ceacf186ed23cb3cbced5cba97e0853d91", "width": 640, "height": 428}, "resolutions": [{"url": "https://external-preview.redd.it/ytAh_HoPvhxhbLN8xUANRtY73AQ1EaEakmzjqRoIwWU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e4ba0a98dddf9ec8a33d2e043afe8a9e9ff3551", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/ytAh_HoPvhxhbLN8xUANRtY73AQ1EaEakmzjqRoIwWU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c48335f113661e96164d9162a6bc90a9451a6f5d", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/ytAh_HoPvhxhbLN8xUANRtY73AQ1EaEakmzjqRoIwWU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cc47d42700c4a534f97c9356a1163396ce0e374e", "width": 320, "height": 214}, {"url": "https://external-preview.redd.it/ytAh_HoPvhxhbLN8xUANRtY73AQ1EaEakmzjqRoIwWU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ff7a9998b37d55bfc01b3eea97eb744a40f5544a", "width": 640, "height": 428}], "variants": {}, "id": "m6zZAb-zGHpzXpSmgqJsVzsfY6iSHgTlbdnDlvVMQfk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ydbkaf", "is_robot_indexable": true, "report_reasons": null, "author": "Waterfront_xD", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ydbkaf/object_detection_model_learns_backgrounds_and_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ydbkaf/object_detection_model_learns_backgrounds_and_not/", "subreddit_subscribers": 815379, "created_utc": 1666721410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am trying to forecast some data - the data isn't necessarily linear but I am limited by what I have access to (SQL Server and SSRS) so I have forecasted using line regression. I need to include a confidence interval so I have done so as shown in the first picture (99%). \n\nThe issues I have with this are \n\n1. It seems awfully narrow - equation used is 2.576\\*SD/sqrt(n) = 0.473 (I have tried both normal distribution and t distribution and they give the same answer - n = 30 so its right on the threshold for now). \n\n2. We are looking for something that fans out as the sample size increases as shown in the second picture. So far the only solution I have been able to figure out is by modifying the slopes of the max and min forecast lines but that takes away the statistical accuracy. \n\nIs there any form of confidence interval that affects the slope? Or a CI specifically meant for time series?\n\n[99&amp;#37; Confidence Interval](https://preview.redd.it/bqog1ybs1zv91.png?width=2920&amp;format=png&amp;auto=webp&amp;s=bf20b881aaf5b623625940687064aff1bb08ef5a)\n\n[99&amp;#37; Confidence interval with slope modification](https://preview.redd.it/je5hf82r3zv91.png?width=2920&amp;format=png&amp;auto=webp&amp;s=678a937eae1edca0338eacce73e28b5e135e6a53)", "author_fullname": "t2_e3a1k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confidence Intervals for Linear Regression", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"bqog1ybs1zv91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/bqog1ybs1zv91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a3fb90296f38e2eb9fe795b451afb646d4f80e6a"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/bqog1ybs1zv91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb70a33978d76b4412589ab91ceae2f6be389ac7"}, {"y": 179, "x": 320, "u": "https://preview.redd.it/bqog1ybs1zv91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e3a31af23fc00d38ccf5066f068c86cd805bff1"}, {"y": 358, "x": 640, "u": "https://preview.redd.it/bqog1ybs1zv91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=374e8818cc50d9b114418b6e5098c35dcc04eeb9"}, {"y": 537, "x": 960, "u": "https://preview.redd.it/bqog1ybs1zv91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7f59fbbe5ba8790e83ee3ea0104938d8fcbc208f"}, {"y": 604, "x": 1080, "u": "https://preview.redd.it/bqog1ybs1zv91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=907b6769ce5d9fef59a52ad8f2c5e198e447869d"}], "s": {"y": 1634, "x": 2920, "u": "https://preview.redd.it/bqog1ybs1zv91.png?width=2920&amp;format=png&amp;auto=webp&amp;s=bf20b881aaf5b623625940687064aff1bb08ef5a"}, "id": "bqog1ybs1zv91"}, "je5hf82r3zv91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/je5hf82r3zv91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7db4d44be57914c2a871ee7edf1041d250ad7d08"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/je5hf82r3zv91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f3c93d7755a645a80429da7ffffd437b92d6ee83"}, {"y": 179, "x": 320, "u": "https://preview.redd.it/je5hf82r3zv91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=81985ae8b830fcf6f5d5a4c2debc2244003d3749"}, {"y": 358, "x": 640, "u": "https://preview.redd.it/je5hf82r3zv91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e1e9fa218dabc92c61fb0560e6298d632fc53ce"}, {"y": 537, "x": 960, "u": "https://preview.redd.it/je5hf82r3zv91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a33f2e93588d1aecbcecc013c9191bcc518fbb51"}, {"y": 604, "x": 1080, "u": "https://preview.redd.it/je5hf82r3zv91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b164b915b632df4880fe0c7e204dc59cd364d51d"}], "s": {"y": 1634, "x": 2920, "u": "https://preview.redd.it/je5hf82r3zv91.png?width=2920&amp;format=png&amp;auto=webp&amp;s=678a937eae1edca0338eacce73e28b5e135e6a53"}, "id": "je5hf82r3zv91"}}, "name": "t3_yd87up", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/75lqCnhGAJlP7-1vu4j3-9vQg-WbAjF3ChnGHBTZkBE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666713287.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to forecast some data - the data isn&amp;#39;t necessarily linear but I am limited by what I have access to (SQL Server and SSRS) so I have forecasted using line regression. I need to include a confidence interval so I have done so as shown in the first picture (99%). &lt;/p&gt;\n\n&lt;p&gt;The issues I have with this are &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;It seems awfully narrow - equation used is 2.576*SD/sqrt(n) = 0.473 (I have tried both normal distribution and t distribution and they give the same answer - n = 30 so its right on the threshold for now). &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We are looking for something that fans out as the sample size increases as shown in the second picture. So far the only solution I have been able to figure out is by modifying the slopes of the max and min forecast lines but that takes away the statistical accuracy. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Is there any form of confidence interval that affects the slope? Or a CI specifically meant for time series?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bqog1ybs1zv91.png?width=2920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bf20b881aaf5b623625940687064aff1bb08ef5a\"&gt;99&amp;#37; Confidence Interval&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/je5hf82r3zv91.png?width=2920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=678a937eae1edca0338eacce73e28b5e135e6a53\"&gt;99&amp;#37; Confidence interval with slope modification&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yd87up", "is_robot_indexable": true, "report_reasons": null, "author": "afrojacksparrow", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yd87up/confidence_intervals_for_linear_regression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yd87up/confidence_intervals_for_linear_regression/", "subreddit_subscribers": 815379, "created_utc": 1666713287.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "TLDR: Colab is taking longer to connect to Redshift than Jupyter, and I want to know why\n\nI am a Product Analyst, and I work with several PM's to get them the data that they need- nothing fancy, they really just want csv's that they can load into pivot tables. For reasons not in my control, we are not doing  dashboards (not my call), and I need to get multiple PM's multiple files on a regular basis. \n\nI am using Jupyter notebook hooked up directly to our Redshift database. I figured I could save myself some work by uploading the notebooks to Google Colab (the company is already on G-Suite), and then the PM's can just run the script, which will automatically pull the data, do the manipulations, and download the CSV for them to use. \n\nWhen I tried this, notebooks that ran in 2 minutes or less in Jupyter resulted in run times so long in Colab that the connection to Redshift was shut down.\n\nAny ideas on why Colab is taking so much longer to connect to Redshift?", "author_fullname": "t2_13b1f3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jupyter Notebook vs Google Colab Performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ycxxnz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666679837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: Colab is taking longer to connect to Redshift than Jupyter, and I want to know why&lt;/p&gt;\n\n&lt;p&gt;I am a Product Analyst, and I work with several PM&amp;#39;s to get them the data that they need- nothing fancy, they really just want csv&amp;#39;s that they can load into pivot tables. For reasons not in my control, we are not doing  dashboards (not my call), and I need to get multiple PM&amp;#39;s multiple files on a regular basis. &lt;/p&gt;\n\n&lt;p&gt;I am using Jupyter notebook hooked up directly to our Redshift database. I figured I could save myself some work by uploading the notebooks to Google Colab (the company is already on G-Suite), and then the PM&amp;#39;s can just run the script, which will automatically pull the data, do the manipulations, and download the CSV for them to use. &lt;/p&gt;\n\n&lt;p&gt;When I tried this, notebooks that ran in 2 minutes or less in Jupyter resulted in run times so long in Colab that the connection to Redshift was shut down.&lt;/p&gt;\n\n&lt;p&gt;Any ideas on why Colab is taking so much longer to connect to Redshift?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ycxxnz", "is_robot_indexable": true, "report_reasons": null, "author": "aaquad", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ycxxnz/jupyter_notebook_vs_google_colab_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ycxxnz/jupyter_notebook_vs_google_colab_performance/", "subreddit_subscribers": 815379, "created_utc": 1666679837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Just trying to gauge how you all determine your sample sizes. I\u2019ve always just kinda rule-of-thumb gone with minimum sample of 100 but no more than 1000, and aimed for maybe 10% of the population count. \n\nAdditionally, how do you all determine iterations for calculating the means of sample means? Here I just pull a number out of my butt that\u2019s kinda big\u2026\n\nWhat I\u2019m finding on a toy data set (from real data) is that if my sample sizes are kinda big, like 1000, then my distribution of sample means is basically a single bar around some number that is very different than my population mean. If I reduce my sample size to 30-50 ish, my mean of sample means is closer to my population mean and the distribution is more \u201cnormal\u201d in a sense. \n\nMy population is in the 1,000,000 range for counts of observations.", "author_fullname": "t2_d5gr1nxb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sample size and iterations for Mean of Sample Means", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ycrfd5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666658533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just trying to gauge how you all determine your sample sizes. I\u2019ve always just kinda rule-of-thumb gone with minimum sample of 100 but no more than 1000, and aimed for maybe 10% of the population count. &lt;/p&gt;\n\n&lt;p&gt;Additionally, how do you all determine iterations for calculating the means of sample means? Here I just pull a number out of my butt that\u2019s kinda big\u2026&lt;/p&gt;\n\n&lt;p&gt;What I\u2019m finding on a toy data set (from real data) is that if my sample sizes are kinda big, like 1000, then my distribution of sample means is basically a single bar around some number that is very different than my population mean. If I reduce my sample size to 30-50 ish, my mean of sample means is closer to my population mean and the distribution is more \u201cnormal\u201d in a sense. &lt;/p&gt;\n\n&lt;p&gt;My population is in the 1,000,000 range for counts of observations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ycrfd5", "is_robot_indexable": true, "report_reasons": null, "author": "renok_archnmy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ycrfd5/sample_size_and_iterations_for_mean_of_sample/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ycrfd5/sample_size_and_iterations_for_mean_of_sample/", "subreddit_subscribers": 815379, "created_utc": 1666658533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My friends and I are students working on a machine learning framework that allows devs to train models on other peoples GPUs. Does anyone have any experience working with similar tech (Model/data parallelism, differential privacy, edge computing) or even any feedback on the idea in general.", "author_fullname": "t2_tg5fewzz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Distributed Machine Learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ycoll6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666650471.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My friends and I are students working on a machine learning framework that allows devs to train models on other peoples GPUs. Does anyone have any experience working with similar tech (Model/data parallelism, differential privacy, edge computing) or even any feedback on the idea in general.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ycoll6", "is_robot_indexable": true, "report_reasons": null, "author": "avnertothemoon", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ycoll6/distributed_machine_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ycoll6/distributed_machine_learning/", "subreddit_subscribers": 815379, "created_utc": 1666650471.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_mlz5ca9w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for any feedback/criticisms on my resume for DA/DS/SWE internships", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_yde3ib", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/_PNZoq4En_6j2K5APG6tnZPIS8cpLHGQXyBLinIc9q8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666727814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/uyaibxqzb0w91.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/uyaibxqzb0w91.jpg?auto=webp&amp;s=5e2ee6b68ebad138ee6073eb555dbef4b4506cf8", "width": 1170, "height": 1486}, "resolutions": [{"url": "https://preview.redd.it/uyaibxqzb0w91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=91515f2b9d74f158ca0a83e1deedc7e3b215bb22", "width": 108, "height": 137}, {"url": "https://preview.redd.it/uyaibxqzb0w91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80b7c3831a15c1888ff54c0fb8dd0ff3b1629efd", "width": 216, "height": 274}, {"url": "https://preview.redd.it/uyaibxqzb0w91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=02f077b7403d0f3fc3593e1654888f0d69116e08", "width": 320, "height": 406}, {"url": "https://preview.redd.it/uyaibxqzb0w91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bd676276836b826e0d15b2ce6c17f1bd27de767e", "width": 640, "height": 812}, {"url": "https://preview.redd.it/uyaibxqzb0w91.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=069d887089477b122c7116941cde33568356af31", "width": 960, "height": 1219}, {"url": "https://preview.redd.it/uyaibxqzb0w91.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=caf64a6f6b1793b01a6edbcff401c0c60de07e3d", "width": 1080, "height": 1371}], "variants": {}, "id": "O4kS0UIQfAZ9V8lDKqY3BNrTkvTK0shktQPDq5dl60s"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "yde3ib", "is_robot_indexable": true, "report_reasons": null, "author": "kingdemonfalconmusic", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yde3ib/looking_for_any_feedbackcriticisms_on_my_resume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/uyaibxqzb0w91.jpg", "subreddit_subscribers": 815379, "created_utc": 1666727814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What's it like working at Amazon as a data scientist? I'm a jr data scientist with 2 yoe looking to transition to FAANG+, but with the economic situation, Amazon is the only company I'm seriously considering. I had a recruiter reach out to me last week and I wanted to hear from those who have worked in data science or machine learning engineer roles.\n\nIs the work life balance as bad as people make it seem and the way the company treats its workers?  Which orgs within Amazon are working on the most interesting data science business problems?", "author_fullname": "t2_1ns77nex", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experiences Working at Amazon Data Science Roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yda8t7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": "", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "seniorflair", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666718183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s it like working at Amazon as a data scientist? I&amp;#39;m a jr data scientist with 2 yoe looking to transition to FAANG+, but with the economic situation, Amazon is the only company I&amp;#39;m seriously considering. I had a recruiter reach out to me last week and I wanted to hear from those who have worked in data science or machine learning engineer roles.&lt;/p&gt;\n\n&lt;p&gt;Is the work life balance as bad as people make it seem and the way the company treats its workers?  Which orgs within Amazon are working on the most interesting data science business problems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Scientist MS|MBA ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yda8t7", "is_robot_indexable": true, "report_reasons": null, "author": "DJAlaskaAndrew", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/datascience/comments/yda8t7/experiences_working_at_amazon_data_science_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yda8t7/experiences_working_at_amazon_data_science_roles/", "subreddit_subscribers": 815379, "created_utc": 1666718183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi!\n\nI have managed to write a script in python (using PyCharm) that can extract data from PDF files.  \nHowever, my script uses a link to the PDF to 'open' it.\n\nNow I need to turn it into a service that can be accessed by other people online and the desired output is:  \n\n\nThey drag the PDF file and get the data + data is also stored in a database.\n\n&amp;#x200B;\n\nSeems a lil overwhelming right now and I was wondering you someone here might help me look in the right direction or online tutorials on yt maybe.\n\nThanks for any and all help in advance!", "author_fullname": "t2_fmamadvj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Turning my python script into a web with PDF file as an input", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yczqjq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666687249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I have managed to write a script in python (using PyCharm) that can extract data from PDF files.&lt;br/&gt;\nHowever, my script uses a link to the PDF to &amp;#39;open&amp;#39; it.&lt;/p&gt;\n\n&lt;p&gt;Now I need to turn it into a service that can be accessed by other people online and the desired output is:  &lt;/p&gt;\n\n&lt;p&gt;They drag the PDF file and get the data + data is also stored in a database.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Seems a lil overwhelming right now and I was wondering you someone here might help me look in the right direction or online tutorials on yt maybe.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any and all help in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yczqjq", "is_robot_indexable": true, "report_reasons": null, "author": "FoxSinofSloth", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yczqjq/turning_my_python_script_into_a_web_with_pdf_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yczqjq/turning_my_python_script_into_a_web_with_pdf_file/", "subreddit_subscribers": 815379, "created_utc": 1666687249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Apologies if this is a dumb question, but I've been doing a lot of searching and have found nothing on this topic.\n\nI am trying to put together a logistic regression model to predict the outcome of MMA fights (win or loss). (I know sorry for the sports regression).\n\nHow do I run my model without using data from future fights that have not happened yet? E.g. if the fight occurs on 1/1/22, any info on the fighters in fights that occurred after that date should not be included and every fight up to that date is included?\n\nI am working in python if there are any specific solutions I should be aware of.\n\nAppreciate any input.", "author_fullname": "t2_4ndy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Logistic Regression Model - How to include only data from events that already occurred?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ycrs1j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666659576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if this is a dumb question, but I&amp;#39;ve been doing a lot of searching and have found nothing on this topic.&lt;/p&gt;\n\n&lt;p&gt;I am trying to put together a logistic regression model to predict the outcome of MMA fights (win or loss). (I know sorry for the sports regression).&lt;/p&gt;\n\n&lt;p&gt;How do I run my model without using data from future fights that have not happened yet? E.g. if the fight occurs on 1/1/22, any info on the fighters in fights that occurred after that date should not be included and every fight up to that date is included?&lt;/p&gt;\n\n&lt;p&gt;I am working in python if there are any specific solutions I should be aware of.&lt;/p&gt;\n\n&lt;p&gt;Appreciate any input.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "ycrs1j", "is_robot_indexable": true, "report_reasons": null, "author": "tookie22", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/ycrs1j/logistic_regression_model_how_to_include_only/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/ycrs1j/logistic_regression_model_how_to_include_only/", "subreddit_subscribers": 815379, "created_utc": 1666659576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Labeling and AI companies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yd5hbc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_rsdl04ak", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "Hello folks,\n\nCan you explain please how AI companies chose to what data labeling company they should outsource their data set for labeling?  \n\nWhat metrics or things they search in data labeling companies before they pick one?\n\nplease share with your experience,\n\nThank you in advance", "author_fullname": "t2_rsdl04ak", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Labeling and AI companies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yd5fmj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666706090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks,&lt;/p&gt;\n\n&lt;p&gt;Can you explain please how AI companies chose to what data labeling company they should outsource their data set for labeling?  &lt;/p&gt;\n\n&lt;p&gt;What metrics or things they search in data labeling companies before they pick one?&lt;/p&gt;\n\n&lt;p&gt;please share with your experience,&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yd5fmj", "is_robot_indexable": true, "report_reasons": null, "author": "PayVolt", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/yd5fmj/data_labeling_and_ai_companies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/ArtificialInteligence/comments/yd5fmj/data_labeling_and_ai_companies/", "subreddit_subscribers": 87153, "created_utc": 1666706090.0, "num_crossposts": 4, "media": null, "is_video": false}], "created": 1666706212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/ArtificialInteligence/comments/yd5fmj/data_labeling_and_ai_companies/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yd5hbc", "is_robot_indexable": true, "report_reasons": null, "author": "PayVolt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_yd5fmj", "author_flair_text_color": null, "permalink": "/r/datascience/comments/yd5hbc/data_labeling_and_ai_companies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/ArtificialInteligence/comments/yd5fmj/data_labeling_and_ai_companies/", "subreddit_subscribers": 815379, "created_utc": 1666706212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "You can know someone works in user-centric-design is if their concept pitch includes the phrase \"it somehow [does X]\". And this irritates me more than it should.", "author_fullname": "t2_1wm8goh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It somehow does magic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yd250j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666696201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You can know someone works in user-centric-design is if their concept pitch includes the phrase &amp;quot;it somehow [does X]&amp;quot;. And this irritates me more than it should.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yd250j", "is_robot_indexable": true, "report_reasons": null, "author": "yourmamaman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yd250j/it_somehow_does_magic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yd250j/it_somehow_does_magic/", "subreddit_subscribers": 815379, "created_utc": 1666696201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nI have just finished my master's in business administration and started looking for my first job in data analytics or data science. I have had 6 courses about programming, and one of them was basically about the completion of 4 data science projects. My master's thesis was a great pain for me, but I managed to make it work (simulation with evolution algorithm for a popular logistics problem). I am also learning some stuff from my family on the run (my brother is doing a computer science degree, my husband is a data engineer).\nIn addition, I have speedrun one popular data science course on some online learning platform, where I also had some data science related stuff to do. I made the course quickly since I had already learned most of the stuff at the university.\n\nNow I am starting to receive refusals to my job applications. How do I go about it? Should I go after each and every company to ask why they turned me down or should I just move on with my day and keep spamming job applications? \n\nThis is so frustrating. I thought I had worked up some half decent portfolio that would reduce the number of refusals. I don't know how I should feel about this tedious recruiting process. I started applying just this weekend, but this frustration feeling is so overwhelming for me that I really start thinking I should just stop trying and quit for good. How can I cope with this situation? \n\nHonestly I feel so tired of having to work my ass off every time I have to make a next major step in my life. \n\nAny feedback is appreciated. \n\nIn case it's important, I've put together my CV in latex, it's one page, looking alright.\n\nAnother detail, which is probably important too, I am a female with two kids, and I have been working in business consulting over the last five years. I don't maintain any social media, so the recruiters probably don't know about the kids; however, in Europe we are required to disclose photos to CVs, so maybe I'm too ugly or whatever? I didn't use any beach photo of course.\n\nThank you!", "author_fullname": "t2_4aipbey1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggles with first job in data field", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yd1qsy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666694917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have just finished my master&amp;#39;s in business administration and started looking for my first job in data analytics or data science. I have had 6 courses about programming, and one of them was basically about the completion of 4 data science projects. My master&amp;#39;s thesis was a great pain for me, but I managed to make it work (simulation with evolution algorithm for a popular logistics problem). I am also learning some stuff from my family on the run (my brother is doing a computer science degree, my husband is a data engineer).\nIn addition, I have speedrun one popular data science course on some online learning platform, where I also had some data science related stuff to do. I made the course quickly since I had already learned most of the stuff at the university.&lt;/p&gt;\n\n&lt;p&gt;Now I am starting to receive refusals to my job applications. How do I go about it? Should I go after each and every company to ask why they turned me down or should I just move on with my day and keep spamming job applications? &lt;/p&gt;\n\n&lt;p&gt;This is so frustrating. I thought I had worked up some half decent portfolio that would reduce the number of refusals. I don&amp;#39;t know how I should feel about this tedious recruiting process. I started applying just this weekend, but this frustration feeling is so overwhelming for me that I really start thinking I should just stop trying and quit for good. How can I cope with this situation? &lt;/p&gt;\n\n&lt;p&gt;Honestly I feel so tired of having to work my ass off every time I have to make a next major step in my life. &lt;/p&gt;\n\n&lt;p&gt;Any feedback is appreciated. &lt;/p&gt;\n\n&lt;p&gt;In case it&amp;#39;s important, I&amp;#39;ve put together my CV in latex, it&amp;#39;s one page, looking alright.&lt;/p&gt;\n\n&lt;p&gt;Another detail, which is probably important too, I am a female with two kids, and I have been working in business consulting over the last five years. I don&amp;#39;t maintain any social media, so the recruiters probably don&amp;#39;t know about the kids; however, in Europe we are required to disclose photos to CVs, so maybe I&amp;#39;m too ugly or whatever? I didn&amp;#39;t use any beach photo of course.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "yd1qsy", "is_robot_indexable": true, "report_reasons": null, "author": "Brasilian_sandwich", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yd1qsy/struggles_with_first_job_in_data_field/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yd1qsy/struggles_with_first_job_in_data_field/", "subreddit_subscribers": 815379, "created_utc": 1666694917.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}