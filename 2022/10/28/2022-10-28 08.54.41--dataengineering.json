{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_n5n50", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It's cron all the way down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_yez0ad", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 180, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 180, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/MwYU41FHiIcEBAI85vrnfPDevtFvvInRFv0d-dk1TIc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666894200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/yaeal5qi2ew91.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/yaeal5qi2ew91.png?auto=webp&amp;s=852a00695350cf3b53d3b12f6e0b768ffb674e52", "width": 491, "height": 668}, "resolutions": [{"url": "https://preview.redd.it/yaeal5qi2ew91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd87e1377ad8483c4d7be12d098843c6e94b552d", "width": 108, "height": 146}, {"url": "https://preview.redd.it/yaeal5qi2ew91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=884cf89534e839ae088fbad7b62c86c8339fd0f0", "width": 216, "height": 293}, {"url": "https://preview.redd.it/yaeal5qi2ew91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=679d791cac63dfb5ebc616c8bcc18d16bf314d2d", "width": 320, "height": 435}], "variants": {}, "id": "4S2EpRx0f1SPFIO8tuJkQjwqwfmjqojxHLWrE24pFwU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "yez0ad", "is_robot_indexable": true, "report_reasons": null, "author": "FireflyCaptain", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yez0ad/its_cron_all_the_way_down/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/yaeal5qi2ew91.png", "subreddit_subscribers": 78118, "created_utc": 1666894200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know it varies on background, but what do you expect from a junior / medior / senior DE? What are the \"must-know\" questions based on seniority? \n\nDo you usually do live coding? If yes what kind of problems do you focus on?\n\nIf the candidate has a personal project do you care about it? Even if its a medior/senior candidate?", "author_fullname": "t2_xcba5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Technical interviewers! Based on seniority what do you usually expect from candidates?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yesj6s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666878728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know it varies on background, but what do you expect from a junior / medior / senior DE? What are the &amp;quot;must-know&amp;quot; questions based on seniority? &lt;/p&gt;\n\n&lt;p&gt;Do you usually do live coding? If yes what kind of problems do you focus on?&lt;/p&gt;\n\n&lt;p&gt;If the candidate has a personal project do you care about it? Even if its a medior/senior candidate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "yesj6s", "is_robot_indexable": true, "report_reasons": null, "author": "mackbenc", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yesj6s/technical_interviewers_based_on_seniority_what_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yesj6s/technical_interviewers_based_on_seniority_what_do/", "subreddit_subscribers": 78118, "created_utc": 1666878728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to reuse TaskGroups in Airflow and make better DAGs!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 115, "top_awarded_type": null, "hide_score": false, "name": "t3_yet4s9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kQej6qOOo8JGH9ju51v0FDrUiUDJVpdZ3-F0OA8Z6Xk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666880210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/feed/update/urn:li:activity:6991398253325869056/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wjBoxa4Xip5X_Ad91SVIwaPQTeC0x7_DdtkeJbEE4UM.jpg?auto=webp&amp;s=a0763b5976d29e8e4384e2962bc4b36f4bcc686a", "width": 1070, "height": 884}, "resolutions": [{"url": "https://external-preview.redd.it/wjBoxa4Xip5X_Ad91SVIwaPQTeC0x7_DdtkeJbEE4UM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=200ec977c8639c6dbac5d6f33a9ef1a21441e02c", "width": 108, "height": 89}, {"url": "https://external-preview.redd.it/wjBoxa4Xip5X_Ad91SVIwaPQTeC0x7_DdtkeJbEE4UM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3047e22b937f4b8151b527b23aa77afd4eb06997", "width": 216, "height": 178}, {"url": "https://external-preview.redd.it/wjBoxa4Xip5X_Ad91SVIwaPQTeC0x7_DdtkeJbEE4UM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5eec2d89144e34c161e689b3da410e9aa830fd8", "width": 320, "height": 264}, {"url": "https://external-preview.redd.it/wjBoxa4Xip5X_Ad91SVIwaPQTeC0x7_DdtkeJbEE4UM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0929bb8f062b909c1a14b85e6c84046f6d875afc", "width": 640, "height": 528}, {"url": "https://external-preview.redd.it/wjBoxa4Xip5X_Ad91SVIwaPQTeC0x7_DdtkeJbEE4UM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aef87130e11caa93c395a6fc3425dbd43080da0d", "width": 960, "height": 793}], "variants": {}, "id": "0Tq7PdBUStgC5qLm6ITurkrMJDr7r6CwK2viNt1k3dY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yet4s9", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yet4s9/how_to_reuse_taskgroups_in_airflow_and_make/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/feed/update/urn:li:activity:6991398253325869056/", "subreddit_subscribers": 78118, "created_utc": 1666880210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to ask this, I have read about batch processing and stream processing at a high level but in my project, I am doing data ingestion from an OLTP system during its freeze time. this is batch processing right??", "author_fullname": "t2_6f6khk66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data ingestion from an OLTP system", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yeqbm0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666872683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to ask this, I have read about batch processing and stream processing at a high level but in my project, I am doing data ingestion from an OLTP system during its freeze time. this is batch processing right??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yeqbm0", "is_robot_indexable": true, "report_reasons": null, "author": "mainak17", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yeqbm0/data_ingestion_from_an_oltp_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yeqbm0/data_ingestion_from_an_oltp_system/", "subreddit_subscribers": 78118, "created_utc": 1666872683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In Berlin, what should be the salary expectation for someone who has been working as a DE for 3 years and before that as Backend Dev 3 more years? \n\nI mostly worked at small companies where the role required both a bit backend and data experience, I fit in this pretty well. For example at my current place we work with Neo4j, ElasticSearch and GraphQL, separate for different tenants/customers. I am responsible for all the steps for integration of a new data source, from analysis and modelling to pushing, to exposing the data via ES and GraphQL API. We do ETLs on Databricks with PySpark and Pandas. I don't manage teams but I do guide one or two juniors we have in my team.\n\nSo far I am quite okay with my current pay but I feel like I am constantly underselling myself specially because I suck at negotiations.", "author_fullname": "t2_12lkky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Salary expectations for Berlin, Germany", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yewd6n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666888050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In Berlin, what should be the salary expectation for someone who has been working as a DE for 3 years and before that as Backend Dev 3 more years? &lt;/p&gt;\n\n&lt;p&gt;I mostly worked at small companies where the role required both a bit backend and data experience, I fit in this pretty well. For example at my current place we work with Neo4j, ElasticSearch and GraphQL, separate for different tenants/customers. I am responsible for all the steps for integration of a new data source, from analysis and modelling to pushing, to exposing the data via ES and GraphQL API. We do ETLs on Databricks with PySpark and Pandas. I don&amp;#39;t manage teams but I do guide one or two juniors we have in my team.&lt;/p&gt;\n\n&lt;p&gt;So far I am quite okay with my current pay but I feel like I am constantly underselling myself specially because I suck at negotiations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yewd6n", "is_robot_indexable": true, "report_reasons": null, "author": "ratulotron", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yewd6n/salary_expectations_for_berlin_germany/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yewd6n/salary_expectations_for_berlin_germany/", "subreddit_subscribers": 78118, "created_utc": 1666888050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Will put the sign up link in the comments. You'll need to apply to join their Slack org, then check the #oa-book-club channel to review dates and other information.", "author_fullname": "t2_4041g9mz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Operational Analytics (Club) Book Club: Fundamentals of Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yf34ge", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666904151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Will put the sign up link in the comments. You&amp;#39;ll need to apply to join their Slack org, then check the #oa-book-club channel to review dates and other information.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yf34ge", "is_robot_indexable": true, "report_reasons": null, "author": "aamoscodes", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yf34ge/operational_analytics_club_book_club_fundamentals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yf34ge/operational_analytics_club_book_club_fundamentals/", "subreddit_subscribers": 78118, "created_utc": 1666904151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Today we used requests and os libraries to write object oriented programs that help us to fetch data from the web (html files) and store them in a temp folder. We put these tasks in apache airflow and saw how airflow schedules these tasks to fetch web data. Here is the video https://www.youtube.com/watch?v=NvU5kxgtUpE", "author_fullname": "t2_ck47kwls", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "using airflow to fetch web data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yf4seh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1666908085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today we used requests and os libraries to write object oriented programs that help us to fetch data from the web (html files) and store them in a temp folder. We put these tasks in apache airflow and saw how airflow schedules these tasks to fetch web data. Here is the video &lt;a href=\"https://www.youtube.com/watch?v=NvU5kxgtUpE\"&gt;https://www.youtube.com/watch?v=NvU5kxgtUpE&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1e25_scX4wJ_YpFSTVZczuYMCTsmW9UHt9TpSMsPODw.jpg?auto=webp&amp;s=1113a2c334403c48e65782a13567f1a6acbb818e", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/1e25_scX4wJ_YpFSTVZczuYMCTsmW9UHt9TpSMsPODw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a653a99d1b5d5c137de5483d47ff0f7defe6f857", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/1e25_scX4wJ_YpFSTVZczuYMCTsmW9UHt9TpSMsPODw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=341ec7eb87d1c4e59481c7316495ffd986decbbc", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/1e25_scX4wJ_YpFSTVZczuYMCTsmW9UHt9TpSMsPODw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c02396ddc791a3cc73e936e7c4cbaf0f70a83f1", "width": 320, "height": 240}], "variants": {}, "id": "eE41GQD9d0XY9OK9AaKrtI1yeFDpGfgy-bQXha-6oVM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yf4seh", "is_robot_indexable": true, "report_reasons": null, "author": "DaliCodes", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yf4seh/using_airflow_to_fetch_web_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yf4seh/using_airflow_to_fetch_web_data/", "subreddit_subscribers": 78118, "created_utc": 1666908085.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \n\n  \nI am converting a unix time column to a datetime human readable format, and while using the function below I have a weird behaviour.  \n\nThe column creationDate contains unix stamps like : `1666828800032`\n\nWhat is also weird is the year is the only anomaly. the rest is fine.\n\nUsing external conversion tools showed that the original format is correct.\n\nThanks!  \n\n\nhttps://preview.redd.it/2olqs5azcdw91.png?width=600&amp;format=png&amp;auto=webp&amp;s=a558bbd1c23bd076c08598c2d5ac2b64853f7b56", "author_fullname": "t2_o1lpjxry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark from_unixtime year not working as expected.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 130, "top_awarded_type": null, "hide_score": false, "media_metadata": {"2olqs5azcdw91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 100, "x": 108, "u": "https://preview.redd.it/2olqs5azcdw91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ddf7e7485806641393a4da2538164c318b750bc5"}, {"y": 200, "x": 216, "u": "https://preview.redd.it/2olqs5azcdw91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d13e56eda982a17567ee7faae55903d133a8386e"}, {"y": 297, "x": 320, "u": "https://preview.redd.it/2olqs5azcdw91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=45c94555e8c1eaf762319ea04a440fb4dea817e8"}], "s": {"y": 558, "x": 600, "u": "https://preview.redd.it/2olqs5azcdw91.png?width=600&amp;format=png&amp;auto=webp&amp;s=a558bbd1c23bd076c08598c2d5ac2b64853f7b56"}, "id": "2olqs5azcdw91"}}, "name": "t3_yevc6n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1QsNuqylwhtXWrMSxGyb7QH1hchOGlSwEnWhrnhiVl8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666885543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I am converting a unix time column to a datetime human readable format, and while using the function below I have a weird behaviour.  &lt;/p&gt;\n\n&lt;p&gt;The column creationDate contains unix stamps like : &lt;code&gt;1666828800032&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;What is also weird is the year is the only anomaly. the rest is fine.&lt;/p&gt;\n\n&lt;p&gt;Using external conversion tools showed that the original format is correct.&lt;/p&gt;\n\n&lt;p&gt;Thanks!  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2olqs5azcdw91.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a558bbd1c23bd076c08598c2d5ac2b64853f7b56\"&gt;https://preview.redd.it/2olqs5azcdw91.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a558bbd1c23bd076c08598c2d5ac2b64853f7b56&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yevc6n", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Freedom9865", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yevc6n/pyspark_from_unixtime_year_not_working_as_expected/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yevc6n/pyspark_from_unixtime_year_not_working_as_expected/", "subreddit_subscribers": 78118, "created_utc": 1666885543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the difference between a data engineer and etl developer? What would be the differentiating use of tools/languages? What methodologies/best practices should either know? I am aware there is often overlap of responsibilities/similarities in tasks between the two, but need to know if a role I applied to internally should be titled ETL developer or am right in asking for the role to be titled Data Engineer instead.", "author_fullname": "t2_3h5wixaf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer vs ETL developer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yezrg0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666896029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the difference between a data engineer and etl developer? What would be the differentiating use of tools/languages? What methodologies/best practices should either know? I am aware there is often overlap of responsibilities/similarities in tasks between the two, but need to know if a role I applied to internally should be titled ETL developer or am right in asking for the role to be titled Data Engineer instead.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yezrg0", "is_robot_indexable": true, "report_reasons": null, "author": "J0hnDutt00n", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yezrg0/data_engineer_vs_etl_developer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yezrg0/data_engineer_vs_etl_developer/", "subreddit_subscribers": 78118, "created_utc": 1666896029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! Curious to know how folks in the community address corrections and adjustments in datapoints in their data pipelines. \n\nExample is if a sensor returns a wild value and you want to null that value, or replace it with something else, where does that sit in your process. \n\nI\u2019ve got an excel file right now that matches a few fields up and corrects it that way, but I don\u2019t think it\u2019s scales very well. Basically have to reprocess the entire pipeline (ETL) to reflect the changes. \n\nThoughts?", "author_fullname": "t2_ahu1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Correcting data in pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yermwm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666876344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Curious to know how folks in the community address corrections and adjustments in datapoints in their data pipelines. &lt;/p&gt;\n\n&lt;p&gt;Example is if a sensor returns a wild value and you want to null that value, or replace it with something else, where does that sit in your process. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve got an excel file right now that matches a few fields up and corrects it that way, but I don\u2019t think it\u2019s scales very well. Basically have to reprocess the entire pipeline (ETL) to reflect the changes. &lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yermwm", "is_robot_indexable": true, "report_reasons": null, "author": "Namur007", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yermwm/correcting_data_in_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yermwm/correcting_data_in_pipeline/", "subreddit_subscribers": 78118, "created_utc": 1666876344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "1. Create Iceberg Catalog\n2. Create Iceberg Table\n3. Insert records into table", "author_fullname": "t2_ce2ldlob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video: 2 minute demonstration of how to get started with Iceberg tables in Dremio Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_yeqeef", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/qyoi0hgfbcw91/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/qyoi0hgfbcw91/DASH_96.mp4", "dash_url": "https://v.redd.it/qyoi0hgfbcw91/DASHPlaylist.mpd?a=1669539281%2CYmFmZWRjNjRhN2UwNDI2MTFiYThlYjVjMjgxNmY2NzI5MTNlZjdiZWQ4ZjFhNDQwNzRiYmUzOTJiNDQ5NjIxYg%3D%3D&amp;v=1&amp;f=sd", "duration": 141, "hls_url": "https://v.redd.it/qyoi0hgfbcw91/HLSPlaylist.m3u8?a=1669539281%2CZTY1ODA2N2Q0ODM1N2NiYTEwNGUxOTIxODFhOTJmYzQzM2FjYWZmOTE3MWM3NDE2MTlhNDcyNmMzNDUyZjU2OQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/HDN-bKsKkKwNMcWMnGUXXD7dPmMrKubBurEYWuqDEY8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666872911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;Create Iceberg Catalog&lt;/li&gt;\n&lt;li&gt;Create Iceberg Table&lt;/li&gt;\n&lt;li&gt;Insert records into table&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/qyoi0hgfbcw91", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?format=pjpg&amp;auto=webp&amp;s=f5bcc0925bdd28b75394e336614e675e0a960d11", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=98edb62f8b5c3c43a08cef522e186022210c8512", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=66672738d1777cadab320c12a1610bd061522088", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9ae9292e417d21e9e14590df37fa779875b2656c", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2c46a75476d46040d8accc03b605fa85eec7aff7", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=94c86fe0248ddb4e98fbf19abb0475b36d8b1ff7", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3f64f552197572eac8f07cc340947566596f257c", "width": 1080, "height": 607}], "variants": {}, "id": "qQtKOSP1_UbVF1Z1wBujWLI9flsKMyJ3n30SqjAE-Dg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yeqeef", "is_robot_indexable": true, "report_reasons": null, "author": "amdatalakehouse", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yeqeef/video_2_minute_demonstration_of_how_to_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/qyoi0hgfbcw91", "subreddit_subscribers": 78118, "created_utc": 1666872911.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/qyoi0hgfbcw91/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/qyoi0hgfbcw91/DASH_96.mp4", "dash_url": "https://v.redd.it/qyoi0hgfbcw91/DASHPlaylist.mpd?a=1669539281%2CYmFmZWRjNjRhN2UwNDI2MTFiYThlYjVjMjgxNmY2NzI5MTNlZjdiZWQ4ZjFhNDQwNzRiYmUzOTJiNDQ5NjIxYg%3D%3D&amp;v=1&amp;f=sd", "duration": 141, "hls_url": "https://v.redd.it/qyoi0hgfbcw91/HLSPlaylist.m3u8?a=1669539281%2CZTY1ODA2N2Q0ODM1N2NiYTEwNGUxOTIxODFhOTJmYzQzM2FjYWZmOTE3MWM3NDE2MTlhNDcyNmMzNDUyZjU2OQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I'm just starting to get exposure into AWS and I'm having a hard time understanding if an architecture that was handed to me makes any sense. Wanted to hear people's takes on how they would approach this problem.\n\nThere is a snowflake datalake that contains data representing the back end of an existing application.\n\nOur team is replacing that application, and as part of that process we need to hydrate our existing our new application database (dynamoDB) with the old data.\n\nThe data model has shifted between the two applications, but in theory we have a mapping between the old application data data and the api endpoints of the new application  (not the new application data model).\n\nThe proposal was to use AWS Glue to extract data from Snowflake into S3, and from S3 run a lambda or further leverage Glue to transform the data and feed it into the API. Hitting the dynamoDB backend directly was rejected due to some backend work that occurs in the new data model.\n\nThis would be a one time activity with the caveat being that there would need to be some delta loading while the current application is shut down and replaced with the new application.\n\nThe proposed solution so far has been use AWS Glue -&gt; S3 (Read from SFLK into S3) and then use lambdas or some other kind of compute to decompose the tables into individual api calls.\n\nThere are obvious red flags with hitting the API with that much load, as well as how slow it would be to decompose a table into rows and feed those piecemeal into an API.\n\nMy current thoughts after trying to absorb all of this is to try to get Snowflake to directly COPY that into S3 and from there try persuading the API team to expose some kind of batch endpoint or just perform a proper mapping from the old DB to DynamoDB (which would be challenging because the old DB was a relational model).\n\nAny help or experience in the same vein would be gratefully appreciated!", "author_fullname": "t2_6d49ikq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on hydrating an AWS hosted API with data from Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yf736o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666913648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m just starting to get exposure into AWS and I&amp;#39;m having a hard time understanding if an architecture that was handed to me makes any sense. Wanted to hear people&amp;#39;s takes on how they would approach this problem.&lt;/p&gt;\n\n&lt;p&gt;There is a snowflake datalake that contains data representing the back end of an existing application.&lt;/p&gt;\n\n&lt;p&gt;Our team is replacing that application, and as part of that process we need to hydrate our existing our new application database (dynamoDB) with the old data.&lt;/p&gt;\n\n&lt;p&gt;The data model has shifted between the two applications, but in theory we have a mapping between the old application data data and the api endpoints of the new application  (not the new application data model).&lt;/p&gt;\n\n&lt;p&gt;The proposal was to use AWS Glue to extract data from Snowflake into S3, and from S3 run a lambda or further leverage Glue to transform the data and feed it into the API. Hitting the dynamoDB backend directly was rejected due to some backend work that occurs in the new data model.&lt;/p&gt;\n\n&lt;p&gt;This would be a one time activity with the caveat being that there would need to be some delta loading while the current application is shut down and replaced with the new application.&lt;/p&gt;\n\n&lt;p&gt;The proposed solution so far has been use AWS Glue -&amp;gt; S3 (Read from SFLK into S3) and then use lambdas or some other kind of compute to decompose the tables into individual api calls.&lt;/p&gt;\n\n&lt;p&gt;There are obvious red flags with hitting the API with that much load, as well as how slow it would be to decompose a table into rows and feed those piecemeal into an API.&lt;/p&gt;\n\n&lt;p&gt;My current thoughts after trying to absorb all of this is to try to get Snowflake to directly COPY that into S3 and from there try persuading the API team to expose some kind of batch endpoint or just perform a proper mapping from the old DB to DynamoDB (which would be challenging because the old DB was a relational model).&lt;/p&gt;\n\n&lt;p&gt;Any help or experience in the same vein would be gratefully appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yf736o", "is_robot_indexable": true, "report_reasons": null, "author": "poppinstacks", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yf736o/thoughts_on_hydrating_an_aws_hosted_api_with_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yf736o/thoughts_on_hydrating_an_aws_hosted_api_with_data/", "subreddit_subscribers": 78118, "created_utc": 1666913648.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an interview coming up with Tesla for DE and wanted to know if anyone here has given interviews at Tesla for DE. Would really help :)", "author_fullname": "t2_ahi836bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tesla Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yf23eu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666901657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an interview coming up with Tesla for DE and wanted to know if anyone here has given interviews at Tesla for DE. Would really help :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yf23eu", "is_robot_indexable": true, "report_reasons": null, "author": "yyforthewin", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yf23eu/tesla_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yf23eu/tesla_data_engineering/", "subreddit_subscribers": 78118, "created_utc": 1666901657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need a solution that scales and shrinks as needed.", "author_fullname": "t2_tfwkz0xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hey! Any tips on how I can build a Webhook on Google Cloud Run that receives JSON POST requests, do a little transformation and store them into a Google MySQL DB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yewc5i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666887956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need a solution that scales and shrinks as needed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yewc5i", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Object_7904", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yewc5i/hey_any_tips_on_how_i_can_build_a_webhook_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yewc5i/hey_any_tips_on_how_i_can_build_a_webhook_on/", "subreddit_subscribers": 78118, "created_utc": 1666887956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in the early stages of building out a data model / data warehouse in my company. We are loosely following star schema design patterns. \n\nOne of the common analyses our users need is sales / orders by new and repeat users at all levels in our product hierarchy.\n\nIs anyone aware of any design patterns or approaches we could use to model our data?", "author_fullname": "t2_128qvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modelling new and repeat customers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yextvb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666891360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in the early stages of building out a data model / data warehouse in my company. We are loosely following star schema design patterns. &lt;/p&gt;\n\n&lt;p&gt;One of the common analyses our users need is sales / orders by new and repeat users at all levels in our product hierarchy.&lt;/p&gt;\n\n&lt;p&gt;Is anyone aware of any design patterns or approaches we could use to model our data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yextvb", "is_robot_indexable": true, "report_reasons": null, "author": "mathewtrivett", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yextvb/modelling_new_and_repeat_customers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yextvb/modelling_new_and_repeat_customers/", "subreddit_subscribers": 78118, "created_utc": 1666891360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Analytics and Machine Learning Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_yem6ft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HZ6A6FZR12gQEweQfgjsgYZedZfX94MkQLRMW-LG0xM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666858439.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/gooddata-developers/cooperation-between-data-analytics-and-machine-learning-54ffb047cf20", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?auto=webp&amp;s=e7e9a3f526d2ea9aec7a0bd9d23008daac98cfca", "width": 1200, "height": 674}, "resolutions": [{"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1c9972279d5e784c0f55ae4d020c937480fc269", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6f9b48bb241c6d0df7fd64ef4f7ae9fbcce28223", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b32f57ad7deb5067ede595d7650a208b0a7904fe", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f2a34a2e220039250cab0108c31e617a40e3991", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9931e08b2e33079bcfda16c7bc1ae08fab7de3f6", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b4e845936cae4b532e14f205736887a5f163736", "width": 1080, "height": 606}], "variants": {}, "id": "1j7JSz5-gepwKFTbsjTMWqzYcHRUTKEnkYetTXyg8JY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yem6ft", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yem6ft/data_analytics_and_machine_learning_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/gooddata-developers/cooperation-between-data-analytics-and-machine-learning-54ffb047cf20", "subreddit_subscribers": 78118, "created_utc": 1666858439.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a web activity in ADF which returns a JSON object, part of the structure of the object is like this:\n\n    {\n        \"links\": [\n            {\n                \"rel\": \"next\",\n                \"href\": \"https://some-service?limit=10&amp;offset=10\"\n            },\n            {\n                \"rel\": \"last\",\n                \"href\": \"https://some-service?limit=10&amp;offset=9990\"\n            },\n            {\n                \"rel\": \"self\",\n                \"href\": \"https://some-service?limit=10&amp;offset=0\"\n            }\n        ]\n\nI want to run a set variable task after this one and set the variable to the value of the \"next\" URL, it's part of a loop. \n\nThe problem is I don't know how to address that part of the JSON in the set variable task. I know I could go  \n\n    @activity('Web1').output.links[0].href \n\nBut the problem is that after the first call two more elements are added to the list \"first\" and \"previous\".\n\nWhat I really want to do is dynamically look for the entry where rel = \"next\"\n\nBut I'm not sure how to code that. Any thoughts?", "author_fullname": "t2_vj5jo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to access json items in list with the same name", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yeu808", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666882837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a web activity in ADF which returns a JSON object, part of the structure of the object is like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n    &amp;quot;links&amp;quot;: [\n        {\n            &amp;quot;rel&amp;quot;: &amp;quot;next&amp;quot;,\n            &amp;quot;href&amp;quot;: &amp;quot;https://some-service?limit=10&amp;amp;offset=10&amp;quot;\n        },\n        {\n            &amp;quot;rel&amp;quot;: &amp;quot;last&amp;quot;,\n            &amp;quot;href&amp;quot;: &amp;quot;https://some-service?limit=10&amp;amp;offset=9990&amp;quot;\n        },\n        {\n            &amp;quot;rel&amp;quot;: &amp;quot;self&amp;quot;,\n            &amp;quot;href&amp;quot;: &amp;quot;https://some-service?limit=10&amp;amp;offset=0&amp;quot;\n        }\n    ]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I want to run a set variable task after this one and set the variable to the value of the &amp;quot;next&amp;quot; URL, it&amp;#39;s part of a loop. &lt;/p&gt;\n\n&lt;p&gt;The problem is I don&amp;#39;t know how to address that part of the JSON in the set variable task. I know I could go  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@activity(&amp;#39;Web1&amp;#39;).output.links[0].href \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But the problem is that after the first call two more elements are added to the list &amp;quot;first&amp;quot; and &amp;quot;previous&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;What I really want to do is dynamically look for the entry where rel = &amp;quot;next&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m not sure how to code that. Any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yeu808", "is_robot_indexable": true, "report_reasons": null, "author": "Enigma1984", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yeu808/how_to_access_json_items_in_list_with_the_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yeu808/how_to_access_json_items_in_list_with_the_same/", "subreddit_subscribers": 78118, "created_utc": 1666882837.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}