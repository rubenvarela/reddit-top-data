{"kind": "Listing", "data": {"after": "t3_yerqya", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I understand how to perform PCA , and why it's done and the theory behind it and how features are reduced in lower dimensional using eigen vectors and how to normalize the data before finding PCA. \n\nMy question is: in Linear Regression (LR), if I have (say)10 features then my LR looks like this y=c1x1+c2x2+.....+ c10x10. If I reduce my features to (say) 2 components then my LR looks like y=C1PC1+C2PC2 (where C = constant &amp; PC = principle components).\n\nhow is this equation useful to me because now my y is represented in terms of Principle components (PC) instead of actual variables. \n\nI haven't been able to find this answer online. Please help.", "author_fullname": "t2_13t60b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can someone please explain what to do next after getting PCA (Principle component analysis)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yex3pq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 264, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 264, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_1": 1}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666889642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand how to perform PCA , and why it&amp;#39;s done and the theory behind it and how features are reduced in lower dimensional using eigen vectors and how to normalize the data before finding PCA. &lt;/p&gt;\n\n&lt;p&gt;My question is: in Linear Regression (LR), if I have (say)10 features then my LR looks like this y=c1x1+c2x2+.....+ c10x10. If I reduce my features to (say) 2 components then my LR looks like y=C1PC1+C2PC2 (where C = constant &amp;amp; PC = principle components).&lt;/p&gt;\n\n&lt;p&gt;how is this equation useful to me because now my y is represented in terms of Principle components (PC) instead of actual variables. &lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t been able to find this answer online. Please help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 100, "id": "gid_1", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Shows the Silver Award... and that's it.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Silver", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yex3pq", "is_robot_indexable": true, "report_reasons": null, "author": "drugsarebadmky", "discussion_type": null, "num_comments": 108, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yex3pq/can_someone_please_explain_what_to_do_next_after/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yex3pq/can_someone_please_explain_what_to_do_next_after/", "subreddit_subscribers": 815895, "created_utc": 1666889642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a working junior data scientist. I am in a Masters in Data Science program as well as self studying data science technologies on the side. However, it is overwhelming to give focus to both + work full time, so I need help deciding how to split my time, or if I should split my time at all. \n\nHow should I split my time between MS courses and self studying the modern data technologies and frameworks being used in the real world?", "author_fullname": "t2_dbas4m3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I split my time between my studying my Masters in DS program and self studying?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yf7ahw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 69, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 69, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666914158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a working junior data scientist. I am in a Masters in Data Science program as well as self studying data science technologies on the side. However, it is overwhelming to give focus to both + work full time, so I need help deciding how to split my time, or if I should split my time at all. &lt;/p&gt;\n\n&lt;p&gt;How should I split my time between MS courses and self studying the modern data technologies and frameworks being used in the real world?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yf7ahw", "is_robot_indexable": true, "report_reasons": null, "author": "Proof_Hyena4223", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yf7ahw/how_should_i_split_my_time_between_my_studying_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yf7ahw/how_should_i_split_my_time_between_my_studying_my/", "subreddit_subscribers": 815895, "created_utc": 1666914158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys so I'm a month in at my new job as a data engineer. There are so many initiatives and so much data. I'm very new to this kind of thing to be honest with you and was hired for my general knowledge of key databases and some coding skills that I just kind of picked up at my previous role at the same company. So we are running this model in sas and they already put me in charge of data governance. I have meetings with our consultants but I basically need to drive them and ensure they are doing what they are supposed to be doing. \n\nSo I guess my question is what should I be focusing on? What are the typical key metrics that you use to validate the model, the input output data? What can I do to be successful at this? \n\nIt's a lot of new info so very overwhelming, any help is very appreciated. \n\nThank youu!!!!", "author_fullname": "t2_a8yt6gr6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "new to ml ops, having trouble", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yf6zs0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666913415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys so I&amp;#39;m a month in at my new job as a data engineer. There are so many initiatives and so much data. I&amp;#39;m very new to this kind of thing to be honest with you and was hired for my general knowledge of key databases and some coding skills that I just kind of picked up at my previous role at the same company. So we are running this model in sas and they already put me in charge of data governance. I have meetings with our consultants but I basically need to drive them and ensure they are doing what they are supposed to be doing. &lt;/p&gt;\n\n&lt;p&gt;So I guess my question is what should I be focusing on? What are the typical key metrics that you use to validate the model, the input output data? What can I do to be successful at this? &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a lot of new info so very overwhelming, any help is very appreciated. &lt;/p&gt;\n\n&lt;p&gt;Thank youu!!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yf6zs0", "is_robot_indexable": true, "report_reasons": null, "author": "MembershipNice2192", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yf6zs0/new_to_ml_ops_having_trouble/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yf6zs0/new_to_ml_ops_having_trouble/", "subreddit_subscribers": 815895, "created_utc": 1666913415.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hi, \n\nI've just published my recent article titled \"**The baseline for Precision-Recall curve: A Bayesian approach\".** \n\nThis article is a review of the nuts and bolts \ud83d\udd29 of Precision-Recall curve. Demystifying its baseline mathematically and empirically using some fun\ud83c\udf88**Python** codes for better understanding.  \n\n\nlink: [https://itnext.io/the-baseline-for-precision-recall-curve-a-bayesian-approach-1611c690607](https://itnext.io/the-baseline-for-precision-recall-curve-a-bayesian-approach-1611c690607)", "author_fullname": "t2_69i5g4ss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The baseline for Precision-Recall curve: A Bayesian approach", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yeviy5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1666885989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just published my recent article titled &amp;quot;&lt;strong&gt;The baseline for Precision-Recall curve: A Bayesian approach&amp;quot;.&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;This article is a review of the nuts and bolts \ud83d\udd29 of Precision-Recall curve. Demystifying its baseline mathematically and empirically using some fun\ud83c\udf88&lt;strong&gt;Python&lt;/strong&gt; codes for better understanding.  &lt;/p&gt;\n\n&lt;p&gt;link: &lt;a href=\"https://itnext.io/the-baseline-for-precision-recall-curve-a-bayesian-approach-1611c690607\"&gt;https://itnext.io/the-baseline-for-precision-recall-curve-a-bayesian-approach-1611c690607&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/F57Nwp4h7vdlKYYnI-ZxsRA61Ea7_hbfYggGZVc5c_A.jpg?auto=webp&amp;s=fdc4fcfcf9c72a528071c7d8843d4c818c55fb18", "width": 1200, "height": 515}, "resolutions": [{"url": "https://external-preview.redd.it/F57Nwp4h7vdlKYYnI-ZxsRA61Ea7_hbfYggGZVc5c_A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ac8441b5f7e94a0dae0b357609e1dda0ddbfdaec", "width": 108, "height": 46}, {"url": "https://external-preview.redd.it/F57Nwp4h7vdlKYYnI-ZxsRA61Ea7_hbfYggGZVc5c_A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d976fa8c68593b3336a6e15a9496b0afce6e3aef", "width": 216, "height": 92}, {"url": "https://external-preview.redd.it/F57Nwp4h7vdlKYYnI-ZxsRA61Ea7_hbfYggGZVc5c_A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fe7c60d55379e8db1a49731ee3ec57ee3142d0f8", "width": 320, "height": 137}, {"url": "https://external-preview.redd.it/F57Nwp4h7vdlKYYnI-ZxsRA61Ea7_hbfYggGZVc5c_A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7b95720527e1a78613f9caf2ce171f2b53bf7aee", "width": 640, "height": 274}, {"url": "https://external-preview.redd.it/F57Nwp4h7vdlKYYnI-ZxsRA61Ea7_hbfYggGZVc5c_A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0500a44fedc7b23fc645d5b6338c6665ee751a55", "width": 960, "height": 412}, {"url": "https://external-preview.redd.it/F57Nwp4h7vdlKYYnI-ZxsRA61Ea7_hbfYggGZVc5c_A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fbedd1028edba9e7c245801c0fd0bf2a3302d192", "width": 1080, "height": 463}], "variants": {}, "id": "GAsNMjcpg4eCcxJSspR7kI3t_WueG7Tpsw39Rsa1ZWU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yeviy5", "is_robot_indexable": true, "report_reasons": null, "author": "s_arme", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yeviy5/the_baseline_for_precisionrecall_curve_a_bayesian/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yeviy5/the_baseline_for_precisionrecall_curve_a_bayesian/", "subreddit_subscribers": 815895, "created_utc": 1666885989.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_30o1hb10", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Predictive analytics in Venture investing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_yesu7y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vdJiMiIVinGYL0Iy_mb-tFzBvTmfr6PnNGPUZ6pKvAM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666879488.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "parsers.vc", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://parsers.vc/blog/predictive-analytics-in-venture-investing/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oe8oczGXfZb5rpoM5ieKJfNllZql8qPUEEf5aYvYvWY.jpg?auto=webp&amp;s=2173ad66f527e425c033ff88d2a944c9918e7177", "width": 1104, "height": 736}, "resolutions": [{"url": "https://external-preview.redd.it/oe8oczGXfZb5rpoM5ieKJfNllZql8qPUEEf5aYvYvWY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc6f2d5745de9e6c67128e3de3d1dc5479dc17e9", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/oe8oczGXfZb5rpoM5ieKJfNllZql8qPUEEf5aYvYvWY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2857a33558c222328a235f88b705a9622feb195e", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/oe8oczGXfZb5rpoM5ieKJfNllZql8qPUEEf5aYvYvWY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=529c3585884372e23cdcf13f287ebff500fb41e7", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/oe8oczGXfZb5rpoM5ieKJfNllZql8qPUEEf5aYvYvWY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d89057078b02ce088c55357cfe6af60c29c5ce6", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/oe8oczGXfZb5rpoM5ieKJfNllZql8qPUEEf5aYvYvWY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7dadd17ef98fe2522236fc76d01a25733f774fa3", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/oe8oczGXfZb5rpoM5ieKJfNllZql8qPUEEf5aYvYvWY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1ff0d81311f0db15edb0982c738c731cf571cf0f", "width": 1080, "height": 720}], "variants": {}, "id": "x4e4t3q1y1fhczqKO2Mh5jJWbb6BvXFs0A1NzCTGjC8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "yesu7y", "is_robot_indexable": true, "report_reasons": null, "author": "Gill_Chloet", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yesu7y/predictive_analytics_in_venture_investing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://parsers.vc/blog/predictive-analytics-in-venture-investing/", "subreddit_subscribers": 815895, "created_utc": 1666879488.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a tad confused and a noob, please be patient.\n\nI was watching one of my co-workers analyse and visualise some data and he said he was going to normalise the data. For normalisation I normally used the mean, standard deviation and standarise the data using those.\n\nHowever, this is what he did:\n\nCalculated the regression equation.\n\nUsing the independent variable coefficient and the intercept (from the previous step) he calculated the predicted y.\n\nProceeded to substract the actual y against the predicted y and then add the average of all of the predicted y.\n\nWhat essentially happened is that he got rid of the independent variable coefficient (if you were to graph it, the trendline would be flat). Now:\n\nIS this actually normalising data or is it integration? if neither, what would this be? and why should I do it?\n\nI am asking since he had to go.", "author_fullname": "t2_q2cte", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple question about normalising and integrating data.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfe464", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666931955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a tad confused and a noob, please be patient.&lt;/p&gt;\n\n&lt;p&gt;I was watching one of my co-workers analyse and visualise some data and he said he was going to normalise the data. For normalisation I normally used the mean, standard deviation and standarise the data using those.&lt;/p&gt;\n\n&lt;p&gt;However, this is what he did:&lt;/p&gt;\n\n&lt;p&gt;Calculated the regression equation.&lt;/p&gt;\n\n&lt;p&gt;Using the independent variable coefficient and the intercept (from the previous step) he calculated the predicted y.&lt;/p&gt;\n\n&lt;p&gt;Proceeded to substract the actual y against the predicted y and then add the average of all of the predicted y.&lt;/p&gt;\n\n&lt;p&gt;What essentially happened is that he got rid of the independent variable coefficient (if you were to graph it, the trendline would be flat). Now:&lt;/p&gt;\n\n&lt;p&gt;IS this actually normalising data or is it integration? if neither, what would this be? and why should I do it?&lt;/p&gt;\n\n&lt;p&gt;I am asking since he had to go.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yfe464", "is_robot_indexable": true, "report_reasons": null, "author": "manuelgcg", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yfe464/simple_question_about_normalising_and_integrating/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yfe464/simple_question_about_normalising_and_integrating/", "subreddit_subscribers": 815895, "created_utc": 1666931955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_2xjdvpun", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "kaggle is wild (\u2060\u30fb\u2060o\u2060\u30fb\u2060)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_yfnbab", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JshsXvQAazpuSWBL8tKtnyIMsS3vQo3IAhN5_X602OM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666961386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/especdi14lw91.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/especdi14lw91.png?auto=webp&amp;s=66ef51efdccb64609e9121272ab8a805ea32f190", "width": 1080, "height": 1473}, "resolutions": [{"url": "https://preview.redd.it/especdi14lw91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a147cd6ed91ccc113c6ecfc8d849ca698e448268", "width": 108, "height": 147}, {"url": "https://preview.redd.it/especdi14lw91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9cc6bed0b48683bde9b4691d10fc35ffedbfece", "width": 216, "height": 294}, {"url": "https://preview.redd.it/especdi14lw91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d36deb6a41910ee278703e1e523afdc433e4cb05", "width": 320, "height": 436}, {"url": "https://preview.redd.it/especdi14lw91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=18fbfc9dd98895410b8fd25f93ab2dc90bd3029a", "width": 640, "height": 872}, {"url": "https://preview.redd.it/especdi14lw91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b899ad1c04a2dc5fdd09603161d6baeedf5d6f15", "width": 960, "height": 1309}, {"url": "https://preview.redd.it/especdi14lw91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a89a5de533e66fa94a3095270db0429d6e2845f9", "width": 1080, "height": 1473}], "variants": {}, "id": "ozHlhbL--sD-arUoVdRtVi4kezXQMNn-ItoyjzUHlNU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "yfnbab", "is_robot_indexable": true, "report_reasons": null, "author": "deepcontractor", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yfnbab/kaggle_is_wild_o/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/especdi14lw91.png", "subreddit_subscribers": 815895, "created_utc": 1666961386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Currently a Biochemistry grad working in research for a blood banking company. Hate it. Thinking about a masters in data science or pursuing certs on my free time without a masters. \n\nWould anyone like to give a realistic run down of what a typical day looks like for you working in data science? \n\n\nAlso, is data science similar to computer science in the aspect that you don\u2019t need a degree just need to know your stuff to secure a job? (Unlike the translational side of science, degree means everything\ud83d\ude43) \n\nThanks in advance!", "author_fullname": "t2_9csm8sf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Switch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yez6vq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666894655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently a Biochemistry grad working in research for a blood banking company. Hate it. Thinking about a masters in data science or pursuing certs on my free time without a masters. &lt;/p&gt;\n\n&lt;p&gt;Would anyone like to give a realistic run down of what a typical day looks like for you working in data science? &lt;/p&gt;\n\n&lt;p&gt;Also, is data science similar to computer science in the aspect that you don\u2019t need a degree just need to know your stuff to secure a job? (Unlike the translational side of science, degree means everything\ud83d\ude43) &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yez6vq", "is_robot_indexable": true, "report_reasons": null, "author": "SprayPsychological86", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yez6vq/career_switch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yez6vq/career_switch/", "subreddit_subscribers": 815895, "created_utc": 1666894655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am working on a ML project and need data for population density (I live in small country with not much data) . Is it possible to find a website from which I can download satellite pictures (compiled every day for an year) at night of my town, so I can estimate the population concentration by the given lights. Thanks.", "author_fullname": "t2_aws8tyj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Satellite images data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfir8m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666946824.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a ML project and need data for population density (I live in small country with not much data) . Is it possible to find a website from which I can download satellite pictures (compiled every day for an year) at night of my town, so I can estimate the population concentration by the given lights. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yfir8m", "is_robot_indexable": true, "report_reasons": null, "author": "AnyJello605", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yfir8m/satellite_images_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yfir8m/satellite_images_data/", "subreddit_subscribers": 815895, "created_utc": 1666946824.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been working as a Data Scientist for only a short time. I spent a year working as a graduate and have recently been promoted to a permenant position. I find that within my team everyone seems to have side projects going on or be reading textbooks and listening to podcasts about Data Science in their free time. I enjoy working in Data Science and am always eager to learn more, but to be honest, I still see it as work and like to completely disconnect outside of my job to pursue and enjoy other things. I have no real desire to extend the time I spend working on or reading about the field outside of my work hours. Is this likely to hurt me in my career?", "author_fullname": "t2_jip2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I succeed as a Data Scientist without wanting to do extra outside of work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfhubz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666943468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working as a Data Scientist for only a short time. I spent a year working as a graduate and have recently been promoted to a permenant position. I find that within my team everyone seems to have side projects going on or be reading textbooks and listening to podcasts about Data Science in their free time. I enjoy working in Data Science and am always eager to learn more, but to be honest, I still see it as work and like to completely disconnect outside of my job to pursue and enjoy other things. I have no real desire to extend the time I spend working on or reading about the field outside of my work hours. Is this likely to hurt me in my career?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yfhubz", "is_robot_indexable": true, "report_reasons": null, "author": "RastaSalad", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yfhubz/can_i_succeed_as_a_data_scientist_without_wanting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yfhubz/can_i_succeed_as_a_data_scientist_without_wanting/", "subreddit_subscribers": 815895, "created_utc": 1666943468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nMost of the e2e DS tutorials cover feature engineering (OHE, LE, Norm, etc.) and splitting complete data into train and test sets. As you know, in real life, there is no x\\_train and y\\_train. You need to do labeling using your raw data. However, there are no tutorials covering **tabular** data labeling. Are there any recommended tutorials, git repositories, or blog posts to learn and exercise for labeling? It can be related churn labeling, likelihood to buy, or spend forecasting etc.\n\nTLDR: Looking for tutorials about **tabular data labeling** (creating target variable).", "author_fullname": "t2_rehw0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for tabular data prep/labeling tutorials", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfhhhp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666942149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Most of the e2e DS tutorials cover feature engineering (OHE, LE, Norm, etc.) and splitting complete data into train and test sets. As you know, in real life, there is no x_train and y_train. You need to do labeling using your raw data. However, there are no tutorials covering &lt;strong&gt;tabular&lt;/strong&gt; data labeling. Are there any recommended tutorials, git repositories, or blog posts to learn and exercise for labeling? It can be related churn labeling, likelihood to buy, or spend forecasting etc.&lt;/p&gt;\n\n&lt;p&gt;TLDR: Looking for tutorials about &lt;strong&gt;tabular data labeling&lt;/strong&gt; (creating target variable).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yfhhhp", "is_robot_indexable": true, "report_reasons": null, "author": "silverstone1903", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yfhhhp/looking_for_tabular_data_preplabeling_tutorials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yfhhhp/looking_for_tabular_data_preplabeling_tutorials/", "subreddit_subscribers": 815895, "created_utc": 1666942149.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Pretty much what the title says. Do you have any you'd recommend?\n\nFor reference, I'm about to get out of undergrad. I didn't practice much of statistics but through side projects doing data analysis &amp; machine learning, I realized how important this was kind of late imo.  I'm vaguely familiar with calculus, it's been a while since I've looked at multivariable, I already have a book for that for review. \n\nLooking for statistics mainly. But if you do happen to have one for linear algebra you'd recommend, that'd be appreciated as well. ; )\n\nAnything to help understand these models more lol.", "author_fullname": "t2_u7pn8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations for introductory statistical texts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfi7k2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666945150.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666944783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty much what the title says. Do you have any you&amp;#39;d recommend?&lt;/p&gt;\n\n&lt;p&gt;For reference, I&amp;#39;m about to get out of undergrad. I didn&amp;#39;t practice much of statistics but through side projects doing data analysis &amp;amp; machine learning, I realized how important this was kind of late imo.  I&amp;#39;m vaguely familiar with calculus, it&amp;#39;s been a while since I&amp;#39;ve looked at multivariable, I already have a book for that for review. &lt;/p&gt;\n\n&lt;p&gt;Looking for statistics mainly. But if you do happen to have one for linear algebra you&amp;#39;d recommend, that&amp;#39;d be appreciated as well. ; )&lt;/p&gt;\n\n&lt;p&gt;Anything to help understand these models more lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yfi7k2", "is_robot_indexable": true, "report_reasons": null, "author": "pekkalacd", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yfi7k2/recommendations_for_introductory_statistical_texts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yfi7k2/recommendations_for_introductory_statistical_texts/", "subreddit_subscribers": 815895, "created_utc": 1666944783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "i basically have 1-2-3 (high medium low quality) and want to plot the counts, stacked, on a histogram. Only thing is I want something like an interactive button to bring the selected histogram to the front  \n\n\nWhat can I easily do this with in python?  \n\n\nI will then make it web runnable on steamlit", "author_fullname": "t2_lww73a7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I make an interactive histogram in python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfaot7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666922693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i basically have 1-2-3 (high medium low quality) and want to plot the counts, stacked, on a histogram. Only thing is I want something like an interactive button to bring the selected histogram to the front  &lt;/p&gt;\n\n&lt;p&gt;What can I easily do this with in python?  &lt;/p&gt;\n\n&lt;p&gt;I will then make it web runnable on steamlit&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yfaot7", "is_robot_indexable": true, "report_reasons": null, "author": "fartuni4", "discussion_type": null, "num_comments": 9, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yfaot7/how_can_i_make_an_interactive_histogram_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yfaot7/how_can_i_make_an_interactive_histogram_in_python/", "subreddit_subscribers": 815895, "created_utc": 1666922693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve recently started at a new company in a hybrid DS/engineering role. It\u2019s an older company in the utility industry, so there\u2019s a pretty healthy mix of \u201ctraditional\u201d &amp; modern ways of keeping track of things (events, equipment, project status, etc). Thankfully we do have some Oracle DBs in place that a couple members on the team are using to build new dashboards and analytics scripts from. However, lots of existing/historical work tracking is done through random Excel sheets scattered about the network drives. There always exists a desire to move forward and advance our current practices to increase effectiveness &amp; efficiency within the company\u2019s data handling schemes. \n\nWe can develop new scripts/dashboards and do all kinds of cool stuff on that side of things. I\u2019m just curious as to what methods other people may be using to bridge gaps between scattered Excel data/reporting sheets and more centralized dashboarding/SW development practices. \n\nIf you couldn\u2019t tell, I\u2019m a bit new the DS working side. I\u2019ve done lots with Python, SQL, and a few dashboard platforms, but I\u2019m still new to the real world applications of DS. \n\nThanks!", "author_fullname": "t2_dv1qy8c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bridging the gap between old &amp; new methods of DS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yf5883", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666909123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve recently started at a new company in a hybrid DS/engineering role. It\u2019s an older company in the utility industry, so there\u2019s a pretty healthy mix of \u201ctraditional\u201d &amp;amp; modern ways of keeping track of things (events, equipment, project status, etc). Thankfully we do have some Oracle DBs in place that a couple members on the team are using to build new dashboards and analytics scripts from. However, lots of existing/historical work tracking is done through random Excel sheets scattered about the network drives. There always exists a desire to move forward and advance our current practices to increase effectiveness &amp;amp; efficiency within the company\u2019s data handling schemes. &lt;/p&gt;\n\n&lt;p&gt;We can develop new scripts/dashboards and do all kinds of cool stuff on that side of things. I\u2019m just curious as to what methods other people may be using to bridge gaps between scattered Excel data/reporting sheets and more centralized dashboarding/SW development practices. &lt;/p&gt;\n\n&lt;p&gt;If you couldn\u2019t tell, I\u2019m a bit new the DS working side. I\u2019ve done lots with Python, SQL, and a few dashboard platforms, but I\u2019m still new to the real world applications of DS. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yf5883", "is_robot_indexable": true, "report_reasons": null, "author": "ljh78", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yf5883/bridging_the_gap_between_old_new_methods_of_ds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yf5883/bridging_the_gap_between_old_new_methods_of_ds/", "subreddit_subscribers": 815895, "created_utc": 1666909123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently participating in datacamp competition,I need some help selecting my model for the output\n\n# Project Goal\n\n**Using the network analysis, in which departments would you recommend the HR team focus to boost collaboration?**\n\n# Data Set Info\n\n\\#### Messages has information on the sender, receiver, and time.\n\n\\- \"sender\" - represents the employee id of the employee sending the message.\n\n\\- \"receiver\" - represents the employee id of the employee receiving the message.\n\n\\- \"timestamp\" - the date of the message.\n\n\\- \"message\\_length\" - the length in words of the message.\n\n&amp;#x200B;\n\n\\#### Employees has information on each employee;\n\n\\- \"id\" - represents the employee id of the employee.\n\n\\- \"department\" - is the department within the company. \n\n\\- \"location\" - is the country where the employee lives.\n\n\\- \"age\" - is the age of the employee.\n\n[https://drive.google.com/drive/folders/1aijyRGETvelAkT6x0ew2skIjmxxryHPV?usp=sharing](https://drive.google.com/drive/folders/1aijyRGETvelAkT6x0ew2skIjmxxryHPV?usp=sharing)\n\n&amp;#x200B;\n\n# My work till now\n\nI haven't made many changes to the dataset. just tried to find the co-relation between different attributes and some visualization which was required for the project ( lmk if they are shit)\n\n&amp;#x200B;\n\n[Messages sent based on time and department](https://preview.redd.it/nnodk91egjw91.png?width=944&amp;format=png&amp;auto=webp&amp;s=86c5bc7a3bb11bc32bfd5244ba38950cd62448ae)\n\n[Number of messages sent based on location](https://preview.redd.it/2judthi3gjw91.png?width=807&amp;format=png&amp;auto=webp&amp;s=e322fd8d1a42b741953b973e1844f80f32e3e74f)\n\n&amp;#x200B;\n\n[No. of messages sent based on department](https://preview.redd.it/eio4rqp8gjw91.png?width=1433&amp;format=png&amp;auto=webp&amp;s=516fc47091a330c7ba78bc6bd3f273f7e17ab06c)\n\nObservations: \n\n1. All Branches has sales as largest department except US\n\n2. Largest department in US is operations, second is Brazil and UK has no Operation dep\n\n3. Germany has highest Sales department, with 68.45% of total branch and Brazil Coming in no. 2\n\n4. It's weird that only first 2 months have high conversations\n\nCreate a report that covers the following:  \n\n  1. Which departments are the most/least active? \n\ni)  Most:-  Sales(1.55k msg)\n\nii) least:- Marketing(16 msgs)\n\n  2. Which employee has the most connections?  - \n\ni)  Sender  :- 605(459, Admin)\n\nii) Receiver:- 281(60,Sales) \n\n  3. Which Department has the most connections? \n\ni) sender Department:- Sales(1.5k)\n\nii) receiver Dep:- Sales(1.23k)\n\n&amp;#x200B;\n\n&gt;**I know my data might be just random with no path followed, well this is partially truth, since 3 factors are affecting the message sent 1. Time, 2. Location, 3. Department**\n\n&amp;#x200B;\n\n&gt;My first thoughts were to look it as a time series model but after looking at the msg based on time graph It was dissolved  \n&gt;  \n&gt;My second thoughts was to look it as a network model like graph with sender and receiver as nodes and msgs as edge weight but I don't much clue on how to deal with it", "author_fullname": "t2_3hg5hlo7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best model for network Analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 53, "top_awarded_type": null, "hide_score": true, "media_metadata": {"2judthi3gjw91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 104, "x": 108, "u": "https://preview.redd.it/2judthi3gjw91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4914cfa061400ddc67bbc6af034e7cc37c95987"}, {"y": 208, "x": 216, "u": "https://preview.redd.it/2judthi3gjw91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=022e2127a831c48e0daf76bfb488c2bb4fb6adbd"}, {"y": 309, "x": 320, "u": "https://preview.redd.it/2judthi3gjw91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e1601daacb1a977defefddaeaa5f4eb30dddc19"}, {"y": 618, "x": 640, "u": "https://preview.redd.it/2judthi3gjw91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=211a3e2852378fcb708ce709e160d1e99043c0d3"}], "s": {"y": 780, "x": 807, "u": "https://preview.redd.it/2judthi3gjw91.png?width=807&amp;format=png&amp;auto=webp&amp;s=e322fd8d1a42b741953b973e1844f80f32e3e74f"}, "id": "2judthi3gjw91"}, "eio4rqp8gjw91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/eio4rqp8gjw91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f8a97a8c91a8fc1683183f90a8e666bd78754f0"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/eio4rqp8gjw91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d1790cfe40d4db06a90e14f31f1dff2f08f9b585"}, {"y": 179, "x": 320, "u": "https://preview.redd.it/eio4rqp8gjw91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3da26c8fd3fec597272eb7a6de46c673430a4360"}, {"y": 359, "x": 640, "u": "https://preview.redd.it/eio4rqp8gjw91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=32211baab0e86ab02cc4239a4526d79b33fb2c48"}, {"y": 538, "x": 960, "u": "https://preview.redd.it/eio4rqp8gjw91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b4e5f40dda1a65162828b42e28c16a39d2a57877"}, {"y": 605, "x": 1080, "u": "https://preview.redd.it/eio4rqp8gjw91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15eeae2ce4bd5cb4470b65f9b639f9cda73c2852"}], "s": {"y": 804, "x": 1433, "u": "https://preview.redd.it/eio4rqp8gjw91.png?width=1433&amp;format=png&amp;auto=webp&amp;s=516fc47091a330c7ba78bc6bd3f273f7e17ab06c"}, "id": "eio4rqp8gjw91"}, "nnodk91egjw91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 41, "x": 108, "u": "https://preview.redd.it/nnodk91egjw91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a43d24da16d39c3ea2b498a5ec1b8ae271fb538"}, {"y": 82, "x": 216, "u": "https://preview.redd.it/nnodk91egjw91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=05d4ae3b24915ddaa5c76e05fd7b4517165cdec4"}, {"y": 122, "x": 320, "u": "https://preview.redd.it/nnodk91egjw91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c39b177336e6a6e580d097d7c9ca9c9c96b0a0aa"}, {"y": 245, "x": 640, "u": "https://preview.redd.it/nnodk91egjw91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ebb17ec1627a8a2e4d4f054228cece4b0ded4de7"}], "s": {"y": 362, "x": 944, "u": "https://preview.redd.it/nnodk91egjw91.png?width=944&amp;format=png&amp;auto=webp&amp;s=86c5bc7a3bb11bc32bfd5244ba38950cd62448ae"}, "id": "nnodk91egjw91"}}, "name": "t3_yfmry5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/kGVMenG6zW_kSwqvSBLqHuFFu6I-vOVHawBUyiiZPTE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666959881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently participating in datacamp competition,I need some help selecting my model for the output&lt;/p&gt;\n\n&lt;h1&gt;Project Goal&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Using the network analysis, in which departments would you recommend the HR team focus to boost collaboration?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;Data Set Info&lt;/h1&gt;\n\n&lt;p&gt;#### Messages has information on the sender, receiver, and time.&lt;/p&gt;\n\n&lt;p&gt;- &amp;quot;sender&amp;quot; - represents the employee id of the employee sending the message.&lt;/p&gt;\n\n&lt;p&gt;- &amp;quot;receiver&amp;quot; - represents the employee id of the employee receiving the message.&lt;/p&gt;\n\n&lt;p&gt;- &amp;quot;timestamp&amp;quot; - the date of the message.&lt;/p&gt;\n\n&lt;p&gt;- &amp;quot;message_length&amp;quot; - the length in words of the message.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;#### Employees has information on each employee;&lt;/p&gt;\n\n&lt;p&gt;- &amp;quot;id&amp;quot; - represents the employee id of the employee.&lt;/p&gt;\n\n&lt;p&gt;- &amp;quot;department&amp;quot; - is the department within the company. &lt;/p&gt;\n\n&lt;p&gt;- &amp;quot;location&amp;quot; - is the country where the employee lives.&lt;/p&gt;\n\n&lt;p&gt;- &amp;quot;age&amp;quot; - is the age of the employee.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://drive.google.com/drive/folders/1aijyRGETvelAkT6x0ew2skIjmxxryHPV?usp=sharing\"&gt;https://drive.google.com/drive/folders/1aijyRGETvelAkT6x0ew2skIjmxxryHPV?usp=sharing&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h1&gt;My work till now&lt;/h1&gt;\n\n&lt;p&gt;I haven&amp;#39;t made many changes to the dataset. just tried to find the co-relation between different attributes and some visualization which was required for the project ( lmk if they are shit)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nnodk91egjw91.png?width=944&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=86c5bc7a3bb11bc32bfd5244ba38950cd62448ae\"&gt;Messages sent based on time and department&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2judthi3gjw91.png?width=807&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e322fd8d1a42b741953b973e1844f80f32e3e74f\"&gt;Number of messages sent based on location&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/eio4rqp8gjw91.png?width=1433&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=516fc47091a330c7ba78bc6bd3f273f7e17ab06c\"&gt;No. of messages sent based on department&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Observations: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;All Branches has sales as largest department except US&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Largest department in US is operations, second is Brazil and UK has no Operation dep&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Germany has highest Sales department, with 68.45% of total branch and Brazil Coming in no. 2&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;It&amp;#39;s weird that only first 2 months have high conversations&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Create a report that covers the following:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which departments are the most/least active? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;i)  Most:-  Sales(1.55k msg)&lt;/p&gt;\n\n&lt;p&gt;ii) least:- Marketing(16 msgs)&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which employee has the most connections?  - &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;i)  Sender  :- 605(459, Admin)&lt;/p&gt;\n\n&lt;p&gt;ii) Receiver:- 281(60,Sales) &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which Department has the most connections? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;i) sender Department:- Sales(1.5k)&lt;/p&gt;\n\n&lt;p&gt;ii) receiver Dep:- Sales(1.23k)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;I know my data might be just random with no path followed, well this is partially truth, since 3 factors are affecting the message sent 1. Time, 2. Location, 3. Department&lt;/strong&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;My first thoughts were to look it as a time series model but after looking at the msg based on time graph It was dissolved  &lt;/p&gt;\n\n&lt;p&gt;My second thoughts was to look it as a network model like graph with sender and receiver as nodes and msgs as edge weight but I don&amp;#39;t much clue on how to deal with it&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yfmry5", "is_robot_indexable": true, "report_reasons": null, "author": "SupaSTaZz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yfmry5/best_model_for_network_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yfmry5/best_model_for_network_analysis/", "subreddit_subscribers": 815895, "created_utc": 1666959881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to analyze how total nitrogen and total sugar levels respond to the control or if there is even a relationship between the two. Does this require a multivariate multiple regression? If not, what would be the most effective method of statistical analysis?\n\nhttps://preview.redd.it/yni8thsyfjw91.png?width=528&amp;format=png&amp;auto=webp&amp;s=6cc1388aec9b64d31b7e59d455c3a41ea5172eb7", "author_fullname": "t2_ql5c6gd2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does this dataset require multivariate regression?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"yni8thsyfjw91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 137, "x": 108, "u": "https://preview.redd.it/yni8thsyfjw91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=335cd0864fa6ccb862b22c69277f6a29eae4694e"}, {"y": 275, "x": 216, "u": "https://preview.redd.it/yni8thsyfjw91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7993b653259039211910cf46077bf0be2836d730"}, {"y": 407, "x": 320, "u": "https://preview.redd.it/yni8thsyfjw91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b4e5a99f2013a27bcd41a26671b356e2b1e2f68d"}], "s": {"y": 673, "x": 528, "u": "https://preview.redd.it/yni8thsyfjw91.png?width=528&amp;format=png&amp;auto=webp&amp;s=6cc1388aec9b64d31b7e59d455c3a41ea5172eb7"}, "id": "yni8thsyfjw91"}}, "name": "t3_yfmlhj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/73xjUj3M73hEgaTJDU909GSBkO-QtQt2saX-NuyL81Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666959367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to analyze how total nitrogen and total sugar levels respond to the control or if there is even a relationship between the two. Does this require a multivariate multiple regression? If not, what would be the most effective method of statistical analysis?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yni8thsyfjw91.png?width=528&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6cc1388aec9b64d31b7e59d455c3a41ea5172eb7\"&gt;https://preview.redd.it/yni8thsyfjw91.png?width=528&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6cc1388aec9b64d31b7e59d455c3a41ea5172eb7&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yfmlhj", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianNew3975", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yfmlhj/does_this_dataset_require_multivariate_regression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yfmlhj/does_this_dataset_require_multivariate_regression/", "subreddit_subscribers": 815895, "created_utc": 1666959367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Need assistance helping a set of patients. I have a dataset of events recording a non-continuous time value, meaning I get a WhatsApp message every time such event happens. Now the data set is In the order of thousands so I was wondering if there is a way to predict when the next event will be so we can:\n\n1) Measure a closer before and after on a set of physiological features \n2) try to counteract the onset with a special drug", "author_fullname": "t2_es9dac8d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help predicting the future!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yflwvf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666957378.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need assistance helping a set of patients. I have a dataset of events recording a non-continuous time value, meaning I get a WhatsApp message every time such event happens. Now the data set is In the order of thousands so I was wondering if there is a way to predict when the next event will be so we can:&lt;/p&gt;\n\n&lt;p&gt;1) Measure a closer before and after on a set of physiological features \n2) try to counteract the onset with a special drug&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yflwvf", "is_robot_indexable": true, "report_reasons": null, "author": "SlowAndLate", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yflwvf/help_predicting_the_future/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yflwvf/help_predicting_the_future/", "subreddit_subscribers": 815895, "created_utc": 1666957378.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I\u2019m starting a new job as a data consultant and I\u2019m going to work with talend and salesforce, can someone tell if it is hard to learn? I\u2019m a Junior Developer and never worked with data. What are The Essentials to be good at this new job?", "author_fullname": "t2_tnivskor", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do I need to be a data consultant?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yflufi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666957184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019m starting a new job as a data consultant and I\u2019m going to work with talend and salesforce, can someone tell if it is hard to learn? I\u2019m a Junior Developer and never worked with data. What are The Essentials to be good at this new job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yflufi", "is_robot_indexable": true, "report_reasons": null, "author": "Hot-Opportunity8991", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yflufi/what_do_i_need_to_be_a_data_consultant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yflufi/what_do_i_need_to_be_a_data_consultant/", "subreddit_subscribers": 815895, "created_utc": 1666957184.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For an example, lets say i take on an NLP project to anonymise and classify some text.\n\nTypically id start with a simple approch to solve the problem and meet the scope of the project. This might mean using some prebuilt package to anonymise, and a fairly simple tree model to classify.\n\nThis meets the scope and means we can progress to get something into production, wit the view that when its productionised, we can hot swap improvements in the pipeline.\n\nMy question is, how do you justify to the business these improvements if they view the problem as solved? \n\nFor small changes like replacing a random forest with a lgbm, thats not so bad. However, i mean like replacing the anonymisation package with a more custom CNN or LSTM approach? Thats a much bigger change.\n\nI feel like the answer is \u2018why bother if the business doesn\u2019t see value in it/the current solution doing the job\u2019. But part of our teams role is to progress data science in the business - so we cant just go off what the business thinks as they dont know the art of the possible.\n\nYes its down to the data scientist to communicate the value of the work, but how would you justify such a change in this usecase?", "author_fullname": "t2_gm8b3iu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For those in industry - how do you know when to/ justify revisiting a project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfl2t4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666954860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For an example, lets say i take on an NLP project to anonymise and classify some text.&lt;/p&gt;\n\n&lt;p&gt;Typically id start with a simple approch to solve the problem and meet the scope of the project. This might mean using some prebuilt package to anonymise, and a fairly simple tree model to classify.&lt;/p&gt;\n\n&lt;p&gt;This meets the scope and means we can progress to get something into production, wit the view that when its productionised, we can hot swap improvements in the pipeline.&lt;/p&gt;\n\n&lt;p&gt;My question is, how do you justify to the business these improvements if they view the problem as solved? &lt;/p&gt;\n\n&lt;p&gt;For small changes like replacing a random forest with a lgbm, thats not so bad. However, i mean like replacing the anonymisation package with a more custom CNN or LSTM approach? Thats a much bigger change.&lt;/p&gt;\n\n&lt;p&gt;I feel like the answer is \u2018why bother if the business doesn\u2019t see value in it/the current solution doing the job\u2019. But part of our teams role is to progress data science in the business - so we cant just go off what the business thinks as they dont know the art of the possible.&lt;/p&gt;\n\n&lt;p&gt;Yes its down to the data scientist to communicate the value of the work, but how would you justify such a change in this usecase?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yfl2t4", "is_robot_indexable": true, "report_reasons": null, "author": "poppycocknbalderdash", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yfl2t4/for_those_in_industry_how_do_you_know_when_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yfl2t4/for_those_in_industry_how_do_you_know_when_to/", "subreddit_subscribers": 815895, "created_utc": 1666954860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello yall,\n\nI am doing a project for my economics class and was wondering if anyone in this subreddit has any interesting articles or studies that can show causality between tax-breaks and  economic growth or income. I need to isolate and use methods such as difference-in-difference or other methods to somehow show that a tax break helped or didn't help a state or city. Any thoughts or suggestions or other ways to look at this project? \n\nP.S. my professor said, how will I isolate for the tax-break? What if a big company had just moved into town and brought jobs and investments? How can I show that it was the tax-break that helped or didn't help. \n\nTHIS IS NOT HOMEWORK HELP, I JUST NEED DIFFERENT WAYS TO APPROACH THIS.", "author_fullname": "t2_7chxigz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Isolating for main outcome variables for a tax-break project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfbebv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666924533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello yall,&lt;/p&gt;\n\n&lt;p&gt;I am doing a project for my economics class and was wondering if anyone in this subreddit has any interesting articles or studies that can show causality between tax-breaks and  economic growth or income. I need to isolate and use methods such as difference-in-difference or other methods to somehow show that a tax break helped or didn&amp;#39;t help a state or city. Any thoughts or suggestions or other ways to look at this project? &lt;/p&gt;\n\n&lt;p&gt;P.S. my professor said, how will I isolate for the tax-break? What if a big company had just moved into town and brought jobs and investments? How can I show that it was the tax-break that helped or didn&amp;#39;t help. &lt;/p&gt;\n\n&lt;p&gt;THIS IS NOT HOMEWORK HELP, I JUST NEED DIFFERENT WAYS TO APPROACH THIS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yfbebv", "is_robot_indexable": true, "report_reasons": null, "author": "bakerintheforest", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yfbebv/isolating_for_main_outcome_variables_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yfbebv/isolating_for_main_outcome_variables_for_a/", "subreddit_subscribers": 815895, "created_utc": 1666924533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm working as a Senior Data Scientist in one of the top companies. I work from 9-5 and they are doing good stuff. But somehow I'm not enjoying it. I'm not excited about the work I'm doing. I want to work on some side projects that can be either be learning for me or help some small scale business with their work. Long term, I think I want to start my own startup or work for one. Any suggestions are appreciated.", "author_fullname": "t2_ji5eacqx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for side project tech/ DS projects to be more productive in free time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfb9p0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666924202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working as a Senior Data Scientist in one of the top companies. I work from 9-5 and they are doing good stuff. But somehow I&amp;#39;m not enjoying it. I&amp;#39;m not excited about the work I&amp;#39;m doing. I want to work on some side projects that can be either be learning for me or help some small scale business with their work. Long term, I think I want to start my own startup or work for one. Any suggestions are appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yfb9p0", "is_robot_indexable": true, "report_reasons": null, "author": "Rude-Evidence246", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yfb9p0/looking_for_side_project_tech_ds_projects_to_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yfb9p0/looking_for_side_project_tech_ds_projects_to_be/", "subreddit_subscribers": 815895, "created_utc": 1666924202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m curious to know what the consensus on best online master\u2019s in data science is? I am a little less price dependent as my company would cover $30k or so.\n\nThe 3 I keep seeing as being rated pretty well are Georgia Tech, Texas, and Illinois.\n\nIs this accurate?\n\nWould love to hear where different people went, how the program was, and roughly how much time they spent per class per week.\n\nThank you so much in advance!", "author_fullname": "t2_17qwjq21", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best online master\u2019s in data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfaiou", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666922270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m curious to know what the consensus on best online master\u2019s in data science is? I am a little less price dependent as my company would cover $30k or so.&lt;/p&gt;\n\n&lt;p&gt;The 3 I keep seeing as being rated pretty well are Georgia Tech, Texas, and Illinois.&lt;/p&gt;\n\n&lt;p&gt;Is this accurate?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear where different people went, how the program was, and roughly how much time they spent per class per week.&lt;/p&gt;\n\n&lt;p&gt;Thank you so much in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yfaiou", "is_robot_indexable": true, "report_reasons": null, "author": "PowerfulSquirrel4", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yfaiou/best_online_masters_in_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yfaiou/best_online_masters_in_data_science/", "subreddit_subscribers": 815895, "created_utc": 1666922270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve been working as an applied ML engineer for almost a decade now, and in recent years I find I\u2019m spending a lot less time fiddling with model architectures and a lot more time digging into the data - rebalancing the data, removing bad labels, figuring out features we should add, and sourcing new labeled examples of cases where the model is weak.\n\nOften I\u2019ve found this means collaborating with less-technical folks (e.g. PMs, managers, annotators, data domain experts, etc.) on the team to dig through individual examples in our training set. While the engineers and data scientists on the team are usually comfortable looking at examples in a Python notebook environment, with the non-technical folks, I often need to screen-share my notebook with them, or we hack together a Google spreadsheet where we can comment/collaborate on identifying issues with mispredicted examples. Sometimes I\u2019ll write a script to generate an HTML dump that visualizes a slice of examples just so that we can gain more intuition than what we get from putting it all in a spreadsheet.\n\nI\u2019ve been working on a prototype to ideally reduce a lot of the friction behind this hand-off and collaboration process. It\u2019s still rough around the edges, but the basic flow is:\n\n* You import a slice of your dataset / predictions via a one-liner function call in a Python notebook or by manually uploading from your hard drive via a GUI.\n* You can then quickly customize how the dataset examples are visualized via a web interface: you can choose grid/table layouts, pick a subset of features to display, modify text size and color, render URLs as images, visualize bounding boxes, format enums to be color-coded and human-readable, or you can do something entirely custom.\n* You then get a link that you can share with the rest of your team so that everyone can comment and collaborate by looking at individual model predictions and diagnose next steps for improving the model.\n\nAnyway, I was wondering if there was anyone here who\u2019d be interested in trying out the prototype and providing some feedback on how to improve it? Alternatively, are there any existing tools/processes you all like to use to make this collaboration process easier? Thanks!\n\n**TL;DR:** Looking for feedback on a prototype of a tool to help ML engineers &amp; data scientists easily collaborate with non-technical team members when curating a dataset.", "author_fullname": "t2_qwbxg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for feedback on a dataset collaboration tool for ML projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yf8dvd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666916904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been working as an applied ML engineer for almost a decade now, and in recent years I find I\u2019m spending a lot less time fiddling with model architectures and a lot more time digging into the data - rebalancing the data, removing bad labels, figuring out features we should add, and sourcing new labeled examples of cases where the model is weak.&lt;/p&gt;\n\n&lt;p&gt;Often I\u2019ve found this means collaborating with less-technical folks (e.g. PMs, managers, annotators, data domain experts, etc.) on the team to dig through individual examples in our training set. While the engineers and data scientists on the team are usually comfortable looking at examples in a Python notebook environment, with the non-technical folks, I often need to screen-share my notebook with them, or we hack together a Google spreadsheet where we can comment/collaborate on identifying issues with mispredicted examples. Sometimes I\u2019ll write a script to generate an HTML dump that visualizes a slice of examples just so that we can gain more intuition than what we get from putting it all in a spreadsheet.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been working on a prototype to ideally reduce a lot of the friction behind this hand-off and collaboration process. It\u2019s still rough around the edges, but the basic flow is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You import a slice of your dataset / predictions via a one-liner function call in a Python notebook or by manually uploading from your hard drive via a GUI.&lt;/li&gt;\n&lt;li&gt;You can then quickly customize how the dataset examples are visualized via a web interface: you can choose grid/table layouts, pick a subset of features to display, modify text size and color, render URLs as images, visualize bounding boxes, format enums to be color-coded and human-readable, or you can do something entirely custom.&lt;/li&gt;\n&lt;li&gt;You then get a link that you can share with the rest of your team so that everyone can comment and collaborate by looking at individual model predictions and diagnose next steps for improving the model.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyway, I was wondering if there was anyone here who\u2019d be interested in trying out the prototype and providing some feedback on how to improve it? Alternatively, are there any existing tools/processes you all like to use to make this collaboration process easier? Thanks!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Looking for feedback on a prototype of a tool to help ML engineers &amp;amp; data scientists easily collaborate with non-technical team members when curating a dataset.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yf8dvd", "is_robot_indexable": true, "report_reasons": null, "author": "arkmastermind", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yf8dvd/looking_for_feedback_on_a_dataset_collaboration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yf8dvd/looking_for_feedback_on_a_dataset_collaboration/", "subreddit_subscribers": 815895, "created_utc": 1666916904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Ever wondered how Tesla's autopilot is able to make so many predictions in real time? It's because instead of \n\ndesigning multiple neural networks for different tasks, they design neural networks with a common backbone \n\ndoing multiple tasks. These neural networks are called Hydranets. Having known about them I revived\n\nmy old project on a self-driving car and designed a hydranet for predicting both the steering angle and throttle\n\nin a single pass. To know more you can visit this blog link:\n\n&amp;#x200B;\n\n[https://medium.com/geekculture/building-a-hydranet-for-self-driving-car-simulation-cd08543feffe](https://medium.com/geekculture/building-a-hydranet-for-self-driving-car-simulation-cd08543feffe)\n\n&amp;#x200B;\n\nThere is also a youtube link in the blog which shows the working of the system in real time.", "author_fullname": "t2_5n4y6isg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a HydraNet for Self-driving car simulation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yevcvu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1666885587.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever wondered how Tesla&amp;#39;s autopilot is able to make so many predictions in real time? It&amp;#39;s because instead of &lt;/p&gt;\n\n&lt;p&gt;designing multiple neural networks for different tasks, they design neural networks with a common backbone &lt;/p&gt;\n\n&lt;p&gt;doing multiple tasks. These neural networks are called Hydranets. Having known about them I revived&lt;/p&gt;\n\n&lt;p&gt;my old project on a self-driving car and designed a hydranet for predicting both the steering angle and throttle&lt;/p&gt;\n\n&lt;p&gt;in a single pass. To know more you can visit this blog link:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/geekculture/building-a-hydranet-for-self-driving-car-simulation-cd08543feffe\"&gt;https://medium.com/geekculture/building-a-hydranet-for-self-driving-car-simulation-cd08543feffe&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;There is also a youtube link in the blog which shows the working of the system in real time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rZAv-LIYN1pNN2jrBY-dd020w0oO2q4PEdUT0VwdJWA.jpg?auto=webp&amp;s=9130a9ceacdb694d01ee9f715d34a83e707be4ab", "width": 1200, "height": 960}, "resolutions": [{"url": "https://external-preview.redd.it/rZAv-LIYN1pNN2jrBY-dd020w0oO2q4PEdUT0VwdJWA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12ab01ec1559421ed9490d0a8fdcaffe96a3ae78", "width": 108, "height": 86}, {"url": "https://external-preview.redd.it/rZAv-LIYN1pNN2jrBY-dd020w0oO2q4PEdUT0VwdJWA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=082e2064f243fde2f8c8094a1372748ba9739e77", "width": 216, "height": 172}, {"url": "https://external-preview.redd.it/rZAv-LIYN1pNN2jrBY-dd020w0oO2q4PEdUT0VwdJWA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4322a861db8f22dbcb2b8055589d990cb12997df", "width": 320, "height": 256}, {"url": "https://external-preview.redd.it/rZAv-LIYN1pNN2jrBY-dd020w0oO2q4PEdUT0VwdJWA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=80d8dfa24d1f29a38b18ebfa3747a34ff256e6f1", "width": 640, "height": 512}, {"url": "https://external-preview.redd.it/rZAv-LIYN1pNN2jrBY-dd020w0oO2q4PEdUT0VwdJWA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=db5966268af13cece89b0902f6f239ad1377f59f", "width": 960, "height": 768}, {"url": "https://external-preview.redd.it/rZAv-LIYN1pNN2jrBY-dd020w0oO2q4PEdUT0VwdJWA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb41fbd93d0ceab9b7716f7144273210464cabf4", "width": 1080, "height": 864}], "variants": {}, "id": "iIqcll9FNIE46fGW_tKFBGy_JWXm1X25XkkJhC1GDFc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yevcvu", "is_robot_indexable": true, "report_reasons": null, "author": "VikasOjha666", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yevcvu/building_a_hydranet_for_selfdriving_car_simulation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yevcvu/building_a_hydranet_for_selfdriving_car_simulation/", "subreddit_subscribers": 815895, "created_utc": 1666885587.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "The first offer is a 4-months data science internship role in marketing domain(they're known to hire fulltime if the intern was good). \nThe other one is a fulltime BI dev role in insurance (will learn some ETL during). Downside here is that there will be no building data models. Just sql and BI. \nThis will be my first job in the field. Which one will be better in the long run? \nThank you", "author_fullname": "t2_9mpmbpc8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to become a data scientist and I have 2 job offers. What do I choose?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yerqya", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666876652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The first offer is a 4-months data science internship role in marketing domain(they&amp;#39;re known to hire fulltime if the intern was good). \nThe other one is a fulltime BI dev role in insurance (will learn some ETL during). Downside here is that there will be no building data models. Just sql and BI. \nThis will be my first job in the field. Which one will be better in the long run? \nThank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "yerqya", "is_robot_indexable": true, "report_reasons": null, "author": "anonynimiti", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/yerqya/i_want_to_become_a_data_scientist_and_i_have_2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/yerqya/i_want_to_become_a_data_scientist_and_i_have_2/", "subreddit_subscribers": 815895, "created_utc": 1666876652.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}