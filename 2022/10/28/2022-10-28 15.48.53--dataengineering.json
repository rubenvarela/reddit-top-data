{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_n5n50", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It's cron all the way down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_yez0ad", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 221, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 221, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/MwYU41FHiIcEBAI85vrnfPDevtFvvInRFv0d-dk1TIc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666894200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/yaeal5qi2ew91.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/yaeal5qi2ew91.png?auto=webp&amp;s=852a00695350cf3b53d3b12f6e0b768ffb674e52", "width": 491, "height": 668}, "resolutions": [{"url": "https://preview.redd.it/yaeal5qi2ew91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd87e1377ad8483c4d7be12d098843c6e94b552d", "width": 108, "height": 146}, {"url": "https://preview.redd.it/yaeal5qi2ew91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=884cf89534e839ae088fbad7b62c86c8339fd0f0", "width": 216, "height": 293}, {"url": "https://preview.redd.it/yaeal5qi2ew91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=679d791cac63dfb5ebc616c8bcc18d16bf314d2d", "width": 320, "height": 435}], "variants": {}, "id": "4S2EpRx0f1SPFIO8tuJkQjwqwfmjqojxHLWrE24pFwU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "yez0ad", "is_robot_indexable": true, "report_reasons": null, "author": "FireflyCaptain", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yez0ad/its_cron_all_the_way_down/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/yaeal5qi2ew91.png", "subreddit_subscribers": 78147, "created_utc": 1666894200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was reading about how compression works in parquet file format while doing so I landed on this table from data bricks docs.\n\nCan someone please explain? Thanks in advance\n\nhttps://preview.redd.it/zzek3263ijw91.png?width=865&amp;format=png&amp;auto=webp&amp;s=a30d4963e7652282f4045049f8128459a7e43f5b", "author_fullname": "t2_drv960av", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does 1 TB of data stored in S3 when scanned come out to be 1.15 TB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 96, "top_awarded_type": null, "hide_score": false, "media_metadata": {"zzek3263ijw91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 74, "x": 108, "u": "https://preview.redd.it/zzek3263ijw91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef09a4e09530a64327c03a971c025f117c292340"}, {"y": 148, "x": 216, "u": "https://preview.redd.it/zzek3263ijw91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d2e6e1fea75b67a5b95780e55118cd5646aa987c"}, {"y": 220, "x": 320, "u": "https://preview.redd.it/zzek3263ijw91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0c1d505bf8b0f916a86930e1ddd66b6cfcb207e5"}, {"y": 440, "x": 640, "u": "https://preview.redd.it/zzek3263ijw91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a94b6d024c8eb764584c5903a4b77cf61a362ee4"}], "s": {"y": 595, "x": 865, "u": "https://preview.redd.it/zzek3263ijw91.png?width=865&amp;format=png&amp;auto=webp&amp;s=a30d4963e7652282f4045049f8128459a7e43f5b"}, "id": "zzek3263ijw91"}}, "name": "t3_yfmqs9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/vOs_2W0ushxi485NvhoHrwsShRSNTkxSQMbjS7iNVw8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666959785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was reading about how compression works in parquet file format while doing so I landed on this table from data bricks docs.&lt;/p&gt;\n\n&lt;p&gt;Can someone please explain? Thanks in advance&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zzek3263ijw91.png?width=865&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a30d4963e7652282f4045049f8128459a7e43f5b\"&gt;https://preview.redd.it/zzek3263ijw91.png?width=865&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a30d4963e7652282f4045049f8128459a7e43f5b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yfmqs9", "is_robot_indexable": true, "report_reasons": null, "author": "Difficult_Buyer7243", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yfmqs9/how_does_1_tb_of_data_stored_in_s3_when_scanned/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yfmqs9/how_does_1_tb_of_data_stored_in_s3_when_scanned/", "subreddit_subscribers": 78147, "created_utc": 1666959785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In Berlin, what should be the salary expectation for someone who has been working as a DE for 3 years and before that as Backend Dev 3 more years? \n\nI mostly worked at small companies where the role required both a bit backend and data experience, I fit in this pretty well. For example at my current place we work with Neo4j, ElasticSearch and GraphQL, separate for different tenants/customers. I am responsible for all the steps for integration of a new data source, from analysis and modelling to pushing, to exposing the data via ES and GraphQL API. We do ETLs on Databricks with PySpark and Pandas. I don't manage teams but I do guide one or two juniors we have in my team.\n\nSo far I am quite okay with my current pay but I feel like I am constantly underselling myself specially because I suck at negotiations.", "author_fullname": "t2_12lkky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Salary expectations for Berlin, Germany", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yewd6n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666888050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In Berlin, what should be the salary expectation for someone who has been working as a DE for 3 years and before that as Backend Dev 3 more years? &lt;/p&gt;\n\n&lt;p&gt;I mostly worked at small companies where the role required both a bit backend and data experience, I fit in this pretty well. For example at my current place we work with Neo4j, ElasticSearch and GraphQL, separate for different tenants/customers. I am responsible for all the steps for integration of a new data source, from analysis and modelling to pushing, to exposing the data via ES and GraphQL API. We do ETLs on Databricks with PySpark and Pandas. I don&amp;#39;t manage teams but I do guide one or two juniors we have in my team.&lt;/p&gt;\n\n&lt;p&gt;So far I am quite okay with my current pay but I feel like I am constantly underselling myself specially because I suck at negotiations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yewd6n", "is_robot_indexable": true, "report_reasons": null, "author": "ratulotron", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yewd6n/salary_expectations_for_berlin_germany/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yewd6n/salary_expectations_for_berlin_germany/", "subreddit_subscribers": 78147, "created_utc": 1666888050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Will put the sign up link in the comments. You'll need to apply to join their Slack org, then check the #oa-book-club channel to review dates and other information.", "author_fullname": "t2_4041g9mz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Operational Analytics (Club) Book Club: Fundamentals of Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yf34ge", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666904151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Will put the sign up link in the comments. You&amp;#39;ll need to apply to join their Slack org, then check the #oa-book-club channel to review dates and other information.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yf34ge", "is_robot_indexable": true, "report_reasons": null, "author": "aamoscodes", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yf34ge/operational_analytics_club_book_club_fundamentals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yf34ge/operational_analytics_club_book_club_fundamentals/", "subreddit_subscribers": 78147, "created_utc": 1666904151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone\n\nAfter putting some thought into it I have decided the best way to showcase my skills in data integration/DE is to build a portfolio of ETL pipelines (with transformations, cleaning, modeling etc. etc.) based on a fairly complex database/dataset.\n\nThe problem now is I cannot find the kind of DB i'm looking for, since resources like Kaggle and other websites are focused on data science/statistics side, so the database they make available are mostly very simple CSV files that are really good to do DS (apply algorithms etc.) but are not that interesting from the point of view of someone who does data integration. How can I showcase my skills if the base data is one CSV with 10 columns? \n\nIdeally the database I'm looking for would have:\n\n* messy, redundant, non conformed data, lots of table not modeled in a star schema (ex: a DB dump from an application maybe?)\n* different source types (some sql, some files, some apis I guess)\n* High volume, at least more than 1GB so that I'm forced to put a bit of optimization\n\nThese are the first that come to mind, if you have any suggestion or can point me to a github repo or something that can help me I would be extremely grateful!\n\nTHANKS", "author_fullname": "t2_ehpb7yoc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fairly complex database/set to build a ETL portfolio on - any help?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfjmvq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666949986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone&lt;/p&gt;\n\n&lt;p&gt;After putting some thought into it I have decided the best way to showcase my skills in data integration/DE is to build a portfolio of ETL pipelines (with transformations, cleaning, modeling etc. etc.) based on a fairly complex database/dataset.&lt;/p&gt;\n\n&lt;p&gt;The problem now is I cannot find the kind of DB i&amp;#39;m looking for, since resources like Kaggle and other websites are focused on data science/statistics side, so the database they make available are mostly very simple CSV files that are really good to do DS (apply algorithms etc.) but are not that interesting from the point of view of someone who does data integration. How can I showcase my skills if the base data is one CSV with 10 columns? &lt;/p&gt;\n\n&lt;p&gt;Ideally the database I&amp;#39;m looking for would have:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;messy, redundant, non conformed data, lots of table not modeled in a star schema (ex: a DB dump from an application maybe?)&lt;/li&gt;\n&lt;li&gt;different source types (some sql, some files, some apis I guess)&lt;/li&gt;\n&lt;li&gt;High volume, at least more than 1GB so that I&amp;#39;m forced to put a bit of optimization&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;These are the first that come to mind, if you have any suggestion or can point me to a github repo or something that can help me I would be extremely grateful!&lt;/p&gt;\n\n&lt;p&gt;THANKS&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yfjmvq", "is_robot_indexable": true, "report_reasons": null, "author": "schizo_coder", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yfjmvq/fairly_complex_databaseset_to_build_a_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yfjmvq/fairly_complex_databaseset_to_build_a_etl/", "subreddit_subscribers": 78147, "created_utc": 1666949986.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an interview coming up with Tesla for DE and wanted to know if anyone here has given interviews at Tesla for DE. Would really help :)", "author_fullname": "t2_ahi836bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tesla Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yf23eu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666901657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an interview coming up with Tesla for DE and wanted to know if anyone here has given interviews at Tesla for DE. Would really help :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yf23eu", "is_robot_indexable": true, "report_reasons": null, "author": "yyforthewin", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yf23eu/tesla_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yf23eu/tesla_data_engineering/", "subreddit_subscribers": 78147, "created_utc": 1666901657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the difference between a data engineer and etl developer? What would be the differentiating use of tools/languages? What methodologies/best practices should either know? I am aware there is often overlap of responsibilities/similarities in tasks between the two, but need to know if a role I applied to internally should be titled ETL developer or am right in asking for the role to be titled Data Engineer instead.", "author_fullname": "t2_3h5wixaf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer vs ETL developer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yezrg0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666896029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the difference between a data engineer and etl developer? What would be the differentiating use of tools/languages? What methodologies/best practices should either know? I am aware there is often overlap of responsibilities/similarities in tasks between the two, but need to know if a role I applied to internally should be titled ETL developer or am right in asking for the role to be titled Data Engineer instead.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yezrg0", "is_robot_indexable": true, "report_reasons": null, "author": "J0hnDutt00n", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yezrg0/data_engineer_vs_etl_developer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yezrg0/data_engineer_vs_etl_developer/", "subreddit_subscribers": 78147, "created_utc": 1666896029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Today we used requests and os libraries to write object oriented programs that help us to fetch data from the web (html files) and store them in a temp folder. We put these tasks in apache airflow and saw how airflow schedules these tasks to fetch web data. Here is the video https://www.youtube.com/watch?v=NvU5kxgtUpE", "author_fullname": "t2_ck47kwls", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "using airflow to fetch web data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yf4seh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1666908085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today we used requests and os libraries to write object oriented programs that help us to fetch data from the web (html files) and store them in a temp folder. We put these tasks in apache airflow and saw how airflow schedules these tasks to fetch web data. Here is the video &lt;a href=\"https://www.youtube.com/watch?v=NvU5kxgtUpE\"&gt;https://www.youtube.com/watch?v=NvU5kxgtUpE&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1e25_scX4wJ_YpFSTVZczuYMCTsmW9UHt9TpSMsPODw.jpg?auto=webp&amp;s=1113a2c334403c48e65782a13567f1a6acbb818e", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/1e25_scX4wJ_YpFSTVZczuYMCTsmW9UHt9TpSMsPODw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a653a99d1b5d5c137de5483d47ff0f7defe6f857", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/1e25_scX4wJ_YpFSTVZczuYMCTsmW9UHt9TpSMsPODw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=341ec7eb87d1c4e59481c7316495ffd986decbbc", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/1e25_scX4wJ_YpFSTVZczuYMCTsmW9UHt9TpSMsPODw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c02396ddc791a3cc73e936e7c4cbaf0f70a83f1", "width": 320, "height": 240}], "variants": {}, "id": "eE41GQD9d0XY9OK9AaKrtI1yeFDpGfgy-bQXha-6oVM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yf4seh", "is_robot_indexable": true, "report_reasons": null, "author": "DaliCodes", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yf4seh/using_airflow_to_fetch_web_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yf4seh/using_airflow_to_fetch_web_data/", "subreddit_subscribers": 78147, "created_utc": 1666908085.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is anyone from Perth, Western Australia who knows a bit about the tech consultancy space in Perth willing to catch up for lunch? My treat :) Or alternatively, coffee catchup or a phone call or email exchange. Whatever is preferable!\n\nI'm a BI developer / mech eng working in a oil and gas company and would like to transition to a tech consultancy in the near future. I would love to dip my hands in some data engineering at some point too... But I have no clue on what companies are players in Perth, and which are good/bad or big/small etc. Any conversation would be immensely helpful!\n\nI've heard of a few and chatted to two of them... Empired, insight, servian, data divers, journey one, velrada... visagio? Deloitte? Don't know what's what!\n\nAlso going to post in Power BI subreddit to see if anyone pops up.\n\nThanks!", "author_fullname": "t2_g0zthzyh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone in Perth WA working know much about tech consultancies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfnj2h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666961967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone from Perth, Western Australia who knows a bit about the tech consultancy space in Perth willing to catch up for lunch? My treat :) Or alternatively, coffee catchup or a phone call or email exchange. Whatever is preferable!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a BI developer / mech eng working in a oil and gas company and would like to transition to a tech consultancy in the near future. I would love to dip my hands in some data engineering at some point too... But I have no clue on what companies are players in Perth, and which are good/bad or big/small etc. Any conversation would be immensely helpful!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve heard of a few and chatted to two of them... Empired, insight, servian, data divers, journey one, velrada... visagio? Deloitte? Don&amp;#39;t know what&amp;#39;s what!&lt;/p&gt;\n\n&lt;p&gt;Also going to post in Power BI subreddit to see if anyone pops up.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yfnj2h", "is_robot_indexable": true, "report_reasons": null, "author": "FarLeading2825", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yfnj2h/anyone_in_perth_wa_working_know_much_about_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yfnj2h/anyone_in_perth_wa_working_know_much_about_tech/", "subreddit_subscribers": 78147, "created_utc": 1666961967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \n\n  \nI am converting a unix time column to a datetime human readable format, and while using the function below I have a weird behaviour.  \n\nThe column creationDate contains unix stamps like : `1666828800032`\n\nWhat is also weird is the year is the only anomaly. the rest is fine.\n\nUsing external conversion tools showed that the original format is correct.\n\nThanks!  \n\n\nhttps://preview.redd.it/2olqs5azcdw91.png?width=600&amp;format=png&amp;auto=webp&amp;s=a558bbd1c23bd076c08598c2d5ac2b64853f7b56", "author_fullname": "t2_o1lpjxry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark from_unixtime year not working as expected.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 130, "top_awarded_type": null, "hide_score": false, "media_metadata": {"2olqs5azcdw91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 100, "x": 108, "u": "https://preview.redd.it/2olqs5azcdw91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ddf7e7485806641393a4da2538164c318b750bc5"}, {"y": 200, "x": 216, "u": "https://preview.redd.it/2olqs5azcdw91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d13e56eda982a17567ee7faae55903d133a8386e"}, {"y": 297, "x": 320, "u": "https://preview.redd.it/2olqs5azcdw91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=45c94555e8c1eaf762319ea04a440fb4dea817e8"}], "s": {"y": 558, "x": 600, "u": "https://preview.redd.it/2olqs5azcdw91.png?width=600&amp;format=png&amp;auto=webp&amp;s=a558bbd1c23bd076c08598c2d5ac2b64853f7b56"}, "id": "2olqs5azcdw91"}}, "name": "t3_yevc6n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1QsNuqylwhtXWrMSxGyb7QH1hchOGlSwEnWhrnhiVl8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666885543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I am converting a unix time column to a datetime human readable format, and while using the function below I have a weird behaviour.  &lt;/p&gt;\n\n&lt;p&gt;The column creationDate contains unix stamps like : &lt;code&gt;1666828800032&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;What is also weird is the year is the only anomaly. the rest is fine.&lt;/p&gt;\n\n&lt;p&gt;Using external conversion tools showed that the original format is correct.&lt;/p&gt;\n\n&lt;p&gt;Thanks!  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2olqs5azcdw91.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a558bbd1c23bd076c08598c2d5ac2b64853f7b56\"&gt;https://preview.redd.it/2olqs5azcdw91.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a558bbd1c23bd076c08598c2d5ac2b64853f7b56&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yevc6n", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Freedom9865", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yevc6n/pyspark_from_unixtime_year_not_working_as_expected/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yevc6n/pyspark_from_unixtime_year_not_working_as_expected/", "subreddit_subscribers": 78147, "created_utc": 1666885543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Exactly *Isn't* dbt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 97, "top_awarded_type": null, "hide_score": false, "name": "t3_yfj9df", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NN16eeNur8YGrU_clqf9T7E3Jt-ExFHPYLVQ6B8UIVs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666948652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "stkbailey.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://stkbailey.substack.com/p/what-exactly-isnt-dbt", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Y4iDwkP0oerE2Pj-fhyg2yX9_6e-r0ObtE4xaxqRozM.jpg?auto=webp&amp;s=70aa9f2b5b4c26268850dc9e88748a39e288ade8", "width": 859, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Y4iDwkP0oerE2Pj-fhyg2yX9_6e-r0ObtE4xaxqRozM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f7c7821b0bfa34d2028b62ddfde8030efc68d2b", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/Y4iDwkP0oerE2Pj-fhyg2yX9_6e-r0ObtE4xaxqRozM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ee5d8a0e4f758f624fbf75e4a138824c522c102e", "width": 216, "height": 150}, {"url": "https://external-preview.redd.it/Y4iDwkP0oerE2Pj-fhyg2yX9_6e-r0ObtE4xaxqRozM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2e80cb8283ddbf15f991043b6db2b41052783c9", "width": 320, "height": 223}, {"url": "https://external-preview.redd.it/Y4iDwkP0oerE2Pj-fhyg2yX9_6e-r0ObtE4xaxqRozM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b71953a84550ff68e5dd8bf0538901414b70492e", "width": 640, "height": 447}], "variants": {}, "id": "SUYqamml5Hw7wKOmN2-Fy2Iqs5U9GU7olebjOzvWR0s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yfj9df", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yfj9df/what_exactly_isnt_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://stkbailey.substack.com/p/what-exactly-isnt-dbt", "subreddit_subscribers": 78147, "created_utc": 1666948652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What's your take on [Datafusion](https://github.com/apache/arrow-datafusion) ? Will it someday replace Spark ?", "author_fullname": "t2_to6i6cdw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datafusion VS Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfoahz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1666963921.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s your take on &lt;a href=\"https://github.com/apache/arrow-datafusion\"&gt;Datafusion&lt;/a&gt; ? Will it someday replace Spark ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HZqOAXIgvL5Fo3Oz08xl0Fp7xRzRt_EtIZiVi6yqQ5c.jpg?auto=webp&amp;s=19afd3a18cc3990fd32177cb77a11e397bf26fbc", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/HZqOAXIgvL5Fo3Oz08xl0Fp7xRzRt_EtIZiVi6yqQ5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=091a60c1ae110a3e132ef51e223673c6d5e53513", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/HZqOAXIgvL5Fo3Oz08xl0Fp7xRzRt_EtIZiVi6yqQ5c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ee91871a5ebbfb44b15ce416c0895801f85e309c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/HZqOAXIgvL5Fo3Oz08xl0Fp7xRzRt_EtIZiVi6yqQ5c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=08277c50a765ed94e876f2edf3017668191a1bd3", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/HZqOAXIgvL5Fo3Oz08xl0Fp7xRzRt_EtIZiVi6yqQ5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=14e1e9db417529080a0ee4ae66784fa3c4f98671", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/HZqOAXIgvL5Fo3Oz08xl0Fp7xRzRt_EtIZiVi6yqQ5c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=493ecc03e2625df8eea5c8ab9c4e582c312b3e52", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/HZqOAXIgvL5Fo3Oz08xl0Fp7xRzRt_EtIZiVi6yqQ5c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cbd32efc776357b7a4e38ea57fce9c7f3aa9dc22", "width": 1080, "height": 540}], "variants": {}, "id": "SkYkuh2Nq5i_pGiCN2mNn1WsEw1iyAet-hT43UgHILc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yfoahz", "is_robot_indexable": true, "report_reasons": null, "author": "LabAway8794", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yfoahz/datafusion_vs_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yfoahz/datafusion_vs_spark/", "subreddit_subscribers": 78147, "created_utc": 1666963921.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, what is the SQL client you use for BigData. I am using Dbeaver (as it's free) but honestly because of the metadata of BigData databases refresh takes forever. Also it shows the tool has connected to the DB but until it syncs the metadata queries fail with connectivity issues. This is quite irritating.\n\nSo what tips do you have for using SQL client for BigData databases and is it paid?", "author_fullname": "t2_umfhx4d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which SQL Client for BigData do you use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yfo4kx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666963509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, what is the SQL client you use for BigData. I am using Dbeaver (as it&amp;#39;s free) but honestly because of the metadata of BigData databases refresh takes forever. Also it shows the tool has connected to the DB but until it syncs the metadata queries fail with connectivity issues. This is quite irritating.&lt;/p&gt;\n\n&lt;p&gt;So what tips do you have for using SQL client for BigData databases and is it paid?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yfo4kx", "is_robot_indexable": true, "report_reasons": null, "author": "demince", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yfo4kx/which_sql_client_for_bigdata_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yfo4kx/which_sql_client_for_bigdata_do_you_use/", "subreddit_subscribers": 78147, "created_utc": 1666963509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I'm just starting to get exposure into AWS and I'm having a hard time understanding if an architecture that was handed to me makes any sense. Wanted to hear people's takes on how they would approach this problem.\n\nThere is a snowflake datalake that contains data representing the back end of an existing application.\n\nOur team is replacing that application, and as part of that process we need to hydrate our existing our new application database (dynamoDB) with the old data.\n\nThe data model has shifted between the two applications, but in theory we have a mapping between the old application data data and the api endpoints of the new application  (not the new application data model).\n\nThe proposal was to use AWS Glue to extract data from Snowflake into S3, and from S3 run a lambda or further leverage Glue to transform the data and feed it into the API. Hitting the dynamoDB backend directly was rejected due to some backend work that occurs in the new data model.\n\nThis would be a one time activity with the caveat being that there would need to be some delta loading while the current application is shut down and replaced with the new application.\n\nThe proposed solution so far has been use AWS Glue -&gt; S3 (Read from SFLK into S3) and then use lambdas or some other kind of compute to decompose the tables into individual api calls.\n\nThere are obvious red flags with hitting the API with that much load, as well as how slow it would be to decompose a table into rows and feed those piecemeal into an API.\n\nMy current thoughts after trying to absorb all of this is to try to get Snowflake to directly COPY that into S3 and from there try persuading the API team to expose some kind of batch endpoint or just perform a proper mapping from the old DB to DynamoDB (which would be challenging because the old DB was a relational model).\n\nAny help or experience in the same vein would be gratefully appreciated!", "author_fullname": "t2_6d49ikq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on hydrating an AWS hosted API with data from Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yf736o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666913648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m just starting to get exposure into AWS and I&amp;#39;m having a hard time understanding if an architecture that was handed to me makes any sense. Wanted to hear people&amp;#39;s takes on how they would approach this problem.&lt;/p&gt;\n\n&lt;p&gt;There is a snowflake datalake that contains data representing the back end of an existing application.&lt;/p&gt;\n\n&lt;p&gt;Our team is replacing that application, and as part of that process we need to hydrate our existing our new application database (dynamoDB) with the old data.&lt;/p&gt;\n\n&lt;p&gt;The data model has shifted between the two applications, but in theory we have a mapping between the old application data data and the api endpoints of the new application  (not the new application data model).&lt;/p&gt;\n\n&lt;p&gt;The proposal was to use AWS Glue to extract data from Snowflake into S3, and from S3 run a lambda or further leverage Glue to transform the data and feed it into the API. Hitting the dynamoDB backend directly was rejected due to some backend work that occurs in the new data model.&lt;/p&gt;\n\n&lt;p&gt;This would be a one time activity with the caveat being that there would need to be some delta loading while the current application is shut down and replaced with the new application.&lt;/p&gt;\n\n&lt;p&gt;The proposed solution so far has been use AWS Glue -&amp;gt; S3 (Read from SFLK into S3) and then use lambdas or some other kind of compute to decompose the tables into individual api calls.&lt;/p&gt;\n\n&lt;p&gt;There are obvious red flags with hitting the API with that much load, as well as how slow it would be to decompose a table into rows and feed those piecemeal into an API.&lt;/p&gt;\n\n&lt;p&gt;My current thoughts after trying to absorb all of this is to try to get Snowflake to directly COPY that into S3 and from there try persuading the API team to expose some kind of batch endpoint or just perform a proper mapping from the old DB to DynamoDB (which would be challenging because the old DB was a relational model).&lt;/p&gt;\n\n&lt;p&gt;Any help or experience in the same vein would be gratefully appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yf736o", "is_robot_indexable": true, "report_reasons": null, "author": "poppinstacks", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yf736o/thoughts_on_hydrating_an_aws_hosted_api_with_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yf736o/thoughts_on_hydrating_an_aws_hosted_api_with_data/", "subreddit_subscribers": 78147, "created_utc": 1666913648.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need a solution that scales and shrinks as needed.", "author_fullname": "t2_tfwkz0xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hey! Any tips on how I can build a Webhook on Google Cloud Run that receives JSON POST requests, do a little transformation and store them into a Google MySQL DB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yewc5i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666887956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need a solution that scales and shrinks as needed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yewc5i", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Object_7904", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yewc5i/hey_any_tips_on_how_i_can_build_a_webhook_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yewc5i/hey_any_tips_on_how_i_can_build_a_webhook_on/", "subreddit_subscribers": 78147, "created_utc": 1666887956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi.\n\nI am new to Airflow, and I am writing a DAG to read a file from a GCS Bucket and insert it into BigQuery. \n\nSo far, I have written the following code, which has my schema:\n```\nload_csv = gcs_to_bq.GoogleCloudStorageToBigQueryOperator(\n    task_id='gcs_to_bq_example',\n    bucket='cloud-samples-data',\n    source_objects=['bigquery/us-states/us-states.csv'],\n    destination_project_dataset_table='airflow_test.gcs_to_bq_table',\n    schema_fields=[\n        {'name': 'name', 'type': 'STRING', 'mode': 'NULLABLE'},\n        {'name': 'insertion_date', 'type': 'DATE', 'mode': 'NULLABLE'},\n    ],\n    write_disposition='WRITE_TRUNCATE',\n    dag=dag)\n```\n\nIn my schema, I have a column called ```insertion_date```, and I want it to store the current time stamp of when the data gets inserted into BigQuery.\n\nFor example, as today is 28-10-2022, if I run my DAG and execute it successfully today, then the ```insertion_date``` column will store today's date.\n\nAny help would be appreciated.", "author_fullname": "t2_13ussz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow: How to get the current timestamp of when data is inserted into BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_yfpc11", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666966505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi.&lt;/p&gt;\n\n&lt;p&gt;I am new to Airflow, and I am writing a DAG to read a file from a GCS Bucket and insert it into BigQuery. &lt;/p&gt;\n\n&lt;p&gt;So far, I have written the following code, which has my schema:\n&lt;code&gt;\nload_csv = gcs_to_bq.GoogleCloudStorageToBigQueryOperator(\n    task_id=&amp;#39;gcs_to_bq_example&amp;#39;,\n    bucket=&amp;#39;cloud-samples-data&amp;#39;,\n    source_objects=[&amp;#39;bigquery/us-states/us-states.csv&amp;#39;],\n    destination_project_dataset_table=&amp;#39;airflow_test.gcs_to_bq_table&amp;#39;,\n    schema_fields=[\n        {&amp;#39;name&amp;#39;: &amp;#39;name&amp;#39;, &amp;#39;type&amp;#39;: &amp;#39;STRING&amp;#39;, &amp;#39;mode&amp;#39;: &amp;#39;NULLABLE&amp;#39;},\n        {&amp;#39;name&amp;#39;: &amp;#39;insertion_date&amp;#39;, &amp;#39;type&amp;#39;: &amp;#39;DATE&amp;#39;, &amp;#39;mode&amp;#39;: &amp;#39;NULLABLE&amp;#39;},\n    ],\n    write_disposition=&amp;#39;WRITE_TRUNCATE&amp;#39;,\n    dag=dag)\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;In my schema, I have a column called &lt;code&gt;insertion_date&lt;/code&gt;, and I want it to store the current time stamp of when the data gets inserted into BigQuery.&lt;/p&gt;\n\n&lt;p&gt;For example, as today is 28-10-2022, if I run my DAG and execute it successfully today, then the &lt;code&gt;insertion_date&lt;/code&gt; column will store today&amp;#39;s date.&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yfpc11", "is_robot_indexable": true, "report_reasons": null, "author": "saucyhambon", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yfpc11/airflow_how_to_get_the_current_timestamp_of_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yfpc11/airflow_how_to_get_the_current_timestamp_of_when/", "subreddit_subscribers": 78147, "created_utc": 1666966505.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in the early stages of building out a data model / data warehouse in my company. We are loosely following star schema design patterns. \n\nOne of the common analyses our users need is sales / orders by new and repeat users at all levels in our product hierarchy.\n\nIs anyone aware of any design patterns or approaches we could use to model our data?", "author_fullname": "t2_128qvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modelling new and repeat customers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yextvb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666891360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in the early stages of building out a data model / data warehouse in my company. We are loosely following star schema design patterns. &lt;/p&gt;\n\n&lt;p&gt;One of the common analyses our users need is sales / orders by new and repeat users at all levels in our product hierarchy.&lt;/p&gt;\n\n&lt;p&gt;Is anyone aware of any design patterns or approaches we could use to model our data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yextvb", "is_robot_indexable": true, "report_reasons": null, "author": "mathewtrivett", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yextvb/modelling_new_and_repeat_customers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yextvb/modelling_new_and_repeat_customers/", "subreddit_subscribers": 78147, "created_utc": 1666891360.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}