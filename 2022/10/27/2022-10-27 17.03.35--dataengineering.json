{"kind": "Listing", "data": {"after": "t3_yek5dh", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, \n\nI am a data engineer with almost 4 years of experience. I am currently based in Switzerland and my total compensation is around 180k in the insurance industry. \n\nI feel that as a data engineer I won't be able to significantly increase my income anymore in Europe. \n\nDo you have some tips how to create an additional source of income as a DE in Europe? My evenings and weekends are relatively free, but I am not sure if it is enough time for a side gig.", "author_fullname": "t2_5p9fb9t6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Additional sources of income for a data engineer with a day job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yelbn4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666855730.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666855148.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I am a data engineer with almost 4 years of experience. I am currently based in Switzerland and my total compensation is around 180k in the insurance industry. &lt;/p&gt;\n\n&lt;p&gt;I feel that as a data engineer I won&amp;#39;t be able to significantly increase my income anymore in Europe. &lt;/p&gt;\n\n&lt;p&gt;Do you have some tips how to create an additional source of income as a DE in Europe? My evenings and weekends are relatively free, but I am not sure if it is enough time for a side gig.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yelbn4", "is_robot_indexable": true, "report_reasons": null, "author": "DigAggressive2982", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yelbn4/additional_sources_of_income_for_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yelbn4/additional_sources_of_income_for_a_data_engineer/", "subreddit_subscribers": 78031, "created_utc": 1666855148.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to come up with a list of companies that have the strongest data engineering teams. I would think teams with location services would have great streaming data expertise, like Uber or Lyft. I would also think logistics companies like Flexport might have great data teams too for automated decision making. What other categories would you sniff down to come up with a list? \n\n&amp;#x200B;\n\nThe goal is just trying to find people who are top notch in the space to talk to and maybe write some Medium articles about things they think are interesting issues in the space today, or find some people to maybe do a podcast. This would be a pet project while I am a grad student since people seem to be responsive to cold emails while I'm a student, and that goodwill probably won't continue when I'm a regular old working adult again. I think it would be fun.\n\n&amp;#x200B;\n\nThank you.", "author_fullname": "t2_rcx1cqyo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think are the companies that have the strongest data engineering teams?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ye8pma", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666817771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to come up with a list of companies that have the strongest data engineering teams. I would think teams with location services would have great streaming data expertise, like Uber or Lyft. I would also think logistics companies like Flexport might have great data teams too for automated decision making. What other categories would you sniff down to come up with a list? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The goal is just trying to find people who are top notch in the space to talk to and maybe write some Medium articles about things they think are interesting issues in the space today, or find some people to maybe do a podcast. This would be a pet project while I am a grad student since people seem to be responsive to cold emails while I&amp;#39;m a student, and that goodwill probably won&amp;#39;t continue when I&amp;#39;m a regular old working adult again. I think it would be fun.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ye8pma", "is_robot_indexable": true, "report_reasons": null, "author": "----bubba----", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ye8pma/what_do_you_think_are_the_companies_that_have_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ye8pma/what_do_you_think_are_the_companies_that_have_the/", "subreddit_subscribers": 78031, "created_utc": 1666817771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As in title. No context given, open interpretation. I'm interested in responses. :)", "author_fullname": "t2_9e7m1qmr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What makes a good data engineer? What differentiates good data engineer from the rest?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yeclpm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666827798.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As in title. No context given, open interpretation. I&amp;#39;m interested in responses. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yeclpm", "is_robot_indexable": true, "report_reasons": null, "author": "LewWariat", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yeclpm/what_makes_a_good_data_engineer_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yeclpm/what_makes_a_good_data_engineer_what/", "subreddit_subscribers": 78031, "created_utc": 1666827798.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know it varies on background, but what do you expect from a junior / medior / senior DE? What are the \"must-know\" questions based on seniority? \n\nDo you usually do live coding? If yes what kind of problems do you focus on?\n\nIf the candidate has a personal project do you care about it? Even if its a medior/senior candidate?", "author_fullname": "t2_xcba5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Technical interviewers! Based on seniority what do you usually expect from candidates?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yesj6s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666878728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know it varies on background, but what do you expect from a junior / medior / senior DE? What are the &amp;quot;must-know&amp;quot; questions based on seniority? &lt;/p&gt;\n\n&lt;p&gt;Do you usually do live coding? If yes what kind of problems do you focus on?&lt;/p&gt;\n\n&lt;p&gt;If the candidate has a personal project do you care about it? Even if its a medior/senior candidate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "yesj6s", "is_robot_indexable": true, "report_reasons": null, "author": "mackbenc", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yesj6s/technical_interviewers_based_on_seniority_what_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yesj6s/technical_interviewers_based_on_seniority_what_do/", "subreddit_subscribers": 78031, "created_utc": 1666878728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What the title says.", "author_fullname": "t2_52v5oro5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "You have $500 to spend on education and professional development before the end of the year. How do you spend it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yecawp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666833342.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666826968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What the title says.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yecawp", "is_robot_indexable": true, "report_reasons": null, "author": "skiwhatwhat", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yecawp/you_have_500_to_spend_on_education_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yecawp/you_have_500_to_spend_on_education_and/", "subreddit_subscribers": 78031, "created_utc": 1666826968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI'm hoping someone can help me here.\n\nI'm connecting to three tables in Snowflake - header, batch\\_data, employees. Both detail tables connect to the header table on header.header\\_id. I'm trying to create a nested dataset that can be converted into **both** xml and json.\n\nSimplified, but assume the tables are as follows:\n\nheader: header\\_id, batch\n\nbatch\\_data: header\\_id, amount, description\n\nemployees: header\\_id, person\n\nThere is a one to many header &gt; batch\\_data, and also a one to many header &gt; employees.\n\nThe structure will be somewhat like this (json view):\n\n    {\n      \"id\": 123,\n      \"batch\": \"something\"\n      \"batch_data\": [\n        {\n          \"amount\": 15.30,\n          \"description\": \"some description\"\n        },\n        {\n          \"amount\": 16.74,\n          \"description\": \"some other description\"\n        }\n      ],\n      \"employees\": [\n        {\n          \"person\": \"John\"\n        },\n        {\n          \"person\": \"Jane\"\n        }\n      ]\n    }\n\nI can't simply do a pandas.merge as I don't want to fan out the dataset when there are multiple batch records and multiple employee records for the same header.\n\nI'm feeling considerably lost.\n\nAny guidance would be really appreciated!", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python question I'm too afraid to ask on Stack Overflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yeh4ka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666840771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping someone can help me here.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m connecting to three tables in Snowflake - header, batch_data, employees. Both detail tables connect to the header table on header.header_id. I&amp;#39;m trying to create a nested dataset that can be converted into &lt;strong&gt;both&lt;/strong&gt; xml and json.&lt;/p&gt;\n\n&lt;p&gt;Simplified, but assume the tables are as follows:&lt;/p&gt;\n\n&lt;p&gt;header: header_id, batch&lt;/p&gt;\n\n&lt;p&gt;batch_data: header_id, amount, description&lt;/p&gt;\n\n&lt;p&gt;employees: header_id, person&lt;/p&gt;\n\n&lt;p&gt;There is a one to many header &amp;gt; batch_data, and also a one to many header &amp;gt; employees.&lt;/p&gt;\n\n&lt;p&gt;The structure will be somewhat like this (json view):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;id&amp;quot;: 123,\n  &amp;quot;batch&amp;quot;: &amp;quot;something&amp;quot;\n  &amp;quot;batch_data&amp;quot;: [\n    {\n      &amp;quot;amount&amp;quot;: 15.30,\n      &amp;quot;description&amp;quot;: &amp;quot;some description&amp;quot;\n    },\n    {\n      &amp;quot;amount&amp;quot;: 16.74,\n      &amp;quot;description&amp;quot;: &amp;quot;some other description&amp;quot;\n    }\n  ],\n  &amp;quot;employees&amp;quot;: [\n    {\n      &amp;quot;person&amp;quot;: &amp;quot;John&amp;quot;\n    },\n    {\n      &amp;quot;person&amp;quot;: &amp;quot;Jane&amp;quot;\n    }\n  ]\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I can&amp;#39;t simply do a pandas.merge as I don&amp;#39;t want to fan out the dataset when there are multiple batch records and multiple employee records for the same header.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m feeling considerably lost.&lt;/p&gt;\n\n&lt;p&gt;Any guidance would be really appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yeh4ka", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yeh4ka/python_question_im_too_afraid_to_ask_on_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yeh4ka/python_question_im_too_afraid_to_ask_on_stack/", "subreddit_subscribers": 78031, "created_utc": 1666840771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_aehfkmkb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Zero to Hero! - Session 2 | Data Pipeline to Data Lake | Chal...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_yefd2d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/W8FCirJB5C0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Databricks Zero to Hero! - Session 2 | Data Pipeline to Data Lake | Challenges\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Databricks Zero to Hero! - Session 2 | Data Pipeline to Data Lake | Challenges", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/W8FCirJB5C0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Databricks Zero to Hero! - Session 2 | Data Pipeline to Data Lake | Challenges\"&gt;&lt;/iframe&gt;", "author_name": "Cloud and Data Science", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/W8FCirJB5C0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/c/CloudDataScience"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/W8FCirJB5C0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Databricks Zero to Hero! - Session 2 | Data Pipeline to Data Lake | Challenges\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/yefd2d", "height": 200}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/drcVPOOR6yHmRP_rouZPmeaOAxp-8NC6zQoJs2ARBqU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666835648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtube.com/watch?v=W8FCirJB5C0&amp;feature=share", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IFtNyI7Qnu-NjG5S3kmINMrvINRJieC1DAklgGr7c18.jpg?auto=webp&amp;s=e965a0ddebb354ebf6ed1e281feddf66a64ec50f", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/IFtNyI7Qnu-NjG5S3kmINMrvINRJieC1DAklgGr7c18.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=11cbdb9e2952643145d10d60d8dee81bfc2dd5e5", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/IFtNyI7Qnu-NjG5S3kmINMrvINRJieC1DAklgGr7c18.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9d4a669c935a6040cf840907be6a0b43ba285892", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/IFtNyI7Qnu-NjG5S3kmINMrvINRJieC1DAklgGr7c18.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=adc30e049ba01c711fb8f88211fb847ffb1d0e63", "width": 320, "height": 240}], "variants": {}, "id": "vWI321XYGFXz-xCeOWkIjCQMwnKbHVrg7u1WUMofqQE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "yefd2d", "is_robot_indexable": true, "report_reasons": null, "author": "Successful-Aide3077", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yefd2d/databricks_zero_to_hero_session_2_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtube.com/watch?v=W8FCirJB5C0&amp;feature=share", "subreddit_subscribers": 78031, "created_utc": 1666835648.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Databricks Zero to Hero! - Session 2 | Data Pipeline to Data Lake | Challenges", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/W8FCirJB5C0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Databricks Zero to Hero! - Session 2 | Data Pipeline to Data Lake | Challenges\"&gt;&lt;/iframe&gt;", "author_name": "Cloud and Data Science", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/W8FCirJB5C0/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/c/CloudDataScience"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So the story behind the picture is that I got hired as a data engineer about half a year ago because I had some experience with Power BI and ERP software development... the project I started on was already a massive sh\\*tshow with restricted access to source systems, missing layers from the architecture and a completely clueless senior DE guy... it turned out they hired me bc they hoped that I could model their entire data architecture :'D \n\nSo, I panicked and resorted to Kimball... which turned out to be absolutely useless, because of late-arriving data, SCD-s, joins, etc. The project failed, naturally. Then for the next project, I tried the medallion architecture and the same problems arose, especially with streaming, because of the recommendation of \"business-level aggregates\" in the Gold Zone... after a lot of conceptualizing and experimentation I managed to put this together, which I think is more or less in line with the Data Mesh principles and works in practice so far...\n\nSoo.. any opinions on this?\n\n[https://github.com/harigabor96/Wildfires](https://github.com/harigabor96/Wildfires)\n\nhttps://preview.redd.it/yi1apy76v6w91.jpg?width=1260&amp;format=pjpg&amp;auto=webp&amp;s=b89ba65fa2b2d028494e3ee7926677327396bf9d", "author_fullname": "t2_djj7qjn9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Lakehouse and Data Mesh and streaming (Any thoughts on this?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": false, "media_metadata": {"yi1apy76v6w91": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/yi1apy76v6w91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8303e2bf3bb164b9033f4582ccb992baf12676ec"}, {"y": 126, "x": 216, "u": "https://preview.redd.it/yi1apy76v6w91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd7df82f310710b9a94b57d91316ecd6b117d6b9"}, {"y": 187, "x": 320, "u": "https://preview.redd.it/yi1apy76v6w91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=39f7cfa2e2e25654e4bc653601ce0b886855aaa6"}, {"y": 374, "x": 640, "u": "https://preview.redd.it/yi1apy76v6w91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=38a5af5b807fe4d91673cce6bf0f2f9e4bc03b64"}, {"y": 561, "x": 960, "u": "https://preview.redd.it/yi1apy76v6w91.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=04c265947c6c31c9dd09a3506ba8bbd27a0fef23"}, {"y": 631, "x": 1080, "u": "https://preview.redd.it/yi1apy76v6w91.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=65c621b3ff7a1b0ca3204fb3c9d34aea9c234f73"}], "s": {"y": 737, "x": 1260, "u": "https://preview.redd.it/yi1apy76v6w91.jpg?width=1260&amp;format=pjpg&amp;auto=webp&amp;s=b89ba65fa2b2d028494e3ee7926677327396bf9d"}, "id": "yi1apy76v6w91"}}, "name": "t3_ye4ufw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jLCd78aMHGZAGq0utff3DgABmgVBUonSDzmICJsD26U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1666808003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So the story behind the picture is that I got hired as a data engineer about half a year ago because I had some experience with Power BI and ERP software development... the project I started on was already a massive sh*tshow with restricted access to source systems, missing layers from the architecture and a completely clueless senior DE guy... it turned out they hired me bc they hoped that I could model their entire data architecture :&amp;#39;D &lt;/p&gt;\n\n&lt;p&gt;So, I panicked and resorted to Kimball... which turned out to be absolutely useless, because of late-arriving data, SCD-s, joins, etc. The project failed, naturally. Then for the next project, I tried the medallion architecture and the same problems arose, especially with streaming, because of the recommendation of &amp;quot;business-level aggregates&amp;quot; in the Gold Zone... after a lot of conceptualizing and experimentation I managed to put this together, which I think is more or less in line with the Data Mesh principles and works in practice so far...&lt;/p&gt;\n\n&lt;p&gt;Soo.. any opinions on this?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/harigabor96/Wildfires\"&gt;https://github.com/harigabor96/Wildfires&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yi1apy76v6w91.jpg?width=1260&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b89ba65fa2b2d028494e3ee7926677327396bf9d\"&gt;https://preview.redd.it/yi1apy76v6w91.jpg?width=1260&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b89ba65fa2b2d028494e3ee7926677327396bf9d&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FkW7mCXc2avVdJwTfUIs21rnHVxX63nO8l12JObyhsk.jpg?auto=webp&amp;s=520010252982ebb55cbe9ddcd189724fe9e9f81f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/FkW7mCXc2avVdJwTfUIs21rnHVxX63nO8l12JObyhsk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7127201ec5c604db9e24e91a649e5be43e62c323", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/FkW7mCXc2avVdJwTfUIs21rnHVxX63nO8l12JObyhsk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7e8a70db0742ea7a571184b3b4ba2bcfaa6d937", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/FkW7mCXc2avVdJwTfUIs21rnHVxX63nO8l12JObyhsk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=76f7874740f3242554c9b6a8935e485d82a22dee", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/FkW7mCXc2avVdJwTfUIs21rnHVxX63nO8l12JObyhsk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe909fa508bd585d53ea090e812b45a76072522e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/FkW7mCXc2avVdJwTfUIs21rnHVxX63nO8l12JObyhsk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8309a0a8719c4e0e662591fdc84ec9dedcbd23a1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/FkW7mCXc2avVdJwTfUIs21rnHVxX63nO8l12JObyhsk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2316e7976352494d9c5ce94530ee41d52336219e", "width": 1080, "height": 540}], "variants": {}, "id": "i5b-sLyb4PlqLN44G2ZQeDSEfkArgGdPDDHGr2EHBYY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "ye4ufw", "is_robot_indexable": true, "report_reasons": null, "author": "patka96", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ye4ufw/data_lakehouse_and_data_mesh_and_streaming_any/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ye4ufw/data_lakehouse_and_data_mesh_and_streaming_any/", "subreddit_subscribers": 78031, "created_utc": 1666808003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to ask this, I have read about batch processing and stream processing at a high level but in my project, I am doing data ingestion from an OLTP system during its freeze time. this is batch processing right??", "author_fullname": "t2_6f6khk66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data ingestion from an OLTP system", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yeqbm0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666872683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to ask this, I have read about batch processing and stream processing at a high level but in my project, I am doing data ingestion from an OLTP system during its freeze time. this is batch processing right??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yeqbm0", "is_robot_indexable": true, "report_reasons": null, "author": "mainak17", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yeqbm0/data_ingestion_from_an_oltp_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yeqbm0/data_ingestion_from_an_oltp_system/", "subreddit_subscribers": 78031, "created_utc": 1666872683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m doing a data science master\u2019s and don\u2019t come from a very technical background prior. Sure, ML models are cool and all, but honestly I get more satisfaction out of cleaning and processing data earlier in the lifecycle and I\u2019m more analytics minded. I don\u2019t have credentials to be doing neural nets and feel like chasing the data science title would be a waste and an opportunity cost.\n\nWith this kind of disposition I feel like I might be a better fit for data engineering.\n\nWhat would you advise? Any encouraging words to fight the inevitable impostor syndrome? Haha. Thanks all", "author_fullname": "t2_k1u5d2ej", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I pivot to data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yel4ih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666854373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m doing a data science master\u2019s and don\u2019t come from a very technical background prior. Sure, ML models are cool and all, but honestly I get more satisfaction out of cleaning and processing data earlier in the lifecycle and I\u2019m more analytics minded. I don\u2019t have credentials to be doing neural nets and feel like chasing the data science title would be a waste and an opportunity cost.&lt;/p&gt;\n\n&lt;p&gt;With this kind of disposition I feel like I might be a better fit for data engineering.&lt;/p&gt;\n\n&lt;p&gt;What would you advise? Any encouraging words to fight the inevitable impostor syndrome? Haha. Thanks all&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yel4ih", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Country_3732", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yel4ih/should_i_pivot_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yel4ih/should_i_pivot_to_data_engineering/", "subreddit_subscribers": 78031, "created_utc": 1666854373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to reuse TaskGroups in Airflow and make better DAGs!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 115, "top_awarded_type": null, "hide_score": false, "name": "t3_yet4s9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kQej6qOOo8JGH9ju51v0FDrUiUDJVpdZ3-F0OA8Z6Xk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666880210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/feed/update/urn:li:activity:6991398253325869056/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wjBoxa4Xip5X_Ad91SVIwaPQTeC0x7_DdtkeJbEE4UM.jpg?auto=webp&amp;s=a0763b5976d29e8e4384e2962bc4b36f4bcc686a", "width": 1070, "height": 884}, "resolutions": [{"url": "https://external-preview.redd.it/wjBoxa4Xip5X_Ad91SVIwaPQTeC0x7_DdtkeJbEE4UM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=200ec977c8639c6dbac5d6f33a9ef1a21441e02c", "width": 108, "height": 89}, {"url": "https://external-preview.redd.it/wjBoxa4Xip5X_Ad91SVIwaPQTeC0x7_DdtkeJbEE4UM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3047e22b937f4b8151b527b23aa77afd4eb06997", "width": 216, "height": 178}, {"url": "https://external-preview.redd.it/wjBoxa4Xip5X_Ad91SVIwaPQTeC0x7_DdtkeJbEE4UM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5eec2d89144e34c161e689b3da410e9aa830fd8", "width": 320, "height": 264}, {"url": "https://external-preview.redd.it/wjBoxa4Xip5X_Ad91SVIwaPQTeC0x7_DdtkeJbEE4UM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0929bb8f062b909c1a14b85e6c84046f6d875afc", "width": 640, "height": 528}, {"url": "https://external-preview.redd.it/wjBoxa4Xip5X_Ad91SVIwaPQTeC0x7_DdtkeJbEE4UM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aef87130e11caa93c395a6fc3425dbd43080da0d", "width": 960, "height": 793}], "variants": {}, "id": "0Tq7PdBUStgC5qLm6ITurkrMJDr7r6CwK2viNt1k3dY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yet4s9", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yet4s9/how_to_reuse_taskgroups_in_airflow_and_make/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/feed/update/urn:li:activity:6991398253325869056/", "subreddit_subscribers": 78031, "created_utc": 1666880210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Today we setup apache airflow then configured it to use postgresql database to store its metadata. We then created an admin user for airflow. We also saw how to run airflow scheduler and airflow webserver, then accessing airflow ui using the admin user we created.\n\n[https://www.youtube.com/watch?v=auyfJq5b9Io](https://www.youtube.com/watch?v=auyfJq5b9Io)", "author_fullname": "t2_ck47kwls", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "setting up apache airflow with postgresql database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ye740n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1666813774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today we setup apache airflow then configured it to use postgresql database to store its metadata. We then created an admin user for airflow. We also saw how to run airflow scheduler and airflow webserver, then accessing airflow ui using the admin user we created.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=auyfJq5b9Io\"&gt;https://www.youtube.com/watch?v=auyfJq5b9Io&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/G8-O0ArN1bSbIfnHJUdgotMMl0eZPR8oSkVR-KsLLEk.jpg?auto=webp&amp;s=7b0289da8c607d4cdac189336ad241a4d81d6969", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/G8-O0ArN1bSbIfnHJUdgotMMl0eZPR8oSkVR-KsLLEk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=213c3411aa0eeeaba61746d79467160ee690eb45", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/G8-O0ArN1bSbIfnHJUdgotMMl0eZPR8oSkVR-KsLLEk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2b5172896415c126df8253ace47538d589c54de9", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/G8-O0ArN1bSbIfnHJUdgotMMl0eZPR8oSkVR-KsLLEk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a94212859acff29d69eaa834e29829ffffe4ee7", "width": 320, "height": 240}], "variants": {}, "id": "-TADoFjtrhvQNtQ7SEEhUeMI9_22dHgvpwW4v-UlCXU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "ye740n", "is_robot_indexable": true, "report_reasons": null, "author": "DaliCodes", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ye740n/setting_up_apache_airflow_with_postgresql_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ye740n/setting_up_apache_airflow_with_postgresql_database/", "subreddit_subscribers": 78031, "created_utc": 1666813774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are a SaaS company in the cybersecurity industry. Looking to hear what you have used as a BI Visualization solution in a similar situation. At our company\u2019s core we are a O365 Microsoft shop but have recently had a large VC funding injection and are moving to scrap our old products on Azure/Dynamics and recreate our new products on AWS (App) and Snowflake (Analytics). We use PowerBI for reporting to our internal stakeholders and will continue to do so.\n\nRight now I\u2019m leaning towards PowerBI embedded as we want our analytics teams who are experts in PBI to manage and deploy these client dashboards and let app focus on what they do best outside analytics and just import these dashboards into IFrames on the apps themselves. \n\nMy main concern is that we are on really tight timelines so will this a heavy lift, especially being on Azure and AWS both connecting through Privatelink? My second concern is we  will have thousands of clients that we\u2019ll have to distribute these dashboards too and the management of IAM access roles between app and PBI will be too complex. \n\nAre there other solutions you recommend? Can we pass a user token to PBI from the app to only expose certain data and will it get out of hand to manage? Thanks for your help on this.", "author_fullname": "t2_811imwee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Client facing Viz Embedded Solution for SaaS company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yehwcb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666843166.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are a SaaS company in the cybersecurity industry. Looking to hear what you have used as a BI Visualization solution in a similar situation. At our company\u2019s core we are a O365 Microsoft shop but have recently had a large VC funding injection and are moving to scrap our old products on Azure/Dynamics and recreate our new products on AWS (App) and Snowflake (Analytics). We use PowerBI for reporting to our internal stakeholders and will continue to do so.&lt;/p&gt;\n\n&lt;p&gt;Right now I\u2019m leaning towards PowerBI embedded as we want our analytics teams who are experts in PBI to manage and deploy these client dashboards and let app focus on what they do best outside analytics and just import these dashboards into IFrames on the apps themselves. &lt;/p&gt;\n\n&lt;p&gt;My main concern is that we are on really tight timelines so will this a heavy lift, especially being on Azure and AWS both connecting through Privatelink? My second concern is we  will have thousands of clients that we\u2019ll have to distribute these dashboards too and the management of IAM access roles between app and PBI will be too complex. &lt;/p&gt;\n\n&lt;p&gt;Are there other solutions you recommend? Can we pass a user token to PBI from the app to only expose certain data and will it get out of hand to manage? Thanks for your help on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yehwcb", "is_robot_indexable": true, "report_reasons": null, "author": "Reasonable_Top_6420", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yehwcb/client_facing_viz_embedded_solution_for_saas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yehwcb/client_facing_viz_embedded_solution_for_saas/", "subreddit_subscribers": 78031, "created_utc": 1666843166.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey All, which is more effective on AWS? Is it Hudi or Iceberg? \nWe are starting to build Transactional Data Lake on AWS and I wanted to gather opinion on these two platforms.", "author_fullname": "t2_th2jok2c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hudi or Iceberg on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ye76p4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666813957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All, which is more effective on AWS? Is it Hudi or Iceberg? \nWe are starting to build Transactional Data Lake on AWS and I wanted to gather opinion on these two platforms.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ye76p4", "is_robot_indexable": true, "report_reasons": null, "author": "padikaha", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/ye76p4/hudi_or_iceberg_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ye76p4/hudi_or_iceberg_on_aws/", "subreddit_subscribers": 78031, "created_utc": 1666813957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Claire and I wrote about \"Roman roads\" in data engineering: [https://dagster.io/blog/roman-roads-assets-ops](https://dagster.io/blog/roman-roads-assets-ops).\n\n&amp;#x200B;\n\nHere's the TLDR:\n\n* A Roman road is something you create while trying to accomplish a task, that helps you or others accomplish similar tasks in the future.\n* Without Roman roads, each time you build a new report or ML model, you end up re-cleaning the data, re-answering questions, re-implementing logic.\n* Roman roads in data engineering come in two forms: data assets and functions.", "author_fullname": "t2_1jjs655y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What code artifacts are reusable in data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ye5awt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1666809154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Claire and I wrote about &amp;quot;Roman roads&amp;quot; in data engineering: &lt;a href=\"https://dagster.io/blog/roman-roads-assets-ops\"&gt;https://dagster.io/blog/roman-roads-assets-ops&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the TLDR:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A Roman road is something you create while trying to accomplish a task, that helps you or others accomplish similar tasks in the future.&lt;/li&gt;\n&lt;li&gt;Without Roman roads, each time you build a new report or ML model, you end up re-cleaning the data, re-answering questions, re-implementing logic.&lt;/li&gt;\n&lt;li&gt;Roman roads in data engineering come in two forms: data assets and functions.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NliydTzZLIJcb7g_jTIV6UFJsuXhEu8gUW4K13qCTHY.jpg?auto=webp&amp;s=8d0ffc55d6e90d36da7e1fb13baa2255434c66de", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/NliydTzZLIJcb7g_jTIV6UFJsuXhEu8gUW4K13qCTHY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d17e1b2ebc2198860ea9b55a6521e26c25efb98", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/NliydTzZLIJcb7g_jTIV6UFJsuXhEu8gUW4K13qCTHY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ebc7ed8b745e4a5a1bcd7cad076a9d17c1fa325", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/NliydTzZLIJcb7g_jTIV6UFJsuXhEu8gUW4K13qCTHY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=23193568a8a524dc994ac341e80a105f7ad4523c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/NliydTzZLIJcb7g_jTIV6UFJsuXhEu8gUW4K13qCTHY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=86a30f8f813b7f38a63e6a573c9f786341f756e6", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/NliydTzZLIJcb7g_jTIV6UFJsuXhEu8gUW4K13qCTHY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7f15b4649bedd04248c8c305485020f16b38fd1", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/NliydTzZLIJcb7g_jTIV6UFJsuXhEu8gUW4K13qCTHY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7eb9824ec64d1fb6d3913d7e2c1ab4bf1cc32d9b", "width": 1080, "height": 567}], "variants": {}, "id": "c0F4EpzuA2i3RH3sGKzGiRjjpuprWx8O4Id2Omhfk3s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "ye5awt", "is_robot_indexable": true, "report_reasons": null, "author": "FrequentAthlete", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ye5awt/what_code_artifacts_are_reusable_in_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ye5awt/what_code_artifacts_are_reusable_in_data/", "subreddit_subscribers": 78031, "created_utc": 1666809154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am at my wit's end and decided to take my chances with the kind data experts in Reddit: I am trying to insert the result of a SQL query in a parquet file (linked tables)  through a SQL script in Synapse and I get error messages or any version of the path to the table not being recognised... Any suggestions, help, resources, or tips?", "author_fullname": "t2_1uxs6s29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Issue trying to insert a SQL query result into a parquet file (table)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ye28r3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666801555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am at my wit&amp;#39;s end and decided to take my chances with the kind data experts in Reddit: I am trying to insert the result of a SQL query in a parquet file (linked tables)  through a SQL script in Synapse and I get error messages or any version of the path to the table not being recognised... Any suggestions, help, resources, or tips?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ye28r3", "is_robot_indexable": true, "report_reasons": null, "author": "mvgame74", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ye28r3/issue_trying_to_insert_a_sql_query_result_into_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ye28r3/issue_trying_to_insert_a_sql_query_result_into_a/", "subreddit_subscribers": 78031, "created_utc": 1666801555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! Curious to know how folks in the community address corrections and adjustments in datapoints in their data pipelines. \n\nExample is if a sensor returns a wild value and you want to null that value, or replace it with something else, where does that sit in your process. \n\nI\u2019ve got an excel file right now that matches a few fields up and corrects it that way, but I don\u2019t think it\u2019s scales very well. Basically have to reprocess the entire pipeline (ETL) to reflect the changes. \n\nThoughts?", "author_fullname": "t2_ahu1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Correcting data in pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yermwm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666876344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Curious to know how folks in the community address corrections and adjustments in datapoints in their data pipelines. &lt;/p&gt;\n\n&lt;p&gt;Example is if a sensor returns a wild value and you want to null that value, or replace it with something else, where does that sit in your process. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve got an excel file right now that matches a few fields up and corrects it that way, but I don\u2019t think it\u2019s scales very well. Basically have to reprocess the entire pipeline (ETL) to reflect the changes. &lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yermwm", "is_robot_indexable": true, "report_reasons": null, "author": "Namur007", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yermwm/correcting_data_in_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yermwm/correcting_data_in_pipeline/", "subreddit_subscribers": 78031, "created_utc": 1666876344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3g7ch6cj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time Analytics Database Architecture with Dhruba Borthakur (creator of HDFS &amp; RocksDB)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_yekm2i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Km5880rqbCO76pyqAVh74co_R7ees_UJgsJZ3yd9PAY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666852421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "rockset.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://rockset.com/videos/real-time-analytics-software-architecture-with-dhruba-borthakur/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6CWeTgF6po6pMQbaVMZ-BZfpStVIyQIrCnFfLjusodc.jpg?auto=webp&amp;s=524499db2b7772d4c7529dc7d770ca7f8e5fc1fb", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/6CWeTgF6po6pMQbaVMZ-BZfpStVIyQIrCnFfLjusodc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0669791390a4a1ee3e629bc4009c128cf6cc2220", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/6CWeTgF6po6pMQbaVMZ-BZfpStVIyQIrCnFfLjusodc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=684cec6bd50fa76c9c057b07837f8604c042ba67", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/6CWeTgF6po6pMQbaVMZ-BZfpStVIyQIrCnFfLjusodc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e0b0ba92185c55974645965ee4cb2e32042a8488", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/6CWeTgF6po6pMQbaVMZ-BZfpStVIyQIrCnFfLjusodc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb49fc8b3dcf0b1e24818b37043cd7127814249c", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/6CWeTgF6po6pMQbaVMZ-BZfpStVIyQIrCnFfLjusodc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c5dd698dbcab2c6ce3cacb08f70727629f484641", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/6CWeTgF6po6pMQbaVMZ-BZfpStVIyQIrCnFfLjusodc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7576f70750d9b7fcecfb151c264b7124f2b228ba", "width": 1080, "height": 564}], "variants": {}, "id": "rQkLQl-IFo_4RNyw15f11BiVBkPO5JYpPpXNSa80ZDc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yekm2i", "is_robot_indexable": true, "report_reasons": null, "author": "ssb61", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yekm2i/realtime_analytics_database_architecture_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://rockset.com/videos/real-time-analytics-software-architecture-with-dhruba-borthakur/", "subreddit_subscribers": 78031, "created_utc": 1666852421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_buql0vn1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL query to find Facebook common friends", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_yei2bt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/khlyseDlZJs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Hard - SQL Query - Interview - Find users with more than 3 common friends\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Hard - SQL Query - Interview - Find users with more than 3 common friends", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/khlyseDlZJs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Hard - SQL Query - Interview - Find users with more than 3 common friends\"&gt;&lt;/iframe&gt;", "author_name": "Select From Data", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/khlyseDlZJs/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/channel/UCPgrXg1ZT5YvlB1Ulg5PTXA"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/khlyseDlZJs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Hard - SQL Query - Interview - Find users with more than 3 common friends\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/yei2bt", "height": 200}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/81cUIjGVmbdcflULmLQMQQgscH47wsTJNDvGxteX568.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666843663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/khlyseDlZJs", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SCWWj-R9u62dXSUG8L8ikqgTaZgF68WCZ3xmBCM3a40.jpg?auto=webp&amp;s=4b812947c8a19ed5b6bccfff11c56ce1dd84fcdf", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/SCWWj-R9u62dXSUG8L8ikqgTaZgF68WCZ3xmBCM3a40.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=716d60ab595c6c600ee91355fd815fde08e51699", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/SCWWj-R9u62dXSUG8L8ikqgTaZgF68WCZ3xmBCM3a40.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=188d5e7099eed3df209cd6872d247a00b913a50f", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/SCWWj-R9u62dXSUG8L8ikqgTaZgF68WCZ3xmBCM3a40.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f72e84aae6297a2a29f8279ef8d1362b829e007", "width": 320, "height": 240}], "variants": {}, "id": "rnOWOQZ-e_inMSh_J9JNadMVThOcBDclsGdjslQcdYQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "yei2bt", "is_robot_indexable": true, "report_reasons": null, "author": "Spirited_Novel2792", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yei2bt/sql_query_to_find_facebook_common_friends/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/khlyseDlZJs", "subreddit_subscribers": 78031, "created_utc": 1666843663.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Hard - SQL Query - Interview - Find users with more than 3 common friends", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/khlyseDlZJs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Hard - SQL Query - Interview - Find users with more than 3 common friends\"&gt;&lt;/iframe&gt;", "author_name": "Select From Data", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/khlyseDlZJs/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/channel/UCPgrXg1ZT5YvlB1Ulg5PTXA"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Due to a mgmt decision, we've to transition data pipeline from existing AWS to Azure soon-ish. \n\nTrouble is i'm new to cloud &amp; am finding azure documentations more complicated than aws \n\nCurrently, It looks like -\n\n**snowflake (account usage and some other) -&gt; S3 (raw)-&gt; Glue (lambda trigger) -&gt; S3 (cleaned) -&gt; athena queries -&gt; rds**\n\nHow would you replicate this on azure? if you HAD to follow the same pattern of tooling..", "author_fullname": "t2_piwlmz4s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS -&gt; Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ye3jaa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666804727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Due to a mgmt decision, we&amp;#39;ve to transition data pipeline from existing AWS to Azure soon-ish. &lt;/p&gt;\n\n&lt;p&gt;Trouble is i&amp;#39;m new to cloud &amp;amp; am finding azure documentations more complicated than aws &lt;/p&gt;\n\n&lt;p&gt;Currently, It looks like -&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;snowflake (account usage and some other) -&amp;gt; S3 (raw)-&amp;gt; Glue (lambda trigger) -&amp;gt; S3 (cleaned) -&amp;gt; athena queries -&amp;gt; rds&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;How would you replicate this on azure? if you HAD to follow the same pattern of tooling..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "ye3jaa", "is_robot_indexable": true, "report_reasons": null, "author": "Fun_Story2003", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ye3jaa/aws_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ye3jaa/aws_azure/", "subreddit_subscribers": 78031, "created_utc": 1666804727.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \n\n  \nI am converting a unix time column to a datetime human readable format, and while using the function below I have a weird behaviour.  \n\nThe column creationDate contains unix stamps like : `1666828800032`\n\nWhat is also weird is the year is the only anomaly. the rest is fine.\n\nUsing external conversion tools showed that the original format is correct.\n\nThanks!  \n\n\nhttps://preview.redd.it/2olqs5azcdw91.png?width=600&amp;format=png&amp;auto=webp&amp;s=a558bbd1c23bd076c08598c2d5ac2b64853f7b56", "author_fullname": "t2_o1lpjxry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark from_unixtime year not working as expected.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 130, "top_awarded_type": null, "hide_score": true, "media_metadata": {"2olqs5azcdw91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 100, "x": 108, "u": "https://preview.redd.it/2olqs5azcdw91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ddf7e7485806641393a4da2538164c318b750bc5"}, {"y": 200, "x": 216, "u": "https://preview.redd.it/2olqs5azcdw91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d13e56eda982a17567ee7faae55903d133a8386e"}, {"y": 297, "x": 320, "u": "https://preview.redd.it/2olqs5azcdw91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=45c94555e8c1eaf762319ea04a440fb4dea817e8"}], "s": {"y": 558, "x": 600, "u": "https://preview.redd.it/2olqs5azcdw91.png?width=600&amp;format=png&amp;auto=webp&amp;s=a558bbd1c23bd076c08598c2d5ac2b64853f7b56"}, "id": "2olqs5azcdw91"}}, "name": "t3_yevc6n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1QsNuqylwhtXWrMSxGyb7QH1hchOGlSwEnWhrnhiVl8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666885543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I am converting a unix time column to a datetime human readable format, and while using the function below I have a weird behaviour.  &lt;/p&gt;\n\n&lt;p&gt;The column creationDate contains unix stamps like : &lt;code&gt;1666828800032&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;What is also weird is the year is the only anomaly. the rest is fine.&lt;/p&gt;\n\n&lt;p&gt;Using external conversion tools showed that the original format is correct.&lt;/p&gt;\n\n&lt;p&gt;Thanks!  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2olqs5azcdw91.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a558bbd1c23bd076c08598c2d5ac2b64853f7b56\"&gt;https://preview.redd.it/2olqs5azcdw91.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a558bbd1c23bd076c08598c2d5ac2b64853f7b56&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yevc6n", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Freedom9865", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yevc6n/pyspark_from_unixtime_year_not_working_as_expected/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yevc6n/pyspark_from_unixtime_year_not_working_as_expected/", "subreddit_subscribers": 78031, "created_utc": 1666885543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a web activity in ADF which returns a JSON object, part of the structure of the object is like this:\n\n    {\n        \"links\": [\n            {\n                \"rel\": \"next\",\n                \"href\": \"https://some-service?limit=10&amp;offset=10\"\n            },\n            {\n                \"rel\": \"last\",\n                \"href\": \"https://some-service?limit=10&amp;offset=9990\"\n            },\n            {\n                \"rel\": \"self\",\n                \"href\": \"https://some-service?limit=10&amp;offset=0\"\n            }\n        ]\n\nI want to run a set variable task after this one and set the variable to the value of the \"next\" URL, it's part of a loop. \n\nThe problem is I don't know how to address that part of the JSON in the set variable task. I know I could go  \n\n    @activity('Web1').output.links[0].href \n\nBut the problem is that after the first call two more elements are added to the list \"first\" and \"previous\".\n\nWhat I really want to do is dynamically look for the entry where rel = \"next\"\n\nBut I'm not sure how to code that. Any thoughts?", "author_fullname": "t2_vj5jo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to access json items in list with the same name", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yeu808", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666882837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a web activity in ADF which returns a JSON object, part of the structure of the object is like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n    &amp;quot;links&amp;quot;: [\n        {\n            &amp;quot;rel&amp;quot;: &amp;quot;next&amp;quot;,\n            &amp;quot;href&amp;quot;: &amp;quot;https://some-service?limit=10&amp;amp;offset=10&amp;quot;\n        },\n        {\n            &amp;quot;rel&amp;quot;: &amp;quot;last&amp;quot;,\n            &amp;quot;href&amp;quot;: &amp;quot;https://some-service?limit=10&amp;amp;offset=9990&amp;quot;\n        },\n        {\n            &amp;quot;rel&amp;quot;: &amp;quot;self&amp;quot;,\n            &amp;quot;href&amp;quot;: &amp;quot;https://some-service?limit=10&amp;amp;offset=0&amp;quot;\n        }\n    ]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I want to run a set variable task after this one and set the variable to the value of the &amp;quot;next&amp;quot; URL, it&amp;#39;s part of a loop. &lt;/p&gt;\n\n&lt;p&gt;The problem is I don&amp;#39;t know how to address that part of the JSON in the set variable task. I know I could go  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@activity(&amp;#39;Web1&amp;#39;).output.links[0].href \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But the problem is that after the first call two more elements are added to the list &amp;quot;first&amp;quot; and &amp;quot;previous&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;What I really want to do is dynamically look for the entry where rel = &amp;quot;next&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m not sure how to code that. Any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yeu808", "is_robot_indexable": true, "report_reasons": null, "author": "Enigma1984", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yeu808/how_to_access_json_items_in_list_with_the_same/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yeu808/how_to_access_json_items_in_list_with_the_same/", "subreddit_subscribers": 78031, "created_utc": 1666882837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "1. Create Iceberg Catalog\n2. Create Iceberg Table\n3. Insert records into table", "author_fullname": "t2_ce2ldlob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video: 2 minute demonstration of how to get started with Iceberg tables in Dremio Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_yeqeef", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/qyoi0hgfbcw91/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/qyoi0hgfbcw91/DASH_96.mp4", "dash_url": "https://v.redd.it/qyoi0hgfbcw91/DASHPlaylist.mpd?a=1669482215%2CMGVlMTIwOGY0NDg1ZjRkMDFiZDQwZTZlM2IwYTRhNjAyMTUxODQxNzQ4ZjllYjEzOWY4MzcyNDJlZGUwY2FhZg%3D%3D&amp;v=1&amp;f=sd", "duration": 141, "hls_url": "https://v.redd.it/qyoi0hgfbcw91/HLSPlaylist.m3u8?a=1669482215%2CZmYyNjZkNmM1ZDFlMmI2ZGU2Nzg4ZmE0MGIwOTNiMTA0Y2JmMTBhZjU5ZjIyODY0ZjlkMjRmZTY2MDFlNzgyOA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/HDN-bKsKkKwNMcWMnGUXXD7dPmMrKubBurEYWuqDEY8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666872911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;Create Iceberg Catalog&lt;/li&gt;\n&lt;li&gt;Create Iceberg Table&lt;/li&gt;\n&lt;li&gt;Insert records into table&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/qyoi0hgfbcw91", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?format=pjpg&amp;auto=webp&amp;s=f5bcc0925bdd28b75394e336614e675e0a960d11", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=98edb62f8b5c3c43a08cef522e186022210c8512", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=66672738d1777cadab320c12a1610bd061522088", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9ae9292e417d21e9e14590df37fa779875b2656c", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2c46a75476d46040d8accc03b605fa85eec7aff7", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=94c86fe0248ddb4e98fbf19abb0475b36d8b1ff7", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/zabDP_2fdwCp92duIqc1DjS7sAA0Mx5tW5BjOJ9-eio.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3f64f552197572eac8f07cc340947566596f257c", "width": 1080, "height": 607}], "variants": {}, "id": "qQtKOSP1_UbVF1Z1wBujWLI9flsKMyJ3n30SqjAE-Dg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yeqeef", "is_robot_indexable": true, "report_reasons": null, "author": "amdatalakehouse", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yeqeef/video_2_minute_demonstration_of_how_to_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/qyoi0hgfbcw91", "subreddit_subscribers": 78031, "created_utc": 1666872911.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/qyoi0hgfbcw91/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/qyoi0hgfbcw91/DASH_96.mp4", "dash_url": "https://v.redd.it/qyoi0hgfbcw91/DASHPlaylist.mpd?a=1669482215%2CMGVlMTIwOGY0NDg1ZjRkMDFiZDQwZTZlM2IwYTRhNjAyMTUxODQxNzQ4ZjllYjEzOWY4MzcyNDJlZGUwY2FhZg%3D%3D&amp;v=1&amp;f=sd", "duration": 141, "hls_url": "https://v.redd.it/qyoi0hgfbcw91/HLSPlaylist.m3u8?a=1669482215%2CZmYyNjZkNmM1ZDFlMmI2ZGU2Nzg4ZmE0MGIwOTNiMTA0Y2JmMTBhZjU5ZjIyODY0ZjlkMjRmZTY2MDFlNzgyOA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Analytics and Machine Learning Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_yem6ft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HZ6A6FZR12gQEweQfgjsgYZedZfX94MkQLRMW-LG0xM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666858439.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/gooddata-developers/cooperation-between-data-analytics-and-machine-learning-54ffb047cf20", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?auto=webp&amp;s=e7e9a3f526d2ea9aec7a0bd9d23008daac98cfca", "width": 1200, "height": 674}, "resolutions": [{"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1c9972279d5e784c0f55ae4d020c937480fc269", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6f9b48bb241c6d0df7fd64ef4f7ae9fbcce28223", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b32f57ad7deb5067ede595d7650a208b0a7904fe", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f2a34a2e220039250cab0108c31e617a40e3991", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9931e08b2e33079bcfda16c7bc1ae08fab7de3f6", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/XRuipPTWZVrIPOyRvPoLp4It9uAXVUq-obPS3ffMHgE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b4e845936cae4b532e14f205736887a5f163736", "width": 1080, "height": 606}], "variants": {}, "id": "1j7JSz5-gepwKFTbsjTMWqzYcHRUTKEnkYetTXyg8JY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yem6ft", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yem6ft/data_analytics_and_machine_learning_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/gooddata-developers/cooperation-between-data-analytics-and-machine-learning-54ffb047cf20", "subreddit_subscribers": 78031, "created_utc": 1666858439.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_b7f9ay9o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to use PySpark and Spark SQL , MatPlotLib and Seaborn in Azure Synapse Analytics and Spark Pool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_yek5dh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.66, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tft8JP4lz3w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How to use PySpark and Spark SQL , MatPlotLib and Seaborn in Azure Synapse Analytics and Spark Pool\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How to use PySpark and Spark SQL , MatPlotLib and Seaborn in Azure Synapse Analytics and Spark Pool", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tft8JP4lz3w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How to use PySpark and Spark SQL , MatPlotLib and Seaborn in Azure Synapse Analytics and Spark Pool\"&gt;&lt;/iframe&gt;", "author_name": "SoftWiz Circle", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/tft8JP4lz3w/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/c/SoftWizCircle"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tft8JP4lz3w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How to use PySpark and Spark SQL , MatPlotLib and Seaborn in Azure Synapse Analytics and Spark Pool\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/yek5dh", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HTK4Vy2LXOmEt7VwlfW6XN7X7UI54ssamahFSxFkBow.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666850724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/tft8JP4lz3w", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Z2S53KPbtRZFz7n_LeTFzMqQRlsEEE8C2FoEkSTz-VQ.jpg?auto=webp&amp;s=bd129459e3e5517530e56f247bf1fc0da7c3d4cd", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Z2S53KPbtRZFz7n_LeTFzMqQRlsEEE8C2FoEkSTz-VQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0676722bf6bf5a2193826acf7551b34342fd87b0", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Z2S53KPbtRZFz7n_LeTFzMqQRlsEEE8C2FoEkSTz-VQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a79a22b400437692ebae08215518039b6401075", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Z2S53KPbtRZFz7n_LeTFzMqQRlsEEE8C2FoEkSTz-VQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc2c6e8e1fd6b21b872e9288fb18a476b2082687", "width": 320, "height": 240}], "variants": {}, "id": "8Ie7cbCMxNmPiEHsSQ3e113mN17nzQiD48ghwzEjLIs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yek5dh", "is_robot_indexable": true, "report_reasons": null, "author": "balramprasad", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yek5dh/how_to_use_pyspark_and_spark_sql_matplotlib_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/tft8JP4lz3w", "subreddit_subscribers": 78031, "created_utc": 1666850724.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How to use PySpark and Spark SQL , MatPlotLib and Seaborn in Azure Synapse Analytics and Spark Pool", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/tft8JP4lz3w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"How to use PySpark and Spark SQL , MatPlotLib and Seaborn in Azure Synapse Analytics and Spark Pool\"&gt;&lt;/iframe&gt;", "author_name": "SoftWiz Circle", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/tft8JP4lz3w/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/c/SoftWizCircle"}}, "is_video": false}}], "before": null}}