{"kind": "Listing", "data": {"after": "t3_y6lag3", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "The `data \\w+` gold rush has been a blessing and a curse, blessing in that many of us are getting filthy rich off it, curse in that many (frankly unskilled) people see the job market and think \u201cwow I gotta get me a piece of that\u201d and proceed to bombard every specialist board with mentorship requests and e-begging for a crumb of interview. \n\nFrankly I wouldn\u2019t mind this if the people asking had done some cursory research beforehand and asked politely, but it seems like every jerkoff who\u2019s caught a whiff of an Excel spreadsheet thinks they can land a FAANG job overnight and, instead of looking on Google for \u201chow to data job pls to help\u201d and seeing the ten trillion useless Medium articles made by the endless morons trying to resume pad and slip their jimmy into an Amazon L3 role that would tell them practically everything they need to know (even if by and large anything posted on Medium is worthless) they choose to pepper subs like /r/dataengineering, /r/dataanalysis, and this one with the same \u201chow to data job please give me six figures\u201d - it\u2019s like asking /r/personalfinance \u201chelp how do I own a bank account\u201d repeated for every hapless schmuck who\u2019s been hiding their Benjamins in granny\u2019s cookie tin for the last sixteen years of their childhood. \n\nNot even getting into the fact that doing basic research on the topic at hand is probably *the* fundamental skill for any data-*whatever* role, what\u2019s even funnier is that I\u2019d hazard a guess that most of us who *actually work in the industry* have better things to do during the day, so the people answering questions are probably majority kids trying to get their first data-whatever job - blind leading the blind all over again. \n\nTLDR: Screw you guys I\u2019m going to /r/Statistics", "author_fullname": "t2_gtodeuwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[UNHINGED RANT] It\u2019s kind of annoying to see that, in general, most data-related spaces are flush with \u201chow do I get a job\u201d and comparatively little discussion around the actual topic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6w5ab", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 396, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 396, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666065579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The &lt;code&gt;data \\w+&lt;/code&gt; gold rush has been a blessing and a curse, blessing in that many of us are getting filthy rich off it, curse in that many (frankly unskilled) people see the job market and think \u201cwow I gotta get me a piece of that\u201d and proceed to bombard every specialist board with mentorship requests and e-begging for a crumb of interview. &lt;/p&gt;\n\n&lt;p&gt;Frankly I wouldn\u2019t mind this if the people asking had done some cursory research beforehand and asked politely, but it seems like every jerkoff who\u2019s caught a whiff of an Excel spreadsheet thinks they can land a FAANG job overnight and, instead of looking on Google for \u201chow to data job pls to help\u201d and seeing the ten trillion useless Medium articles made by the endless morons trying to resume pad and slip their jimmy into an Amazon L3 role that would tell them practically everything they need to know (even if by and large anything posted on Medium is worthless) they choose to pepper subs like &lt;a href=\"/r/dataengineering\"&gt;/r/dataengineering&lt;/a&gt;, &lt;a href=\"/r/dataanalysis\"&gt;/r/dataanalysis&lt;/a&gt;, and this one with the same \u201chow to data job please give me six figures\u201d - it\u2019s like asking &lt;a href=\"/r/personalfinance\"&gt;/r/personalfinance&lt;/a&gt; \u201chelp how do I own a bank account\u201d repeated for every hapless schmuck who\u2019s been hiding their Benjamins in granny\u2019s cookie tin for the last sixteen years of their childhood. &lt;/p&gt;\n\n&lt;p&gt;Not even getting into the fact that doing basic research on the topic at hand is probably &lt;em&gt;the&lt;/em&gt; fundamental skill for any data-&lt;em&gt;whatever&lt;/em&gt; role, what\u2019s even funnier is that I\u2019d hazard a guess that most of us who &lt;em&gt;actually work in the industry&lt;/em&gt; have better things to do during the day, so the people answering questions are probably majority kids trying to get their first data-whatever job - blind leading the blind all over again. &lt;/p&gt;\n\n&lt;p&gt;TLDR: Screw you guys I\u2019m going to &lt;a href=\"/r/Statistics\"&gt;/r/Statistics&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6w5ab", "is_robot_indexable": true, "report_reasons": null, "author": "sigma_hyperborean", "discussion_type": null, "num_comments": 83, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6w5ab/unhinged_rant_its_kind_of_annoying_to_see_that_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6w5ab/unhinged_rant_its_kind_of_annoying_to_see_that_in/", "subreddit_subscribers": 814150, "created_utc": 1666065579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was recently hired as the first (and only) data scientist for a municipal government. My position will essentially be starting from a totally blank slate: other city employees are familiar with Microsoft Power BI and Excel, and that's it. \n\nMy question is this: what languages and/or software tools should I set up as the \"standard\" going forward? I have mediocre Python skills, and I'm *very* good with R, but I'm worried that R is maybe too niche to build a whole DS program around? Maybe it's asking too much to expect future city employees to be familiar with R?\n\nOn the other hand, I'm not sure I will *ever* have other DS-inclined coworkers... So maybe this is a moot point? If I want to cater my code to my real coworkers, is either R or Python better at interacting with Microsoft Excel (meeting them where they are)? Sorry if this is a weird set of questions, I just want to try and think things through before I go too far down any path, hopefully to \"future proof\" my work.", "author_fullname": "t2_7nhni", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting a city government DS program: which language should I use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6nr7c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666043207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was recently hired as the first (and only) data scientist for a municipal government. My position will essentially be starting from a totally blank slate: other city employees are familiar with Microsoft Power BI and Excel, and that&amp;#39;s it. &lt;/p&gt;\n\n&lt;p&gt;My question is this: what languages and/or software tools should I set up as the &amp;quot;standard&amp;quot; going forward? I have mediocre Python skills, and I&amp;#39;m &lt;em&gt;very&lt;/em&gt; good with R, but I&amp;#39;m worried that R is maybe too niche to build a whole DS program around? Maybe it&amp;#39;s asking too much to expect future city employees to be familiar with R?&lt;/p&gt;\n\n&lt;p&gt;On the other hand, I&amp;#39;m not sure I will &lt;em&gt;ever&lt;/em&gt; have other DS-inclined coworkers... So maybe this is a moot point? If I want to cater my code to my real coworkers, is either R or Python better at interacting with Microsoft Excel (meeting them where they are)? Sorry if this is a weird set of questions, I just want to try and think things through before I go too far down any path, hopefully to &amp;quot;future proof&amp;quot; my work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6nr7c", "is_robot_indexable": true, "report_reasons": null, "author": "dankatheist420", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6nr7c/starting_a_city_government_ds_program_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6nr7c/starting_a_city_government_ds_program_which/", "subreddit_subscribers": 814150, "created_utc": 1666043207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am trying to use nltk VADER to do sentiment analysis to see the emotions associated with a certain word. For example, let\u2019s say the keyword is \u201cpizza\u201d. Unfortunately, If I were the analyze the sentence \n\n\u201cI hate broccoli, carrots, and turnips, and fries are awful, although pizza is delicious\u201d\n\nIt would have a fairly high \u201cnegative\u201d score because the sentence, overall, is fairly negative. However, clearly there is actually positive sentiment towards the keyword \u201cpizza\u201d. Is there a specific model or a clever way to analyze the sentiment toward a particular word like this?", "author_fullname": "t2_g7jmnu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sentiment analysis for a single keyword", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6dyla", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666020327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to use nltk VADER to do sentiment analysis to see the emotions associated with a certain word. For example, let\u2019s say the keyword is \u201cpizza\u201d. Unfortunately, If I were the analyze the sentence &lt;/p&gt;\n\n&lt;p&gt;\u201cI hate broccoli, carrots, and turnips, and fries are awful, although pizza is delicious\u201d&lt;/p&gt;\n\n&lt;p&gt;It would have a fairly high \u201cnegative\u201d score because the sentence, overall, is fairly negative. However, clearly there is actually positive sentiment towards the keyword \u201cpizza\u201d. Is there a specific model or a clever way to analyze the sentiment toward a particular word like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6dyla", "is_robot_indexable": true, "report_reasons": null, "author": "_hairyberry_", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6dyla/sentiment_analysis_for_a_single_keyword/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6dyla/sentiment_analysis_for_a_single_keyword/", "subreddit_subscribers": 814150, "created_utc": 1666020327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm just going to make up an example here.  Imagine you want to predict a stock price.  Your feature is today's price and your target is tomorrow's price.  \n\nHere would be a simple case:\nFeature = [100, 105, 110, 115, 120...]\nTarget = [105, 110, 115, 120, 125...]\n\nAre we allowed to do cross validation on this?  \n\nEven though we are never using future data to predict the past, I still feel like cross validation should not be allowed in this case.  Better approaches would be a walk-forward validation or a rolling window validation.  \n\nAm I correct in my assumption that cross validation should not be allowed?", "author_fullname": "t2_cn54oiy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cross Validation with Time Series", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6v261", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666062429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just going to make up an example here.  Imagine you want to predict a stock price.  Your feature is today&amp;#39;s price and your target is tomorrow&amp;#39;s price.  &lt;/p&gt;\n\n&lt;p&gt;Here would be a simple case:\nFeature = [100, 105, 110, 115, 120...]\nTarget = [105, 110, 115, 120, 125...]&lt;/p&gt;\n\n&lt;p&gt;Are we allowed to do cross validation on this?  &lt;/p&gt;\n\n&lt;p&gt;Even though we are never using future data to predict the past, I still feel like cross validation should not be allowed in this case.  Better approaches would be a walk-forward validation or a rolling window validation.  &lt;/p&gt;\n\n&lt;p&gt;Am I correct in my assumption that cross validation should not be allowed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6v261", "is_robot_indexable": true, "report_reasons": null, "author": "BlackLotus8888", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6v261/cross_validation_with_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6v261/cross_validation_with_time_series/", "subreddit_subscribers": 814150, "created_utc": 1666062429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I notice that some times after completing a really exciting/impactful project I tend to feel pretty \u2018meh\u2019 about doing other work for a bit of time (even follow on work). Do you feel the same way? I dunno, maybe I want some victory laps or something \ud83d\ude02", "author_fullname": "t2_1xj38rmh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Peaks and valleys", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6mvcz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666041122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I notice that some times after completing a really exciting/impactful project I tend to feel pretty \u2018meh\u2019 about doing other work for a bit of time (even follow on work). Do you feel the same way? I dunno, maybe I want some victory laps or something \ud83d\ude02&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6mvcz", "is_robot_indexable": true, "report_reasons": null, "author": "eomar2828", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6mvcz/peaks_and_valleys/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6mvcz/peaks_and_valleys/", "subreddit_subscribers": 814150, "created_utc": 1666041122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Basically the title, and will it be easy to apply the material on python instead?", "author_fullname": "t2_lus61uk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Andrew Ng Machine Learning Course still a good choice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6yd0l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666072408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically the title, and will it be easy to apply the material on python instead?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6yd0l", "is_robot_indexable": true, "report_reasons": null, "author": "mohamedk97", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6yd0l/is_andrew_ng_machine_learning_course_still_a_good/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6yd0l/is_andrew_ng_machine_learning_course_still_a_good/", "subreddit_subscribers": 814150, "created_utc": 1666072408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Let's say I have a sequence of terms. Each term could be any string of alphabetical characters. `['abc', 'stg', 'drT']`. Now let's say I have millions of such sequences of varying length (might be 1 term, might be 1000) and some of them are 'bad'. But their combination matters toward that badness. So I'm baselining and looking for outliers, which should have a high correlation with 'bad' in my dataset. Term might indicate 'badness' in one combination but is totally benign in another.\n\nSo that's our search-space and problem statement. I am not educated in DS. But I understand the basic idea: Correlate traits and relationships of features in one or more data-sets to answer a question / solve a problem. But which features to select? It seems like an interesting question, since as analysts we don't know everything and if we try to select feature manually, there's a good chance we might miss something. Right? There might be some combination of features which, when correlated with some combination of other features ends up being a good indicator that we wouldn't have manually expected. Maybe 3 features plotted on a 3D chart with K-Means clustering or just human viewing reveals something interesting / creates a valuable signal that we might not have guessed or tested if we were hand-picking the features we thought might be useful.\n\nSo I'm computing an exhaustive list of features globally and locally, like:\n\nHow standard / deviant is the feature locally?\n\n* local occurence count\n* local occurence as % of total local feature count\n\nHow standard / deviant is the feature globally?\n\n* global occurence count\n* global occurence count as % of global feature count\n\nHow is the feature related to other features locally?\n\n* for each term: for each other term: min distance, where distance is in terms of index\n* for each term: for each other term: max distance, where distance is in terms of index\n* for each term: for each other term: avg distance, where distance is in terms of index\n* for each term: for each other term: min distance, where distance is in terms of character count\n* for each term: for each other term: max distance, where distance is in terms of character count\n* for each term: for each other term: avg distance, where distance is in terms of character count\n\nHow globally standard / deviant is the feature's relationships to other features locally?\n\n* for each term: for each other term: min distance as % of all global min distances between these two symbols, where distance is in terms of index\n* for each term: for each other term: max distance as % of all global max distances between these two symbols, where distance is in terms of index\n* for each term: for each other term: avg distance as % of all global avg distances between these two symbols, where distance is in terms of index\n* for each term: for each other term: min distance as % of all global min distances between these two symbols, where distance is in terms of character count\n* for each term: for each other term: max distance as % of all global max distances between these two symbols, where distance is in terms of character count\n* for each term: for each other term: avg distance as % of all global avg distances between these two symbols, where distance is in terms of character count\n\nThat's the theory I'm experimenting with, planning to just produce every possible chart, perhaps thousands, and then do some simple clustering on all of them and see if features associated with known-bad sample have a statistical tendency to end up in distinct clusters in any of those plots.\n\nAm I potentially on the right track? Is this a known strategy in data science? It's my first real run-in with cleaning data, isolating / computing features, so I'm still really just shooting in the dark at this point.", "author_fullname": "t2_8yytz5qc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it sometimes a good strategy to identify / prep all possible features, model them all dynamically (with recursive code) and measure which collection of features end up correlating more often than others to identify whatever signal you're looking for?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6txbs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666063704.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666059259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say I have a sequence of terms. Each term could be any string of alphabetical characters. &lt;code&gt;[&amp;#39;abc&amp;#39;, &amp;#39;stg&amp;#39;, &amp;#39;drT&amp;#39;]&lt;/code&gt;. Now let&amp;#39;s say I have millions of such sequences of varying length (might be 1 term, might be 1000) and some of them are &amp;#39;bad&amp;#39;. But their combination matters toward that badness. So I&amp;#39;m baselining and looking for outliers, which should have a high correlation with &amp;#39;bad&amp;#39; in my dataset. Term might indicate &amp;#39;badness&amp;#39; in one combination but is totally benign in another.&lt;/p&gt;\n\n&lt;p&gt;So that&amp;#39;s our search-space and problem statement. I am not educated in DS. But I understand the basic idea: Correlate traits and relationships of features in one or more data-sets to answer a question / solve a problem. But which features to select? It seems like an interesting question, since as analysts we don&amp;#39;t know everything and if we try to select feature manually, there&amp;#39;s a good chance we might miss something. Right? There might be some combination of features which, when correlated with some combination of other features ends up being a good indicator that we wouldn&amp;#39;t have manually expected. Maybe 3 features plotted on a 3D chart with K-Means clustering or just human viewing reveals something interesting / creates a valuable signal that we might not have guessed or tested if we were hand-picking the features we thought might be useful.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m computing an exhaustive list of features globally and locally, like:&lt;/p&gt;\n\n&lt;p&gt;How standard / deviant is the feature locally?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;local occurence count&lt;/li&gt;\n&lt;li&gt;local occurence as % of total local feature count&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How standard / deviant is the feature globally?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;global occurence count&lt;/li&gt;\n&lt;li&gt;global occurence count as % of global feature count&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How is the feature related to other features locally?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;for each term: for each other term: min distance, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: max distance, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: avg distance, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: min distance, where distance is in terms of character count&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: max distance, where distance is in terms of character count&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: avg distance, where distance is in terms of character count&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How globally standard / deviant is the feature&amp;#39;s relationships to other features locally?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;for each term: for each other term: min distance as % of all global min distances between these two symbols, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: max distance as % of all global max distances between these two symbols, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: avg distance as % of all global avg distances between these two symbols, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: min distance as % of all global min distances between these two symbols, where distance is in terms of character count&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: max distance as % of all global max distances between these two symbols, where distance is in terms of character count&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: avg distance as % of all global avg distances between these two symbols, where distance is in terms of character count&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That&amp;#39;s the theory I&amp;#39;m experimenting with, planning to just produce every possible chart, perhaps thousands, and then do some simple clustering on all of them and see if features associated with known-bad sample have a statistical tendency to end up in distinct clusters in any of those plots.&lt;/p&gt;\n\n&lt;p&gt;Am I potentially on the right track? Is this a known strategy in data science? It&amp;#39;s my first real run-in with cleaning data, isolating / computing features, so I&amp;#39;m still really just shooting in the dark at this point.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6txbs", "is_robot_indexable": true, "report_reasons": null, "author": "Jonathan-Todd", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6txbs/is_it_sometimes_a_good_strategy_to_identify_prep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6txbs/is_it_sometimes_a_good_strategy_to_identify_prep/", "subreddit_subscribers": 814150, "created_utc": 1666059259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "[Ergest Xheblati](https://www.linkedin.com/in/ergestxheblati/) is the author of [*Minimum Viable SQL Patterns*](https://ergestx.gumroad.com/l/sqlpatterns), and he's spent the last 15 years mastering SQL.  \n\nIn this [SQL workshop](https://youtu.be/UFiZx5NlzL4), Ergest teaches various SQL principles (patters) to help you take your SQL skills from intermediate to expert. More specifically, you'll learn about:   \n\ud83c\udfaf Query composition patterns - How to make your complex queries shorter, more legible, and more performant   \n\ud83c\udfaf Query maintainability patterns - Constructing CTEs that can be reused. In software engineering, it's called the DRY principle (don't repeat yourself)  \n\ud83c\udfaf Query robustness patterns - Constructing queries that don't break when the underlying data changes in unpredictable ways  \n\ud83c\udfaf Query performance patterns - Make your queries faster (and cheaper) regardless of specific database you\u2019re using.\n\nToward the end of the workshop, Ergest answers over a dozen questions from SQL professionals all over the world.  \n\nWatch the full workshop \ud83d\udc49 [**here**](https://www.youtube.com/watch?v=UFiZx5NlzL4) \ud83d\udc48. It's free, and don't worry, you're not being sold on a SaaS product \ud83e\udd23  \n\n\nIf you enjoyed this video, please join Ergest and +1,000 like minded data professionals in our Slack Community, the [OA Club](https://www.operationalanalytics.club/)! We're creating content like this all the time!", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Workshop recording: Making SQL more efficient, readable, and easier to debug", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6n2tm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666041610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.linkedin.com/in/ergestxheblati/\"&gt;Ergest Xheblati&lt;/a&gt; is the author of &lt;a href=\"https://ergestx.gumroad.com/l/sqlpatterns\"&gt;&lt;em&gt;Minimum Viable SQL Patterns&lt;/em&gt;&lt;/a&gt;, and he&amp;#39;s spent the last 15 years mastering SQL.  &lt;/p&gt;\n\n&lt;p&gt;In this &lt;a href=\"https://youtu.be/UFiZx5NlzL4\"&gt;SQL workshop&lt;/a&gt;, Ergest teaches various SQL principles (patters) to help you take your SQL skills from intermediate to expert. More specifically, you&amp;#39;ll learn about:&lt;br/&gt;\n\ud83c\udfaf Query composition patterns - How to make your complex queries shorter, more legible, and more performant&lt;br/&gt;\n\ud83c\udfaf Query maintainability patterns - Constructing CTEs that can be reused. In software engineering, it&amp;#39;s called the DRY principle (don&amp;#39;t repeat yourself)&lt;br/&gt;\n\ud83c\udfaf Query robustness patterns - Constructing queries that don&amp;#39;t break when the underlying data changes in unpredictable ways&lt;br/&gt;\n\ud83c\udfaf Query performance patterns - Make your queries faster (and cheaper) regardless of specific database you\u2019re using.&lt;/p&gt;\n\n&lt;p&gt;Toward the end of the workshop, Ergest answers over a dozen questions from SQL professionals all over the world.  &lt;/p&gt;\n\n&lt;p&gt;Watch the full workshop \ud83d\udc49 &lt;a href=\"https://www.youtube.com/watch?v=UFiZx5NlzL4\"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt; \ud83d\udc48. It&amp;#39;s free, and don&amp;#39;t worry, you&amp;#39;re not being sold on a SaaS product \ud83e\udd23  &lt;/p&gt;\n\n&lt;p&gt;If you enjoyed this video, please join Ergest and +1,000 like minded data professionals in our Slack Community, the &lt;a href=\"https://www.operationalanalytics.club/\"&gt;OA Club&lt;/a&gt;! We&amp;#39;re creating content like this all the time!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6n2tm", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6n2tm/sql_workshop_recording_making_sql_more_efficient/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6n2tm/sql_workshop_recording_making_sql_more_efficient/", "subreddit_subscribers": 814150, "created_utc": 1666041610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I'm relatively new to time-series modeling and am not quite sure how to approach this problem. Let's say I have data on 100 students and their 100 most recent test scores (assuming that each test is the same across all students and of varying difficulty successively), and I want to understand what kind of exponential smoothing value is optimal to predict each student's next test score.\n\nBecause there must be some information contained within the population's overall performance across tests, I want to leverage that. But am I able to do so using something like R's forecast package's ETS function? I know that it can handle a single-subject time series but how can it assess the overall alpha value given 100 subjects?", "author_fullname": "t2_21w9l2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Method for Optimal Exponential Smoothing of Time Series across samples?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6va5a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666063062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m relatively new to time-series modeling and am not quite sure how to approach this problem. Let&amp;#39;s say I have data on 100 students and their 100 most recent test scores (assuming that each test is the same across all students and of varying difficulty successively), and I want to understand what kind of exponential smoothing value is optimal to predict each student&amp;#39;s next test score.&lt;/p&gt;\n\n&lt;p&gt;Because there must be some information contained within the population&amp;#39;s overall performance across tests, I want to leverage that. But am I able to do so using something like R&amp;#39;s forecast package&amp;#39;s ETS function? I know that it can handle a single-subject time series but how can it assess the overall alpha value given 100 subjects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6va5a", "is_robot_indexable": true, "report_reasons": null, "author": "DataScience0", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6va5a/method_for_optimal_exponential_smoothing_of_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6va5a/method_for_optimal_exponential_smoothing_of_time/", "subreddit_subscribers": 814150, "created_utc": 1666063062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Throughout my life, I haven't really known what I wanted to do as a job until now. I love the idea of working from home and I'm also excited to start learning to code, but the affordable university I have chosen doesn't have a computer science course, which would be more suited for me imo as it would be more programming oriented, however, I suppose I can program on my own in my free time.\n\nWhat they do have is the computational mathematics course, I've never heard of this before but it is heavily math based with a few programming classes sprinkled here and there. As I don't know much about these classes yet, I'd like to ask your advice if taking all of the ones listed is what will definitely help me gain the framework to have the brain of a data scientist. So here they are:\n\nSo they have calculus and analytic geometry, intro to statistic analysis and intro to discrete mathematics, matrix and linear algebra (I've heard that this one is very useful for the career), abstract algebraic structures, multivariable calculus, applied differential equations, probability I, and mathematical molding and simulation.\n\nThe reason why I listed all of them is because I want to hear your advice to know for sure if taking all of these is worth it or if some of it might be a waste. If any of you are familiar with these courses please let me know. I don't know much yet what path I should take, so any advice would be appreciated.\n\nAlso, if you can tell me what websites or links you used to study certain programming courses, or the names of books that teach them, I would like to hear it as well.\n\nThanks.", "author_fullname": "t2_jemak9dw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Going for a bachelors in computational mathematics, will this help me gain the fundamental skills to become a Data Scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6mmvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666040587.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Throughout my life, I haven&amp;#39;t really known what I wanted to do as a job until now. I love the idea of working from home and I&amp;#39;m also excited to start learning to code, but the affordable university I have chosen doesn&amp;#39;t have a computer science course, which would be more suited for me imo as it would be more programming oriented, however, I suppose I can program on my own in my free time.&lt;/p&gt;\n\n&lt;p&gt;What they do have is the computational mathematics course, I&amp;#39;ve never heard of this before but it is heavily math based with a few programming classes sprinkled here and there. As I don&amp;#39;t know much about these classes yet, I&amp;#39;d like to ask your advice if taking all of the ones listed is what will definitely help me gain the framework to have the brain of a data scientist. So here they are:&lt;/p&gt;\n\n&lt;p&gt;So they have calculus and analytic geometry, intro to statistic analysis and intro to discrete mathematics, matrix and linear algebra (I&amp;#39;ve heard that this one is very useful for the career), abstract algebraic structures, multivariable calculus, applied differential equations, probability I, and mathematical molding and simulation.&lt;/p&gt;\n\n&lt;p&gt;The reason why I listed all of them is because I want to hear your advice to know for sure if taking all of these is worth it or if some of it might be a waste. If any of you are familiar with these courses please let me know. I don&amp;#39;t know much yet what path I should take, so any advice would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Also, if you can tell me what websites or links you used to study certain programming courses, or the names of books that teach them, I would like to hear it as well.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6mmvo", "is_robot_indexable": true, "report_reasons": null, "author": "SleepingPooper", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6mmvo/going_for_a_bachelors_in_computational/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6mmvo/going_for_a_bachelors_in_computational/", "subreddit_subscribers": 814150, "created_utc": 1666040587.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm choosing between popular methods like keybert, spacy, textrank, rake and yake to create a baseline for keyword extraction. So nothing fancy at this point. What criteria should I consider? It's going to be used on open domain scientific texts.", "author_fullname": "t2_3ow77bmq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to select a model for Keyword extraction?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6e7nq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666020914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m choosing between popular methods like keybert, spacy, textrank, rake and yake to create a baseline for keyword extraction. So nothing fancy at this point. What criteria should I consider? It&amp;#39;s going to be used on open domain scientific texts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6e7nq", "is_robot_indexable": true, "report_reasons": null, "author": "soldierpie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6e7nq/how_to_select_a_model_for_keyword_extraction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6e7nq/how_to_select_a_model_for_keyword_extraction/", "subreddit_subscribers": 814150, "created_utc": 1666020914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve been in and around the sub for a while now and there\u2019s always waves of posts with high engagement that meme about how different/similar data science/analytics/data engineering are. I\u2019d like to get some perspectives and start discussion around this phenomena that perennially plagues data fields. \n\nTo this end, and to prevent this becoming an absolute circlejerk - I\u2019d like to encourage the following response pattern:\n\nDefinitions\nData Science\nData Analytics\nData Engineering\n\nRoles:\nData Scientist\nData Analyst\nData Engineer\n\nFreeform:\nYour experience with these distinctions in practice\n\nI\u2019d like discussion to ensue in the replies so everyone is working off shared viewpoints. So many times, we struggle to have meaningful conversation because \u201cdata science\u201d means two fundamentally different things to each user.\n\nAdded benefit of structured replies on this post - y\u2019all can do your NLP black magic on it for your portfolio.", "author_fullname": "t2_24pdm31x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Structured conversation on definitions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6cnlw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666016878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been in and around the sub for a while now and there\u2019s always waves of posts with high engagement that meme about how different/similar data science/analytics/data engineering are. I\u2019d like to get some perspectives and start discussion around this phenomena that perennially plagues data fields. &lt;/p&gt;\n\n&lt;p&gt;To this end, and to prevent this becoming an absolute circlejerk - I\u2019d like to encourage the following response pattern:&lt;/p&gt;\n\n&lt;p&gt;Definitions\nData Science\nData Analytics\nData Engineering&lt;/p&gt;\n\n&lt;p&gt;Roles:\nData Scientist\nData Analyst\nData Engineer&lt;/p&gt;\n\n&lt;p&gt;Freeform:\nYour experience with these distinctions in practice&lt;/p&gt;\n\n&lt;p&gt;I\u2019d like discussion to ensue in the replies so everyone is working off shared viewpoints. So many times, we struggle to have meaningful conversation because \u201cdata science\u201d means two fundamentally different things to each user.&lt;/p&gt;\n\n&lt;p&gt;Added benefit of structured replies on this post - y\u2019all can do your NLP black magic on it for your portfolio.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6cnlw", "is_robot_indexable": true, "report_reasons": null, "author": "lawrebx", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6cnlw/structured_conversation_on_definitions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6cnlw/structured_conversation_on_definitions/", "subreddit_subscribers": 814150, "created_utc": 1666016878.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nMy background:\n\n* BSc Mathematics from Russel Group, MSc Statistics in progress from Top 3 university\n* 3 month quant analytics consulting internship + 12 months full-time data scientist at FinTech after graduating (c. \u00a338k total comp, outside of London)\n\nI'm starting to apply to London-based data science/AI/quant analytics graduate schemes, and I am hoping to land something in the \u00a365k range, although there seems to be massive variance in the comp for graduate schemes - some as low as \u00a325k, with FANGG type employers closer to the \u00a375k mark. Grad scheme applications also seem much more time consuming than I'd like, and it's hard to keep up with MSc work and many applications at the same time.\n\nTwo questions:\n\n* Is \u00a365k total comp a reasonable target for me, starting in September 2023?\n* Should I be focusing on grad schemes now and try to get an offer, or do I have a better chance waiting until 2023 and applying to a standard full-time role?\n\nReally appreciate any advice, happy to have a chat if anyone wants to reach out. Thanks!", "author_fullname": "t2_4g4zpv1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[UK] Grad Schemes and Salary Expectations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y77tpe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666101523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;My background:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;BSc Mathematics from Russel Group, MSc Statistics in progress from Top 3 university&lt;/li&gt;\n&lt;li&gt;3 month quant analytics consulting internship + 12 months full-time data scientist at FinTech after graduating (c. \u00a338k total comp, outside of London)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m starting to apply to London-based data science/AI/quant analytics graduate schemes, and I am hoping to land something in the \u00a365k range, although there seems to be massive variance in the comp for graduate schemes - some as low as \u00a325k, with FANGG type employers closer to the \u00a375k mark. Grad scheme applications also seem much more time consuming than I&amp;#39;d like, and it&amp;#39;s hard to keep up with MSc work and many applications at the same time.&lt;/p&gt;\n\n&lt;p&gt;Two questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is \u00a365k total comp a reasonable target for me, starting in September 2023?&lt;/li&gt;\n&lt;li&gt;Should I be focusing on grad schemes now and try to get an offer, or do I have a better chance waiting until 2023 and applying to a standard full-time role?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Really appreciate any advice, happy to have a chat if anyone wants to reach out. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y77tpe", "is_robot_indexable": true, "report_reasons": null, "author": "EatsShootsAndLeavz", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y77tpe/uk_grad_schemes_and_salary_expectations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y77tpe/uk_grad_schemes_and_salary_expectations/", "subreddit_subscribers": 814150, "created_utc": 1666101523.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CYBERSOC HACKLAB PROJECT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y77rcw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_7qir32tk", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "cybersocitlibrary", "selftext": "Information technology and cyber security require a strong technical foundation in knowledge and skills. Most employees learn about the basics in school, on the job, or through training or certifications. After mastering the foundations and becoming a professional in IT and cyber security, how does one improve their knowledge? They need hands-on technical training in an isolated and secure lab environment in order to stay current and improve their technical skills. The most effective strategy for doing this is to build virtualized hacking laboratories. The CYBERSOC HACKLAB PROJECT can provide specialized training, be used to prepare employees for a scenario, and help employees in improving their technical abilities, making them better all-around technical staff. You may learn how to build your own hacking lab on this channel.\n\nSubscribe to our Channel at [https://bit.ly/3D1HkcN](https://bit.ly/3D1HkcN)  Thank you for your support and God Bless!", "author_fullname": "t2_7qir32tk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CYBERSOC HACKLAB PROJECT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/cybersocitlibrary", "hidden": false, "pwls": null, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y70yq4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1666081539.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.cybersocitlibrary", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Information technology and cyber security require a strong technical foundation in knowledge and skills. Most employees learn about the basics in school, on the job, or through training or certifications. After mastering the foundations and becoming a professional in IT and cyber security, how does one improve their knowledge? They need hands-on technical training in an isolated and secure lab environment in order to stay current and improve their technical skills. The most effective strategy for doing this is to build virtualized hacking laboratories. The CYBERSOC HACKLAB PROJECT can provide specialized training, be used to prepare employees for a scenario, and help employees in improving their technical abilities, making them better all-around technical staff. You may learn how to build your own hacking lab on this channel.&lt;/p&gt;\n\n&lt;p&gt;Subscribe to our Channel at &lt;a href=\"https://bit.ly/3D1HkcN\"&gt;https://bit.ly/3D1HkcN&lt;/a&gt;  Thank you for your support and God Bless!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YZwrWxk6g1GSAR3eruJk-HikT6JwKK-Yz3rR2X62g78.jpg?auto=webp&amp;s=dc4ed5cae4898ea896acb6520dc6322eb7601686", "width": 900, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/YZwrWxk6g1GSAR3eruJk-HikT6JwKK-Yz3rR2X62g78.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0dfbca34ab64640fa5feca71576dd26b74991d4e", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/YZwrWxk6g1GSAR3eruJk-HikT6JwKK-Yz3rR2X62g78.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ca714c952db94e9c9488f9f228bfa72baa42c13", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/YZwrWxk6g1GSAR3eruJk-HikT6JwKK-Yz3rR2X62g78.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=57a888d39d000c6ddf90ee7c2c368f40302cd349", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/YZwrWxk6g1GSAR3eruJk-HikT6JwKK-Yz3rR2X62g78.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=31cc2497e7a890dc78a36c7682557e68762f09ad", "width": 640, "height": 640}], "variants": {}, "id": "XE_yW0dkHKdc-XpE5ityrR9JG1ujmll591g8v1_FXtk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_68chyo", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y70yq4", "is_robot_indexable": true, "report_reasons": null, "author": "cybersocdm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/cybersocitlibrary/comments/y70yq4/cybersoc_hacklab_project/", "parent_whitelist_status": null, "stickied": false, "url": "https://old.reddit.com/r/cybersocitlibrary/comments/y70yq4/cybersoc_hacklab_project/", "subreddit_subscribers": 1066, "created_utc": 1666081539.0, "num_crossposts": 29, "media": null, "is_video": false}], "created": 1666101360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.cybersocitlibrary", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/cybersocitlibrary/comments/y70yq4/cybersoc_hacklab_project/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YZwrWxk6g1GSAR3eruJk-HikT6JwKK-Yz3rR2X62g78.jpg?auto=webp&amp;s=dc4ed5cae4898ea896acb6520dc6322eb7601686", "width": 900, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/YZwrWxk6g1GSAR3eruJk-HikT6JwKK-Yz3rR2X62g78.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0dfbca34ab64640fa5feca71576dd26b74991d4e", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/YZwrWxk6g1GSAR3eruJk-HikT6JwKK-Yz3rR2X62g78.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ca714c952db94e9c9488f9f228bfa72baa42c13", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/YZwrWxk6g1GSAR3eruJk-HikT6JwKK-Yz3rR2X62g78.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=57a888d39d000c6ddf90ee7c2c368f40302cd349", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/YZwrWxk6g1GSAR3eruJk-HikT6JwKK-Yz3rR2X62g78.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=31cc2497e7a890dc78a36c7682557e68762f09ad", "width": 640, "height": 640}], "variants": {}, "id": "XE_yW0dkHKdc-XpE5ityrR9JG1ujmll591g8v1_FXtk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y77rcw", "is_robot_indexable": true, "report_reasons": null, "author": "cybersocdm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_y70yq4", "author_flair_text_color": null, "permalink": "/r/datascience/comments/y77rcw/cybersoc_hacklab_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/cybersocitlibrary/comments/y70yq4/cybersoc_hacklab_project/", "subreddit_subscribers": 814150, "created_utc": 1666101360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to start posting content here on Reddit once a week in a data related subreddit. Stuff like cool libraries, updated laws and regulations about handling data, and other cool stuff. It\u2019s something that\u2019ll help me learn and add value to a community that\u2019s been really helpful to me. So what kind of stuff do you want to see more of here?", "author_fullname": "t2_2gd0s9mj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are you interested in?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y770h7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666099485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to start posting content here on Reddit once a week in a data related subreddit. Stuff like cool libraries, updated laws and regulations about handling data, and other cool stuff. It\u2019s something that\u2019ll help me learn and add value to a community that\u2019s been really helpful to me. So what kind of stuff do you want to see more of here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y770h7", "is_robot_indexable": true, "report_reasons": null, "author": "milkmanbran", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y770h7/what_are_you_interested_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y770h7/what_are_you_interested_in/", "subreddit_subscribers": 814150, "created_utc": 1666099485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_amfdjuba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Q] If you have an input variable that has a 0.5 Pearson Correlation with the Target Variable using the whole dataset, what would be your \"expected\" test/blind set performance (in R^2) when applying a machine learning model?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y762ix", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666097058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y762ix", "is_robot_indexable": true, "report_reasons": null, "author": "limedove", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y762ix/q_if_you_have_an_input_variable_that_has_a_05/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y762ix/q_if_you_have_an_input_variable_that_has_a_05/", "subreddit_subscribers": 814150, "created_utc": 1666097058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am doing a analysis to understand a decreasing trend on a KPI. My main goal here was explain which metrics are impacting this KPI and, if possible, how much are they impacting.\n\nStarted doing a lot of visualization and trying to find any kind of corelation between the KPI and other important metrics. I have found a metric, lets call it X, that it is VERY positive correlated (0.9) with my KPI.\n\nAt the end of the analysis tried to create an Linear Regression using all of the variables that seemed to be important/impacting my KPI. Ended up with some pretty good metrics (R squared and MAE).\n\nSomething, although, is making me very confuse... even if my X variable have a pretty strong positive correlation, its coefficient (when checking the Linear Regression) is negative. Does it makes sense ? Shouldnt the coefficient be positive as well as the correlation metric ?\n\nThe negative coefficient tells me that if I increase the X metric, my KPI will deacrese, which goes the opposite way of what my correlation says...\n\nAny help will be appreciated", "author_fullname": "t2_8ej2plie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linear Regression Coefficients", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y75gzh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666095370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am doing a analysis to understand a decreasing trend on a KPI. My main goal here was explain which metrics are impacting this KPI and, if possible, how much are they impacting.&lt;/p&gt;\n\n&lt;p&gt;Started doing a lot of visualization and trying to find any kind of corelation between the KPI and other important metrics. I have found a metric, lets call it X, that it is VERY positive correlated (0.9) with my KPI.&lt;/p&gt;\n\n&lt;p&gt;At the end of the analysis tried to create an Linear Regression using all of the variables that seemed to be important/impacting my KPI. Ended up with some pretty good metrics (R squared and MAE).&lt;/p&gt;\n\n&lt;p&gt;Something, although, is making me very confuse... even if my X variable have a pretty strong positive correlation, its coefficient (when checking the Linear Regression) is negative. Does it makes sense ? Shouldnt the coefficient be positive as well as the correlation metric ?&lt;/p&gt;\n\n&lt;p&gt;The negative coefficient tells me that if I increase the X metric, my KPI will deacrese, which goes the opposite way of what my correlation says...&lt;/p&gt;\n\n&lt;p&gt;Any help will be appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y75gzh", "is_robot_indexable": true, "report_reasons": null, "author": "miiguelkf", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y75gzh/linear_regression_coefficients/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y75gzh/linear_regression_coefficients/", "subreddit_subscribers": 814150, "created_utc": 1666095370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys,\n\nI've just been admitted to IT Masters Degree course with Data Science specialization, but from what I can see in the curriculum, there is not much math involved over the 2 years the programme goes on.\n\nHence my question - do you know any comprehensive online stats courses focusing on math? Could be expensive ones (eg. if Harvard offers any relevant classes), that's not an issue - I just need some truly reliable source that can supplement my uni studies in the long run.", "author_fullname": "t2_zydxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning statistics for Data Science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y74kvk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666092864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just been admitted to IT Masters Degree course with Data Science specialization, but from what I can see in the curriculum, there is not much math involved over the 2 years the programme goes on.&lt;/p&gt;\n\n&lt;p&gt;Hence my question - do you know any comprehensive online stats courses focusing on math? Could be expensive ones (eg. if Harvard offers any relevant classes), that&amp;#39;s not an issue - I just need some truly reliable source that can supplement my uni studies in the long run.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y74kvk", "is_robot_indexable": true, "report_reasons": null, "author": "SqayUp", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y74kvk/learning_statistics_for_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y74kvk/learning_statistics_for_data_science/", "subreddit_subscribers": 814150, "created_utc": 1666092864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Which solutions do you see for the following problem?\n\nGiven full training session (15-20 minutes) wearable sensors time-series data (accelerometer, etc.), identify the \"moment\" when a specific test was performed. The test can be a combination of activities: sit2stand+walk+stand2sit or walk+turn+walk+turn or something similar. Obviously, duration can be different, but the test is present within the session. Walking detection model is available, there is some training data, but not excessive amounts. I am just trying to see what the best approach would be? I am thinking about training a convolutional NN with short window size of 1/2 sec to detect each required test element and then see where they best align together. Also how to deal with the fact if the model doesn't find such a test, but it certainly was done?", "author_fullname": "t2_f3y80e7s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Series of events identification within given time-series data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y70yok", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666081534.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which solutions do you see for the following problem?&lt;/p&gt;\n\n&lt;p&gt;Given full training session (15-20 minutes) wearable sensors time-series data (accelerometer, etc.), identify the &amp;quot;moment&amp;quot; when a specific test was performed. The test can be a combination of activities: sit2stand+walk+stand2sit or walk+turn+walk+turn or something similar. Obviously, duration can be different, but the test is present within the session. Walking detection model is available, there is some training data, but not excessive amounts. I am just trying to see what the best approach would be? I am thinking about training a convolutional NN with short window size of 1/2 sec to detect each required test element and then see where they best align together. Also how to deal with the fact if the model doesn&amp;#39;t find such a test, but it certainly was done?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y70yok", "is_robot_indexable": true, "report_reasons": null, "author": "docoja1739", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y70yok/series_of_events_identification_within_given/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y70yok/series_of_events_identification_within_given/", "subreddit_subscribers": 814150, "created_utc": 1666081534.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I one-hot-encoded features and want to calculate similarity with the Jaccard index.\nBut, I am 100% sure that features have different importance for my clustering (i.e. some features are more important than others to calculate distance between my users )\n\nThinking about how Jaccard is calculated, the most obvious answer is to duplicate the important features as they count twice in these cases for the similarity.\nIs there any other way?", "author_fullname": "t2_5ydjc5q5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Weight features in jaccard distance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y70g2h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666079644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I one-hot-encoded features and want to calculate similarity with the Jaccard index.\nBut, I am 100% sure that features have different importance for my clustering (i.e. some features are more important than others to calculate distance between my users )&lt;/p&gt;\n\n&lt;p&gt;Thinking about how Jaccard is calculated, the most obvious answer is to duplicate the important features as they count twice in these cases for the similarity.\nIs there any other way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y70g2h", "is_robot_indexable": true, "report_reasons": null, "author": "alexreddor", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y70g2h/weight_features_in_jaccard_distance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y70g2h/weight_features_in_jaccard_distance/", "subreddit_subscribers": 814150, "created_utc": 1666079644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I have recently joined a newly formed data science team in a bank and my background has mainly been in retail and marketing analytics and modeling.\n\nNow from a banks perspective credit decisoning and delinquency modeling in the commercial space are obviously good use cases, but I was wondering if someone could provide examples of other use cases as well which would be relevant to institutional banking and lending practices.\n\nAn example could be credit limit optimization of borrowers. Any suggestion would be greatly appreciated! TIA", "author_fullname": "t2_2nqwlzun", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for professional project recommendations in the commercial and institutional banking space", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6vgur", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666063604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have recently joined a newly formed data science team in a bank and my background has mainly been in retail and marketing analytics and modeling.&lt;/p&gt;\n\n&lt;p&gt;Now from a banks perspective credit decisoning and delinquency modeling in the commercial space are obviously good use cases, but I was wondering if someone could provide examples of other use cases as well which would be relevant to institutional banking and lending practices.&lt;/p&gt;\n\n&lt;p&gt;An example could be credit limit optimization of borrowers. Any suggestion would be greatly appreciated! TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6vgur", "is_robot_indexable": true, "report_reasons": null, "author": "saiko1993", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6vgur/looking_for_professional_project_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6vgur/looking_for_professional_project_recommendations/", "subreddit_subscribers": 814150, "created_utc": 1666063604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My goal is to learn hypothesis testing. My understanding is that we do not have the population data. Therefore, we do not know the estimated population mean. So, we guess the population mean. If that so, can we change the population mean until the p-value is below 0.05, so we have enough evidence to reject the null hypothesis?\n\nThe code\n```\nimport pandas as pd\nfrom scipy import stats\n\nsample = pd.Series([1,2,3,4,5,6,7,8,9,10])\n\n# Hypothesis Testing:\n#\n# H0: \u03bc &lt;= 3.5 (previously it was \u03bc &lt;= 5.5, \n#               then 5, then 4.5, I change it until I get p value less than 0.05)\n# H1: \u03bc &gt; 3.5\n\nt_stat, p_value = stats.ttest_1samp(a = sample, popmean=3.5, alternative=\"greater\")\n\nprint(\"sample mean:\", sample.mean())\nprint(\"t-statistic: {0:.2f} p-value: {1:.2f}\".format(t_stat, p_value))\n\nprint(\"since we do not have the population data, \"\n      \"popmean=3.5 is not meaningful, \"\n      \"we can change it until we get enough evidence to reject the null hypothesis\")\n\nprint(\"critical value: 5%, therefore confidence interval: 90%\")\n\nprint(\"because p-value is below 0.05, we have enough evidence to reject the null hypothesis. \"\n      \"therefore, we can conclude that the mean population is more than 3.5\")\n\nprint(\"population standard deviation {0:.2f}\".format(sample.std(ddof=0)))\n\nprint(\"if we imagine the sample as the weight of waste, the conclusion is, \"\n      \"90% of population data is between population mean + 3 * population standard deviation, \"\n      \"in this case 3.5 +- 3 * 2.87\")\n```", "author_fullname": "t2_5yye765o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can we change the estimated population mean in Hypothesis Testing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6v9fc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666063002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My goal is to learn hypothesis testing. My understanding is that we do not have the population data. Therefore, we do not know the estimated population mean. So, we guess the population mean. If that so, can we change the population mean until the p-value is below 0.05, so we have enough evidence to reject the null hypothesis?&lt;/p&gt;\n\n&lt;p&gt;The code\n```\nimport pandas as pd\nfrom scipy import stats&lt;/p&gt;\n\n&lt;p&gt;sample = pd.Series([1,2,3,4,5,6,7,8,9,10])&lt;/p&gt;\n\n&lt;h1&gt;Hypothesis Testing:&lt;/h1&gt;\n\n&lt;h1&gt;H0: \u03bc &amp;lt;= 3.5 (previously it was \u03bc &amp;lt;= 5.5,&lt;/h1&gt;\n\n&lt;h1&gt;then 5, then 4.5, I change it until I get p value less than 0.05)&lt;/h1&gt;\n\n&lt;h1&gt;H1: \u03bc &amp;gt; 3.5&lt;/h1&gt;\n\n&lt;p&gt;t_stat, p_value = stats.ttest_1samp(a = sample, popmean=3.5, alternative=&amp;quot;greater&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;sample mean:&amp;quot;, sample.mean())\nprint(&amp;quot;t-statistic: {0:.2f} p-value: {1:.2f}&amp;quot;.format(t_stat, p_value))&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;since we do not have the population data, &amp;quot;\n      &amp;quot;popmean=3.5 is not meaningful, &amp;quot;\n      &amp;quot;we can change it until we get enough evidence to reject the null hypothesis&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;critical value: 5%, therefore confidence interval: 90%&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;because p-value is below 0.05, we have enough evidence to reject the null hypothesis. &amp;quot;\n      &amp;quot;therefore, we can conclude that the mean population is more than 3.5&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;population standard deviation {0:.2f}&amp;quot;.format(sample.std(ddof=0)))&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;if we imagine the sample as the weight of waste, the conclusion is, &amp;quot;\n      &amp;quot;90% of population data is between population mean + 3 * population standard deviation, &amp;quot;\n      &amp;quot;in this case 3.5 +- 3 * 2.87&amp;quot;)\n```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6v9fc", "is_robot_indexable": true, "report_reasons": null, "author": "kidfromtheast", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6v9fc/can_we_change_the_estimated_population_mean_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6v9fc/can_we_change_the_estimated_population_mean_in/", "subreddit_subscribers": 814150, "created_utc": 1666063002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_amfdjuba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Q] What are AutoML packages easily useable with M1 Macs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6su7s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666056304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6su7s", "is_robot_indexable": true, "report_reasons": null, "author": "limedove", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6su7s/q_what_are_automl_packages_easily_useable_with_m1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6su7s/q_what_are_automl_packages_easily_useable_with_m1/", "subreddit_subscribers": 814150, "created_utc": 1666056304.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating a framework for high-performance, cloud-native object storage need not be a mystery. Learn more from The Buyer\u2019s Guide to Software Defined #ObjectStorage to understand the key capabilities.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_y6r3pz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_90p5s", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5NmL1wYqCwWgwXIXv_DIZnr6NSP7CfeFWGiZ7AEgxNQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "minio", "selftext": "", "author_fullname": "t2_90p5s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating a framework for high-performance, cloud-native object storage need not be a mystery. Learn more from The Buyer\u2019s Guide to Software Defined #ObjectStorage to understand the key capabilities.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/minio", "hidden": false, "pwls": null, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_y6r3fi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5NmL1wYqCwWgwXIXv_DIZnr6NSP7CfeFWGiZ7AEgxNQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666051553.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.min.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.min.io/the-buyers-guide-to-software-defined-object-storage/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?auto=webp&amp;s=4c9d700643a7a50f07c41d3b8224cc2271fe9eda", "width": 2000, "height": 2500}, "resolutions": [{"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a7269efca35f3bea41359d9025aa0874089f6aa", "width": 108, "height": 135}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f69046d0f8d7aa8b06b9424f3dd7657e611f35b", "width": 216, "height": 270}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cce2526b804aa922ac5567b83b8d8423772dd99", "width": 320, "height": 400}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df9fd20727ca180aacd3779768de4ab4e82dd314", "width": 640, "height": 800}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a472c8451f0e2b54e8788df3e76c8598910bfe80", "width": 960, "height": 1200}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=00af793804249180718bb5711c7906431b23e9d8", "width": 1080, "height": 1350}], "variants": {}, "id": "P4iZZpW8gmFxT4qxHV2RCB-P5u7REB9eId2NuZ5PUpI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "427dbc5c-5868-11ec-81a8-7e4604f56887", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_3ahsh", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#00a6a5", "id": "y6r3fi", "is_robot_indexable": true, "report_reasons": null, "author": "prtkgpt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/minio/comments/y6r3fi/creating_a_framework_for_highperformance/", "parent_whitelist_status": null, "stickied": false, "url": "https://blog.min.io/the-buyers-guide-to-software-defined-object-storage/", "subreddit_subscribers": 502, "created_utc": 1666051553.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1666051575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.min.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://blog.min.io/the-buyers-guide-to-software-defined-object-storage/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?auto=webp&amp;s=4c9d700643a7a50f07c41d3b8224cc2271fe9eda", "width": 2000, "height": 2500}, "resolutions": [{"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a7269efca35f3bea41359d9025aa0874089f6aa", "width": 108, "height": 135}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f69046d0f8d7aa8b06b9424f3dd7657e611f35b", "width": 216, "height": 270}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cce2526b804aa922ac5567b83b8d8423772dd99", "width": 320, "height": 400}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df9fd20727ca180aacd3779768de4ab4e82dd314", "width": 640, "height": 800}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a472c8451f0e2b54e8788df3e76c8598910bfe80", "width": 960, "height": 1200}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=00af793804249180718bb5711c7906431b23e9d8", "width": 1080, "height": 1350}], "variants": {}, "id": "P4iZZpW8gmFxT4qxHV2RCB-P5u7REB9eId2NuZ5PUpI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6r3pz", "is_robot_indexable": true, "report_reasons": null, "author": "prtkgpt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_y6r3fi", "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6r3pz/creating_a_framework_for_highperformance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.min.io/the-buyers-guide-to-software-defined-object-storage/", "subreddit_subscribers": 814150, "created_utc": 1666051575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all,\n\nAm currently working on a side project analyzing a data set for anomaly\u2019s with unsupervised clustering models. As part of my pipeline, I\u2019d like to implement a feature scaling function. I\u2019ve seem to hit a wall and am stuck wondering if feature scaling is necessary/effective for encoded categorical columns. Considering their integer value merely represents a key to a categorical label, wouldn\u2019t scaling them possibly be detrimental to my model?\n\nAny advice on how to handle this is much appreciated.\n\nEDIT: Forgot to mention that some columns have up to 1800+ different types of possible values. OneHotEncoding doesn\u2019t seem to be possible", "author_fullname": "t2_ceqd7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature Scaling for Anomaly Detection Dataset", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6lag3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666038372.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666037548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;Am currently working on a side project analyzing a data set for anomaly\u2019s with unsupervised clustering models. As part of my pipeline, I\u2019d like to implement a feature scaling function. I\u2019ve seem to hit a wall and am stuck wondering if feature scaling is necessary/effective for encoded categorical columns. Considering their integer value merely represents a key to a categorical label, wouldn\u2019t scaling them possibly be detrimental to my model?&lt;/p&gt;\n\n&lt;p&gt;Any advice on how to handle this is much appreciated.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Forgot to mention that some columns have up to 1800+ different types of possible values. OneHotEncoding doesn\u2019t seem to be possible&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6lag3", "is_robot_indexable": true, "report_reasons": null, "author": "DJK923", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6lag3/feature_scaling_for_anomaly_detection_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6lag3/feature_scaling_for_anomaly_detection_dataset/", "subreddit_subscribers": 814150, "created_utc": 1666037548.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}