{"kind": "Listing", "data": {"after": "t3_y6bity", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am maintaining a page for all Kaggle Solutions here - [https://kaggle.datagyan.co.in/](https://kaggle.datagyan.co.in/)\n\nIf anyone is interested to contribute to this page(GitHub repo), do hit me up!", "author_fullname": "t2_60oji2x5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kaggle Solutions Page", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y68ntp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Networking", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666006213.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am maintaining a page for all Kaggle Solutions here - &lt;a href=\"https://kaggle.datagyan.co.in/\"&gt;https://kaggle.datagyan.co.in/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If anyone is interested to contribute to this page(GitHub repo), do hit me up!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y68ntp", "is_robot_indexable": true, "report_reasons": null, "author": "rakash_ram", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y68ntp/kaggle_solutions_page/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y68ntp/kaggle_solutions_page/", "subreddit_subscribers": 814100, "created_utc": 1666006213.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was recently hired as the first (and only) data scientist for a municipal government. My position will essentially be starting from a totally blank slate: other city employees are familiar with Microsoft Power BI and Excel, and that's it. \n\nMy question is this: what languages and/or software tools should I set up as the \"standard\" going forward? I have mediocre Python skills, and I'm *very* good with R, but I'm worried that R is maybe too niche to build a whole DS program around? Maybe it's asking too much to expect future city employees to be familiar with R?\n\nOn the other hand, I'm not sure I will *ever* have other DS-inclined coworkers... So maybe this is a moot point? If I want to cater my code to my real coworkers, is either R or Python better at interacting with Microsoft Excel (meeting them where they are)? Sorry if this is a weird set of questions, I just want to try and think things through before I go too far down any path, hopefully to \"future proof\" my work.", "author_fullname": "t2_7nhni", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting a city government DS program: which language should I use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6nr7c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666043207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was recently hired as the first (and only) data scientist for a municipal government. My position will essentially be starting from a totally blank slate: other city employees are familiar with Microsoft Power BI and Excel, and that&amp;#39;s it. &lt;/p&gt;\n\n&lt;p&gt;My question is this: what languages and/or software tools should I set up as the &amp;quot;standard&amp;quot; going forward? I have mediocre Python skills, and I&amp;#39;m &lt;em&gt;very&lt;/em&gt; good with R, but I&amp;#39;m worried that R is maybe too niche to build a whole DS program around? Maybe it&amp;#39;s asking too much to expect future city employees to be familiar with R?&lt;/p&gt;\n\n&lt;p&gt;On the other hand, I&amp;#39;m not sure I will &lt;em&gt;ever&lt;/em&gt; have other DS-inclined coworkers... So maybe this is a moot point? If I want to cater my code to my real coworkers, is either R or Python better at interacting with Microsoft Excel (meeting them where they are)? Sorry if this is a weird set of questions, I just want to try and think things through before I go too far down any path, hopefully to &amp;quot;future proof&amp;quot; my work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6nr7c", "is_robot_indexable": true, "report_reasons": null, "author": "dankatheist420", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6nr7c/starting_a_city_government_ds_program_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6nr7c/starting_a_city_government_ds_program_which/", "subreddit_subscribers": 814100, "created_utc": 1666043207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "The `data \\w+` gold rush has been a blessing and a curse, blessing in that many of us are getting filthy rich off it, curse in that many (frankly unskilled) people see the job market and think \u201cwow I gotta get me a piece of that\u201d and proceed to bombard every specialist board with mentorship requests and e-begging for a crumb of interview. \n\nFrankly I wouldn\u2019t mind this if the people asking had done some cursory research beforehand and asked politely, but it seems like every jerkoff who\u2019s caught a whiff of an Excel spreadsheet thinks they can land a FAANG job overnight and, instead of looking on Google for \u201chow to data job pls to help\u201d and seeing the ten trillion useless Medium articles made by the endless morons trying to resume pad and slip their jimmy into an Amazon L3 role that would tell them practically everything they need to know (even if by and large anything posted on Medium is worthless) they choose to pepper subs like /r/dataengineering, /r/dataanalysis, and this one with the same \u201chow to data job please give me six figures\u201d - it\u2019s like asking /r/personalfinance \u201chelp how do I own a bank account\u201d repeated for every hapless schmuck who\u2019s been hiding their Benjamins in granny\u2019s cookie tin for the last sixteen years of their childhood. \n\nNot even getting into the fact that doing basic research on the topic at hand is probably *the* fundamental skill for any data-*whatever* role, what\u2019s even funnier is that I\u2019d hazard a guess that most of us who *actually work in the industry* have better things to do during the day, so the people answering questions are probably majority kids trying to get their first data-whatever job - blind leading the blind all over again. \n\nTLDR: Screw you guys I\u2019m going to /r/Statistics", "author_fullname": "t2_gtodeuwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[UNHINGED RANT] It\u2019s kind of annoying to see that, in general, most data-related spaces are flush with \u201chow do I get a job\u201d and comparatively little discussion around the actual topic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y6w5ab", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666065579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The &lt;code&gt;data \\w+&lt;/code&gt; gold rush has been a blessing and a curse, blessing in that many of us are getting filthy rich off it, curse in that many (frankly unskilled) people see the job market and think \u201cwow I gotta get me a piece of that\u201d and proceed to bombard every specialist board with mentorship requests and e-begging for a crumb of interview. &lt;/p&gt;\n\n&lt;p&gt;Frankly I wouldn\u2019t mind this if the people asking had done some cursory research beforehand and asked politely, but it seems like every jerkoff who\u2019s caught a whiff of an Excel spreadsheet thinks they can land a FAANG job overnight and, instead of looking on Google for \u201chow to data job pls to help\u201d and seeing the ten trillion useless Medium articles made by the endless morons trying to resume pad and slip their jimmy into an Amazon L3 role that would tell them practically everything they need to know (even if by and large anything posted on Medium is worthless) they choose to pepper subs like &lt;a href=\"/r/dataengineering\"&gt;/r/dataengineering&lt;/a&gt;, &lt;a href=\"/r/dataanalysis\"&gt;/r/dataanalysis&lt;/a&gt;, and this one with the same \u201chow to data job please give me six figures\u201d - it\u2019s like asking &lt;a href=\"/r/personalfinance\"&gt;/r/personalfinance&lt;/a&gt; \u201chelp how do I own a bank account\u201d repeated for every hapless schmuck who\u2019s been hiding their Benjamins in granny\u2019s cookie tin for the last sixteen years of their childhood. &lt;/p&gt;\n\n&lt;p&gt;Not even getting into the fact that doing basic research on the topic at hand is probably &lt;em&gt;the&lt;/em&gt; fundamental skill for any data-&lt;em&gt;whatever&lt;/em&gt; role, what\u2019s even funnier is that I\u2019d hazard a guess that most of us who &lt;em&gt;actually work in the industry&lt;/em&gt; have better things to do during the day, so the people answering questions are probably majority kids trying to get their first data-whatever job - blind leading the blind all over again. &lt;/p&gt;\n\n&lt;p&gt;TLDR: Screw you guys I\u2019m going to &lt;a href=\"/r/Statistics\"&gt;/r/Statistics&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6w5ab", "is_robot_indexable": true, "report_reasons": null, "author": "sigma_hyperborean", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6w5ab/unhinged_rant_its_kind_of_annoying_to_see_that_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6w5ab/unhinged_rant_its_kind_of_annoying_to_see_that_in/", "subreddit_subscribers": 814100, "created_utc": 1666065579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am trying to use nltk VADER to do sentiment analysis to see the emotions associated with a certain word. For example, let\u2019s say the keyword is \u201cpizza\u201d. Unfortunately, If I were the analyze the sentence \n\n\u201cI hate broccoli, carrots, and turnips, and fries are awful, although pizza is delicious\u201d\n\nIt would have a fairly high \u201cnegative\u201d score because the sentence, overall, is fairly negative. However, clearly there is actually positive sentiment towards the keyword \u201cpizza\u201d. Is there a specific model or a clever way to analyze the sentiment toward a particular word like this?", "author_fullname": "t2_g7jmnu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sentiment analysis for a single keyword", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6dyla", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666020327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to use nltk VADER to do sentiment analysis to see the emotions associated with a certain word. For example, let\u2019s say the keyword is \u201cpizza\u201d. Unfortunately, If I were the analyze the sentence &lt;/p&gt;\n\n&lt;p&gt;\u201cI hate broccoli, carrots, and turnips, and fries are awful, although pizza is delicious\u201d&lt;/p&gt;\n\n&lt;p&gt;It would have a fairly high \u201cnegative\u201d score because the sentence, overall, is fairly negative. However, clearly there is actually positive sentiment towards the keyword \u201cpizza\u201d. Is there a specific model or a clever way to analyze the sentiment toward a particular word like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6dyla", "is_robot_indexable": true, "report_reasons": null, "author": "_hairyberry_", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6dyla/sentiment_analysis_for_a_single_keyword/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6dyla/sentiment_analysis_for_a_single_keyword/", "subreddit_subscribers": 814100, "created_utc": 1666020327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I know it was probably discussed here a lot but I just wanted to see a sample of all the working people in here - How does your everyday work look like? What do you actually do? What languages you use?\n\nFor me - Junior, working in a fintech startup. Doing pure ML - from preprocessing to modeling. No Data Analysis and no SQL, 100% Python.", "author_fullname": "t2_3hqmko1r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Scientists here - what are your everyday tasks in your job? How does it look like?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6884v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666004948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know it was probably discussed here a lot but I just wanted to see a sample of all the working people in here - How does your everyday work look like? What do you actually do? What languages you use?&lt;/p&gt;\n\n&lt;p&gt;For me - Junior, working in a fintech startup. Doing pure ML - from preprocessing to modeling. No Data Analysis and no SQL, 100% Python.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6884v", "is_robot_indexable": true, "report_reasons": null, "author": "nuriel8833", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6884v/data_scientists_here_what_are_your_everyday_tasks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6884v/data_scientists_here_what_are_your_everyday_tasks/", "subreddit_subscribers": 814100, "created_utc": 1666004948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What do you guys think is the most lucrative industry for DS? Preferably entry to mid level\n\nAnd what does their stack or most frequently used tools/languages look like? Just looking to get into the most profitable industry + role quickly from being a BI Analyst with a few years experience\n\nI know my friend worked in retail, then worked 1.5 yrs as a data analyst, 7 months as a PM, and THEN he became a senior data scientist making real estate prediction models and makes 300k total comp in NYC. So in 2 years he went from working retail to making 300k+ which is pretty insane to me", "author_fullname": "t2_lh7t6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most Lucrative Sector &amp; Role for Data Scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6bcs4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666069364.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666013418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you guys think is the most lucrative industry for DS? Preferably entry to mid level&lt;/p&gt;\n\n&lt;p&gt;And what does their stack or most frequently used tools/languages look like? Just looking to get into the most profitable industry + role quickly from being a BI Analyst with a few years experience&lt;/p&gt;\n\n&lt;p&gt;I know my friend worked in retail, then worked 1.5 yrs as a data analyst, 7 months as a PM, and THEN he became a senior data scientist making real estate prediction models and makes 300k total comp in NYC. So in 2 years he went from working retail to making 300k+ which is pretty insane to me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6bcs4", "is_robot_indexable": true, "report_reasons": null, "author": "iemg88", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6bcs4/most_lucrative_sector_role_for_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6bcs4/most_lucrative_sector_role_for_data_scientist/", "subreddit_subscribers": 814100, "created_utc": 1666013418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm just going to make up an example here.  Imagine you want to predict a stock price.  Your feature is today's price and your target is tomorrow's price.  \n\nHere would be a simple case:\nFeature = [100, 105, 110, 115, 120...]\nTarget = [105, 110, 115, 120, 125...]\n\nAre we allowed to do cross validation on this?  \n\nEven though we are never using future data to predict the past, I still feel like cross validation should not be allowed in this case.  Better approaches would be a walk-forward validation or a rolling window validation.  \n\nAm I correct in my assumption that cross validation should not be allowed?", "author_fullname": "t2_cn54oiy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cross Validation with Time Series", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6v261", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666062429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just going to make up an example here.  Imagine you want to predict a stock price.  Your feature is today&amp;#39;s price and your target is tomorrow&amp;#39;s price.  &lt;/p&gt;\n\n&lt;p&gt;Here would be a simple case:\nFeature = [100, 105, 110, 115, 120...]\nTarget = [105, 110, 115, 120, 125...]&lt;/p&gt;\n\n&lt;p&gt;Are we allowed to do cross validation on this?  &lt;/p&gt;\n\n&lt;p&gt;Even though we are never using future data to predict the past, I still feel like cross validation should not be allowed in this case.  Better approaches would be a walk-forward validation or a rolling window validation.  &lt;/p&gt;\n\n&lt;p&gt;Am I correct in my assumption that cross validation should not be allowed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6v261", "is_robot_indexable": true, "report_reasons": null, "author": "BlackLotus8888", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6v261/cross_validation_with_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6v261/cross_validation_with_time_series/", "subreddit_subscribers": 814100, "created_utc": 1666062429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "[Ergest Xheblati](https://www.linkedin.com/in/ergestxheblati/) is the author of [*Minimum Viable SQL Patterns*](https://ergestx.gumroad.com/l/sqlpatterns), and he's spent the last 15 years mastering SQL.  \n\nIn this [SQL workshop](https://youtu.be/UFiZx5NlzL4), Ergest teaches various SQL principles (patters) to help you take your SQL skills from intermediate to expert. More specifically, you'll learn about:   \n\ud83c\udfaf Query composition patterns - How to make your complex queries shorter, more legible, and more performant   \n\ud83c\udfaf Query maintainability patterns - Constructing CTEs that can be reused. In software engineering, it's called the DRY principle (don't repeat yourself)  \n\ud83c\udfaf Query robustness patterns - Constructing queries that don't break when the underlying data changes in unpredictable ways  \n\ud83c\udfaf Query performance patterns - Make your queries faster (and cheaper) regardless of specific database you\u2019re using.\n\nToward the end of the workshop, Ergest answers over a dozen questions from SQL professionals all over the world.  \n\nWatch the full workshop \ud83d\udc49 [**here**](https://www.youtube.com/watch?v=UFiZx5NlzL4) \ud83d\udc48. It's free, and don't worry, you're not being sold on a SaaS product \ud83e\udd23  \n\n\nIf you enjoyed this video, please join Ergest and +1,000 like minded data professionals in our Slack Community, the [OA Club](https://www.operationalanalytics.club/)! We're creating content like this all the time!", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Workshop recording: Making SQL more efficient, readable, and easier to debug", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6n2tm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666041610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.linkedin.com/in/ergestxheblati/\"&gt;Ergest Xheblati&lt;/a&gt; is the author of &lt;a href=\"https://ergestx.gumroad.com/l/sqlpatterns\"&gt;&lt;em&gt;Minimum Viable SQL Patterns&lt;/em&gt;&lt;/a&gt;, and he&amp;#39;s spent the last 15 years mastering SQL.  &lt;/p&gt;\n\n&lt;p&gt;In this &lt;a href=\"https://youtu.be/UFiZx5NlzL4\"&gt;SQL workshop&lt;/a&gt;, Ergest teaches various SQL principles (patters) to help you take your SQL skills from intermediate to expert. More specifically, you&amp;#39;ll learn about:&lt;br/&gt;\n\ud83c\udfaf Query composition patterns - How to make your complex queries shorter, more legible, and more performant&lt;br/&gt;\n\ud83c\udfaf Query maintainability patterns - Constructing CTEs that can be reused. In software engineering, it&amp;#39;s called the DRY principle (don&amp;#39;t repeat yourself)&lt;br/&gt;\n\ud83c\udfaf Query robustness patterns - Constructing queries that don&amp;#39;t break when the underlying data changes in unpredictable ways&lt;br/&gt;\n\ud83c\udfaf Query performance patterns - Make your queries faster (and cheaper) regardless of specific database you\u2019re using.&lt;/p&gt;\n\n&lt;p&gt;Toward the end of the workshop, Ergest answers over a dozen questions from SQL professionals all over the world.  &lt;/p&gt;\n\n&lt;p&gt;Watch the full workshop \ud83d\udc49 &lt;a href=\"https://www.youtube.com/watch?v=UFiZx5NlzL4\"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt; \ud83d\udc48. It&amp;#39;s free, and don&amp;#39;t worry, you&amp;#39;re not being sold on a SaaS product \ud83e\udd23  &lt;/p&gt;\n\n&lt;p&gt;If you enjoyed this video, please join Ergest and +1,000 like minded data professionals in our Slack Community, the &lt;a href=\"https://www.operationalanalytics.club/\"&gt;OA Club&lt;/a&gt;! We&amp;#39;re creating content like this all the time!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6n2tm", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6n2tm/sql_workshop_recording_making_sql_more_efficient/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6n2tm/sql_workshop_recording_making_sql_more_efficient/", "subreddit_subscribers": 814100, "created_utc": 1666041610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I notice that some times after completing a really exciting/impactful project I tend to feel pretty \u2018meh\u2019 about doing other work for a bit of time (even follow on work). Do you feel the same way? I dunno, maybe I want some victory laps or something \ud83d\ude02", "author_fullname": "t2_1xj38rmh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Peaks and valleys", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6mvcz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666041122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I notice that some times after completing a really exciting/impactful project I tend to feel pretty \u2018meh\u2019 about doing other work for a bit of time (even follow on work). Do you feel the same way? I dunno, maybe I want some victory laps or something \ud83d\ude02&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6mvcz", "is_robot_indexable": true, "report_reasons": null, "author": "eomar2828", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6mvcz/peaks_and_valleys/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6mvcz/peaks_and_valleys/", "subreddit_subscribers": 814100, "created_utc": 1666041122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Let's say I have a sequence of terms. Each term could be any string of alphabetical characters. `['abc', 'stg', 'drT']`. Now let's say I have millions of such sequences of varying length (might be 1 term, might be 1000) and some of them are 'bad'. But their combination matters toward that badness. So I'm baselining and looking for outliers, which should have a high correlation with 'bad' in my dataset. Term might indicate 'badness' in one combination but is totally benign in another.\n\nSo that's our search-space and problem statement. I am not educated in DS. But I understand the basic idea: Correlate traits and relationships of features in one or more data-sets to answer a question / solve a problem. But which features to select? It seems like an interesting question, since as analysts we don't know everything and if we try to select feature manually, there's a good chance we might miss something. Right? There might be some combination of features which, when correlated with some combination of other features ends up being a good indicator that we wouldn't have manually expected. Maybe 3 features plotted on a 3D chart with K-Means clustering or just human viewing reveals something interesting / creates a valuable signal that we might not have guessed or tested if we were hand-picking the features we thought might be useful.\n\nSo I'm computing an exhaustive list of features globally and locally, like:\n\nHow standard / deviant is the feature locally?\n\n* local occurence count\n* local occurence as % of total local feature count\n\nHow standard / deviant is the feature globally?\n\n* global occurence count\n* global occurence count as % of global feature count\n\nHow is the feature related to other features locally?\n\n* for each term: for each other term: min distance, where distance is in terms of index\n* for each term: for each other term: max distance, where distance is in terms of index\n* for each term: for each other term: avg distance, where distance is in terms of index\n* for each term: for each other term: min distance, where distance is in terms of character count\n* for each term: for each other term: max distance, where distance is in terms of character count\n* for each term: for each other term: avg distance, where distance is in terms of character count\n\nHow globally standard / deviant is the feature's relationships to other features locally?\n\n* for each term: for each other term: min distance as % of all global min distances between these two symbols, where distance is in terms of index\n* for each term: for each other term: max distance as % of all global max distances between these two symbols, where distance is in terms of index\n* for each term: for each other term: avg distance as % of all global avg distances between these two symbols, where distance is in terms of index\n* for each term: for each other term: min distance as % of all global min distances between these two symbols, where distance is in terms of character count\n* for each term: for each other term: max distance as % of all global max distances between these two symbols, where distance is in terms of character count\n* for each term: for each other term: avg distance as % of all global avg distances between these two symbols, where distance is in terms of character count\n\nThat's the theory I'm experimenting with, planning to just produce every possible chart, perhaps thousands, and then do some simple clustering on all of them and see if features associated with known-bad sample have a statistical tendency to end up in distinct clusters in any of those plots.\n\nAm I potentially on the right track? Is this a known strategy in data science? It's my first real run-in with cleaning data, isolating / computing features, so I'm still really just shooting in the dark at this point.", "author_fullname": "t2_8yytz5qc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it sometimes a good strategy to identify / prep all possible features, model them all dynamically (with recursive code) and measure which collection of features end up correlating more often than others to identify whatever signal you're looking for?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6txbs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666063704.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666059259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say I have a sequence of terms. Each term could be any string of alphabetical characters. &lt;code&gt;[&amp;#39;abc&amp;#39;, &amp;#39;stg&amp;#39;, &amp;#39;drT&amp;#39;]&lt;/code&gt;. Now let&amp;#39;s say I have millions of such sequences of varying length (might be 1 term, might be 1000) and some of them are &amp;#39;bad&amp;#39;. But their combination matters toward that badness. So I&amp;#39;m baselining and looking for outliers, which should have a high correlation with &amp;#39;bad&amp;#39; in my dataset. Term might indicate &amp;#39;badness&amp;#39; in one combination but is totally benign in another.&lt;/p&gt;\n\n&lt;p&gt;So that&amp;#39;s our search-space and problem statement. I am not educated in DS. But I understand the basic idea: Correlate traits and relationships of features in one or more data-sets to answer a question / solve a problem. But which features to select? It seems like an interesting question, since as analysts we don&amp;#39;t know everything and if we try to select feature manually, there&amp;#39;s a good chance we might miss something. Right? There might be some combination of features which, when correlated with some combination of other features ends up being a good indicator that we wouldn&amp;#39;t have manually expected. Maybe 3 features plotted on a 3D chart with K-Means clustering or just human viewing reveals something interesting / creates a valuable signal that we might not have guessed or tested if we were hand-picking the features we thought might be useful.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m computing an exhaustive list of features globally and locally, like:&lt;/p&gt;\n\n&lt;p&gt;How standard / deviant is the feature locally?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;local occurence count&lt;/li&gt;\n&lt;li&gt;local occurence as % of total local feature count&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How standard / deviant is the feature globally?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;global occurence count&lt;/li&gt;\n&lt;li&gt;global occurence count as % of global feature count&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How is the feature related to other features locally?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;for each term: for each other term: min distance, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: max distance, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: avg distance, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: min distance, where distance is in terms of character count&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: max distance, where distance is in terms of character count&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: avg distance, where distance is in terms of character count&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How globally standard / deviant is the feature&amp;#39;s relationships to other features locally?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;for each term: for each other term: min distance as % of all global min distances between these two symbols, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: max distance as % of all global max distances between these two symbols, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: avg distance as % of all global avg distances between these two symbols, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: min distance as % of all global min distances between these two symbols, where distance is in terms of character count&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: max distance as % of all global max distances between these two symbols, where distance is in terms of character count&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: avg distance as % of all global avg distances between these two symbols, where distance is in terms of character count&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That&amp;#39;s the theory I&amp;#39;m experimenting with, planning to just produce every possible chart, perhaps thousands, and then do some simple clustering on all of them and see if features associated with known-bad sample have a statistical tendency to end up in distinct clusters in any of those plots.&lt;/p&gt;\n\n&lt;p&gt;Am I potentially on the right track? Is this a known strategy in data science? It&amp;#39;s my first real run-in with cleaning data, isolating / computing features, so I&amp;#39;m still really just shooting in the dark at this point.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6txbs", "is_robot_indexable": true, "report_reasons": null, "author": "Jonathan-Todd", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6txbs/is_it_sometimes_a_good_strategy_to_identify_prep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6txbs/is_it_sometimes_a_good_strategy_to_identify_prep/", "subreddit_subscribers": 814100, "created_utc": 1666059259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I have recently joined a newly formed data science team in a bank and my background has mainly been in retail and marketing analytics and modeling.\n\nNow from a banks perspective credit decisoning and delinquency modeling in the commercial space are obviously good use cases, but I was wondering if someone could provide examples of other use cases as well which would be relevant to institutional banking and lending practices.\n\nAn example could be credit limit optimization of borrowers. Any suggestion would be greatly appreciated! TIA", "author_fullname": "t2_2nqwlzun", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for professional project recommendations in the commercial and institutional banking space", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y6vgur", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666063604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have recently joined a newly formed data science team in a bank and my background has mainly been in retail and marketing analytics and modeling.&lt;/p&gt;\n\n&lt;p&gt;Now from a banks perspective credit decisoning and delinquency modeling in the commercial space are obviously good use cases, but I was wondering if someone could provide examples of other use cases as well which would be relevant to institutional banking and lending practices.&lt;/p&gt;\n\n&lt;p&gt;An example could be credit limit optimization of borrowers. Any suggestion would be greatly appreciated! TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6vgur", "is_robot_indexable": true, "report_reasons": null, "author": "saiko1993", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6vgur/looking_for_professional_project_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6vgur/looking_for_professional_project_recommendations/", "subreddit_subscribers": 814100, "created_utc": 1666063604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I'm relatively new to time-series modeling and am not quite sure how to approach this problem. Let's say I have data on 100 students and their 100 most recent test scores (assuming that each test is the same across all students and of varying difficulty successively), and I want to understand what kind of exponential smoothing value is optimal to predict each student's next test score.\n\nBecause there must be some information contained within the population's overall performance across tests, I want to leverage that. But am I able to do so using something like R's forecast package's ETS function? I know that it can handle a single-subject time series but how can it assess the overall alpha value given 100 subjects?", "author_fullname": "t2_21w9l2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Method for Optimal Exponential Smoothing of Time Series across samples?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y6va5a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666063062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m relatively new to time-series modeling and am not quite sure how to approach this problem. Let&amp;#39;s say I have data on 100 students and their 100 most recent test scores (assuming that each test is the same across all students and of varying difficulty successively), and I want to understand what kind of exponential smoothing value is optimal to predict each student&amp;#39;s next test score.&lt;/p&gt;\n\n&lt;p&gt;Because there must be some information contained within the population&amp;#39;s overall performance across tests, I want to leverage that. But am I able to do so using something like R&amp;#39;s forecast package&amp;#39;s ETS function? I know that it can handle a single-subject time series but how can it assess the overall alpha value given 100 subjects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6va5a", "is_robot_indexable": true, "report_reasons": null, "author": "DataScience0", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6va5a/method_for_optimal_exponential_smoothing_of_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6va5a/method_for_optimal_exponential_smoothing_of_time/", "subreddit_subscribers": 814100, "created_utc": 1666063062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating a framework for high-performance, cloud-native object storage need not be a mystery. Learn more from The Buyer\u2019s Guide to Software Defined #ObjectStorage to understand the key capabilities.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_y6r3pz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_90p5s", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5NmL1wYqCwWgwXIXv_DIZnr6NSP7CfeFWGiZ7AEgxNQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "minio", "selftext": "", "author_fullname": "t2_90p5s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating a framework for high-performance, cloud-native object storage need not be a mystery. Learn more from The Buyer\u2019s Guide to Software Defined #ObjectStorage to understand the key capabilities.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/minio", "hidden": false, "pwls": null, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_y6r3fi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5NmL1wYqCwWgwXIXv_DIZnr6NSP7CfeFWGiZ7AEgxNQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666051553.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.min.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.min.io/the-buyers-guide-to-software-defined-object-storage/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?auto=webp&amp;s=4c9d700643a7a50f07c41d3b8224cc2271fe9eda", "width": 2000, "height": 2500}, "resolutions": [{"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a7269efca35f3bea41359d9025aa0874089f6aa", "width": 108, "height": 135}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f69046d0f8d7aa8b06b9424f3dd7657e611f35b", "width": 216, "height": 270}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cce2526b804aa922ac5567b83b8d8423772dd99", "width": 320, "height": 400}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df9fd20727ca180aacd3779768de4ab4e82dd314", "width": 640, "height": 800}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a472c8451f0e2b54e8788df3e76c8598910bfe80", "width": 960, "height": 1200}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=00af793804249180718bb5711c7906431b23e9d8", "width": 1080, "height": 1350}], "variants": {}, "id": "P4iZZpW8gmFxT4qxHV2RCB-P5u7REB9eId2NuZ5PUpI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "427dbc5c-5868-11ec-81a8-7e4604f56887", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_3ahsh", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#00a6a5", "id": "y6r3fi", "is_robot_indexable": true, "report_reasons": null, "author": "prtkgpt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/minio/comments/y6r3fi/creating_a_framework_for_highperformance/", "parent_whitelist_status": null, "stickied": false, "url": "https://blog.min.io/the-buyers-guide-to-software-defined-object-storage/", "subreddit_subscribers": 503, "created_utc": 1666051553.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1666051575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.min.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://blog.min.io/the-buyers-guide-to-software-defined-object-storage/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?auto=webp&amp;s=4c9d700643a7a50f07c41d3b8224cc2271fe9eda", "width": 2000, "height": 2500}, "resolutions": [{"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a7269efca35f3bea41359d9025aa0874089f6aa", "width": 108, "height": 135}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f69046d0f8d7aa8b06b9424f3dd7657e611f35b", "width": 216, "height": 270}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cce2526b804aa922ac5567b83b8d8423772dd99", "width": 320, "height": 400}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df9fd20727ca180aacd3779768de4ab4e82dd314", "width": 640, "height": 800}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a472c8451f0e2b54e8788df3e76c8598910bfe80", "width": 960, "height": 1200}, {"url": "https://external-preview.redd.it/hzUDa9motX-TeigJq1Z0AgffvO7HUBaIpqqIj22FZQA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=00af793804249180718bb5711c7906431b23e9d8", "width": 1080, "height": 1350}], "variants": {}, "id": "P4iZZpW8gmFxT4qxHV2RCB-P5u7REB9eId2NuZ5PUpI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6r3pz", "is_robot_indexable": true, "report_reasons": null, "author": "prtkgpt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_y6r3fi", "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6r3pz/creating_a_framework_for_highperformance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.min.io/the-buyers-guide-to-software-defined-object-storage/", "subreddit_subscribers": 814100, "created_utc": 1666051575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Throughout my life, I haven't really known what I wanted to do as a job until now. I love the idea of working from home and I'm also excited to start learning to code, but the affordable university I have chosen doesn't have a computer science course, which would be more suited for me imo as it would be more programming oriented, however, I suppose I can program on my own in my free time.\n\nWhat they do have is the computational mathematics course, I've never heard of this before but it is heavily math based with a few programming classes sprinkled here and there. As I don't know much about these classes yet, I'd like to ask your advice if taking all of the ones listed is what will definitely help me gain the framework to have the brain of a data scientist. So here they are:\n\nSo they have calculus and analytic geometry, intro to statistic analysis and intro to discrete mathematics, matrix and linear algebra (I've heard that this one is very useful for the career), abstract algebraic structures, multivariable calculus, applied differential equations, probability I, and mathematical molding and simulation.\n\nThe reason why I listed all of them is because I want to hear your advice to know for sure if taking all of these is worth it or if some of it might be a waste. If any of you are familiar with these courses please let me know. I don't know much yet what path I should take, so any advice would be appreciated.\n\nAlso, if you can tell me what websites or links you used to study certain programming courses, or the names of books that teach them, I would like to hear it as well.\n\nThanks.", "author_fullname": "t2_jemak9dw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Going for a bachelors in computational mathematics, will this help me gain the fundamental skills to become a Data Scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6mmvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666040587.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Throughout my life, I haven&amp;#39;t really known what I wanted to do as a job until now. I love the idea of working from home and I&amp;#39;m also excited to start learning to code, but the affordable university I have chosen doesn&amp;#39;t have a computer science course, which would be more suited for me imo as it would be more programming oriented, however, I suppose I can program on my own in my free time.&lt;/p&gt;\n\n&lt;p&gt;What they do have is the computational mathematics course, I&amp;#39;ve never heard of this before but it is heavily math based with a few programming classes sprinkled here and there. As I don&amp;#39;t know much about these classes yet, I&amp;#39;d like to ask your advice if taking all of the ones listed is what will definitely help me gain the framework to have the brain of a data scientist. So here they are:&lt;/p&gt;\n\n&lt;p&gt;So they have calculus and analytic geometry, intro to statistic analysis and intro to discrete mathematics, matrix and linear algebra (I&amp;#39;ve heard that this one is very useful for the career), abstract algebraic structures, multivariable calculus, applied differential equations, probability I, and mathematical molding and simulation.&lt;/p&gt;\n\n&lt;p&gt;The reason why I listed all of them is because I want to hear your advice to know for sure if taking all of these is worth it or if some of it might be a waste. If any of you are familiar with these courses please let me know. I don&amp;#39;t know much yet what path I should take, so any advice would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Also, if you can tell me what websites or links you used to study certain programming courses, or the names of books that teach them, I would like to hear it as well.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6mmvo", "is_robot_indexable": true, "report_reasons": null, "author": "SleepingPooper", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6mmvo/going_for_a_bachelors_in_computational/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6mmvo/going_for_a_bachelors_in_computational/", "subreddit_subscribers": 814100, "created_utc": 1666040587.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm choosing between popular methods like keybert, spacy, textrank, rake and yake to create a baseline for keyword extraction. So nothing fancy at this point. What criteria should I consider? It's going to be used on open domain scientific texts.", "author_fullname": "t2_3ow77bmq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to select a model for Keyword extraction?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6e7nq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666020914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m choosing between popular methods like keybert, spacy, textrank, rake and yake to create a baseline for keyword extraction. So nothing fancy at this point. What criteria should I consider? It&amp;#39;s going to be used on open domain scientific texts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6e7nq", "is_robot_indexable": true, "report_reasons": null, "author": "soldierpie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6e7nq/how_to_select_a_model_for_keyword_extraction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6e7nq/how_to_select_a_model_for_keyword_extraction/", "subreddit_subscribers": 814100, "created_utc": 1666020914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve been in and around the sub for a while now and there\u2019s always waves of posts with high engagement that meme about how different/similar data science/analytics/data engineering are. I\u2019d like to get some perspectives and start discussion around this phenomena that perennially plagues data fields. \n\nTo this end, and to prevent this becoming an absolute circlejerk - I\u2019d like to encourage the following response pattern:\n\nDefinitions\nData Science\nData Analytics\nData Engineering\n\nRoles:\nData Scientist\nData Analyst\nData Engineer\n\nFreeform:\nYour experience with these distinctions in practice\n\nI\u2019d like discussion to ensue in the replies so everyone is working off shared viewpoints. So many times, we struggle to have meaningful conversation because \u201cdata science\u201d means two fundamentally different things to each user.\n\nAdded benefit of structured replies on this post - y\u2019all can do your NLP black magic on it for your portfolio.", "author_fullname": "t2_24pdm31x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Structured conversation on definitions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6cnlw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666016878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been in and around the sub for a while now and there\u2019s always waves of posts with high engagement that meme about how different/similar data science/analytics/data engineering are. I\u2019d like to get some perspectives and start discussion around this phenomena that perennially plagues data fields. &lt;/p&gt;\n\n&lt;p&gt;To this end, and to prevent this becoming an absolute circlejerk - I\u2019d like to encourage the following response pattern:&lt;/p&gt;\n\n&lt;p&gt;Definitions\nData Science\nData Analytics\nData Engineering&lt;/p&gt;\n\n&lt;p&gt;Roles:\nData Scientist\nData Analyst\nData Engineer&lt;/p&gt;\n\n&lt;p&gt;Freeform:\nYour experience with these distinctions in practice&lt;/p&gt;\n\n&lt;p&gt;I\u2019d like discussion to ensue in the replies so everyone is working off shared viewpoints. So many times, we struggle to have meaningful conversation because \u201cdata science\u201d means two fundamentally different things to each user.&lt;/p&gt;\n\n&lt;p&gt;Added benefit of structured replies on this post - y\u2019all can do your NLP black magic on it for your portfolio.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6cnlw", "is_robot_indexable": true, "report_reasons": null, "author": "lawrebx", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6cnlw/structured_conversation_on_definitions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6cnlw/structured_conversation_on_definitions/", "subreddit_subscribers": 814100, "created_utc": 1666016878.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My goal is to learn hypothesis testing. My understanding is that we do not have the population data. Therefore, we do not know the estimated population mean. So, we guess the population mean. If that so, can we change the population mean until the p-value is below 0.05, so we have enough evidence to reject the null hypothesis?\n\nThe code\n```\nimport pandas as pd\nfrom scipy import stats\n\nsample = pd.Series([1,2,3,4,5,6,7,8,9,10])\n\n# Hypothesis Testing:\n#\n# H0: \u03bc &lt;= 3.5 (previously it was \u03bc &lt;= 5.5, \n#               then 5, then 4.5, I change it until I get p value less than 0.05)\n# H1: \u03bc &gt; 3.5\n\nt_stat, p_value = stats.ttest_1samp(a = sample, popmean=3.5, alternative=\"greater\")\n\nprint(\"sample mean:\", sample.mean())\nprint(\"t-statistic: {0:.2f} p-value: {1:.2f}\".format(t_stat, p_value))\n\nprint(\"since we do not have the population data, \"\n      \"popmean=3.5 is not meaningful, \"\n      \"we can change it until we get enough evidence to reject the null hypothesis\")\n\nprint(\"critical value: 5%, therefore confidence interval: 90%\")\n\nprint(\"because p-value is below 0.05, we have enough evidence to reject the null hypothesis. \"\n      \"therefore, we can conclude that the mean population is more than 3.5\")\n\nprint(\"population standard deviation {0:.2f}\".format(sample.std(ddof=0)))\n\nprint(\"if we imagine the sample as the weight of waste, the conclusion is, \"\n      \"90% of population data is between population mean + 3 * population standard deviation, \"\n      \"in this case 3.5 +- 3 * 2.87\")\n```", "author_fullname": "t2_5yye765o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can we change the estimated population mean in Hypothesis Testing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y6v9fc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666063002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My goal is to learn hypothesis testing. My understanding is that we do not have the population data. Therefore, we do not know the estimated population mean. So, we guess the population mean. If that so, can we change the population mean until the p-value is below 0.05, so we have enough evidence to reject the null hypothesis?&lt;/p&gt;\n\n&lt;p&gt;The code\n```\nimport pandas as pd\nfrom scipy import stats&lt;/p&gt;\n\n&lt;p&gt;sample = pd.Series([1,2,3,4,5,6,7,8,9,10])&lt;/p&gt;\n\n&lt;h1&gt;Hypothesis Testing:&lt;/h1&gt;\n\n&lt;h1&gt;H0: \u03bc &amp;lt;= 3.5 (previously it was \u03bc &amp;lt;= 5.5,&lt;/h1&gt;\n\n&lt;h1&gt;then 5, then 4.5, I change it until I get p value less than 0.05)&lt;/h1&gt;\n\n&lt;h1&gt;H1: \u03bc &amp;gt; 3.5&lt;/h1&gt;\n\n&lt;p&gt;t_stat, p_value = stats.ttest_1samp(a = sample, popmean=3.5, alternative=&amp;quot;greater&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;sample mean:&amp;quot;, sample.mean())\nprint(&amp;quot;t-statistic: {0:.2f} p-value: {1:.2f}&amp;quot;.format(t_stat, p_value))&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;since we do not have the population data, &amp;quot;\n      &amp;quot;popmean=3.5 is not meaningful, &amp;quot;\n      &amp;quot;we can change it until we get enough evidence to reject the null hypothesis&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;critical value: 5%, therefore confidence interval: 90%&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;because p-value is below 0.05, we have enough evidence to reject the null hypothesis. &amp;quot;\n      &amp;quot;therefore, we can conclude that the mean population is more than 3.5&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;population standard deviation {0:.2f}&amp;quot;.format(sample.std(ddof=0)))&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;if we imagine the sample as the weight of waste, the conclusion is, &amp;quot;\n      &amp;quot;90% of population data is between population mean + 3 * population standard deviation, &amp;quot;\n      &amp;quot;in this case 3.5 +- 3 * 2.87&amp;quot;)\n```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6v9fc", "is_robot_indexable": true, "report_reasons": null, "author": "kidfromtheast", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6v9fc/can_we_change_the_estimated_population_mean_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6v9fc/can_we_change_the_estimated_population_mean_in/", "subreddit_subscribers": 814100, "created_utc": 1666063002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_3rccrfnr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bayesball: Bayesian Integration in professional Baseball Batting. - Justin Brantley on Twitter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6u2ku", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;What do you get when you cross professional baseball \u26be\ufe0fwith Bayes ? Bayesball! I am super excited to announce a new paper out with &lt;a href=\"https://twitter.com/KordingLab?ref_src=twsrc%5Etfw\"&gt;@KordingLab&lt;/a&gt;, titled &amp;quot;Bayesball: Bayesian Integration in Professional Baseball Batters&amp;quot; &lt;a href=\"https://t.co/w6JmjlP7d7\"&gt;https://t.co/w6JmjlP7d7&lt;/a&gt;&lt;/p&gt;&amp;mdash; Justin Brantley (@JABrantl) &lt;a href=\"https://twitter.com/JABrantl/status/1582087838400532486?ref_src=twsrc%5Etfw\"&gt;October 17, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "height": 200}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/JABrantl/status/1582087838400532486", "author_name": "Justin Brantley", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;What do you get when you cross professional baseball \u26be\ufe0fwith Bayes ? Bayesball! I am super excited to announce a new paper out with &lt;a href=\"https://twitter.com/KordingLab?ref_src=twsrc%5Etfw\"&gt;@KordingLab&lt;/a&gt;, titled &amp;quot;Bayesball: Bayesian Integration in Professional Baseball Batters&amp;quot; &lt;a href=\"https://t.co/w6JmjlP7d7\"&gt;https://t.co/w6JmjlP7d7&lt;/a&gt;&lt;/p&gt;&amp;mdash; Justin Brantley (@JABrantl) &lt;a href=\"https://twitter.com/JABrantl/status/1582087838400532486?ref_src=twsrc%5Etfw\"&gt;October 17, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/JABrantl", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;What do you get when you cross professional baseball \u26be\ufe0fwith Bayes ? Bayesball! I am super excited to announce a new paper out with &lt;a href=\"https://twitter.com/KordingLab?ref_src=twsrc%5Etfw\"&gt;@KordingLab&lt;/a&gt;, titled &amp;quot;Bayesball: Bayesian Integration in Professional Baseball Batters&amp;quot; &lt;a href=\"https://t.co/w6JmjlP7d7\"&gt;https://t.co/w6JmjlP7d7&lt;/a&gt;&lt;/p&gt;&amp;mdash; Justin Brantley (@JABrantl) &lt;a href=\"https://twitter.com/JABrantl/status/1582087838400532486?ref_src=twsrc%5Etfw\"&gt;October 17, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/y6u2ku", "height": 200}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1666059656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "twitter.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://twitter.com/JABrantl/status/1582087838400532486?s=20&amp;t=4IAlOjOm_XpeOG5I1tmVHw", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6u2ku", "is_robot_indexable": true, "report_reasons": null, "author": "GenXAlwaysForgotten", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6u2ku/bayesball_bayesian_integration_in_professional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://twitter.com/JABrantl/status/1582087838400532486?s=20&amp;t=4IAlOjOm_XpeOG5I1tmVHw", "subreddit_subscribers": 814100, "created_utc": 1666059656.0, "num_crossposts": 0, "media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/JABrantl/status/1582087838400532486", "author_name": "Justin Brantley", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;What do you get when you cross professional baseball \u26be\ufe0fwith Bayes ? Bayesball! I am super excited to announce a new paper out with &lt;a href=\"https://twitter.com/KordingLab?ref_src=twsrc%5Etfw\"&gt;@KordingLab&lt;/a&gt;, titled &amp;quot;Bayesball: Bayesian Integration in Professional Baseball Batters&amp;quot; &lt;a href=\"https://t.co/w6JmjlP7d7\"&gt;https://t.co/w6JmjlP7d7&lt;/a&gt;&lt;/p&gt;&amp;mdash; Justin Brantley (@JABrantl) &lt;a href=\"https://twitter.com/JABrantl/status/1582087838400532486?ref_src=twsrc%5Etfw\"&gt;October 17, 2022&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/JABrantl", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all,\n\nAm currently working on a side project analyzing a data set for anomaly\u2019s with unsupervised clustering models. As part of my pipeline, I\u2019d like to implement a feature scaling function. I\u2019ve seem to hit a wall and am stuck wondering if feature scaling is necessary/effective for encoded categorical columns. Considering their integer value merely represents a key to a categorical label, wouldn\u2019t scaling them possibly be detrimental to my model?\n\nAny advice on how to handle this is much appreciated.\n\nEDIT: Forgot to mention that some columns have up to 1800+ different types of possible values. OneHotEncoding doesn\u2019t seem to be possible", "author_fullname": "t2_ceqd7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature Scaling for Anomaly Detection Dataset", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6lag3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666038372.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666037548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;Am currently working on a side project analyzing a data set for anomaly\u2019s with unsupervised clustering models. As part of my pipeline, I\u2019d like to implement a feature scaling function. I\u2019ve seem to hit a wall and am stuck wondering if feature scaling is necessary/effective for encoded categorical columns. Considering their integer value merely represents a key to a categorical label, wouldn\u2019t scaling them possibly be detrimental to my model?&lt;/p&gt;\n\n&lt;p&gt;Any advice on how to handle this is much appreciated.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Forgot to mention that some columns have up to 1800+ different types of possible values. OneHotEncoding doesn\u2019t seem to be possible&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6lag3", "is_robot_indexable": true, "report_reasons": null, "author": "DJK923", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6lag3/feature_scaling_for_anomaly_detection_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6lag3/feature_scaling_for_anomaly_detection_dataset/", "subreddit_subscribers": 814100, "created_utc": 1666037548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7murgear", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science at Spotify Camp Nou", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 48, "top_awarded_type": null, "hide_score": false, "name": "t3_y6hmtt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kWJxhSI8XsIo3FDeLDhVg7bgD_aDR38YKiHXqDz9qRc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666028993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@tomas-ravalli/data-science-at-spotify-camp-nou-cb805189817c", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dgRlW4cw8C8DqYm-cwnQu65kokwKWLApRzEZUIxErv4.jpg?auto=webp&amp;s=4e2afe13deec335939c318adcdb449acceea288c", "width": 1200, "height": 415}, "resolutions": [{"url": "https://external-preview.redd.it/dgRlW4cw8C8DqYm-cwnQu65kokwKWLApRzEZUIxErv4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc24ae7d2243eaed5b3d4c2c2c6c4bdff80464e7", "width": 108, "height": 37}, {"url": "https://external-preview.redd.it/dgRlW4cw8C8DqYm-cwnQu65kokwKWLApRzEZUIxErv4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=564c0878e07bb9c60b808183467780dc422ea1d6", "width": 216, "height": 74}, {"url": "https://external-preview.redd.it/dgRlW4cw8C8DqYm-cwnQu65kokwKWLApRzEZUIxErv4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fd78fc9fba2450021e5cd4fabdd3a139ee24d42f", "width": 320, "height": 110}, {"url": "https://external-preview.redd.it/dgRlW4cw8C8DqYm-cwnQu65kokwKWLApRzEZUIxErv4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c880153dc2b48b8a2fb52270a0d4467b3da417c3", "width": 640, "height": 221}, {"url": "https://external-preview.redd.it/dgRlW4cw8C8DqYm-cwnQu65kokwKWLApRzEZUIxErv4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7060e1b6302dd22da2d2f7a15b01cafb9e79985a", "width": 960, "height": 332}, {"url": "https://external-preview.redd.it/dgRlW4cw8C8DqYm-cwnQu65kokwKWLApRzEZUIxErv4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62ea1e3ed40812c24012c82ceaf29414770c8793", "width": 1080, "height": 373}], "variants": {}, "id": "FYnjBCOQk9ggkcMys6zg9GP5F3vl8UklGJqSncBa6zU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6hmtt", "is_robot_indexable": true, "report_reasons": null, "author": "Complex-Patient361", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6hmtt/data_science_at_spotify_camp_nou/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@tomas-ravalli/data-science-at-spotify-camp-nou-cb805189817c", "subreddit_subscribers": 814100, "created_utc": 1666028993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi Everyone! I hope you're doing well!\n\n  \n***TLDR:*** **Any resources for streaming or batch-processing data through a storage as well as a dashboard for the end users of the apps utilizing this data stored in our storage. Language: Mostly Python. Read below for full details.**\n\n&amp;#x200B;\n\nI'm looking to learn how to build an ETL where I can:\n\n\\- Pull data either in **frequent batches or continuously (streaming) from more than 1000 sources,** all of which will have their own acquisition algorithm (mining, communicating with APIs, Local-fed .csv files etc.) and cleaning\n\n\\- Deposit into a **data storage in the cloud** (no server rooms) following manipulation as relational (or whatever you think would be fastest) database system\n\n\\- Utilize something like Airflow where I can **run / test the run performance and handle errors** as the **data mining algorithms run in parallel**\n\n\\- Use an app full of the **desired dashboards** built using the data we stored previously so the analyses can be displayed using **desired KPIs**\n\n\\- Do all this in a cybersecurity-conscious manner\n\n&amp;#x200B;\n\nAny technology, classes, courses, bootcamps, required languages, cybersecurity resources, parallel running resources appreciated. \n\nI worked as a data scientist all my career so I'm familiar with scraping, Python, data wrangling and relational databases, however, I never had to build a whole infrastructure and I'm up for the challenge. \n\nAny tips &amp; tricks appreciated! :)", "author_fullname": "t2_7cwegb8i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for learning ETL and data acquisition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6g941", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666025707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone! I hope you&amp;#39;re doing well!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;TLDR:&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;Any resources for streaming or batch-processing data through a storage as well as a dashboard for the end users of the apps utilizing this data stored in our storage. Language: Mostly Python. Read below for full details.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to learn how to build an ETL where I can:&lt;/p&gt;\n\n&lt;p&gt;- Pull data either in &lt;strong&gt;frequent batches or continuously (streaming) from more than 1000 sources,&lt;/strong&gt; all of which will have their own acquisition algorithm (mining, communicating with APIs, Local-fed .csv files etc.) and cleaning&lt;/p&gt;\n\n&lt;p&gt;- Deposit into a &lt;strong&gt;data storage in the cloud&lt;/strong&gt; (no server rooms) following manipulation as relational (or whatever you think would be fastest) database system&lt;/p&gt;\n\n&lt;p&gt;- Utilize something like Airflow where I can &lt;strong&gt;run / test the run performance and handle errors&lt;/strong&gt; as the &lt;strong&gt;data mining algorithms run in parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Use an app full of the &lt;strong&gt;desired dashboards&lt;/strong&gt; built using the data we stored previously so the analyses can be displayed using &lt;strong&gt;desired KPIs&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Do all this in a cybersecurity-conscious manner&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any technology, classes, courses, bootcamps, required languages, cybersecurity resources, parallel running resources appreciated. &lt;/p&gt;\n\n&lt;p&gt;I worked as a data scientist all my career so I&amp;#39;m familiar with scraping, Python, data wrangling and relational databases, however, I never had to build a whole infrastructure and I&amp;#39;m up for the challenge. &lt;/p&gt;\n\n&lt;p&gt;Any tips &amp;amp; tricks appreciated! :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6g941", "is_robot_indexable": true, "report_reasons": null, "author": "Text-Agitated", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6g941/resources_for_learning_etl_and_data_acquisition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6g941/resources_for_learning_etl_and_data_acquisition/", "subreddit_subscribers": 814100, "created_utc": 1666025707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_ban3dn2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it beneficial to opt for online course from udemy or to go for offline course in data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6fx5g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666024913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6fx5g", "is_robot_indexable": true, "report_reasons": null, "author": "aviraldobhal", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6fx5g/is_it_beneficial_to_opt_for_online_course_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6fx5g/is_it_beneficial_to_opt_for_online_course_from/", "subreddit_subscribers": 814100, "created_utc": 1666024913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We are excited to announce MinIO Batch Framework \u2013 Starting with Batch Replication", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "name": "t3_y6f66e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_90p5s", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XV8uonuewrdGGxagoaBJhkCIYsPze3-8L9EfZ7K9-lA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "minio", "selftext": "", "author_fullname": "t2_90p5s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We are excited to announce MinIO Batch Framework \u2013 Starting with Batch Replication", "link_flair_richtext": [], "subreddit_name_prefixed": "r/minio", "hidden": false, "pwls": null, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "name": "t3_y6exji", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XV8uonuewrdGGxagoaBJhkCIYsPze3-8L9EfZ7K9-lA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666022611.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.min.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.min.io/announcing-minio-batch-framework-batch-replication/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?auto=webp&amp;s=35f3033a62196b3180305300104eff5df5998834", "width": 2000, "height": 1062}, "resolutions": [{"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=419e5c9a4e68ec0a5b8d5223a4de58afa89c9f9a", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1ccf81a0dfc87a206a2447203785c74bd6498b3", "width": 216, "height": 114}, {"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eef18cd20ba0590629afad867341eccdf3adc31c", "width": 320, "height": 169}, {"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=41d6965a7f8c932d76ad37539cea4317699c512b", "width": 640, "height": 339}, {"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=36c0883af47e8b54547976fe064337a5c1e19c56", "width": 960, "height": 509}, {"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7762acefc27edab756942c8178f0f72203de0cc2", "width": 1080, "height": 573}], "variants": {}, "id": "38VFQVfCPVmBqj7ufsvqVuBgihv2sEu-2oFyFru4NjQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "427dbc5c-5868-11ec-81a8-7e4604f56887", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_3ahsh", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#00a6a5", "id": "y6exji", "is_robot_indexable": true, "report_reasons": null, "author": "prtkgpt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/minio/comments/y6exji/we_are_excited_to_announce_minio_batch_framework/", "parent_whitelist_status": null, "stickied": false, "url": "https://blog.min.io/announcing-minio-batch-framework-batch-replication/", "subreddit_subscribers": 503, "created_utc": 1666022611.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1666023162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.min.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://blog.min.io/announcing-minio-batch-framework-batch-replication/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?auto=webp&amp;s=35f3033a62196b3180305300104eff5df5998834", "width": 2000, "height": 1062}, "resolutions": [{"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=419e5c9a4e68ec0a5b8d5223a4de58afa89c9f9a", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1ccf81a0dfc87a206a2447203785c74bd6498b3", "width": 216, "height": 114}, {"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eef18cd20ba0590629afad867341eccdf3adc31c", "width": 320, "height": 169}, {"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=41d6965a7f8c932d76ad37539cea4317699c512b", "width": 640, "height": 339}, {"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=36c0883af47e8b54547976fe064337a5c1e19c56", "width": 960, "height": 509}, {"url": "https://external-preview.redd.it/k3RafJLb3SPRoll7jhiEXFUytmhc82-OrUHRl7ohOb0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7762acefc27edab756942c8178f0f72203de0cc2", "width": 1080, "height": 573}], "variants": {}, "id": "38VFQVfCPVmBqj7ufsvqVuBgihv2sEu-2oFyFru4NjQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6f66e", "is_robot_indexable": true, "report_reasons": null, "author": "prtkgpt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_y6exji", "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6f66e/we_are_excited_to_announce_minio_batch_framework/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.min.io/announcing-minio-batch-framework-batch-replication/", "subreddit_subscribers": 814100, "created_utc": 1666023162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "If so what kind of variables would I need in my dataset? Thinking of doing a research paper on it.", "author_fullname": "t2_2qi2mssd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In insurance, is claims reserve prediction possible using prediction models?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6btzj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666014671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If so what kind of variables would I need in my dataset? Thinking of doing a research paper on it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6btzj", "is_robot_indexable": true, "report_reasons": null, "author": "cdtmh", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6btzj/in_insurance_is_claims_reserve_prediction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6btzj/in_insurance_is_claims_reserve_prediction/", "subreddit_subscribers": 814100, "created_utc": 1666014671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Background info: I have about 3 years worth of data of an app store. I have every app listing that was created, all of the content of the app listing (parsed out as title, description, category, etc.), each review (text and numerical), and the entire ranking history of each app in the app store. \n\nI have a bit of experience in aggregating and cleaning data, but my question is what type of strategy/model should I be considering if I want to try learn what the biggest factors are in a given app listing that most contributes to a  higher ranking?", "author_fullname": "t2_3wm4o6hy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would be my best approach given the data I have?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6bity", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666013863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background info: I have about 3 years worth of data of an app store. I have every app listing that was created, all of the content of the app listing (parsed out as title, description, category, etc.), each review (text and numerical), and the entire ranking history of each app in the app store. &lt;/p&gt;\n\n&lt;p&gt;I have a bit of experience in aggregating and cleaning data, but my question is what type of strategy/model should I be considering if I want to try learn what the biggest factors are in a given app listing that most contributes to a  higher ranking?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6bity", "is_robot_indexable": true, "report_reasons": null, "author": "jameslafr", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6bity/what_would_be_my_best_approach_given_the_data_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6bity/what_would_be_my_best_approach_given_the_data_i/", "subreddit_subscribers": 814100, "created_utc": 1666013863.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}