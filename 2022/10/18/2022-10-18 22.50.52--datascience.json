{"kind": "Listing", "data": {"after": "t3_y79fgp", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "The `data \\w+` gold rush has been a blessing and a curse, blessing in that many of us are getting filthy rich off it, curse in that many (frankly unskilled) people see the job market and think \u201cwow I gotta get me a piece of that\u201d and proceed to bombard every specialist board with mentorship requests and e-begging for a crumb of interview. \n\nFrankly I wouldn\u2019t mind this if the people asking had done some cursory research beforehand and asked politely, but it seems like every jerkoff who\u2019s caught a whiff of an Excel spreadsheet thinks they can land a FAANG job overnight and, instead of looking on Google for \u201chow to data job pls to help\u201d and seeing the ten trillion useless Medium articles made by the endless morons trying to resume pad and slip their jimmy into an Amazon L3 role that would tell them practically everything they need to know (even if by and large anything posted on Medium is worthless) they choose to pepper subs like /r/dataengineering, /r/dataanalysis, and this one with the same \u201chow to data job please give me six figures\u201d - it\u2019s like asking /r/personalfinance \u201chelp how do I own a bank account\u201d repeated for every hapless schmuck who\u2019s been hiding their Benjamins in granny\u2019s cookie tin for the last sixteen years of their childhood. \n\nNot even getting into the fact that doing basic research on the topic at hand is probably *the* fundamental skill for any data-*whatever* role, what\u2019s even funnier is that I\u2019d hazard a guess that most of us who *actually work in the industry* have better things to do during the day, so the people answering questions are probably majority kids trying to get their first data-whatever job - blind leading the blind all over again. \n\nTLDR: Screw you guys I\u2019m going to /r/Statistics", "author_fullname": "t2_gtodeuwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[UNHINGED RANT] It\u2019s kind of annoying to see that, in general, most data-related spaces are flush with \u201chow do I get a job\u201d and comparatively little discussion around the actual topic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6w5ab", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 637, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 637, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666065579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The &lt;code&gt;data \\w+&lt;/code&gt; gold rush has been a blessing and a curse, blessing in that many of us are getting filthy rich off it, curse in that many (frankly unskilled) people see the job market and think \u201cwow I gotta get me a piece of that\u201d and proceed to bombard every specialist board with mentorship requests and e-begging for a crumb of interview. &lt;/p&gt;\n\n&lt;p&gt;Frankly I wouldn\u2019t mind this if the people asking had done some cursory research beforehand and asked politely, but it seems like every jerkoff who\u2019s caught a whiff of an Excel spreadsheet thinks they can land a FAANG job overnight and, instead of looking on Google for \u201chow to data job pls to help\u201d and seeing the ten trillion useless Medium articles made by the endless morons trying to resume pad and slip their jimmy into an Amazon L3 role that would tell them practically everything they need to know (even if by and large anything posted on Medium is worthless) they choose to pepper subs like &lt;a href=\"/r/dataengineering\"&gt;/r/dataengineering&lt;/a&gt;, &lt;a href=\"/r/dataanalysis\"&gt;/r/dataanalysis&lt;/a&gt;, and this one with the same \u201chow to data job please give me six figures\u201d - it\u2019s like asking &lt;a href=\"/r/personalfinance\"&gt;/r/personalfinance&lt;/a&gt; \u201chelp how do I own a bank account\u201d repeated for every hapless schmuck who\u2019s been hiding their Benjamins in granny\u2019s cookie tin for the last sixteen years of their childhood. &lt;/p&gt;\n\n&lt;p&gt;Not even getting into the fact that doing basic research on the topic at hand is probably &lt;em&gt;the&lt;/em&gt; fundamental skill for any data-&lt;em&gt;whatever&lt;/em&gt; role, what\u2019s even funnier is that I\u2019d hazard a guess that most of us who &lt;em&gt;actually work in the industry&lt;/em&gt; have better things to do during the day, so the people answering questions are probably majority kids trying to get their first data-whatever job - blind leading the blind all over again. &lt;/p&gt;\n\n&lt;p&gt;TLDR: Screw you guys I\u2019m going to &lt;a href=\"/r/Statistics\"&gt;/r/Statistics&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 250, "id": "award_92d8645c-de2c-44ae-8cd7-7b0c6ab25297", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/YouDroppedThis_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/YouDroppedThis_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/YouDroppedThis_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/YouDroppedThis_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/YouDroppedThis_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/YouDroppedThis_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "King", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "You Dropped This", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/fuxn2mr5rts61_YouDroppedThis.png?width=16&amp;height=16&amp;auto=webp&amp;s=e192a2db51a601c4d446aaf9150d9724aff00ec5", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/fuxn2mr5rts61_YouDroppedThis.png?width=32&amp;height=32&amp;auto=webp&amp;s=eb5addd87eb0dfdf5ff11604049989f3cbbe9d91", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/fuxn2mr5rts61_YouDroppedThis.png?width=48&amp;height=48&amp;auto=webp&amp;s=624b1bedeb36b40ed41461889cd72a36997d41d1", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/fuxn2mr5rts61_YouDroppedThis.png?width=64&amp;height=64&amp;auto=webp&amp;s=da947e7662e6083e2f4a8107dd689a61f2607464", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/fuxn2mr5rts61_YouDroppedThis.png?width=128&amp;height=128&amp;auto=webp&amp;s=c103633384519b7d80e7fbb745fbd80fae3ba9a5", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 512, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/fuxn2mr5rts61_YouDroppedThis.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6w5ab", "is_robot_indexable": true, "report_reasons": null, "author": "sigma_hyperborean", "discussion_type": null, "num_comments": 129, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6w5ab/unhinged_rant_its_kind_of_annoying_to_see_that_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6w5ab/unhinged_rant_its_kind_of_annoying_to_see_that_in/", "subreddit_subscribers": 814202, "created_utc": 1666065579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "OK, I know this question will make a lot of you mad. I can see the replies now: Data science means different things at different companies, different algos are used for different contexts, Python vs R, senior vs junior roles, you're focusing on technologies and implementation while neglecting business context, etc. Yeah, I get it. My goal with this post is, with your help, to build a list of technologies/platform/skills from the broad array of what's considered data science and should be known by a DS with about 3 years of experience, in the opinion of the poster (that's you). Nothing is too broad or specific. For example, Naive Bayes, GNNs, containerization, and Docker are all valid answers (yes, I realize Docker is an implementation of containerization, and that's my point..) Please, if you would be so kind, include why you think it's important and to what depth/capability a DS with 3 years of experience should have with it. Since I'm a python person, let's exclude R from the conversation.\n\nWhy am I asking this? In full candor, I feel like I am seriously stagnating. I am really the only DS in my company, so it's very easy to lose sight of where the field is currently because so much of my job is simple excel/pandas stuff. I would potentially like to start interviewing, but I feel like I am far behind where a typical DS with my YOE would be. Like, for example, I have no clue how to use airflow or any non-AWS scheduler, every time I do anything related to ML (a very rare occurrence, unfortunately), I have completely forgotten the sklearn syntax, I know pretty much nothing about actually training an NN, I know in theory what a basic NN is, but I have no idea about graph NNs or graph databases or graph anything. I still have no clue what distinguishes a data warehouse from a database after reading countless articles. I feel like I don't even know what I don't know; for anything I do know, it's hard to tell if what I'm doing is the best practice.\n\nMany thanks in advance!", "author_fullname": "t2_i7wlt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What technologies/skills should a data scientist with ~ 3 years of experience be familiar with?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y78uss", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666104212.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666103915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;OK, I know this question will make a lot of you mad. I can see the replies now: Data science means different things at different companies, different algos are used for different contexts, Python vs R, senior vs junior roles, you&amp;#39;re focusing on technologies and implementation while neglecting business context, etc. Yeah, I get it. My goal with this post is, with your help, to build a list of technologies/platform/skills from the broad array of what&amp;#39;s considered data science and should be known by a DS with about 3 years of experience, in the opinion of the poster (that&amp;#39;s you). Nothing is too broad or specific. For example, Naive Bayes, GNNs, containerization, and Docker are all valid answers (yes, I realize Docker is an implementation of containerization, and that&amp;#39;s my point..) Please, if you would be so kind, include why you think it&amp;#39;s important and to what depth/capability a DS with 3 years of experience should have with it. Since I&amp;#39;m a python person, let&amp;#39;s exclude R from the conversation.&lt;/p&gt;\n\n&lt;p&gt;Why am I asking this? In full candor, I feel like I am seriously stagnating. I am really the only DS in my company, so it&amp;#39;s very easy to lose sight of where the field is currently because so much of my job is simple excel/pandas stuff. I would potentially like to start interviewing, but I feel like I am far behind where a typical DS with my YOE would be. Like, for example, I have no clue how to use airflow or any non-AWS scheduler, every time I do anything related to ML (a very rare occurrence, unfortunately), I have completely forgotten the sklearn syntax, I know pretty much nothing about actually training an NN, I know in theory what a basic NN is, but I have no idea about graph NNs or graph databases or graph anything. I still have no clue what distinguishes a data warehouse from a database after reading countless articles. I feel like I don&amp;#39;t even know what I don&amp;#39;t know; for anything I do know, it&amp;#39;s hard to tell if what I&amp;#39;m doing is the best practice.&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y78uss", "is_robot_indexable": true, "report_reasons": null, "author": "jewami", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y78uss/what_technologiesskills_should_a_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y78uss/what_technologiesskills_should_a_data_scientist/", "subreddit_subscribers": 814202, "created_utc": 1666103915.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm just going to make up an example here.  Imagine you want to predict a stock price.  Your feature is today's price and your target is tomorrow's price.  \n\nHere would be a simple case:\nFeature = [100, 105, 110, 115, 120...]\nTarget = [105, 110, 115, 120, 125...]\n\nAre we allowed to do cross validation on this?  \n\nEven though we are never using future data to predict the past, I still feel like cross validation should not be allowed in this case.  Better approaches would be a walk-forward validation or a rolling window validation.  \n\nAm I correct in my assumption that cross validation should not be allowed?", "author_fullname": "t2_cn54oiy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cross Validation with Time Series", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6v261", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666062429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just going to make up an example here.  Imagine you want to predict a stock price.  Your feature is today&amp;#39;s price and your target is tomorrow&amp;#39;s price.  &lt;/p&gt;\n\n&lt;p&gt;Here would be a simple case:\nFeature = [100, 105, 110, 115, 120...]\nTarget = [105, 110, 115, 120, 125...]&lt;/p&gt;\n\n&lt;p&gt;Are we allowed to do cross validation on this?  &lt;/p&gt;\n\n&lt;p&gt;Even though we are never using future data to predict the past, I still feel like cross validation should not be allowed in this case.  Better approaches would be a walk-forward validation or a rolling window validation.  &lt;/p&gt;\n\n&lt;p&gt;Am I correct in my assumption that cross validation should not be allowed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6v261", "is_robot_indexable": true, "report_reasons": null, "author": "BlackLotus8888", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6v261/cross_validation_with_time_series/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6v261/cross_validation_with_time_series/", "subreddit_subscribers": 814202, "created_utc": 1666062429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Let's say I have a sequence of terms. Each term could be any string of alphabetical characters. `['abc', 'stg', 'drT']`. Now let's say I have millions of such sequences of varying length (might be 1 term, might be 1000) and some of them are 'bad'. But their combination matters toward that badness. So I'm baselining and looking for outliers, which should have a high correlation with 'bad' in my dataset. Term might indicate 'badness' in one combination but is totally benign in another.\n\nSo that's our search-space and problem statement. I am not educated in DS. But I understand the basic idea: Correlate traits and relationships of features in one or more data-sets to answer a question / solve a problem. But which features to select? It seems like an interesting question, since as analysts we don't know everything and if we try to select feature manually, there's a good chance we might miss something. Right? There might be some combination of features which, when correlated with some combination of other features ends up being a good indicator that we wouldn't have manually expected. Maybe 3 features plotted on a 3D chart with K-Means clustering or just human viewing reveals something interesting / creates a valuable signal that we might not have guessed or tested if we were hand-picking the features we thought might be useful.\n\nSo I'm computing an exhaustive list of features globally and locally, like:\n\nHow standard / deviant is the feature locally?\n\n* local occurence count\n* local occurence as % of total local feature count\n\nHow standard / deviant is the feature globally?\n\n* global occurence count\n* global occurence count as % of global feature count\n\nHow is the feature related to other features locally?\n\n* for each term: for each other term: min distance, where distance is in terms of index\n* for each term: for each other term: max distance, where distance is in terms of index\n* for each term: for each other term: avg distance, where distance is in terms of index\n* for each term: for each other term: min distance, where distance is in terms of character count\n* for each term: for each other term: max distance, where distance is in terms of character count\n* for each term: for each other term: avg distance, where distance is in terms of character count\n\nHow globally standard / deviant is the feature's relationships to other features locally?\n\n* for each term: for each other term: min distance as % of all global min distances between these two symbols, where distance is in terms of index\n* for each term: for each other term: max distance as % of all global max distances between these two symbols, where distance is in terms of index\n* for each term: for each other term: avg distance as % of all global avg distances between these two symbols, where distance is in terms of index\n* for each term: for each other term: min distance as % of all global min distances between these two symbols, where distance is in terms of character count\n* for each term: for each other term: max distance as % of all global max distances between these two symbols, where distance is in terms of character count\n* for each term: for each other term: avg distance as % of all global avg distances between these two symbols, where distance is in terms of character count\n\nThat's the theory I'm experimenting with, planning to just produce every possible chart, perhaps thousands, and then do some simple clustering on all of them and see if features associated with known-bad sample have a statistical tendency to end up in distinct clusters in any of those plots.\n\nAm I potentially on the right track? Is this a known strategy in data science? It's my first real run-in with cleaning data, isolating / computing features, so I'm still really just shooting in the dark at this point.", "author_fullname": "t2_8yytz5qc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it sometimes a good strategy to identify / prep all possible features, model them all dynamically (with recursive code) and measure which collection of features end up correlating more often than others to identify whatever signal you're looking for?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6txbs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666063704.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666059259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say I have a sequence of terms. Each term could be any string of alphabetical characters. &lt;code&gt;[&amp;#39;abc&amp;#39;, &amp;#39;stg&amp;#39;, &amp;#39;drT&amp;#39;]&lt;/code&gt;. Now let&amp;#39;s say I have millions of such sequences of varying length (might be 1 term, might be 1000) and some of them are &amp;#39;bad&amp;#39;. But their combination matters toward that badness. So I&amp;#39;m baselining and looking for outliers, which should have a high correlation with &amp;#39;bad&amp;#39; in my dataset. Term might indicate &amp;#39;badness&amp;#39; in one combination but is totally benign in another.&lt;/p&gt;\n\n&lt;p&gt;So that&amp;#39;s our search-space and problem statement. I am not educated in DS. But I understand the basic idea: Correlate traits and relationships of features in one or more data-sets to answer a question / solve a problem. But which features to select? It seems like an interesting question, since as analysts we don&amp;#39;t know everything and if we try to select feature manually, there&amp;#39;s a good chance we might miss something. Right? There might be some combination of features which, when correlated with some combination of other features ends up being a good indicator that we wouldn&amp;#39;t have manually expected. Maybe 3 features plotted on a 3D chart with K-Means clustering or just human viewing reveals something interesting / creates a valuable signal that we might not have guessed or tested if we were hand-picking the features we thought might be useful.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m computing an exhaustive list of features globally and locally, like:&lt;/p&gt;\n\n&lt;p&gt;How standard / deviant is the feature locally?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;local occurence count&lt;/li&gt;\n&lt;li&gt;local occurence as % of total local feature count&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How standard / deviant is the feature globally?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;global occurence count&lt;/li&gt;\n&lt;li&gt;global occurence count as % of global feature count&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How is the feature related to other features locally?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;for each term: for each other term: min distance, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: max distance, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: avg distance, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: min distance, where distance is in terms of character count&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: max distance, where distance is in terms of character count&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: avg distance, where distance is in terms of character count&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;How globally standard / deviant is the feature&amp;#39;s relationships to other features locally?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;for each term: for each other term: min distance as % of all global min distances between these two symbols, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: max distance as % of all global max distances between these two symbols, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: avg distance as % of all global avg distances between these two symbols, where distance is in terms of index&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: min distance as % of all global min distances between these two symbols, where distance is in terms of character count&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: max distance as % of all global max distances between these two symbols, where distance is in terms of character count&lt;/li&gt;\n&lt;li&gt;for each term: for each other term: avg distance as % of all global avg distances between these two symbols, where distance is in terms of character count&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That&amp;#39;s the theory I&amp;#39;m experimenting with, planning to just produce every possible chart, perhaps thousands, and then do some simple clustering on all of them and see if features associated with known-bad sample have a statistical tendency to end up in distinct clusters in any of those plots.&lt;/p&gt;\n\n&lt;p&gt;Am I potentially on the right track? Is this a known strategy in data science? It&amp;#39;s my first real run-in with cleaning data, isolating / computing features, so I&amp;#39;m still really just shooting in the dark at this point.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6txbs", "is_robot_indexable": true, "report_reasons": null, "author": "Jonathan-Todd", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6txbs/is_it_sometimes_a_good_strategy_to_identify_prep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6txbs/is_it_sometimes_a_good_strategy_to_identify_prep/", "subreddit_subscribers": 814202, "created_utc": 1666059259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm (23f) doing my MS in Data Science and Analytics right now (will be finished next year) and I have my BS in Econometrics (focused in R). Also, I'm currently a full time Clinical Data Manager in cancer research, so I'm wanting to switch fields. The rest of my professional experience is in leading tax and social services research projects, economic development, and public policy.\n\nI'm so excited that I have my first interview for a Data Science role, do you have any tips to ace it? They are looking for R or Python experience (I have both but I'm more experienced in R), Git, and some SQL (which I'm actually learning right now and will be doing projects with soon). Eventually, I'm also going to be learning Tableau, but they don't mention requiring it.\n\nIt's also considerably high-paying - like a 50% pay increase from my current role - so it would be a life changer if I got it.", "author_fullname": "t2_8wmnwl26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got my first Data Scientist interview - do you have any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y7i883", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666125994.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666125693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m (23f) doing my MS in Data Science and Analytics right now (will be finished next year) and I have my BS in Econometrics (focused in R). Also, I&amp;#39;m currently a full time Clinical Data Manager in cancer research, so I&amp;#39;m wanting to switch fields. The rest of my professional experience is in leading tax and social services research projects, economic development, and public policy.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m so excited that I have my first interview for a Data Science role, do you have any tips to ace it? They are looking for R or Python experience (I have both but I&amp;#39;m more experienced in R), Git, and some SQL (which I&amp;#39;m actually learning right now and will be doing projects with soon). Eventually, I&amp;#39;m also going to be learning Tableau, but they don&amp;#39;t mention requiring it.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s also considerably high-paying - like a 50% pay increase from my current role - so it would be a life changer if I got it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "y7i883", "is_robot_indexable": true, "report_reasons": null, "author": "UnsafeBaton1041", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y7i883/just_got_my_first_data_scientist_interview_do_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y7i883/just_got_my_first_data_scientist_interview_do_you/", "subreddit_subscribers": 814202, "created_utc": 1666125693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I'm relatively new to time-series modeling and am not quite sure how to approach this problem. Let's say I have data on 100 students and their 100 most recent test scores (assuming that each test is the same across all students and of varying difficulty successively), and I want to understand what kind of exponential smoothing value is optimal to predict each student's next test score.\n\nBecause there must be some information contained within the population's overall performance across tests, I want to leverage that. But am I able to do so using something like R's forecast package's ETS function? I know that it can handle a single-subject time series but how can it assess the overall alpha value given 100 subjects?", "author_fullname": "t2_21w9l2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Method for Optimal Exponential Smoothing of Time Series across samples?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6va5a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666063062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m relatively new to time-series modeling and am not quite sure how to approach this problem. Let&amp;#39;s say I have data on 100 students and their 100 most recent test scores (assuming that each test is the same across all students and of varying difficulty successively), and I want to understand what kind of exponential smoothing value is optimal to predict each student&amp;#39;s next test score.&lt;/p&gt;\n\n&lt;p&gt;Because there must be some information contained within the population&amp;#39;s overall performance across tests, I want to leverage that. But am I able to do so using something like R&amp;#39;s forecast package&amp;#39;s ETS function? I know that it can handle a single-subject time series but how can it assess the overall alpha value given 100 subjects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6va5a", "is_robot_indexable": true, "report_reasons": null, "author": "DataScience0", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6va5a/method_for_optimal_exponential_smoothing_of_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6va5a/method_for_optimal_exponential_smoothing_of_time/", "subreddit_subscribers": 814202, "created_utc": 1666063062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Currently I use DataViz and was wondering if there was something similar,  one with a nice wizard, convenient table filtering, and schemas, but with Git integration and allows for you to work in other coding languages like Python too. \n\nI\u2019m currently looking into DBeaver. I\u2019m pretty new to all of this so sorry if this post isn\u2019t worded too well. Thanks in advance!", "author_fullname": "t2_11re2h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which IDE would be best for PostgreSQL and python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y7i8ju", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666125713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I use DataViz and was wondering if there was something similar,  one with a nice wizard, convenient table filtering, and schemas, but with Git integration and allows for you to work in other coding languages like Python too. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently looking into DBeaver. I\u2019m pretty new to all of this so sorry if this post isn\u2019t worded too well. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y7i8ju", "is_robot_indexable": true, "report_reasons": null, "author": "proudaggie", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y7i8ju/which_ide_would_be_best_for_postgresql_and_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y7i8ju/which_ide_would_be_best_for_postgresql_and_python/", "subreddit_subscribers": 814202, "created_utc": 1666125713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Suppose I sell flip flops and I have it set up with the following row titles:\nUPC, Item Name, Price\n\nI want to get the price people are selling online, like Amazon and have excel automatically populate a row with the title \u201cOnline Price\u201d\n\nWould this be possible via excel or is this type of data project possible through other softwares?", "author_fullname": "t2_2lmomuub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to extract the price of an item on the internet and directly match it to your item price?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y7a24t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666106666.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Suppose I sell flip flops and I have it set up with the following row titles:\nUPC, Item Name, Price&lt;/p&gt;\n\n&lt;p&gt;I want to get the price people are selling online, like Amazon and have excel automatically populate a row with the title \u201cOnline Price\u201d&lt;/p&gt;\n\n&lt;p&gt;Would this be possible via excel or is this type of data project possible through other softwares?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y7a24t", "is_robot_indexable": true, "report_reasons": null, "author": "tryingtogetintoIB", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y7a24t/is_there_a_way_to_extract_the_price_of_an_item_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y7a24t/is_there_a_way_to_extract_the_price_of_an_item_on/", "subreddit_subscribers": 814202, "created_utc": 1666106666.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm working on a project to learn \"to think\" about the problem in a Data Science way; this is because I've mostly done coding and usually machine learning, but nothing focused on problems, so I started one.\n\n&amp;#x200B;\n\nIn this problem, I need to figure out which companies are more likely to grow in the future by looking at some of their data which says how long they've been operating vs. how big they are. I have all this data and...I'm struggling to come up with a path ahead. A friend of mine suggested using random forests and classifying them (since it's categorical data: time being \"first period, second period, etc.\" and their \"growth\" is measured as \"new, middle, big\") however, I am unsure how to do this. \n\n&amp;#x200B;\n\nI did do the random forest and got some values, but they're giving me, as expected, a result like:\n\n&amp;#x200B;\n\n    Company 1 - random forest: 0.98, growth: new, time: second_period\"\n\nWhich...upon looking at it, means nothing to me. I'm unsure about how to go about doing this, I used to do this a few years ago, but ever since I moved to language models and other such things, I feel like I'm super rusty and not even sure where to go from here...could you please help me out? At least how should I be thinking about the problem", "author_fullname": "t2_3lnrl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not sure if I'm doing the right thing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y7jzbv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666129740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a project to learn &amp;quot;to think&amp;quot; about the problem in a Data Science way; this is because I&amp;#39;ve mostly done coding and usually machine learning, but nothing focused on problems, so I started one.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In this problem, I need to figure out which companies are more likely to grow in the future by looking at some of their data which says how long they&amp;#39;ve been operating vs. how big they are. I have all this data and...I&amp;#39;m struggling to come up with a path ahead. A friend of mine suggested using random forests and classifying them (since it&amp;#39;s categorical data: time being &amp;quot;first period, second period, etc.&amp;quot; and their &amp;quot;growth&amp;quot; is measured as &amp;quot;new, middle, big&amp;quot;) however, I am unsure how to do this. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I did do the random forest and got some values, but they&amp;#39;re giving me, as expected, a result like:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Company 1 - random forest: 0.98, growth: new, time: second_period&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Which...upon looking at it, means nothing to me. I&amp;#39;m unsure about how to go about doing this, I used to do this a few years ago, but ever since I moved to language models and other such things, I feel like I&amp;#39;m super rusty and not even sure where to go from here...could you please help me out? At least how should I be thinking about the problem&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y7jzbv", "is_robot_indexable": true, "report_reasons": null, "author": "Proxify", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y7jzbv/not_sure_if_im_doing_the_right_thing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y7jzbv/not_sure_if_im_doing_the_right_thing/", "subreddit_subscribers": 814202, "created_utc": 1666129740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have weather data from stations across the US but of course this data corresponds to single lat/lon coordinates whereas I'd like data at every coordinate across the entire US. Thus, I'm going to need to do some sort of spatial interpolation (or similar).\n\nI've read that krigging (Gaussian Process Regression) is often used for spatial interpolation though I can't see the benefit over something like KNN if I'm not going to be using the uncertainty estimate it comes with. Krigging is very computationally expensive and explaining the theory behind it is difficult at best. Neglecting uncertainty quantification since it won't be used, is the main advantage  properly designed mean and covariance functions in which the model reverts back to in the areas far from data points?", "author_fullname": "t2_91itiala", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Krigging for Spatial Interpolation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y7hwfm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666124931.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have weather data from stations across the US but of course this data corresponds to single lat/lon coordinates whereas I&amp;#39;d like data at every coordinate across the entire US. Thus, I&amp;#39;m going to need to do some sort of spatial interpolation (or similar).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read that krigging (Gaussian Process Regression) is often used for spatial interpolation though I can&amp;#39;t see the benefit over something like KNN if I&amp;#39;m not going to be using the uncertainty estimate it comes with. Krigging is very computationally expensive and explaining the theory behind it is difficult at best. Neglecting uncertainty quantification since it won&amp;#39;t be used, is the main advantage  properly designed mean and covariance functions in which the model reverts back to in the areas far from data points?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y7hwfm", "is_robot_indexable": true, "report_reasons": null, "author": "MGeeeeeezy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y7hwfm/krigging_for_spatial_interpolation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y7hwfm/krigging_for_spatial_interpolation/", "subreddit_subscribers": 814202, "created_utc": 1666124931.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I wasn't aware that amazon offered online learning. I can't seem to find any reviews online, and I wanted to know what this community thought of it ? \n\nHas anyone taken these courses ?", "author_fullname": "t2_13t60b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reviews of courses offered by Amazon for Data science, SQL, python, etc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y7cyqy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666113431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wasn&amp;#39;t aware that amazon offered online learning. I can&amp;#39;t seem to find any reviews online, and I wanted to know what this community thought of it ? &lt;/p&gt;\n\n&lt;p&gt;Has anyone taken these courses ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y7cyqy", "is_robot_indexable": true, "report_reasons": null, "author": "drugsarebadmky", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y7cyqy/reviews_of_courses_offered_by_amazon_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y7cyqy/reviews_of_courses_offered_by_amazon_for_data/", "subreddit_subscribers": 814202, "created_utc": 1666113431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I understand the R2 score both intuitively and technically and also I understand that it is highly intertwined with fitting a line (hyperplane) and a standard metric for linear regression. \n\nFor the first time I came across R2 score being used as a metric (someone else's solution) in a deep neural network regression problem. I strongly felt that it is not appropriate to use R2 in a dnn setting but not able to pinpoint the exact reason why.\n\nCan you tell me if I'm right or wrong in feeling so? And make a solid case for it?", "author_fullname": "t2_7pfh6trc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "R2 score", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y7bsof", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666110740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand the R2 score both intuitively and technically and also I understand that it is highly intertwined with fitting a line (hyperplane) and a standard metric for linear regression. &lt;/p&gt;\n\n&lt;p&gt;For the first time I came across R2 score being used as a metric (someone else&amp;#39;s solution) in a deep neural network regression problem. I strongly felt that it is not appropriate to use R2 in a dnn setting but not able to pinpoint the exact reason why.&lt;/p&gt;\n\n&lt;p&gt;Can you tell me if I&amp;#39;m right or wrong in feeling so? And make a solid case for it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y7bsof", "is_robot_indexable": true, "report_reasons": null, "author": "eternalmathstudent", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y7bsof/r2_score/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y7bsof/r2_score/", "subreddit_subscribers": 814202, "created_utc": 1666110740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Where to start? No, that\u2019s my question.\n\nAn age old problem of many, disparate data sources entered manually. \n\nWho is trying to solve this issue at scale, and is doing it well?\n\nIf the answer is so large, it needs to be broken down, then do, please.", "author_fullname": "t2_5gltv6d9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manual data entry - who\u2019s who", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y7asl8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666108420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Where to start? No, that\u2019s my question.&lt;/p&gt;\n\n&lt;p&gt;An age old problem of many, disparate data sources entered manually. &lt;/p&gt;\n\n&lt;p&gt;Who is trying to solve this issue at scale, and is doing it well?&lt;/p&gt;\n\n&lt;p&gt;If the answer is so large, it needs to be broken down, then do, please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y7asl8", "is_robot_indexable": true, "report_reasons": null, "author": "montana_mija", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y7asl8/manual_data_entry_whos_who/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y7asl8/manual_data_entry_whos_who/", "subreddit_subscribers": 814202, "created_utc": 1666108420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I had some trouble with the softer side of data science when I first started as a data scientist. During my consulting days, I learned a lot about understanding and communicating with clients, which I want to share.\n\n&amp;#x200B;\n\nBelow are some tips:\n\n&amp;#x200B;\n\n ### How to adopt a business mindset\n\n\\- **Understand how the business makes money.** For you to add value to the business it is crucial to understand their revenue streams and how they make money. \n\n\\- **Relate your work to a business KPI.** To measure the impact of your work, it can be very helpful to relate this to a business KPI. Not always possible, but if you can do it will help in understanding how you can generate value.\n\n\\- **Ask for feedback from your manager.** This is very underutilized. Asking for feedback makes you vulnerable, but it will help in understanding what your manager thinks is important. Plus, it shows that you are willing to learn (which is very important in business).\n\n&amp;#x200B;\n\n### How to manage time\n\n\\- **Prioritize tasks before analysis.** Before touching data, making a list of all actions you need to do will give more clarity in the process and help in planning your time better. Data science comes with a lot of uncertainty, which will take some of them away. \n\n\\- **Use a data analytics project template.** A document where you describe your hypotheses, data needed, and stakeholders will make you way more efficient. \n\n\\- **Plan update meetings with stakeholders to create soft deadlines for yourself.** This helps you to work towards a date and get work done. \n\n&amp;#x200B;\n\n### How to communicate insights\n\n\\- **Learn good data visualization practices.** Understand how you structure a Powerpoint deck and how you present recommendations. I advise to google for Mckinsey's Powerpoint breakdown. \n\n\\- **Create a dialogue where questions can be asked.** The one you are communicating with will definitely have questions. Allow room in your communication for insights and ask \"was this clear?\" after complicated parts.\n\n\\- **Show you are trying to help, not to criticize.** Do not position yourself as the person that knows everything and that the other party is wrong. Be helpful and try to understand where they are coming from.\n\n&amp;#x200B;\n\n### How to collaborate with others\n\n\\- **Write code together.** Writing code together helps to stay sharp and spot errors. This will lead to better work.\n\n\\- **Use Kanban boards to assign tasks and track progress.** Use Kanban boards to decompose big assignments into smaller tasks. Keep the board updated to track progress.\n\n\\- **Have multiple update calls per week.** This depends on the project, but having update calls with your peers \\~2 times per week to discuss challenges helps to spend less time on parts where you are stuck. \n\n&amp;#x200B;\n\nHopefully, this was useful for you. If you have more tips, let me know in the comments!", "author_fullname": "t2_6f4y2kr6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some advice on the softer side of data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y7a5ae", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666108194.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666106874.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had some trouble with the softer side of data science when I first started as a data scientist. During my consulting days, I learned a lot about understanding and communicating with clients, which I want to share.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Below are some tips:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;### How to adopt a business mindset&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;Understand how the business makes money.&lt;/strong&gt; For you to add value to the business it is crucial to understand their revenue streams and how they make money. &lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;Relate your work to a business KPI.&lt;/strong&gt; To measure the impact of your work, it can be very helpful to relate this to a business KPI. Not always possible, but if you can do it will help in understanding how you can generate value.&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;Ask for feedback from your manager.&lt;/strong&gt; This is very underutilized. Asking for feedback makes you vulnerable, but it will help in understanding what your manager thinks is important. Plus, it shows that you are willing to learn (which is very important in business).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h3&gt;How to manage time&lt;/h3&gt;\n\n&lt;p&gt;- &lt;strong&gt;Prioritize tasks before analysis.&lt;/strong&gt; Before touching data, making a list of all actions you need to do will give more clarity in the process and help in planning your time better. Data science comes with a lot of uncertainty, which will take some of them away. &lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;Use a data analytics project template.&lt;/strong&gt; A document where you describe your hypotheses, data needed, and stakeholders will make you way more efficient. &lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;Plan update meetings with stakeholders to create soft deadlines for yourself.&lt;/strong&gt; This helps you to work towards a date and get work done. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h3&gt;How to communicate insights&lt;/h3&gt;\n\n&lt;p&gt;- &lt;strong&gt;Learn good data visualization practices.&lt;/strong&gt; Understand how you structure a Powerpoint deck and how you present recommendations. I advise to google for Mckinsey&amp;#39;s Powerpoint breakdown. &lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;Create a dialogue where questions can be asked.&lt;/strong&gt; The one you are communicating with will definitely have questions. Allow room in your communication for insights and ask &amp;quot;was this clear?&amp;quot; after complicated parts.&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;Show you are trying to help, not to criticize.&lt;/strong&gt; Do not position yourself as the person that knows everything and that the other party is wrong. Be helpful and try to understand where they are coming from.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h3&gt;How to collaborate with others&lt;/h3&gt;\n\n&lt;p&gt;- &lt;strong&gt;Write code together.&lt;/strong&gt; Writing code together helps to stay sharp and spot errors. This will lead to better work.&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;Use Kanban boards to assign tasks and track progress.&lt;/strong&gt; Use Kanban boards to decompose big assignments into smaller tasks. Keep the board updated to track progress.&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;Have multiple update calls per week.&lt;/strong&gt; This depends on the project, but having update calls with your peers ~2 times per week to discuss challenges helps to spend less time on parts where you are stuck. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Hopefully, this was useful for you. If you have more tips, let me know in the comments!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y7a5ae", "is_robot_indexable": true, "report_reasons": null, "author": "thomasvarekamp", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y7a5ae/some_advice_on_the_softer_side_of_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y7a5ae/some_advice_on_the_softer_side_of_data_science/", "subreddit_subscribers": 814202, "created_utc": 1666106874.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to start posting content here on Reddit once a week in a data related subreddit. Stuff like cool libraries, updated laws and regulations about handling data, and other cool stuff. It\u2019s something that\u2019ll help me learn and add value to a community that\u2019s been really helpful to me. So what kind of stuff do you want to see more of here?", "author_fullname": "t2_2gd0s9mj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are you interested in?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y770h7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666099485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to start posting content here on Reddit once a week in a data related subreddit. Stuff like cool libraries, updated laws and regulations about handling data, and other cool stuff. It\u2019s something that\u2019ll help me learn and add value to a community that\u2019s been really helpful to me. So what kind of stuff do you want to see more of here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y770h7", "is_robot_indexable": true, "report_reasons": null, "author": "milkmanbran", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y770h7/what_are_you_interested_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y770h7/what_are_you_interested_in/", "subreddit_subscribers": 814202, "created_utc": 1666099485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys,\n\nI've just been admitted to IT Masters Degree course with Data Science specialization, but from what I can see in the curriculum, there is not much math involved over the 2 years the programme goes on.\n\nHence my question - do you know any comprehensive online stats courses focusing on math? Could be expensive ones (eg. if Harvard offers any relevant classes), that's not an issue - I just need some truly reliable source that can supplement my uni studies in the long run.", "author_fullname": "t2_zydxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning statistics for Data Science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y74kvk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666092864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just been admitted to IT Masters Degree course with Data Science specialization, but from what I can see in the curriculum, there is not much math involved over the 2 years the programme goes on.&lt;/p&gt;\n\n&lt;p&gt;Hence my question - do you know any comprehensive online stats courses focusing on math? Could be expensive ones (eg. if Harvard offers any relevant classes), that&amp;#39;s not an issue - I just need some truly reliable source that can supplement my uni studies in the long run.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y74kvk", "is_robot_indexable": true, "report_reasons": null, "author": "SqayUp", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y74kvk/learning_statistics_for_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y74kvk/learning_statistics_for_data_science/", "subreddit_subscribers": 814202, "created_utc": 1666092864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Which solutions do you see for the following problem?\n\nGiven full training session (15-20 minutes) wearable sensors time-series data (accelerometer, etc.), identify the \"moment\" when a specific test was performed. The test can be a combination of activities: sit2stand+walk+stand2sit or walk+turn+walk+turn or something similar. Obviously, duration can be different, but the test is present within the session. Walking detection model is available, there is some training data, but not excessive amounts. I am just trying to see what the best approach would be? I am thinking about training a convolutional NN with short window size of 1/2 sec to detect each required test element and then see where they best align together. Also how to deal with the fact if the model doesn't find such a test, but it certainly was done?", "author_fullname": "t2_f3y80e7s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Series of events identification within given time-series data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y70yok", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666081534.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which solutions do you see for the following problem?&lt;/p&gt;\n\n&lt;p&gt;Given full training session (15-20 minutes) wearable sensors time-series data (accelerometer, etc.), identify the &amp;quot;moment&amp;quot; when a specific test was performed. The test can be a combination of activities: sit2stand+walk+stand2sit or walk+turn+walk+turn or something similar. Obviously, duration can be different, but the test is present within the session. Walking detection model is available, there is some training data, but not excessive amounts. I am just trying to see what the best approach would be? I am thinking about training a convolutional NN with short window size of 1/2 sec to detect each required test element and then see where they best align together. Also how to deal with the fact if the model doesn&amp;#39;t find such a test, but it certainly was done?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y70yok", "is_robot_indexable": true, "report_reasons": null, "author": "docoja1739", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y70yok/series_of_events_identification_within_given/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y70yok/series_of_events_identification_within_given/", "subreddit_subscribers": 814202, "created_utc": 1666081534.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I one-hot-encoded features and want to calculate similarity with the Jaccard index.\nBut, I am 100% sure that features have different importance for my clustering (i.e. some features are more important than others to calculate distance between my users )\n\nThinking about how Jaccard is calculated, the most obvious answer is to duplicate the important features as they count twice in these cases for the similarity.\nIs there any other way?", "author_fullname": "t2_5ydjc5q5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Weight features in jaccard distance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y70g2h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666079644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I one-hot-encoded features and want to calculate similarity with the Jaccard index.\nBut, I am 100% sure that features have different importance for my clustering (i.e. some features are more important than others to calculate distance between my users )&lt;/p&gt;\n\n&lt;p&gt;Thinking about how Jaccard is calculated, the most obvious answer is to duplicate the important features as they count twice in these cases for the similarity.\nIs there any other way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y70g2h", "is_robot_indexable": true, "report_reasons": null, "author": "alexreddor", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y70g2h/weight_features_in_jaccard_distance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y70g2h/weight_features_in_jaccard_distance/", "subreddit_subscribers": 814202, "created_utc": 1666079644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My goal is to learn hypothesis testing. My understanding is that we do not have the population data. Therefore, we do not know the estimated population mean. So, we guess the population mean. If that so, can we change the population mean until the p-value is below 0.05, so we have enough evidence to reject the null hypothesis?\n\nThe code\n```\nimport pandas as pd\nfrom scipy import stats\n\nsample = pd.Series([1,2,3,4,5,6,7,8,9,10])\n\n# Hypothesis Testing:\n#\n# H0: \u03bc &lt;= 3.5 (previously it was \u03bc &lt;= 5.5, \n#               then 5, then 4.5, I change it until I get p value less than 0.05)\n# H1: \u03bc &gt; 3.5\n\nt_stat, p_value = stats.ttest_1samp(a = sample, popmean=3.5, alternative=\"greater\")\n\nprint(\"sample mean:\", sample.mean())\nprint(\"t-statistic: {0:.2f} p-value: {1:.2f}\".format(t_stat, p_value))\n\nprint(\"since we do not have the population data, \"\n      \"popmean=3.5 is not meaningful, \"\n      \"we can change it until we get enough evidence to reject the null hypothesis\")\n\nprint(\"critical value: 5%, therefore confidence interval: 90%\")\n\nprint(\"because p-value is below 0.05, we have enough evidence to reject the null hypothesis. \"\n      \"therefore, we can conclude that the mean population is more than 3.5\")\n\nprint(\"population standard deviation {0:.2f}\".format(sample.std(ddof=0)))\n\nprint(\"if we imagine the sample as the weight of waste, the conclusion is, \"\n      \"90% of population data is between population mean + 3 * population standard deviation, \"\n      \"in this case 3.5 +- 3 * 2.87\")\n```", "author_fullname": "t2_5yye765o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can we change the estimated population mean in Hypothesis Testing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y6v9fc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666063002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My goal is to learn hypothesis testing. My understanding is that we do not have the population data. Therefore, we do not know the estimated population mean. So, we guess the population mean. If that so, can we change the population mean until the p-value is below 0.05, so we have enough evidence to reject the null hypothesis?&lt;/p&gt;\n\n&lt;p&gt;The code\n```\nimport pandas as pd\nfrom scipy import stats&lt;/p&gt;\n\n&lt;p&gt;sample = pd.Series([1,2,3,4,5,6,7,8,9,10])&lt;/p&gt;\n\n&lt;h1&gt;Hypothesis Testing:&lt;/h1&gt;\n\n&lt;h1&gt;H0: \u03bc &amp;lt;= 3.5 (previously it was \u03bc &amp;lt;= 5.5,&lt;/h1&gt;\n\n&lt;h1&gt;then 5, then 4.5, I change it until I get p value less than 0.05)&lt;/h1&gt;\n\n&lt;h1&gt;H1: \u03bc &amp;gt; 3.5&lt;/h1&gt;\n\n&lt;p&gt;t_stat, p_value = stats.ttest_1samp(a = sample, popmean=3.5, alternative=&amp;quot;greater&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;sample mean:&amp;quot;, sample.mean())\nprint(&amp;quot;t-statistic: {0:.2f} p-value: {1:.2f}&amp;quot;.format(t_stat, p_value))&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;since we do not have the population data, &amp;quot;\n      &amp;quot;popmean=3.5 is not meaningful, &amp;quot;\n      &amp;quot;we can change it until we get enough evidence to reject the null hypothesis&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;critical value: 5%, therefore confidence interval: 90%&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;because p-value is below 0.05, we have enough evidence to reject the null hypothesis. &amp;quot;\n      &amp;quot;therefore, we can conclude that the mean population is more than 3.5&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;population standard deviation {0:.2f}&amp;quot;.format(sample.std(ddof=0)))&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;if we imagine the sample as the weight of waste, the conclusion is, &amp;quot;\n      &amp;quot;90% of population data is between population mean + 3 * population standard deviation, &amp;quot;\n      &amp;quot;in this case 3.5 +- 3 * 2.87&amp;quot;)\n```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y6v9fc", "is_robot_indexable": true, "report_reasons": null, "author": "kidfromtheast", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y6v9fc/can_we_change_the_estimated_population_mean_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y6v9fc/can_we_change_the_estimated_population_mean_in/", "subreddit_subscribers": 814202, "created_utc": 1666063002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am in operations, but for the past couple of years I have been dealing with a lot of Data, if not Big Data surely mid-size Date: Excel, Google Sheets, Asana, QuicBooks, and trying to stitch them all together.\n\nI was thinking into migrating my career into Data Science, my guess would be learning SQL and Python, what are the best BootCamps or Certificate programs? Online or in NYC?\n\nThanks in Advance.", "author_fullname": "t2_k5g1lh0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science BootCamp or Certificate for non-Data Scientists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y7ddze", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666114419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in operations, but for the past couple of years I have been dealing with a lot of Data, if not Big Data surely mid-size Date: Excel, Google Sheets, Asana, QuicBooks, and trying to stitch them all together.&lt;/p&gt;\n\n&lt;p&gt;I was thinking into migrating my career into Data Science, my guess would be learning SQL and Python, what are the best BootCamps or Certificate programs? Online or in NYC?&lt;/p&gt;\n\n&lt;p&gt;Thanks in Advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y7ddze", "is_robot_indexable": true, "report_reasons": null, "author": "G4M35", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y7ddze/data_science_bootcamp_or_certificate_for_nondata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y7ddze/data_science_bootcamp_or_certificate_for_nondata/", "subreddit_subscribers": 814202, "created_utc": 1666114419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have an interview coming up where I was given data and asked to do an EDA and create classification models. That part was easy for me as it is my bread and butter and what I enjoy doing. However, for the interview I have to create a presentation with my EDA, summary of my work and final recommendations. The presentation audience is marketing strategists for the company. I have very limited experience doing these types of presentations. Does anyone with experience doing this type of thing have advice on the best way to present my work? \n\nI used Jupyter notebook for everything if that is relevant. Thanks in advance for any help/advice offered!", "author_fullname": "t2_4vipyddb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to present EDA and models to non-technical audience?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y7bem7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666109848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an interview coming up where I was given data and asked to do an EDA and create classification models. That part was easy for me as it is my bread and butter and what I enjoy doing. However, for the interview I have to create a presentation with my EDA, summary of my work and final recommendations. The presentation audience is marketing strategists for the company. I have very limited experience doing these types of presentations. Does anyone with experience doing this type of thing have advice on the best way to present my work? &lt;/p&gt;\n\n&lt;p&gt;I used Jupyter notebook for everything if that is relevant. Thanks in advance for any help/advice offered!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y7bem7", "is_robot_indexable": true, "report_reasons": null, "author": "bballerkt7", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y7bem7/how_to_present_eda_and_models_to_nontechnical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y7bem7/how_to_present_eda_and_models_to_nontechnical/", "subreddit_subscribers": 814202, "created_utc": 1666109848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Labeling ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y7axwp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_rsdl04ak", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "Hello Folks!\n\nCurious to ask where do you suggest to label data and why, share your experience please.\n\nAnd where are the big sharks companies label their data?\n\nThanks in advance ladies and gentlemen !!!", "author_fullname": "t2_rsdl04ak", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Labeling ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y7axbl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666108747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Folks!&lt;/p&gt;\n\n&lt;p&gt;Curious to ask where do you suggest to label data and why, share your experience please.&lt;/p&gt;\n\n&lt;p&gt;And where are the big sharks companies label their data?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance ladies and gentlemen !!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y7axbl", "is_robot_indexable": true, "report_reasons": null, "author": "PayVolt", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/y7axbl/data_labeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/ArtificialInteligence/comments/y7axbl/data_labeling/", "subreddit_subscribers": 86654, "created_utc": 1666108747.0, "num_crossposts": 4, "media": null, "is_video": false}], "created": 1666108790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/ArtificialInteligence/comments/y7axbl/data_labeling/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y7axwp", "is_robot_indexable": true, "report_reasons": null, "author": "PayVolt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_y7axbl", "author_flair_text_color": null, "permalink": "/r/datascience/comments/y7axwp/data_labeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/ArtificialInteligence/comments/y7axbl/data_labeling/", "subreddit_subscribers": 814202, "created_utc": 1666108790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello!\n\nI've got a client that has given me some survey data where the participants chose the \"top 3\" from a list of 11 possible choices.  They did this 4 times.  I also have a variety of demographic (self-report) responses from participants.\n\nI'm a little at a loss with what to do beyond frequencies sliced by various demographics.  I think I can rule out Poisson regression because they aren't true \"counts\" by event, but I don't have a ton of other ideas.\n\nAnybody dealt with data like this before?   Any techniques to get meaning beyond the sliced frequencies?  If you have dealt with this type of data, what was the analysis that your customer/client found valuable?", "author_fullname": "t2_tgzu2t4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analysis paths for \"Choose Top 3\" data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y7adpl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666107418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a client that has given me some survey data where the participants chose the &amp;quot;top 3&amp;quot; from a list of 11 possible choices.  They did this 4 times.  I also have a variety of demographic (self-report) responses from participants.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a little at a loss with what to do beyond frequencies sliced by various demographics.  I think I can rule out Poisson regression because they aren&amp;#39;t true &amp;quot;counts&amp;quot; by event, but I don&amp;#39;t have a ton of other ideas.&lt;/p&gt;\n\n&lt;p&gt;Anybody dealt with data like this before?   Any techniques to get meaning beyond the sliced frequencies?  If you have dealt with this type of data, what was the analysis that your customer/client found valuable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y7adpl", "is_robot_indexable": true, "report_reasons": null, "author": "Kris_Include", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y7adpl/analysis_paths_for_choose_top_3_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y7adpl/analysis_paths_for_choose_top_3_data/", "subreddit_subscribers": 814202, "created_utc": 1666107418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello:\n\n  \nFor work, I have written a script that makes calls to an API, does some transformations, then ships the data off to a SQL server. I've been told to add more logging. Currently, it logs \"wrote {#ofrows} to {table} for time {timeperiod (QTR, MTD, etc.)}\" or 'writing for {url} failed!'. \n\nSoftware engineering skills are not my strongest suit (I'm more statistics than software), so I'm asking for examples or some guidance on what is useful to log and what is not useful to log in ETL scripts. I don't want to clog the logs with garbage, nor do I want to abbreviate past the point of usefulness. I'm thinking of logging the url when it is called, whether that was a success or not, what transformations are taking place and if they are successes, and then if it writes correctly. \n\nThoughts?\n\nMany thanks.", "author_fullname": "t2_lwmkqytr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What, exactly, should we be logging?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y79wog", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666106316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello:&lt;/p&gt;\n\n&lt;p&gt;For work, I have written a script that makes calls to an API, does some transformations, then ships the data off to a SQL server. I&amp;#39;ve been told to add more logging. Currently, it logs &amp;quot;wrote {#ofrows} to {table} for time {timeperiod (QTR, MTD, etc.)}&amp;quot; or &amp;#39;writing for {url} failed!&amp;#39;. &lt;/p&gt;\n\n&lt;p&gt;Software engineering skills are not my strongest suit (I&amp;#39;m more statistics than software), so I&amp;#39;m asking for examples or some guidance on what is useful to log and what is not useful to log in ETL scripts. I don&amp;#39;t want to clog the logs with garbage, nor do I want to abbreviate past the point of usefulness. I&amp;#39;m thinking of logging the url when it is called, whether that was a success or not, what transformations are taking place and if they are successes, and then if it writes correctly. &lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n\n&lt;p&gt;Many thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y79wog", "is_robot_indexable": true, "report_reasons": null, "author": "c0ntrap0sitive", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y79wog/what_exactly_should_we_be_logging/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y79wog/what_exactly_should_we_be_logging/", "subreddit_subscribers": 814202, "created_utc": 1666106316.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Someone pointed out to me recently that there isn\u2019t enough criticism of the data field. If we want data science to be a field that\u2019s taken seriously we have to call out the crap. Each week I\u2019ll be posting bad examples around data/data science. I call it Garbage In Garbage Out. \n\nThis week I discuss a survey in the UK with results that look great on TV but don\u2019t seem to hold up under scrutiny.", "author_fullname": "t2_a0kcgwo4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Terrible survey. Are 37% of teenagers in the UK being prescribed anti-depressants?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_y79fgp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/e3nxw3Qe_ucsoopJc9ZGYIWPqbCMbVT5rMl54ELlgb0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1666105214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datafantic.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Someone pointed out to me recently that there isn\u2019t enough criticism of the data field. If we want data science to be a field that\u2019s taken seriously we have to call out the crap. Each week I\u2019ll be posting bad examples around data/data science. I call it Garbage In Garbage Out. &lt;/p&gt;\n\n&lt;p&gt;This week I discuss a survey in the UK with results that look great on TV but don\u2019t seem to hold up under scrutiny.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.datafantic.com/garbage-in-garbage-out-2/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/K3pUJ6OObwTvMcPgH_SP2WgiL-ToFPsAXAskQORyYEE.jpg?auto=webp&amp;s=333b586bec85843aff3124f8887998eded769a16", "width": 2000, "height": 1333}, "resolutions": [{"url": "https://external-preview.redd.it/K3pUJ6OObwTvMcPgH_SP2WgiL-ToFPsAXAskQORyYEE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d453499c446fe73ff7c32a89db491c965020cf7c", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/K3pUJ6OObwTvMcPgH_SP2WgiL-ToFPsAXAskQORyYEE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=df6dc397b9085e727944f1c89e920abd7a7dd953", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/K3pUJ6OObwTvMcPgH_SP2WgiL-ToFPsAXAskQORyYEE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27e1e1b5ac0451dc1383581a5992a25a9e8acb52", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/K3pUJ6OObwTvMcPgH_SP2WgiL-ToFPsAXAskQORyYEE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=85eed6c03c081084d60fdaa1bfd878f32f2ccc21", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/K3pUJ6OObwTvMcPgH_SP2WgiL-ToFPsAXAskQORyYEE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6d4e821be10d628723217e942f3d5788133964b2", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/K3pUJ6OObwTvMcPgH_SP2WgiL-ToFPsAXAskQORyYEE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=caa41c6a3bd367e4658ba3431e313231b2575db1", "width": 1080, "height": 719}], "variants": {}, "id": "8vDEu7IOPHxgRu6kMgw8xIohoMK7y1A2Uc0GwkihlfY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "y79fgp", "is_robot_indexable": true, "report_reasons": null, "author": "robert_ritz", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y79fgp/terrible_survey_are_37_of_teenagers_in_the_uk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datafantic.com/garbage-in-garbage-out-2/", "subreddit_subscribers": 814202, "created_utc": 1666105214.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}